[
  {
    "ghsa_id": "GHSA-v2qh-f584-6hj8",
    "cve_id": "CVE-2024-51753",
    "url": "https://api.github.com/advisories/GHSA-v2qh-f584-6hj8",
    "html_url": "https://github.com/advisories/GHSA-v2qh-f584-6hj8",
    "summary": "@workos-inc/authkit-remix refresh tokens are logged when the debug flag is enabled",
    "description": "### Impact\nRefresh tokens are logged to the console when the disabled by default `debug` flag, is enabled.\n\n### Patches\nPatched in [https://github.com/workos/authkit-remix/releases/tag/v0.4.1](https://github.com/workos/authkit-remix/releases/tag/v0.4.1)",
    "type": "reviewed",
    "severity": "low",
    "repository_advisory_url": "https://api.github.com/repos/workos/authkit-remix/security-advisories/GHSA-v2qh-f584-6hj8",
    "source_code_location": "https://github.com/workos/authkit-remix",
    "identifiers": [
      {
        "value": "GHSA-v2qh-f584-6hj8",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-51753",
        "type": "CVE"
      }
    ],
    "references": [
      "https://github.com/workos/authkit-remix/security/advisories/GHSA-v2qh-f584-6hj8",
      "https://github.com/workos/authkit-remix/commit/32d5bcd54c795c1e2a3204f8e3977ab9ad57ec06",
      "https://github.com/workos/authkit-remix/releases/tag/v0.4.1",
      "https://nvd.nist.gov/vuln/detail/CVE-2024-51753",
      "https://github.com/advisories/GHSA-v2qh-f584-6hj8"
    ],
    "published_at": "2024-11-05T17:34:47Z",
    "updated_at": "2024-11-05T21:37:21Z",
    "github_reviewed_at": "2024-11-05T17:34:47Z",
    "nvd_published_at": "2024-11-05T20:15:15Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "@workos-inc/authkit-remix"
        },
        "vulnerable_version_range": "< 0.4.1",
        "first_patched_version": "0.4.1",
        "vulnerable_functions": [],
        "vulnerable_version": "0.4.0",
        "patches": {
          ".github/ISSUE_TEMPLATE/bug_report.md": "@@ -0,0 +1,32 @@\n+---\n+name: Bug report\n+about: Create a report to help us improve\n+title: ''\n+labels: ''\n+assignees: PaulAsjes\n+\n+---\n+\n+**Describe the bug**\n+A clear and concise description of what the bug is.\n+\n+**To Reproduce**\n+Steps to reproduce the behavior:\n+1. Go to '...'\n+2. Click on '....'\n+3. Scroll down to '....'\n+4. See error\n+\n+**Expected behavior**\n+A clear and concise description of what you expected to happen.\n+\n+**Screenshots**\n+If applicable, add screenshots to help explain your problem.\n+\n+**Desktop (please complete the following information):**\n+ - OS: [e.g. iOS]\n+ - Browser [e.g. chrome, safari]\n+ - Version [e.g. 22]\n+\n+**Additional context**\n+Add any other context about the problem here.",
          "package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@workos-inc/authkit-remix\",\n-  \"version\": \"0.4.0\",\n+  \"version\": \"0.4.1\",\n   \"description\": \"Authentication and session helpers for using WorkOS & AuthKit with Remix\",\n   \"sideEffects\": false,\n   \"type\": \"commonjs\",",
          "src/authkit-callback-route.ts": "@@ -13,7 +13,7 @@ export function authLoader(options: HandleAuthOptions = {}) {\n \n     const code = url.searchParams.get('code');\n     const state = url.searchParams.get('state');\n-    let returnPathname = state ? JSON.parse(atob(state)).returnPathname : null;\n+    let returnPathname = state && state !== 'null' ? JSON.parse(atob(state)).returnPathname : null;\n \n     if (code) {\n       try {",
          "src/session.ts": "@@ -27,15 +27,15 @@ async function updateSession(request: Request, debug: boolean) {\n   }\n \n   try {\n-    if (debug) console.log('Session invalid. Attempting refresh', session.refreshToken);\n+    if (debug) console.log(`Session invalid. Refreshing access token that ends in ${session.accessToken.slice(-10)}`);\n \n     // If the session is invalid (i.e. the access token has expired) attempt to re-authenticate with the refresh token\n     const { accessToken, refreshToken } = await workos.userManagement.authenticateWithRefreshToken({\n       clientId: WORKOS_CLIENT_ID,\n       refreshToken: session.refreshToken,\n     });\n \n-    if (debug) console.log('Refresh successful:', refreshToken);\n+    if (debug) console.log(`Refresh successful. New access token ends in ${accessToken.slice(-10)}`);\n \n     const newSession = {\n       accessToken,",
          "src/workos.ts": "@@ -1,7 +1,7 @@\n import { WorkOS } from '@workos-inc/node';\n import { WORKOS_API_HOSTNAME, WORKOS_API_HTTPS, WORKOS_API_KEY, WORKOS_API_PORT } from './env-variables.js';\n \n-const VERSION = '0.4.0';\n+const VERSION = '0.4.1';\n \n const options = {\n   apiHostname: WORKOS_API_HOSTNAME,"
        }
      }
    ],
    "cvss": {
      "vector_string": null,
      "score": null
    },
    "cwes": [
      {
        "cwe_id": "CWE-532",
        "name": "Insertion of Sensitive Information into Log File"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": null,
        "score": 0
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:L/AC:L/AT:P/PR:L/UI:N/VC:L/VI:N/VA:N/SC:N/SI:N/SA:N/E:U",
        "score": 0.4
      }
    },
    "epss": {
      "percentage": 0.00045,
      "percentile": 0.16747
    },
    "cve_description": "The AuthKit library for Remix provides convenient helpers for authentication and session management using WorkOS & AuthKit with Remix. In affected versions refresh tokens are logged to the console when the disabled by default `debug` flag, is enabled. This issue has been patched in version 0.4.1. All users are advised to upgrade. There are no known workarounds for this vulnerability."
  },
  {
    "ghsa_id": "GHSA-5wmg-9cvh-qw25",
    "cve_id": "CVE-2024-51752",
    "url": "https://api.github.com/advisories/GHSA-5wmg-9cvh-qw25",
    "html_url": "https://github.com/advisories/GHSA-5wmg-9cvh-qw25",
    "summary": "@workos-inc/authkit-nextjs refresh tokens are logged when the debug flag is enabled",
    "description": "### Impact\nRefresh tokens are logged to the console when the disabled by default `debug` flag, is enabled.\n\n### Patches\nPatched in [https://github.com/workos/authkit-nextjs/releases/tag/v0.13.2](https://github.com/workos/authkit-nextjs/releases/tag/v0.13.2)\n",
    "type": "reviewed",
    "severity": "low",
    "repository_advisory_url": "https://api.github.com/repos/workos/authkit-nextjs/security-advisories/GHSA-5wmg-9cvh-qw25",
    "source_code_location": "https://github.com/workos/authkit-nextjs",
    "identifiers": [
      {
        "value": "GHSA-5wmg-9cvh-qw25",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-51752",
        "type": "CVE"
      }
    ],
    "references": [
      "https://github.com/workos/authkit-nextjs/security/advisories/GHSA-5wmg-9cvh-qw25",
      "https://github.com/workos/authkit-nextjs/commit/15a332632f7560b03cc6d8cc8da24fd2ac931da7",
      "https://github.com/workos/authkit-nextjs/releases/tag/v0.13.2",
      "https://nvd.nist.gov/vuln/detail/CVE-2024-51752",
      "https://github.com/advisories/GHSA-5wmg-9cvh-qw25"
    ],
    "published_at": "2024-11-05T17:34:23Z",
    "updated_at": "2024-11-05T21:37:25Z",
    "github_reviewed_at": "2024-11-05T17:34:23Z",
    "nvd_published_at": "2024-11-05T20:15:15Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "@workos-inc/authkit-nextjs"
        },
        "vulnerable_version_range": "< 0.13.2",
        "first_patched_version": "0.13.2",
        "vulnerable_functions": [],
        "vulnerable_version": "0.13.1",
        "patches": {
          "package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@workos-inc/authkit-nextjs\",\n-  \"version\": \"0.13.1\",\n+  \"version\": \"0.13.2\",\n   \"description\": \"Authentication and session helpers for using WorkOS & AuthKit with Next.js\",\n   \"sideEffects\": false,\n   \"type\": \"module\",",
          "src/session.ts": "@@ -120,7 +120,7 @@ async function updateSession(\n   }\n \n   try {\n-    if (debug) console.log('Session invalid. Attempting refresh', session.refreshToken);\n+    if (debug) console.log(`Session invalid. Refreshing access token that ends in ${session.accessToken.slice(-10)}`);\n \n     const { org_id: organizationId } = decodeJwt<AccessToken>(session.accessToken);\n \n@@ -131,7 +131,7 @@ async function updateSession(\n       organizationId,\n     });\n \n-    if (debug) console.log('Refresh successful:', refreshToken);\n+    if (debug) console.log(`Refresh successful. New access token ends in ${accessToken.slice(-10)}`);\n \n     // Encrypt session with new access and refresh tokens\n     const encryptedSession = await encryptSession({",
          "src/workos.ts": "@@ -1,7 +1,7 @@\n import { WorkOS } from '@workos-inc/node';\n import { WORKOS_API_HOSTNAME, WORKOS_API_KEY, WORKOS_API_HTTPS, WORKOS_API_PORT } from './env-variables.js';\n \n-export const VERSION = '0.13.1';\n+export const VERSION = '0.13.2';\n \n const options = {\n   apiHostname: WORKOS_API_HOSTNAME,"
        }
      }
    ],
    "cvss": {
      "vector_string": null,
      "score": null
    },
    "cwes": [
      {
        "cwe_id": "CWE-532",
        "name": "Insertion of Sensitive Information into Log File"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": null,
        "score": 0
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:L/AC:L/AT:P/PR:L/UI:N/VC:L/VI:N/VA:N/SC:N/SI:N/SA:N/E:U",
        "score": 0.4
      }
    },
    "epss": {
      "percentage": 0.00045,
      "percentile": 0.16747
    },
    "cve_description": "The AuthKit library for Next.js provides convenient helpers for authentication and session management using WorkOS & AuthKit with Next.js. In affected versions refresh tokens are logged to the console when the disabled by default `debug` flag, is enabled. This issue has been patched in version 0.13.2 and all users are advised to upgrade. There are no known workarounds for this vulnerability."
  },
  {
    "ghsa_id": "GHSA-6m59-8fmv-m5f9",
    "cve_id": "CVE-2024-7042",
    "url": "https://api.github.com/advisories/GHSA-6m59-8fmv-m5f9",
    "html_url": "https://github.com/advisories/GHSA-6m59-8fmv-m5f9",
    "summary": "@langchain/community SQL Injection vulnerability",
    "description": "A vulnerability in the GraphCypherQAChain class of langchain-ai/langchainjs versions 0.2.5 and all versions with this class allows for prompt injection, leading to SQL injection. This vulnerability permits unauthorized data manipulation, data exfiltration, denial of service (DoS) by deleting all data, breaches in multi-tenant security environments, and data integrity issues. Attackers can create, update, or delete nodes and relationships without proper authorization, extract sensitive data, disrupt services, access data across different tenants, and compromise the integrity of the database.",
    "type": "reviewed",
    "severity": "low",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/langchain-ai/langchainjs",
    "identifiers": [
      {
        "value": "GHSA-6m59-8fmv-m5f9",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-7042",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-7042",
      "https://github.com/langchain-ai/langchainjs/commit/615b9d9ab30a2d23a2f95fb8d7acfdf4b41ad7a6",
      "https://huntr.com/bounties/b612defb-1104-4fff-9fef-001ab07c7b2d",
      "https://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2024-114.yaml",
      "https://github.com/advisories/GHSA-6m59-8fmv-m5f9"
    ],
    "published_at": "2024-10-29T15:32:05Z",
    "updated_at": "2024-11-01T13:16:11Z",
    "github_reviewed_at": "2024-10-29T19:38:29Z",
    "nvd_published_at": "2024-10-29T13:15:08Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "@langchain/community"
        },
        "vulnerable_version_range": "< 0.3.3",
        "first_patched_version": "0.3.3",
        "vulnerable_functions": [],
        "vulnerable_version": "0.3.2",
        "patches": {
          ".github/scripts/deployDomainVercel.sh": "@@ -0,0 +1,30 @@\n+#!/bin/bash\n+\n+# Check if an argument is provided\n+if [ $# -eq 0 ]; then\n+    echo \"Error: Please provide a version string as an argument.\"\n+    exit 1\n+fi\n+\n+inputString=$1\n+\n+# Check if VERCEL_TOKEN is set\n+if [ -z \"$VERCEL_TOKEN\" ]; then\n+    echo \"Error: VERCEL_TOKEN is not set.\"\n+    exit 1\n+fi\n+\n+# save stdout and stderr to files\n+vercel deploy --prebuilt --token=\"$VERCEL_TOKEN\" >deployment-url.txt 2>error.txt\n+\n+# check the exit code\n+code=$?\n+if [ $code -eq 0 ]; then\n+    # Set the deploymentUrl using the input string\n+    deploymentUrl=\"${inputString}.api.js.langchain.com\"\n+    vercel alias $(cat deployment-url.txt) $deploymentUrl --token=\"$VERCEL_TOKEN\" --scope=\"langchain\"\n+else\n+    # Handle the error\n+    errorMessage=$(cat error.txt)\n+    echo \"There was an error: $errorMessage\"\n+fi\n\\ No newline at end of file",
          ".github/workflows/compatibility.yml": "@@ -6,10 +6,10 @@ on:\n   pull_request:\n     # Only run this workflow if the following directories have changed.\n     paths:\n-      - 'langchain/**'\n-      - 'langchain-core/**'\n-      - 'libs/**'\n-  workflow_dispatch:  # Allows triggering the workflow manually in GitHub UI\n+      - \"langchain/**\"\n+      - \"langchain-core/**\"\n+      - \"libs/**\"\n+  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI\n \n # If another push to the same PR or branch happens while this workflow is still running,\n # cancel the earlier run in favor of the next run.\n@@ -24,7 +24,7 @@ concurrency:\n env:\n   PUPPETEER_SKIP_DOWNLOAD: \"true\"\n   PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: \"true\"\n-  NODE_VERSION: \"18.x\"\n+  NODE_VERSION: \"20.x\"\n   COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}\n \n # Run a separate job for each check in the docker-compose file,\n@@ -75,42 +75,42 @@ jobs:\n       - name: Test LangChain with lowest deps\n         run: docker compose -f dependency_range_tests/docker-compose.yml run langchain-lowest-deps\n \n-  # Community\n-  community-latest-deps:\n-    runs-on: ubuntu-latest\n-    needs: get-changed-files\n-    if: contains(needs.get-changed-files.outputs.changed_files, 'langchain-core/') || contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-openai/') || contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-community/')\n-    steps:\n-      - uses: actions/checkout@v4\n-      - name: Use Node.js ${{ env.NODE_VERSION }}\n-        uses: actions/setup-node@v3\n-        with:\n-          node-version: ${{ env.NODE_VERSION }}\n-          cache: \"yarn\"\n-      - name: Install dependencies\n-        run: yarn install --immutable\n-      - name: Build `@langchain/standard-tests`\n-        run: yarn build --filter=@langchain/standard-tests\n-      - name: Test `@langchain/community` with latest deps\n-        run: docker compose -f dependency_range_tests/docker-compose.yml run community-latest-deps\n+  # # Community\n+  # community-latest-deps:\n+  #   runs-on: ubuntu-latest\n+  #   needs: get-changed-files\n+  #   if: contains(needs.get-changed-files.outputs.changed_files, 'langchain-core/') || contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-openai/') || contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-community/')\n+  #   steps:\n+  #     - uses: actions/checkout@v4\n+  #     - name: Use Node.js ${{ env.NODE_VERSION }}\n+  #       uses: actions/setup-node@v3\n+  #       with:\n+  #         node-version: ${{ env.NODE_VERSION }}\n+  #         cache: \"yarn\"\n+  #     - name: Install dependencies\n+  #       run: yarn install --immutable\n+  #     - name: Build `@langchain/standard-tests`\n+  #       run: yarn build --filter=@langchain/standard-tests\n+  #     - name: Test `@langchain/community` with latest deps\n+  #       run: docker compose -f dependency_range_tests/docker-compose.yml run community-latest-deps\n \n-  community-lowest-deps:\n-    runs-on: ubuntu-latest\n-    needs: get-changed-files\n-    if: contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-community/')\n-    steps:\n-      - uses: actions/checkout@v4\n-      - name: Use Node.js ${{ env.NODE_VERSION }}\n-        uses: actions/setup-node@v3\n-        with:\n-          node-version: ${{ env.NODE_VERSION }}\n-          cache: \"yarn\"\n-      - name: Install dependencies\n-        run: yarn install --immutable\n-      - name: Build `@langchain/standard-tests`\n-        run: yarn build --filter=@langchain/standard-tests\n-      - name: Test `@langchain/community` with lowest deps\n-        run: docker compose -f dependency_range_tests/docker-compose.yml run community-lowest-deps\n+  # community-lowest-deps:\n+  #   runs-on: ubuntu-latest\n+  #   needs: get-changed-files\n+  #   if: contains(needs.get-changed-files.outputs.changed_files, 'libs/langchain-community/')\n+  #   steps:\n+  #     - uses: actions/checkout@v4\n+  #     - name: Use Node.js ${{ env.NODE_VERSION }}\n+  #       uses: actions/setup-node@v3\n+  #       with:\n+  #         node-version: ${{ env.NODE_VERSION }}\n+  #         cache: \"yarn\"\n+  #     - name: Install dependencies\n+  #       run: yarn install --immutable\n+  #     - name: Build `@langchain/standard-tests`\n+  #       run: yarn build --filter=@langchain/standard-tests\n+  #     - name: Test `@langchain/community` with lowest deps\n+  #       run: docker compose -f dependency_range_tests/docker-compose.yml run community-lowest-deps\n \n   community-npm-install:\n     runs-on: ubuntu-latest",
          ".github/workflows/deploy-api-refs-prod.yml": "@@ -3,7 +3,7 @@ name: Deploy API Refs Prod\n on:\n   workflow_dispatch:  # Allows triggering the workflow manually in GitHub UI\n   push:\n-    branches: [\"main\"]\n+    branches: [\"main\", \"v0.2\"]\n \n # If another push to the same PR or branch happens while this workflow is still running,\n # cancel the earlier run in favor of the next run.\n@@ -34,6 +34,21 @@ jobs:\n       - name: Build All Projects\n         run: yarn turbo:command build --filter=!examples --filter=!api_refs --filter=!core_docs --filter=!create-langchain-integration\n       - name: Build Project Artifacts\n-        run: vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}\n-      - name: Deploy Project Artifacts to Vercel\n-        run: vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}\n\\ No newline at end of file\n+        run: |\n+          if [ ${{ github.ref }} = 'refs/heads/main' ]; then\n+            vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}\n+          else\n+            vercel build --token=${{ secrets.VERCEL_TOKEN }}\n+          fi\n+      - name: Deploy to Vercel\n+        env:\n+          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}\n+        run: |\n+          if [ ${{ github.ref }} = 'refs/heads/main' ]; then\n+            vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}\n+          elif [ ${{ github.ref }} = 'refs/heads/v0.2' ]; then\n+            .github/scripts/deployDomainVercel.sh v02\n+          else\n+            echo \"Error: Deployment is only allowed for 'main' or 'v0.2' branches.\"\n+            exit 1\n+          fi\n\\ No newline at end of file",
          ".github/workflows/spam-comment-filter.yml": "@@ -1,43 +0,0 @@\n-name: Spam Comment Filter\n-\n-on:\n-  issue_comment:\n-    types: [created]\n-  pull_request_review_comment:\n-    types: [created]\n-\n-jobs:\n-  filter_spam:\n-    runs-on: ubuntu-latest\n-    steps:\n-      - uses: actions/checkout@v4\n-      - name: Use Node.js 18.x\n-        uses: actions/setup-node@v3\n-        with:\n-          node-version: 18.x\n-      - name: Check issue body against regex\n-        id: regex_check\n-        env:\n-          COMMENT_BODY: ${{ github.event.comment.body }}\n-        run: |\n-          REGEX='^download\\s+(?:https?:\\/\\/)?[\\w-]+(\\.[\\w-]+)+[^\\s]+\\s+password:\\s*.+\\s+in the installer menu, select\\s*.+$'\n-          if echo \"$COMMENT_BODY\" | tr '\\n' ' ' | grep -qiP \"$REGEX\"; then\n-            echo \"REGEX_MATCHED=true\" >> $GITHUB_OUTPUT\n-          else\n-            echo \"REGEX_MATCHED=false\" >> $GITHUB_OUTPUT\n-          fi\n-      - name: Install dependencies\n-        if: steps.regex_check.outputs.REGEX_MATCHED == 'true'\n-        run: cd ./libs/langchain-scripts && yarn workspaces focus\n-      - name: Build scripts\n-        if: steps.regex_check.outputs.REGEX_MATCHED == 'true'\n-        run: cd ./libs/langchain-scripts && yarn build:internal\n-      - name: Run spam detection script\n-        if: steps.regex_check.outputs.REGEX_MATCHED == 'true'\n-        env:\n-          SPAM_COMMENT_GITHUB_TOKEN: ${{ secrets.SPAM_COMMENT_GITHUB_TOKEN }}\n-          COMMENT_JSON: ${{ toJson(github.event.comment) }}\n-          COMMENT_ID: ${{ github.event.comment.id }}\n-          REPO_OWNER: ${{ github.repository_owner }}\n-          REPO_NAME: ${{ github.event.repository.name }}\n-        run: cd ./libs/langchain-scripts && yarn filter_spam_comment",
          ".vercelignore": "@@ -0,0 +1,4 @@\n+node_modules/\n+**/node_modules/\n+.next/\n+**/.next/",
          "CONTRIBUTING.md": "@@ -105,9 +105,9 @@ You can invoke the script by calling `yarn release`. If new dependencies have be\n \n There are three parameters which can be passed to this script, one required and two optional.\n \n-- __Required__: `<workspace name>`. eg: `@langchain/core` The name of the package to release. Can be found in the `name` value of the package's `package.json`\n-- __Optional__: `--bump-deps` eg `--bump-deps` Will find all packages in the repo which depend on this workspace and checkout a new branch, update the dep version, run yarn install, commit & push to new branch. Generally, this is not necessary.\n-- __Optional__: `--tag <tag>` eg `--tag beta` Add a tag to the NPM release. Useful if you want to push a release candidate.\n+- **Required**: `<workspace name>`. eg: `@langchain/core` The name of the package to release. Can be found in the `name` value of the package's `package.json`\n+- **Optional**: `--bump-deps` eg `--bump-deps` Will find all packages in the repo which depend on this workspace and checkout a new branch, update the dep version, run yarn install, commit & push to new branch. Generally, this is not necessary.\n+- **Optional**: `--tag <tag>` eg `--tag beta` Add a tag to the NPM release. Useful if you want to push a release candidate.\n \n This script automatically bumps the package version, creates a new release branch with the changes, pushes the branch to GitHub, uses `release-it` to automatically release to NPM, and more depending on the flags passed.\n \n@@ -323,6 +323,10 @@ Similar to linting, we recognize documentation can be annoying. If you do not wa\n \n Documentation and the skeleton lives under the `docs/` folder. Example code is imported from under the `examples/` folder.\n \n+**If you are contributing an integration, please copy and use the appropriate template from here:**\n+\n+https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-scripts/src/cli/docs/templates\n+\n ### Running examples\n \n If you add a new major piece of functionality, it is helpful to add an",
          "deno.json": "@@ -1,20 +1,20 @@\n {\n   \"imports\": {\n-    \"langchain/\": \"npm:/langchain/\",\n+    \"langchain/\": \"npm:/langchain@0.3.2/\",\n+    \"@langchain/anthropic\": \"npm:@langchain/anthropic@0.3.0\",\n+    \"@langchain/cloudflare\": \"npm:@langchain/cloudflare@0.1.0\",\n+    \"@langchain/community/\": \"npm:/@langchain/community@0.3.0/\",\n+    \"@langchain/openai\": \"npm:@langchain/openai@0.3.0\",\n+    \"@langchain/cohere\": \"npm:@langchain/cohere@0.3.0\",\n+    \"@langchain/textsplitters\": \"npm:@langchain/textsplitters@0.1.0\",\n+    \"@langchain/google-vertexai-web\": \"npm:@langchain/google-vertexai-web@0.1.0\",\n+    \"@langchain/mistralai\": \"npm:@langchain/mistralai@0.1.0\",\n+    \"@langchain/core/\": \"npm:/@langchain/core@0.3.1/\",\n+    \"@langchain/pinecone\": \"npm:@langchain/pinecone@0.1.0\",\n+    \"@langchain/google-common\": \"npm:@langchain/google-common@0.1.0\",\n+    \"@langchain/langgraph\": \"npm:/@langchain/langgraph@0.2.3\",\n+    \"@langchain/langgraph/\": \"npm:/@langchain/langgraph@0.2.3/\",\n     \"@faker-js/faker\": \"npm:@faker-js/faker\",\n-    \"@langchain/anthropic\": \"npm:@langchain/anthropic\",\n-    \"@langchain/cloudflare\": \"npm:@langchain/cloudflare\",\n-    \"@langchain/community/\": \"npm:/@langchain/community@0.2.2/\",\n-    \"@langchain/openai\": \"npm:@langchain/openai\",\n-    \"@langchain/cohere\": \"npm:@langchain/cohere\",\n-    \"@langchain/textsplitters\": \"npm:@langchain/textsplitters\",\n-    \"@langchain/google-vertexai-web\": \"npm:@langchain/google-vertexai-web\",\n-    \"@langchain/mistralai\": \"npm:@langchain/mistralai\",\n-    \"@langchain/core/\": \"npm:/@langchain/core@0.2.16/\",\n-    \"@langchain/pinecone\": \"npm:@langchain/pinecone\",\n-    \"@langchain/google-common\": \"npm:@langchain/google-common\",\n-    \"@langchain/langgraph\": \"npm:/@langchain/langgraph@0.0.21\",\n-    \"@langchain/langgraph/\": \"npm:/@langchain/langgraph@0.0.21/\",\n     \"@microsoft/fetch-event-source\": \"npm:@microsoft/fetch-event-source\",\n     \"@pinecone-database/pinecone\": \"npm:@pinecone-database/pinecone\",\n     \"cheerio\": \"npm:cheerio\",",
          "dependency_range_tests/scripts/with_standard_tests/community/node/update_resolutions_npm.js": "@@ -3,8 +3,8 @@ const fs = require(\"fs\");\n const communityPackageJsonPath = \"/app/monorepo/libs/langchain-community/package.json\";\n const currentPackageJson = JSON.parse(fs.readFileSync(communityPackageJsonPath));\n \n-if (currentPackageJson.devDependencies[\"@langchain/core\"]) {\n-  delete currentPackageJson.devDependencies[\"@langchain/core\"];\n+if (currentPackageJson.devDependencies) {\n+  delete currentPackageJson.devDependencies;\n }\n \n fs.writeFileSync(communityPackageJsonPath, JSON.stringify(currentPackageJson, null, 2));",
          "dependency_range_tests/scripts/with_standard_tests/community/npm-install.sh": "@@ -25,6 +25,5 @@ node \"update_resolutions_npm.js\"\n \n # Navigate back to monorepo root and install dependencies\n cd \"$monorepo_dir\"\n-npm install @langchain/core\n-npm install\n-\n+npm install @langchain/core --production\n+npm install --production",
          "docs/core_docs/.gitignore": "@@ -138,6 +138,8 @@ docs/how_to/multimodal_inputs.md\n docs/how_to/multimodal_inputs.mdx\n docs/how_to/migrate_agent.md\n docs/how_to/migrate_agent.mdx\n+docs/how_to/message_history.md\n+docs/how_to/message_history.mdx\n docs/how_to/merge_message_runs.md\n docs/how_to/merge_message_runs.mdx\n docs/how_to/logprobs.md\n@@ -198,14 +200,14 @@ docs/how_to/character_text_splitter.md\n docs/how_to/character_text_splitter.mdx\n docs/how_to/cancel_execution.md\n docs/how_to/cancel_execution.mdx\n+docs/how_to/callbacks_serverless.md\n+docs/how_to/callbacks_serverless.mdx\n docs/how_to/callbacks_runtime.md\n docs/how_to/callbacks_runtime.mdx\n docs/how_to/callbacks_custom_events.md\n docs/how_to/callbacks_custom_events.mdx\n docs/how_to/callbacks_constructor.md\n docs/how_to/callbacks_constructor.mdx\n-docs/how_to/callbacks_backgrounding.md\n-docs/how_to/callbacks_backgrounding.mdx\n docs/how_to/callbacks_attach.md\n docs/how_to/callbacks_attach.mdx\n docs/how_to/binding.md\n@@ -214,6 +216,14 @@ docs/how_to/assign.md\n docs/how_to/assign.mdx\n docs/how_to/agent_executor.md\n docs/how_to/agent_executor.mdx\n+docs/versions/migrating_memory/conversation_summary_memory.md\n+docs/versions/migrating_memory/conversation_summary_memory.mdx\n+docs/versions/migrating_memory/conversation_buffer_window_memory.md\n+docs/versions/migrating_memory/conversation_buffer_window_memory.mdx\n+docs/versions/migrating_memory/chat_history.md\n+docs/versions/migrating_memory/chat_history.mdx\n+docs/troubleshooting/errors/INVALID_TOOL_RESULTS.md\n+docs/troubleshooting/errors/INVALID_TOOL_RESULTS.mdx\n docs/integrations/vectorstores/weaviate.md\n docs/integrations/vectorstores/weaviate.mdx\n docs/integrations/vectorstores/upstash.md\n@@ -278,14 +288,6 @@ docs/integrations/text_embedding/bedrock.md\n docs/integrations/text_embedding/bedrock.mdx\n docs/integrations/text_embedding/azure_openai.md\n docs/integrations/text_embedding/azure_openai.mdx\n-docs/integrations/retrievers/tavily.md\n-docs/integrations/retrievers/tavily.mdx\n-docs/integrations/retrievers/kendra-retriever.md\n-docs/integrations/retrievers/kendra-retriever.mdx\n-docs/integrations/retrievers/exa.md\n-docs/integrations/retrievers/exa.mdx\n-docs/integrations/retrievers/bedrock-knowledge-bases.md\n-docs/integrations/retrievers/bedrock-knowledge-bases.mdx\n docs/integrations/llms/together.md\n docs/integrations/llms/together.mdx\n docs/integrations/llms/openai.md\n@@ -304,10 +306,36 @@ docs/integrations/llms/cloudflare_workersai.md\n docs/integrations/llms/cloudflare_workersai.mdx\n docs/integrations/llms/bedrock.md\n docs/integrations/llms/bedrock.mdx\n-docs/integrations/llms/arcjet.md\n-docs/integrations/llms/arcjet.mdx\n docs/integrations/llms/azure.md\n docs/integrations/llms/azure.mdx\n+docs/integrations/llms/arcjet.md\n+docs/integrations/llms/arcjet.mdx\n+docs/integrations/retrievers/tavily.md\n+docs/integrations/retrievers/tavily.mdx\n+docs/integrations/retrievers/kendra-retriever.md\n+docs/integrations/retrievers/kendra-retriever.mdx\n+docs/integrations/retrievers/exa.md\n+docs/integrations/retrievers/exa.mdx\n+docs/integrations/retrievers/bm25.md\n+docs/integrations/retrievers/bm25.mdx\n+docs/integrations/retrievers/bedrock-knowledge-bases.md\n+docs/integrations/retrievers/bedrock-knowledge-bases.mdx\n+docs/integrations/retrievers/self_query/weaviate.md\n+docs/integrations/retrievers/self_query/weaviate.mdx\n+docs/integrations/retrievers/self_query/vectara.md\n+docs/integrations/retrievers/self_query/vectara.mdx\n+docs/integrations/retrievers/self_query/supabase.md\n+docs/integrations/retrievers/self_query/supabase.mdx\n+docs/integrations/retrievers/self_query/qdrant.md\n+docs/integrations/retrievers/self_query/qdrant.mdx\n+docs/integrations/retrievers/self_query/pinecone.md\n+docs/integrations/retrievers/self_query/pinecone.mdx\n+docs/integrations/retrievers/self_query/memory.md\n+docs/integrations/retrievers/self_query/memory.mdx\n+docs/integrations/retrievers/self_query/hnswlib.md\n+docs/integrations/retrievers/self_query/hnswlib.mdx\n+docs/integrations/retrievers/self_query/chroma.md\n+docs/integrations/retrievers/self_query/chroma.mdx\n docs/integrations/chat/togetherai.md\n docs/integrations/chat/togetherai.mdx\n docs/integrations/chat/openai.md\n@@ -332,28 +360,12 @@ docs/integrations/chat/bedrock_converse.md\n docs/integrations/chat/bedrock_converse.mdx\n docs/integrations/chat/bedrock.md\n docs/integrations/chat/bedrock.mdx\n-docs/integrations/chat/arcjet.md\n-docs/integrations/chat/arcjet.mdx\n docs/integrations/chat/azure.md\n docs/integrations/chat/azure.mdx\n+docs/integrations/chat/arcjet.md\n+docs/integrations/chat/arcjet.mdx\n docs/integrations/chat/anthropic.md\n docs/integrations/chat/anthropic.mdx\n-docs/integrations/retrievers/self_query/weaviate.md\n-docs/integrations/retrievers/self_query/weaviate.mdx\n-docs/integrations/retrievers/self_query/vectara.md\n-docs/integrations/retrievers/self_query/vectara.mdx\n-docs/integrations/retrievers/self_query/supabase.md\n-docs/integrations/retrievers/self_query/supabase.mdx\n-docs/integrations/retrievers/self_query/qdrant.md\n-docs/integrations/retrievers/self_query/qdrant.mdx\n-docs/integrations/retrievers/self_query/pinecone.md\n-docs/integrations/retrievers/self_query/pinecone.mdx\n-docs/integrations/retrievers/self_query/memory.md\n-docs/integrations/retrievers/self_query/memory.mdx\n-docs/integrations/retrievers/self_query/hnswlib.md\n-docs/integrations/retrievers/self_query/hnswlib.mdx\n-docs/integrations/retrievers/self_query/chroma.md\n-docs/integrations/retrievers/self_query/chroma.mdx\n docs/integrations/document_loaders/web_loaders/web_puppeteer.md\n docs/integrations/document_loaders/web_loaders/web_puppeteer.mdx\n docs/integrations/document_loaders/web_loaders/web_cheerio.md\n@@ -375,4 +387,4 @@ docs/integrations/document_loaders/file_loaders/pdf.mdx\n docs/integrations/document_loaders/file_loaders/directory.md\n docs/integrations/document_loaders/file_loaders/directory.mdx\n docs/integrations/document_loaders/file_loaders/csv.md\n-docs/integrations/document_loaders/file_loaders/csv.mdx\n+docs/integrations/document_loaders/file_loaders/csv.mdx\n\\ No newline at end of file",
          "docs/core_docs/docs/additional_resources/tutorials.mdx": "@@ -1,6 +1,6 @@\n-# Tutorials\n+# External guides\n \n-Below are links to tutorials and courses on LangChain.js. For written guides on common use cases for LangChain.js, check out the [tutorials](/docs/tutorials/) and [how to](/docs/how_to/) sections.\n+Below are links to external tutorials and courses on LangChain.js. For other written guides on common use cases for LangChain.js, check out the [tutorials](/docs/tutorials/) and [how to](/docs/how_to/) sections.\n \n ---\n ",
          "docs/core_docs/docs/concepts.mdx": "@@ -657,7 +657,7 @@ const tools = toolkit.getTools()\n \n By themselves, language models can't take actions - they just output text.\n A big use case for LangChain is creating **agents**.\n-Agents are systems that use an LLM as a reasoning enginer to determine which actions to take and what the inputs to those actions should be.\n+Agents are systems that use an LLM as a reasoning engineer to determine which actions to take and what the inputs to those actions should be.\n The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.\n \n [LangGraph](https://github.com/langchain-ai/langgraphjs) is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.",
          "docs/core_docs/docs/contributing/integrations.mdx": "@@ -96,8 +96,12 @@ For information on running and implementing tests, see the [Testing guide](/docs\n \n ### Write documentation\n \n-Documentation is generated from Jupyter notebooks or `.mdx` files in the `docs/` directory. You should place the notebooks with examples\n-to the relevant `docs/core_docs/docs/integrations` directory in the monorepo root.\n+Please copy and use the appropriate template from here:\n+\n+https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-scripts/src/cli/docs/templates\n+\n+You should place the notebooks with examples\n+in the relevant `docs/core_docs/docs/integrations` directory in the monorepo root.\n \n ### (If Necessary) Deprecate community integration\n ",
          "docs/core_docs/docs/how_to/agent_executor.ipynb": "@@ -103,7 +103,7 @@\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m`[{\\\"title\\\":\\\"Weather in San Francisco\\\",\\\"url\\\":\\\"https://www.weatherapi.com/\\\",\\\"content\\\":\\\"{'location': {'n`\\u001b[39m... 1347 more characters\"\n+       \"\\u001b[32m`[{\\\"title\\\":\\\"Weather in San Francisco\\\",\\\"url\\\":\\\"https://www.weatherapi.com/\\\",\\\"content\\\":\\\"{'location': {'n`\\u001b[39m... 1358 more characters\"\n       ]\n      },\n      \"execution_count\": 1,\n@@ -146,7 +146,8 @@\n        \"  metadata: {\\n\",\n        \"    source: \\u001b[32m\\\"https://docs.smith.langchain.com/overview\\\"\\u001b[39m,\\n\",\n        \"    loc: { lines: { from: \\u001b[33m4\\u001b[39m, to: \\u001b[33m4\\u001b[39m } }\\n\",\n-       \"  }\\n\",\n+       \"  },\\n\",\n+       \"  id: \\u001b[90mundefined\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n@@ -186,18 +187,25 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 7,\n    \"id\": \"7280b031\",\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import { createRetrieverTool } from \\\"langchain/tools/retriever\\\";\\n\",\n+    \"import { z } from \\\"zod\\\";\\n\",\n+    \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n     \"\\n\",\n-    \"const retrieverTool = await createRetrieverTool(retriever, {\\n\",\n-    \"    name: \\\"langsmith_search\\\",\\n\",\n-    \"    description:\\n\",\n-    \"      \\\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\\\",\\n\",\n-    \"  });\"\n+    \"const retrieverTool = tool(async ({ input }, config) => {\\n\",\n+    \"  const docs = await retriever.invoke(input, config);\\n\",\n+    \"  return docs.map((doc) => doc.pageContent).join(\\\"\\\\n\\\\n\\\");\\n\",\n+    \"}, {\\n\",\n+    \"  name: \\\"langsmith_search\\\",\\n\",\n+    \"  description:\\n\",\n+    \"    \\\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\\\",\\n\",\n+    \"  schema: z.object({\\n\",\n+    \"    input: z.string()\\n\",\n+    \"  }),\\n\",\n+    \"});\"\n    ]\n   },\n   {\n@@ -212,7 +220,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 8,\n    \"id\": \"b8e8e710\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -246,7 +254,21 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 9,\n+   \"id\": \"def033a4\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const model = new ChatOpenAI({ model: \\\"gpt-4o-mini\\\", temperature: 0 })\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 10,\n    \"id\": \"c96c960b\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -256,18 +278,16 @@\n        \"\\u001b[32m\\\"Hello! How can I assist you today?\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 5,\n+     \"execution_count\": 10,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n    ],\n    \"source\": [\n-    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n-    \"const model = new ChatOpenAI({ model: \\\"gpt-4\\\", temperature: 0 })\\n\",\n-    \"\\n\",\n-    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n-    \"const response = await model.invoke([new HumanMessage(\\\"hi!\\\")]);\\n\",\n+    \"const response = await model.invoke([{\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"hi!\\\"\\n\",\n+    \"}]);\\n\",\n     \"\\n\",\n     \"response.content;\"\n    ]\n@@ -282,7 +302,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 11,\n    \"id\": \"ba692a74\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -300,7 +320,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 12,\n    \"id\": \"b6a7e925\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -314,7 +334,10 @@\n     }\n    ],\n    \"source\": [\n-    \"const responseWithTools = await modelWithTools.invoke([new HumanMessage(\\\"Hi!\\\")])\\n\",\n+    \"const responseWithTools = await modelWithTools.invoke([{\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"Hi!\\\"\\n\",\n+    \"}])\\n\",\n     \"\\n\",\n     \"console.log(`Content: ${responseWithTools.content}`)\\n\",\n     \"console.log(`Tool calls: ${responseWithTools.tool_calls}`)\"\n@@ -330,7 +353,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 13,\n    \"id\": \"688b465d\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -345,14 +368,18 @@\n       \"    \\\"args\\\": {\\n\",\n       \"      \\\"input\\\": \\\"current weather in San Francisco\\\"\\n\",\n       \"    },\\n\",\n-      \"    \\\"id\\\": \\\"call_VcSjZAZkEOx9lcHNZNXAjXkm\\\"\\n\",\n+      \"    \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"    \\\"id\\\": \\\"call_gtJ5rrjXswO8EIvePrxyGQbR\\\"\\n\",\n       \"  }\\n\",\n       \"]\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n-    \"const responseWithToolCalls = await modelWithTools.invoke([new HumanMessage(\\\"What's the weather in SF?\\\")])\\n\",\n+    \"const responseWithToolCalls = await modelWithTools.invoke([{\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"What's the weather in SF?\\\"\\n\",\n+    \"}])\\n\",\n     \"\\n\",\n     \"console.log(`Content: ${responseWithToolCalls.content}`)\\n\",\n     \"console.log(`Tool calls: ${JSON.stringify(responseWithToolCalls.tool_calls, null, 2)}`)\"\n@@ -382,7 +409,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 14,\n    \"id\": \"af83d3e3\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -409,7 +436,8 @@\n       \"        partialVariables: undefined,\\n\",\n       \"        templateFormat: \\\"f-string\\\",\\n\",\n       \"        template: \\\"You are a helpful assistant\\\",\\n\",\n-      \"        validateTemplate: true\\n\",\n+      \"        validateTemplate: true,\\n\",\n+      \"        additionalContentFields: undefined\\n\",\n       \"      }\\n\",\n       \"    },\\n\",\n       \"    lc_runnable: true,\\n\",\n@@ -432,7 +460,8 @@\n       \"      partialVariables: undefined,\\n\",\n       \"      templateFormat: \\\"f-string\\\",\\n\",\n       \"      template: \\\"You are a helpful assistant\\\",\\n\",\n-      \"      validateTemplate: true\\n\",\n+      \"      validateTemplate: true,\\n\",\n+      \"      additionalContentFields: undefined\\n\",\n       \"    },\\n\",\n       \"    messageClass: undefined,\\n\",\n       \"    chatMessageClass: undefined\\n\",\n@@ -464,7 +493,8 @@\n       \"        partialVariables: undefined,\\n\",\n       \"        templateFormat: \\\"f-string\\\",\\n\",\n       \"        template: \\\"{input}\\\",\\n\",\n-      \"        validateTemplate: true\\n\",\n+      \"        validateTemplate: true,\\n\",\n+      \"        additionalContentFields: undefined\\n\",\n       \"      }\\n\",\n       \"    },\\n\",\n       \"    lc_runnable: true,\\n\",\n@@ -487,7 +517,8 @@\n       \"      partialVariables: undefined,\\n\",\n       \"      templateFormat: \\\"f-string\\\",\\n\",\n       \"      template: \\\"{input}\\\",\\n\",\n-      \"      validateTemplate: true\\n\",\n+      \"      validateTemplate: true,\\n\",\n+      \"      additionalContentFields: undefined\\n\",\n       \"    },\\n\",\n       \"    messageClass: undefined,\\n\",\n       \"    chatMessageClass: undefined\\n\",\n@@ -530,7 +561,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 15,\n    \"id\": \"89cf72b4-6046-4b47-8f27-5522d8cb8036\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -550,7 +581,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 16,\n    \"id\": \"ce33904a\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -577,7 +608,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 17,\n    \"id\": \"114ba50d\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -587,7 +618,7 @@\n        \"{ input: \\u001b[32m\\\"hi!\\\"\\u001b[39m, output: \\u001b[32m\\\"Hello! How can I assist you today?\\\"\\u001b[39m }\"\n       ]\n      },\n-     \"execution_count\": 12,\n+     \"execution_count\": 17,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -608,7 +639,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 18,\n    \"id\": \"3fa4780a\",\n    \"metadata\": {\n     \"scrolled\": true\n@@ -619,13 +650,11 @@\n       \"text/plain\": [\n        \"{\\n\",\n        \"  input: \\u001b[32m\\\"how can langsmith help with testing?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m\\\"LangSmith can be a valuable tool for testing in several ways:\\\\n\\\"\\u001b[39m +\\n\",\n-       \"    \\u001b[32m\\\"\\\\n\\\"\\u001b[39m +\\n\",\n-       \"    \\u001b[32m\\\"1. **Logging Traces**: LangSmith prov\\\"\\u001b[39m... 960 more characters\\n\",\n+       \"  output: \\u001b[32m\\\"LangSmith can assist with testing in several ways, particularly for applications built using large l\\\"\\u001b[39m... 1474 more characters\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 13,\n+     \"execution_count\": 18,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -646,7 +675,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 19,\n    \"id\": \"77c2f769\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -655,11 +684,14 @@\n       \"text/plain\": [\n        \"{\\n\",\n        \"  input: \\u001b[32m\\\"whats the weather in sf?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m\\\"The current weather in San Francisco, California is partly cloudy with a temperature of 12.2°C (54.0\\\"\\u001b[39m... 176 more characters\\n\",\n+       \"  output: \\u001b[32m\\\"The current weather in San Francisco is as follows:\\\\n\\\"\\u001b[39m +\\n\",\n+       \"    \\u001b[32m\\\"\\\\n\\\"\\u001b[39m +\\n\",\n+       \"    \\u001b[32m\\\"- **Temperature**: 15.6°C (60.1°F)\\\\n\\\"\\u001b[39m +\\n\",\n+       \"    \\u001b[32m\\\"- **Conditio\\\"\\u001b[39m... 303 more characters\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 14,\n+     \"execution_count\": 19,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -690,7 +722,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 20,\n    \"id\": \"c4073e35\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -704,7 +736,7 @@\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 15,\n+     \"execution_count\": 20,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -716,7 +748,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 21,\n    \"id\": \"550e0c6e\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -725,58 +757,31 @@\n       \"text/plain\": [\n        \"{\\n\",\n        \"  chat_history: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"hi! my name is bob\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"hi! my name is bob\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Hello Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Hello Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"    { role: \\u001b[32m\\\"user\\\"\\u001b[39m, content: \\u001b[32m\\\"hi! my name is bob\\\"\\u001b[39m },\\n\",\n+       \"    {\\n\",\n+       \"      role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n+       \"      content: \\u001b[32m\\\"Hello Bob! How can I assist you today?\\\"\\u001b[39m\\n\",\n        \"    }\\n\",\n        \"  ],\\n\",\n        \"  input: \\u001b[32m\\\"what's my name?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m\\\"Your name is Bob. How can I assist you further?\\\"\\u001b[39m\\n\",\n+       \"  output: \\u001b[32m\\\"Your name is Bob. How can I help you today, Bob?\\\"\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 16,\n+     \"execution_count\": 21,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n    ],\n    \"source\": [\n-    \"import { AIMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n     \"await agentExecutor.invoke(\\n\",\n-    \"    {\\n\",\n-    \"        chat_history: [\\n\",\n-    \"            new HumanMessage(\\\"hi! my name is bob\\\"),\\n\",\n-    \"            new AIMessage(\\\"Hello Bob! How can I assist you today?\\\"),\\n\",\n-    \"        ],\\n\",\n-    \"        input: \\\"what's my name?\\\",\\n\",\n-    \"    }\\n\",\n+    \"  {\\n\",\n+    \"    chat_history: [\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"hi! my name is bob\\\" },\\n\",\n+    \"      { role: \\\"assistant\\\", content: \\\"Hello Bob! How can I assist you today?\\\" },\\n\",\n+    \"    ],\\n\",\n+    \"    input: \\\"what's my name?\\\",\\n\",\n+    \"  }\\n\",\n     \")\"\n    ]\n   },\n@@ -797,7 +802,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 17,\n+   \"execution_count\": 22,\n    \"id\": \"8edd96e6\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -808,41 +813,23 @@\n        \"  input: \\u001b[32m\\\"hi! I'm bob\\\"\\u001b[39m,\\n\",\n        \"  chat_history: [\\n\",\n        \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"hi! I'm bob\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"hi! I'm bob\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n+       \"      \\\"content\\\": \\\"hi! I'm bob\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {}\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Hello, Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Hello, Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"content\\\": \\\"Hello Bob! How can I assist you today?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {},\\n\",\n+       \"      \\\"tool_calls\\\": [],\\n\",\n+       \"      \\\"invalid_tool_calls\\\": []\\n\",\n        \"    }\\n\",\n        \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Hello, Bob! How can I assist you today?\\\"\\u001b[39m\\n\",\n+       \"  output: \\u001b[32m\\\"Hello Bob! How can I assist you today?\\\"\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 17,\n+     \"execution_count\": 22,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -855,10 +842,10 @@\n     \"const store = {};\\n\",\n     \"\\n\",\n     \"function getMessageHistory(sessionId: string): BaseChatMessageHistory {\\n\",\n-    \"    if (!(sessionId in store)) {\\n\",\n-    \"        store[sessionId] = new ChatMessageHistory();\\n\",\n-    \"    }\\n\",\n-    \"    return store[sessionId];\\n\",\n+    \"  if (!(sessionId in store)) {\\n\",\n+    \"    store[sessionId] = new ChatMessageHistory();\\n\",\n+    \"  }\\n\",\n+    \"  return store[sessionId];\\n\",\n     \"}\\n\",\n     \"\\n\",\n     \"const agentWithChatHistory = new RunnableWithMessageHistory({\\n\",\n@@ -876,7 +863,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 18,\n+   \"execution_count\": 23,\n    \"id\": \"ae627966\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -887,71 +874,35 @@\n        \"  input: \\u001b[32m\\\"what's my name?\\\"\\u001b[39m,\\n\",\n        \"  chat_history: [\\n\",\n        \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"hi! I'm bob\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"hi! I'm bob\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n+       \"      \\\"content\\\": \\\"hi! I'm bob\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {}\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Hello, Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Hello, Bob! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"content\\\": \\\"Hello Bob! How can I assist you today?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {},\\n\",\n+       \"      \\\"tool_calls\\\": [],\\n\",\n+       \"      \\\"invalid_tool_calls\\\": []\\n\",\n        \"    },\\n\",\n        \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"what's my name?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"what's my name?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n+       \"      \\\"content\\\": \\\"what's my name?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {}\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Your name is Bob. How can I assist you further?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Your name is Bob. How can I assist you further?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"content\\\": \\\"Your name is Bob! How can I help you today, Bob?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {},\\n\",\n+       \"      \\\"tool_calls\\\": [],\\n\",\n+       \"      \\\"invalid_tool_calls\\\": []\\n\",\n        \"    }\\n\",\n        \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Your name is Bob. How can I assist you further?\\\"\\u001b[39m\\n\",\n+       \"  output: \\u001b[32m\\\"Your name is Bob! How can I help you today, Bob?\\\"\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 18,\n+     \"execution_count\": 23,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }",
          "docs/core_docs/docs/how_to/chatbots_memory.ipynb": "@@ -1,37 +1,49 @@\n {\n  \"cells\": [\n+  {\n+   \"cell_type\": \"raw\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"---\\n\",\n+    \"sidebar_position: 1\\n\",\n+    \"---\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# How to manage memory\\n\",\n-    \"\\n\",\n-    \":::info Prerequisites\\n\",\n-    \"\\n\",\n-    \"This guide assumes familiarity with the following:\\n\",\n-    \"\\n\",\n-    \"- [Chatbots](/docs/tutorials/chatbot)\\n\",\n-    \"\\n\",\n-    \":::\\n\",\n+    \"# How to add memory to chatbots\\n\",\n     \"\\n\",\n     \"A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\n\",\n     \"\\n\",\n     \"- Simply stuffing previous messages into a chat model prompt.\\n\",\n     \"- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\\n\",\n     \"- More complex modifications like synthesizing summaries for long running conversations.\\n\",\n     \"\\n\",\n-    \"We’ll go into more detail on a few techniques below!\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n+    \"We'll go into more detail on a few techniques below!\\n\",\n+    \"\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"This how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://v03.api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/chatbots_memory/).\\n\",\n+    \"\\n\",\n+    \"The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n     \"## Setup\\n\",\n     \"\\n\",\n-    \"You’ll need to install a few packages, and set any LLM API keys:\\n\",\n+    \"You'll need to install a few packages, select your chat model, and set its enviroment variable.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\"\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/core @langchain/langgraph\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"```\\n\",\n     \"\\n\",\n-    \"Let’s also set up a chat model that we’ll use for the below examples:\\n\",\n+    \"Let's set up a chat model that we'll use for the below examples.\\n\",\n     \"\\n\",\n     \"```{=mdx}\\n\",\n     \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n@@ -46,42 +58,53 @@\n    \"source\": [\n     \"## Message passing\\n\",\n     \"\\n\",\n-    \"The simplest form of memory is simply passing chat history messages into a chain. Here’s an example:\"\n+    \"The simplest form of memory is simply passing chat history messages into a chain. Here's an example:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 22,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const llm = new ChatOpenAI({ model: \\\"gpt-4o\\\" })\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 23,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m`I said \\\"J'adore la programmation,\\\" which means \\\"I love programming\\\" in French.`\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m`I said \\\"J'adore la programmation,\\\" which means \\\"I love programming\\\" in French.`\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m21\\u001b[39m, promptTokens: \\u001b[33m61\\u001b[39m, totalTokens: \\u001b[33m82\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 2,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABSxUXVIBitFRBh9MpasB5jeEHfCA\\\",\\n\",\n+      \"  \\\"content\\\": \\\"I said \\\\\\\"J'adore la programmation,\\\\\\\" which means \\\\\\\"I love programming\\\\\\\" in French.\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 18,\\n\",\n+      \"      \\\"promptTokens\\\": 58,\\n\",\n+      \"      \\\"totalTokens\\\": 76\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 58,\\n\",\n+      \"    \\\"output_tokens\\\": 18,\\n\",\n+      \"    \\\"total_tokens\\\": 76\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n@@ -119,303 +142,191 @@\n     \"We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\"\n    ]\n   },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"## Chat history\\n\",\n-    \"\\n\",\n-    \"It’s perfectly fine to store and pass messages directly as an array, but we can use LangChain’s built-in message history class to store and load messages as well. Instances of this class are responsible for storing and loading chat messages from persistent storage. LangChain integrates with many providers but for this demo we will use an ephemeral demo class.\\n\",\n-    \"\\n\",\n-    \"Here’s an example of the API:\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"[\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Translate this sentence from English to French: I love programming.\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Translate this sentence from English to French: I love programming.\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"J'adore la programmation.\\\"\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"J'adore la programmation.\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {},\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  }\\n\",\n-       \"]\"\n-      ]\n-     },\n-     \"execution_count\": 3,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"import { ChatMessageHistory } from \\\"langchain/stores/message/in_memory\\\";\\n\",\n-    \"\\n\",\n-    \"const demoEphemeralChatMessageHistory = new ChatMessageHistory();\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new HumanMessage(\\n\",\n-    \"    \\\"Translate this sentence from English to French: I love programming.\\\"\\n\",\n-    \"  )\\n\",\n-    \");\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new AIMessage(\\\"J'adore la programmation.\\\")\\n\",\n-    \");\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.getMessages();\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"We can use it directly to store conversation turns for our chain:\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m'You just asked me to translate the sentence \\\"I love programming\\\" from English to French.'\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m'You just asked me to translate the sentence \\\"I love programming\\\" from English to French.'\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m18\\u001b[39m, promptTokens: \\u001b[33m73\\u001b[39m, totalTokens: \\u001b[33m91\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 4,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await demoEphemeralChatMessageHistory.clear();\\n\",\n-    \"\\n\",\n-    \"const input1 =\\n\",\n-    \"  \\\"Translate this sentence from English to French: I love programming.\\\";\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new HumanMessage(input1));\\n\",\n-    \"\\n\",\n-    \"const response = await chain.invoke({\\n\",\n-    \"  messages: await demoEphemeralChatMessageHistory.getMessages(),\\n\",\n-    \"});\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(response);\\n\",\n-    \"\\n\",\n-    \"const input2 = \\\"What did I just ask you?\\\";\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new HumanMessage(input2));\\n\",\n-    \"\\n\",\n-    \"await chain.invoke({\\n\",\n-    \"  messages: await demoEphemeralChatMessageHistory.getMessages(),\\n\",\n-    \"});\"\n-   ]\n-  },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n     \"## Automatic history management\\n\",\n     \"\\n\",\n-    \"The previous examples pass messages to the chain explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also includes an wrapper for LCEL chains that can handle this process automatically called `RunnableWithMessageHistory`.\\n\",\n-    \"\\n\",\n-    \"To show how it works, let’s slightly modify the above prompt to take a final `input` variable that populates a `HumanMessage` template after the chat history. This means that we will expect a `chat_history` parameter that contains all messages BEFORE the current messages instead of all messages:\"\n+    \"The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a `checkpointer` when compiling the graph.\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 24,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"const runnableWithMessageHistoryPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-    \"  [\\n\",\n-    \"    \\\"system\\\",\\n\",\n-    \"    \\\"You are a helpful assistant. Answer all questions to the best of your ability.\\\",\\n\",\n-    \"  ],\\n\",\n-    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n-    \"]);\\n\",\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n     \"\\n\",\n-    \"const chain2 = runnableWithMessageHistoryPrompt.pipe(llm);\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"We’ll pass the latest input to the conversation here and let the `RunnableWithMessageHistory` class wrap our chain and do the work of appending that `input` variable to the chat history.\\n\",\n     \"\\n\",\n-    \"Next, let’s declare our wrapped chain:\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n-   \"metadata\": {},\n-   \"outputs\": [],\n-   \"source\": [\n-    \"import { RunnableWithMessageHistory } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  const systemPrompt = \\n\",\n+    \"    \\\"You are a helpful assistant. \\\" +\\n\",\n+    \"    \\\"Answer all questions to the best of your ability.\\\";\\n\",\n+    \"  const messages = [{ role: \\\"system\\\", content: systemPrompt }, ...state.messages];\\n\",\n+    \"  const response = await llm.invoke(messages);\\n\",\n+    \"  return { messages: response };\\n\",\n+    \"};\\n\",\n     \"\\n\",\n-    \"const demoEphemeralChatMessageHistoryForChain = new ChatMessageHistory();\\n\",\n+    \"const workflow = new StateGraph(MessagesAnnotation)\\n\",\n+    \"// Define the node and edge\\n\",\n+    \"  .addNode(\\\"model\\\", callModel)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n     \"\\n\",\n-    \"const chainWithMessageHistory = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: chain2,\\n\",\n-    \"  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistoryForChain,\\n\",\n-    \"  inputMessagesKey: \\\"input\\\",\\n\",\n-    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n-    \"});\"\n+    \"// Add simple in-memory checkpointer\\n\",\n+    \"// highlight-start\\n\",\n+    \"const memory = new MemorySaver();\\n\",\n+    \"const app = workflow.compile({ checkpointer: memory });\\n\",\n+    \"// highlight-end\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"This class takes a few parameters in addition to the chain that we want to wrap:\\n\",\n-    \"\\n\",\n-    \"- A factory function that returns a message history for a given session id. This allows your chain to handle multiple users at once by loading different messages for different conversations.\\n\",\n-    \"- An `inputMessagesKey` that specifies which part of the input should be tracked and stored in the chat history. In this example, we want to track the string passed in as input.\\n\",\n-    \"- A `historyMessagesKey` that specifies what the previous messages should be injected into the prompt as. Our prompt has a `MessagesPlaceholder` named `chat_history`, so we specify this property to match.\\n\",\n-    \"  (For chains with multiple outputs) an `outputMessagesKey` which specifies which output to store as history. This is the inverse of `inputMessagesKey`.\\n\",\n-    \"\\n\",\n-    \"We can invoke this new chain as normal, with an additional `configurable` field that specifies the particular `sessionId` to pass to the factory function. This is unused for the demo, but in real-world chains, you’ll want to return a chat history corresponding to the passed session:\"\n+    \" We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 25,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m`The translation of \\\"I love programming\\\" in French is \\\"J'adore la programmation.\\\"`\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m`The translation of \\\"I love programming\\\" in French is \\\"J'adore la programmation.\\\"`\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m20\\u001b[39m, promptTokens: \\u001b[33m39\\u001b[39m, totalTokens: \\u001b[33m59\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 8,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"227b82a9-4084-46a5-ac79-ab9a3faa140e\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Translate to French: I love programming.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ\\\",\\n\",\n+      \"      \\\"content\\\": \\\"J'adore la programmation.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 5,\\n\",\n+      \"          \\\"promptTokens\\\": 35,\\n\",\n+      \"          \\\"totalTokens\\\": 40\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 35,\\n\",\n+      \"        \\\"output_tokens\\\": 5,\\n\",\n+      \"        \\\"total_tokens\\\": 40\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await chainWithMessageHistory.invoke(\\n\",\n+    \"await app.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input:\\n\",\n-    \"      \\\"Translate this sentence from English to French: I love programming.\\\",\\n\",\n+    \"    messages: [\\n\",\n+    \"      {\\n\",\n+    \"        role: \\\"user\\\",\\n\",\n+    \"        content: \\\"Translate to French: I love programming.\\\"\\n\",\n+    \"      }\\n\",\n+    \"    ]\\n\",\n     \"  },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n+    \"  {\\n\",\n+    \"    configurable: { thread_id: \\\"1\\\" }\\n\",\n+    \"  }\\n\",\n     \");\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 26,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m'You just asked for the translation of the sentence \\\"I love programming\\\" from English to French.'\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m'You just asked for the translation of the sentence \\\"I love programming\\\" from English to French.'\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m19\\u001b[39m, promptTokens: \\u001b[33m74\\u001b[39m, totalTokens: \\u001b[33m93\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 9,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"1a0560a4-9dcb-47a1-b441-80717e229706\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Translate to French: I love programming.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ\\\",\\n\",\n+      \"      \\\"content\\\": \\\"J'adore la programmation.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 5,\\n\",\n+      \"          \\\"promptTokens\\\": 35,\\n\",\n+      \"          \\\"totalTokens\\\": 40\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"4f233a7d-4b08-4f53-bb60-cf0141a59721\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What did I just ask you?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxVs5QnlPfbihTOmJrCVg1Dh7Ol\\\",\\n\",\n+      \"      \\\"content\\\": \\\"You asked me to translate \\\\\\\"I love programming\\\\\\\" into French.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 13,\\n\",\n+      \"          \\\"promptTokens\\\": 55,\\n\",\n+      \"          \\\"totalTokens\\\": 68\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_9f2bfdaa89\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 55,\\n\",\n+      \"        \\\"output_tokens\\\": 13,\\n\",\n+      \"        \\\"total_tokens\\\": 68\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await chainWithMessageHistory.invoke(\\n\",\n+    \"await app.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input: \\\"What did I just ask you?\\\",\\n\",\n+    \"    messages: [\\n\",\n+    \"      {\\n\",\n+    \"        role: \\\"user\\\",\\n\",\n+    \"        content: \\\"What did I just ask you?\\\"\\n\",\n+    \"      }\\n\",\n+    \"    ]\\n\",\n     \"  },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n+    \"  {\\n\",\n+    \"    configurable: { thread_id: \\\"1\\\" }\\n\",\n+    \"  }\\n\",\n     \");\"\n    ]\n   },\n@@ -429,711 +340,423 @@\n     \"\\n\",\n     \"### Trimming messages\\n\",\n     \"\\n\",\n-    \"LLMs and chat models have limited context windows, and even if you’re not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is to only load and store the most recent `n` messages. Let’s use an example history with some preloaded messages:\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"[\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Hey there! I'm Nemo.\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Hey there! I'm Nemo.\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Hello!\\\"\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Hello!\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {},\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  },\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"How are you today?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"How are you today?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Fine thanks!\\\"\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Fine thanks!\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {},\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  }\\n\",\n-       \"]\"\n-      ]\n-     },\n-     \"execution_count\": 10,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await demoEphemeralChatMessageHistory.clear();\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new HumanMessage(\\\"Hey there! I'm Nemo.\\\")\\n\",\n-    \");\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new AIMessage(\\\"Hello!\\\"));\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new HumanMessage(\\\"How are you today?\\\")\\n\",\n-    \");\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new AIMessage(\\\"Fine thanks!\\\"));\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.getMessages();\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"Let’s use this message history with the `RunnableWithMessageHistory` chain we declared above:\"\n+    \"LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the `app` we declared above:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 27,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m6\\u001b[39m, promptTokens: \\u001b[33m66\\u001b[39m, totalTokens: \\u001b[33m72\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 11,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"63057c3d-f980-4640-97d6-497a9f83ddee\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Hey there! I'm Nemo.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"c9f0c20a-8f55-4909-b281-88f2a45c4f05\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Hello!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {},\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"fd7fb3a0-7bc7-4e84-99a9-731b30637b55\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How are you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"09b0debb-1d4a-4856-8821-b037f5d96ecf\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Fine thanks!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {},\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"edc13b69-25a0-40ac-81b3-175e65dc1a9a\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What's my name?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxWKCTdRuh2ZifXsvFHSo5z5I0J\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Your name is Nemo! How can I assist you today, Nemo?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 14,\\n\",\n+      \"          \\\"promptTokens\\\": 63,\\n\",\n+      \"          \\\"totalTokens\\\": 77\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_a5d11b2ef2\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 63,\\n\",\n+      \"        \\\"output_tokens\\\": 14,\\n\",\n+      \"        \\\"total_tokens\\\": 77\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const chainWithMessageHistory2 = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: chain2,\\n\",\n-    \"  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistory,\\n\",\n-    \"  inputMessagesKey: \\\"input\\\",\\n\",\n-    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n-    \"});\\n\",\n+    \"const demoEphemeralChatHistory = [\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"Hey there! I'm Nemo.\\\" },\\n\",\n+    \"  { role: \\\"assistant\\\", content: \\\"Hello!\\\" },\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"How are you today?\\\" },\\n\",\n+    \"  { role: \\\"assistant\\\", content: \\\"Fine thanks!\\\" },\\n\",\n+    \"];\\n\",\n     \"\\n\",\n-    \"await chainWithMessageHistory2.invoke(\\n\",\n+    \"await app.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input: \\\"What's my name?\\\",\\n\",\n+    \"    messages: [\\n\",\n+    \"      ...demoEphemeralChatHistory,\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"What's my name?\\\" }\\n\",\n+    \"    ]\\n\",\n     \"  },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n+    \"  {\\n\",\n+    \"    configurable: { thread_id: \\\"2\\\" }\\n\",\n+    \"  }\\n\",\n     \");\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We can see the chain remembers the preloaded name.\\n\",\n+    \"We can see the app remembers the preloaded name.\\n\",\n     \"\\n\",\n-    \"But let’s say we have a very small context window, and we want to trim the number of messages passed to the chain to only the 2 most recent ones. We can use the `clear` method to remove messages and re-add them to the history. We don’t have to, but let’s put this method at the front of our chain to ensure it’s always called:\"\n+    \"But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in [trimMessages](/docs/how_to/trim_messages/) util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \\\"token\\\" and keep only the last two messages:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 28,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import {\\n\",\n-    \"  RunnablePassthrough,\\n\",\n-    \"  RunnableSequence,\\n\",\n-    \"} from \\\"@langchain/core/runnables\\\";\\n\",\n-    \"\\n\",\n-    \"const trimMessages = async (_chainInput: Record<string, any>) => {\\n\",\n-    \"  const storedMessages = await demoEphemeralChatMessageHistory.getMessages();\\n\",\n-    \"  if (storedMessages.length <= 2) {\\n\",\n-    \"    return false;\\n\",\n-    \"  }\\n\",\n-    \"  await demoEphemeralChatMessageHistory.clear();\\n\",\n-    \"  for (const message of storedMessages.slice(-2)) {\\n\",\n-    \"    demoEphemeralChatMessageHistory.addMessage(message);\\n\",\n-    \"  }\\n\",\n-    \"  return true;\\n\",\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { trimMessages } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"// Define trimmer\\n\",\n+    \"// highlight-start\\n\",\n+    \"// count each message as 1 \\\"token\\\" (tokenCounter: (msgs) => msgs.length) and keep only the last two messages\\n\",\n+    \"const trimmer = trimMessages({ strategy: \\\"last\\\", maxTokens: 2, tokenCounter: (msgs) => msgs.length });\\n\",\n+    \"// highlight-end\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel2 = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  // highlight-start\\n\",\n+    \"  const trimmedMessages = await trimmer.invoke(state.messages);\\n\",\n+    \"  const systemPrompt = \\n\",\n+    \"    \\\"You are a helpful assistant. \\\" +\\n\",\n+    \"    \\\"Answer all questions to the best of your ability.\\\";\\n\",\n+    \"  const messages = [{ role: \\\"system\\\", content: systemPrompt }, ...trimmedMessages];\\n\",\n+    \"  // highlight-end\\n\",\n+    \"  const response = await llm.invoke(messages);\\n\",\n+    \"  return { messages: response };\\n\",\n     \"};\\n\",\n     \"\\n\",\n-    \"const chainWithTrimming = RunnableSequence.from([\\n\",\n-    \"  RunnablePassthrough.assign({ messages_trimmed: trimMessages }),\\n\",\n-    \"  chainWithMessageHistory2,\\n\",\n-    \"]);\"\n+    \"const workflow2 = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  // Define the node and edge\\n\",\n+    \"  .addNode(\\\"model\\\", callModel2)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"// Add simple in-memory checkpointer\\n\",\n+    \"const app2 = workflow2.compile({ checkpointer: new MemorySaver() });\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Let’s call this new chain and check the messages afterwards:\"\n+    \"Let's call this new app and check the response\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 29,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m26\\u001b[39m, promptTokens: \\u001b[33m53\\u001b[39m, totalTokens: \\u001b[33m79\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 13,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"0d9330a0-d9d1-4aaf-8171-ca1ac6344f7c\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What is my name?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"3a24e88b-7525-4797-9fcd-d751a378d22c\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Fine thanks!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {},\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"276039c8-eba8-4c68-b015-81ec7704140d\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How are you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"2ad4f461-20e1-4982-ba3b-235cb6b02abd\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Hello!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {},\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"52213cae-953a-463d-a4a0-a7368c9ee4db\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Hey there! I'm Nemo.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxWe9BRDl1pmzkNIDawWwU3hvKm\\\",\\n\",\n+      \"      \\\"content\\\": \\\"I'm sorry, but I don't have access to personal information about you unless you've shared it with me during our conversation. How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 30,\\n\",\n+      \"          \\\"promptTokens\\\": 39,\\n\",\n+      \"          \\\"totalTokens\\\": 69\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_3537616b13\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 39,\\n\",\n+      \"        \\\"output_tokens\\\": 30,\\n\",\n+      \"        \\\"total_tokens\\\": 69\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await chainWithTrimming.invoke(\\n\",\n+    \"await app2.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input: \\\"Where does P. Sherman live?\\\",\\n\",\n+    \"    messages: [\\n\",\n+    \"      ...demoEphemeralChatHistory,\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"What is my name?\\\" }\\n\",\n+    \"    ]\\n\",\n     \"  },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n-    \");\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"[\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"What's my name?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"What's my name?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m6\\u001b[39m, promptTokens: \\u001b[33m66\\u001b[39m, totalTokens: \\u001b[33m72\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  },\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Where does P. Sherman live?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Where does P. Sherman live?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m26\\u001b[39m, promptTokens: \\u001b[33m53\\u001b[39m, totalTokens: \\u001b[33m79\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  }\\n\",\n-       \"]\"\n-      ]\n-     },\n-     \"execution_count\": 14,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await demoEphemeralChatMessageHistory.getMessages();\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"And we can see that our history has removed the two oldest messages while still adding the most recent conversation at the end. The next time the chain is called, `trimMessages` will be called again, and only the two most recent messages will be passed to the model. In this case, this means that the model will forget the name we gave it the next time we invoke it:\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"I'm sorry, I don't have access to your personal information. Can I help you with anything else?\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"I'm sorry, I don't have access to your personal information. Can I help you with anything else?\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m22\\u001b[39m, promptTokens: \\u001b[33m73\\u001b[39m, totalTokens: \\u001b[33m95\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 15,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await chainWithTrimming.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input: \\\"What is my name?\\\",\\n\",\n-    \"  },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n+    \"    configurable: { thread_id: \\\"3\\\" }\\n\",\n+    \"  }\\n\",\n     \");\"\n    ]\n   },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"[\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"Where does P. Sherman live?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"Where does P. Sherman live?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie \\\"Finding Nem'\\u001b[39m... 3 more characters,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m26\\u001b[39m, promptTokens: \\u001b[33m53\\u001b[39m, totalTokens: \\u001b[33m79\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  },\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"I'm sorry, I don't have access to your personal information. Can I help you with anything else?\\\"\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"I'm sorry, I don't have access to your personal information. Can I help you with anything else?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m22\\u001b[39m, promptTokens: \\u001b[33m73\\u001b[39m, totalTokens: \\u001b[33m95\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  }\\n\",\n-       \"]\"\n-      ]\n-     },\n-     \"execution_count\": 16,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await demoEphemeralChatMessageHistory.getMessages();\"\n-   ]\n-  },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"### Summary memory\\n\",\n+    \"We can see that `trimMessages` was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.\\n\",\n     \"\\n\",\n-    \"We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our chain. Let’s recreate our chat history and chatbot chain:\"\n+    \"Check out our [how to guide on trimming messages](/docs/how_to/trim_messages/) for more.\"\n    ]\n   },\n   {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 17,\n+   \"cell_type\": \"markdown\",\n    \"metadata\": {},\n-   \"outputs\": [],\n    \"source\": [\n-    \"await demoEphemeralChatMessageHistory.clear();\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new HumanMessage(\\\"Hey there! I'm Nemo.\\\")\\n\",\n-    \");\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new AIMessage(\\\"Hello!\\\"));\\n\",\n-    \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(\\n\",\n-    \"  new HumanMessage(\\\"How are you today?\\\")\\n\",\n-    \");\\n\",\n+    \"### Summary memory\\n\",\n     \"\\n\",\n-    \"await demoEphemeralChatMessageHistory.addMessage(new AIMessage(\\\"Fine thanks!\\\"));\"\n+    \"We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 19,\n+   \"execution_count\": 30,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"const runnableWithSummaryMemoryPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-    \"  [\\n\",\n-    \"    \\\"system\\\",\\n\",\n-    \"    \\\"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\\\",\\n\",\n-    \"  ],\\n\",\n-    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n-    \"]);\\n\",\n-    \"\\n\",\n-    \"const summaryMemoryChain = runnableWithSummaryMemoryPrompt.pipe(llm);\\n\",\n-    \"\\n\",\n-    \"const chainWithMessageHistory3 = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: summaryMemoryChain,\\n\",\n-    \"  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistory,\\n\",\n-    \"  inputMessagesKey: \\\"input\\\",\\n\",\n-    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n-    \"});\"\n+    \"const demoEphemeralChatHistory2 = [\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"Hey there! I'm Nemo.\\\" },\\n\",\n+    \"  { role: \\\"assistant\\\", content: \\\"Hello!\\\" },\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"How are you today?\\\" },\\n\",\n+    \"  { role: \\\"assistant\\\", content: \\\"Fine thanks!\\\" },\\n\",\n+    \"];\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"And now, let’s create a function that will distill previous interactions into a summary. We can add this one to the front of the chain too:\"\n+    \"And now, let's update the model-calling function to distill previous interactions into a summary:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 22,\n+   \"execution_count\": 31,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"const summarizeMessages = async (_chainInput: Record<string, any>) => {\\n\",\n-    \"  const storedMessages = await demoEphemeralChatMessageHistory.getMessages();\\n\",\n-    \"  if (storedMessages.length === 0) {\\n\",\n-    \"    return false;\\n\",\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { RemoveMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel3 = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  const systemPrompt = \\n\",\n+    \"    \\\"You are a helpful assistant. \\\" +\\n\",\n+    \"    \\\"Answer all questions to the best of your ability. \\\" +\\n\",\n+    \"    \\\"The provided chat history includes a summary of the earlier conversation.\\\";\\n\",\n+    \"  const systemMessage = { role: \\\"system\\\", content: systemPrompt };\\n\",\n+    \"  const messageHistory = state.messages.slice(0, -1); // exclude the most recent user input\\n\",\n+    \"  \\n\",\n+    \"  // Summarize the messages if the chat history reaches a certain size\\n\",\n+    \"  if (messageHistory.length >= 4) {\\n\",\n+    \"    const lastHumanMessage = state.messages[state.messages.length - 1];\\n\",\n+    \"    // Invoke the model to generate conversation summary\\n\",\n+    \"    const summaryPrompt = \\n\",\n+    \"      \\\"Distill the above chat messages into a single summary message. \\\" +\\n\",\n+    \"      \\\"Include as many specific details as you can.\\\";\\n\",\n+    \"    const summaryMessage = await llm.invoke([\\n\",\n+    \"      ...messageHistory,\\n\",\n+    \"      { role: \\\"user\\\", content: summaryPrompt }\\n\",\n+    \"    ]);\\n\",\n+    \"\\n\",\n+    \"    // Delete messages that we no longer want to show up\\n\",\n+    \"    const deleteMessages = state.messages.map(m => new RemoveMessage({ id: m.id }));\\n\",\n+    \"    // Re-add user message\\n\",\n+    \"    const humanMessage = { role: \\\"user\\\", content: lastHumanMessage.content };\\n\",\n+    \"    // Call the model with summary & response\\n\",\n+    \"    const response = await llm.invoke([systemMessage, summaryMessage, humanMessage]);\\n\",\n+    \"    return { messages: [summaryMessage, humanMessage, response, ...deleteMessages] };\\n\",\n+    \"  } else {\\n\",\n+    \"    const response = await llm.invoke([systemMessage, ...state.messages]);\\n\",\n+    \"    return { messages: response };\\n\",\n     \"  }\\n\",\n-    \"  const summarizationPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-    \"    new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-    \"    [\\n\",\n-    \"      \\\"user\\\",\\n\",\n-    \"      \\\"Distill the above chat messages into a single summary message. Include as many specific details as you can.\\\",\\n\",\n-    \"    ],\\n\",\n-    \"  ]);\\n\",\n-    \"  const summarizationChain = summarizationPrompt.pipe(llm);\\n\",\n-    \"  const summaryMessage = await summarizationChain.invoke({\\n\",\n-    \"    chat_history: storedMessages,\\n\",\n-    \"  });\\n\",\n-    \"  await demoEphemeralChatMessageHistory.clear();\\n\",\n-    \"  demoEphemeralChatMessageHistory.addMessage(summaryMessage);\\n\",\n-    \"  return true;\\n\",\n     \"};\\n\",\n     \"\\n\",\n-    \"const chainWithSummarization = RunnableSequence.from([\\n\",\n-    \"  RunnablePassthrough.assign({\\n\",\n-    \"    messages_summarized: summarizeMessages,\\n\",\n-    \"  }),\\n\",\n-    \"  chainWithMessageHistory3,\\n\",\n-    \"]);\"\n+    \"const workflow3 = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  // Define the node and edge\\n\",\n+    \"  .addNode(\\\"model\\\", callModel3)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"// Add simple in-memory checkpointer\\n\",\n+    \"const app3 = workflow3.compile({ checkpointer: new MemorySaver() });\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Let’s see if it remembers the name we gave it:\"\n+    \"Let's see if it remembers the name we gave it:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 23,\n+   \"execution_count\": 32,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m'You introduced yourself as \\\"Nemo.\\\"'\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m'You introduced yourself as \\\"Nemo.\\\"'\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    tokenUsage: { completionTokens: \\u001b[33m8\\u001b[39m, promptTokens: \\u001b[33m87\\u001b[39m, totalTokens: \\u001b[33m95\\u001b[39m },\\n\",\n-       \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 23,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxXjFDj6WRo7VLSneBtlAxUumPE\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Nemo greeted the assistant and asked how it was doing, to which the assistant responded that it was fine.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 22,\\n\",\n+      \"          \\\"promptTokens\\\": 60,\\n\",\n+      \"          \\\"totalTokens\\\": 82\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 60,\\n\",\n+      \"        \\\"output_tokens\\\": 22,\\n\",\n+      \"        \\\"total_tokens\\\": 82\\n\",\n+      \"      }\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"8b1309b7-c09e-47fb-9ab3-34047f6973e3\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What did I say my name was?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABSxYAQKiBsQ6oVypO4CLFDsi1HRH\\\",\\n\",\n+      \"      \\\"content\\\": \\\"You mentioned that your name is Nemo.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 73,\\n\",\n+      \"          \\\"totalTokens\\\": 81\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 73,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 81\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await chainWithSummarization.invoke(\\n\",\n+    \"await app3.invoke(\\n\",\n     \"  {\\n\",\n-    \"    input: \\\"What did I say my name was?\\\",\\n\",\n+    \"    messages: [\\n\",\n+    \"      ...demoEphemeralChatHistory2,\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"What did I say my name was?\\\" }\\n\",\n+    \"    ]\\n\",\n     \"  },\\n\",\n     \"  {\\n\",\n-    \"    configurable: { sessionId: \\\"unused\\\" },\\n\",\n+    \"    configurable: { thread_id: \\\"4\\\" }\\n\",\n     \"  }\\n\",\n     \");\"\n    ]\n   },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 24,\n-   \"metadata\": {},\n-   \"outputs\": [\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"[\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"The conversation consists of a greeting from someone named Nemo and a general inquiry about their we\\\"\\u001b[39m... 86 more characters,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"The conversation consists of a greeting from someone named Nemo and a general inquiry about their we\\\"\\u001b[39m... 86 more characters,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m34\\u001b[39m, promptTokens: \\u001b[33m62\\u001b[39m, totalTokens: \\u001b[33m96\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  },\\n\",\n-       \"  HumanMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m\\\"What did I say my name was?\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m\\\"What did I say my name was?\\\"\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: {},\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  AIMessage {\\n\",\n-       \"    lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"    lc_kwargs: {\\n\",\n-       \"      content: \\u001b[32m'You introduced yourself as \\\"Nemo.\\\"'\\u001b[39m,\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"    content: \\u001b[32m'You introduced yourself as \\\"Nemo.\\\"'\\u001b[39m,\\n\",\n-       \"    name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-       \"    response_metadata: {\\n\",\n-       \"      tokenUsage: { completionTokens: \\u001b[33m8\\u001b[39m, promptTokens: \\u001b[33m87\\u001b[39m, totalTokens: \\u001b[33m95\\u001b[39m },\\n\",\n-       \"      finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: []\\n\",\n-       \"  }\\n\",\n-       \"]\"\n-      ]\n-     },\n-     \"execution_count\": 24,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n-   \"source\": [\n-    \"await demoEphemeralChatMessageHistory.getMessages();\"\n-   ]\n-  },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\\n\",\n-    \"\\n\",\n-    \"## Next steps\\n\",\n-    \"\\n\",\n-    \"You've now learned how to manage memory in your chatbots\\n\",\n-    \"\\n\",\n-    \"Next, check out some of the other guides in this section, such as [how to add retrieval to your chatbot](/docs/how_to/chatbots_retrieval).\"\n+    \"Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\"\n    ]\n   }\n  ],\n@@ -1144,14 +767,17 @@\n    \"name\": \"deno\"\n   },\n   \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n    \"file_extension\": \".ts\",\n-   \"mimetype\": \"text/x.typescript\",\n+   \"mimetype\": \"text/typescript\",\n    \"name\": \"typescript\",\n-   \"nb_converter\": \"script\",\n-   \"pygments_lexer\": \"typescript\",\n-   \"version\": \"5.3.3\"\n+   \"version\": \"3.7.2\"\n   }\n  },\n  \"nbformat\": 4,\n- \"nbformat_minor\": 2\n+ \"nbformat_minor\": 4\n }",
          "docs/core_docs/docs/how_to/chatbots_retrieval.ipynb": "@@ -45,6 +45,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n     \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n     \"\\n\",\n     \"const llm = new ChatOpenAI({\\n\",",
          "docs/core_docs/docs/how_to/chatbots_tools.ipynb": "@@ -4,67 +4,107 @@\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# How to use tools\\n\",\n+    \"# How to add tools to chatbots\\n\",\n     \"\\n\",\n     \":::info Prerequisites\\n\",\n     \"\\n\",\n     \"This guide assumes familiarity with the following concepts:\\n\",\n     \"\\n\",\n     \"- [Chatbots](/docs/concepts/#messages)\\n\",\n-    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)\\n\",\n+    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/)\\n\",\n     \"- [Chat history](/docs/concepts/#chat-history)\\n\",\n     \"\\n\",\n     \":::\\n\",\n     \"\\n\",\n     \"This section will cover how to create conversational agents: chatbots that can interact with other systems and APIs using tools.\\n\",\n     \"\\n\",\n-    \"## Setup\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"This how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/chatbots_tools/).\\n\",\n+    \"\\n\",\n+    \"The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n     \"\\n\",\n-    \"For this guide, we’ll be using an [tool calling agent](/docs/how_to/agent_executor) with a single tool for searching the web. The default will be powered by [Tavily](/docs/integrations/tools/tavily_search), but you can switch it out for any similar tool. The rest of this section will assume you’re using Tavily.\\n\",\n+    \"## Setup\\n\",\n     \"\\n\",\n-    \"You’ll need to [sign up for an account on the Tavily website](https://tavily.com), and install the following packages:\\n\",\n+    \"For this guide, we'll be using a [tool calling agent](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#tool-calling-agent) with a single tool for searching the web. The default will be powered by [Tavily](/docs/integrations/tools/tavily_search), but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\\n\",\n     \"\\n\",\n+    \"You'll need to [sign up for an account](https://tavily.com/) on the Tavily website, and install the following packages:\\n\",\n     \"\\n\",\n     \"```{=mdx}\\n\",\n     \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\";\\n\",\n     \"\\n\",\n     \"<Npm2Yarn>\\n\",\n-    \"  @langchain/openai langchain @langchain/core\\n\",\n+    \"  @langchain/core @langchain/langgraph @langchain/community\\n\",\n     \"</Npm2Yarn>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"Let’s also set up a chat model that we’ll use for the below examples.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n+    \"\\n\",\n+    \"<ChatModelTabs customVarName=\\\"llm\\\" />\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"```typescript\\n\",\n+    \"process.env.TAVILY_API_KEY = \\\"YOUR_API_KEY\\\";\\n\",\n     \"```\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Creating an agent\\n\",\n+    \"\\n\",\n+    \"Our end goal is to create an agent that can respond conversationally to user questions while looking up information as needed.\\n\",\n+    \"\\n\",\n+    \"First, let's initialize Tavily and an OpenAI chat model capable of tool calling:\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"code\",\n    \"execution_count\": 1,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import { TavilySearchResults } from \\\"@langchain/community/tools/tavily_search\\\";\\n\",\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n     \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n     \"\\n\",\n+    \"const llm = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o\\\",\\n\",\n+    \"  temperature: 0,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { TavilySearchResults } from \\\"@langchain/community/tools/tavily_search\\\";\\n\",\n+    \"\\n\",\n     \"const tools = [\\n\",\n     \"  new TavilySearchResults({\\n\",\n     \"    maxResults: 1,\\n\",\n     \"  }),\\n\",\n-    \"];\\n\",\n-    \"\\n\",\n-    \"const llm = new ChatOpenAI({\\n\",\n-    \"  model: \\\"gpt-3.5-turbo-1106\\\",\\n\",\n-    \"  temperature: 0,\\n\",\n-    \"});\"\n+    \"];\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"To make our agent conversational, we must also choose a prompt with a placeholder for our chat history. Here’s an example:\\n\"\n+    \"To make our agent conversational, we can also specify a prompt. Here's an example:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 3,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -78,39 +118,28 @@\n     \"    \\\"system\\\",\\n\",\n     \"    \\\"You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\\\",\\n\",\n     \"  ],\\n\",\n-    \"  [\\\"placeholder\\\", \\\"{messages}\\\"],\\n\",\n-    \"  [\\\"placeholder\\\", \\\"{agent_scratchpad}\\\"],\\n\",\n     \"]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Great! Now let’s assemble our agent:\\n\",\n-    \"\\n\",\n-    \"```{=mdx}\\n\",\n-    \":::tip\\n\",\n-    \"As of `langchain` version `0.2.8`, the `createOpenAIToolsAgent` function now supports [OpenAI-formatted tools](https://api.js.langchain.com/interfaces/langchain_core.language_models_base.ToolDefinition.html).\\n\",\n-    \":::\\n\",\n-    \"```\\n\"\n+    \"Great! Now let's assemble our agent using LangGraph's prebuilt [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html), which allows you to create a [tool-calling agent](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#tool-calling-agent):\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 6,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import { AgentExecutor, createToolCallingAgent } from \\\"langchain/agents\\\";\\n\",\n-    \"\\n\",\n-    \"const agent = await createToolCallingAgent({\\n\",\n-    \"  llm,\\n\",\n-    \"  tools,\\n\",\n-    \"  prompt,\\n\",\n-    \"});\\n\",\n+    \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\"\\n\",\n     \"\\n\",\n-    \"const agentExecutor = new AgentExecutor({ agent, tools });\"\n+    \"// messageModifier allows you to preprocess the inputs to the model inside ReAct agent\\n\",\n+    \"// in this case, since we're passing a prompt string, we'll just always add a SystemMessage\\n\",\n+    \"// with this prompt string before any other messages sent to the model\\n\",\n+    \"const agent = createReactAgent({ llm, tools, messageModifier: prompt })\"\n    ]\n   },\n   {\n@@ -119,98 +148,108 @@\n    \"source\": [\n     \"## Running the agent\\n\",\n     \"\\n\",\n-    \"Now that we’ve set up our agent, let’s try interacting with it! It can handle both trivial queries that require no lookup:\\n\"\n+    \"Now that we've set up our agent, let's try interacting with it! It can handle both trivial queries that require no lookup:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 7,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  messages: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    }\\n\",\n-       \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Hi Nemo! It's great to meet you. How can I assist you today?\\\"\\u001b[39m\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 4,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"8c5fa465-e8d8-472a-9434-f574bf74537f\\\",\\n\",\n+      \"      \\\"content\\\": \\\"I'm Nemo!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKLLriRcZin65zLAMB3WUf9Sg1t\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_3537616b13\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 93,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 101\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n-    \"await agentExecutor.invoke({\\n\",\n-    \"  messages: [new HumanMessage(\\\"I'm Nemo!\\\")],\\n\",\n-    \"});\"\n+    \"await agent.invoke({ messages: [{ role: \\\"user\\\", content: \\\"I'm Nemo!\\\" }]})\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Or, it can use of the passed search tool to get up to date information if needed:\\n\"\n+    \"Or, it can use of the passed search tool to get up to date information if needed:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 8,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  messages: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"What is the current conservation status of the Great Barrier Reef?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"What is the current conservation status of the Great Barrier Reef?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    }\\n\",\n-       \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"The Great Barrier Reef has recorded its highest amount of coral cover since the Australian Institute\\\"\\u001b[39m... 688 more characters\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 5,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"65c315b6-2433-4cb1-97c7-b60b5546f518\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What is the current conservation status of the Great Barrier Reef?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKLQn1e4axRhqIhpKMyzWWTGauO\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_3537616b13\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 93,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 101\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await agentExecutor.invoke({\\n\",\n-    \"  messages: [\\n\",\n-    \"    new HumanMessage(\\n\",\n-    \"      \\\"What is the current conservation status of the Great Barrier Reef?\\\"\\n\",\n-    \"    ),\\n\",\n-    \"  ],\\n\",\n-    \"});\"\n+    \"await agent.invoke({ messages: [{ role: \\\"user\\\", content: \\\"What is the current conservation status of the Great Barrier Reef?\\\" }]})\"\n    ]\n   },\n   {\n@@ -219,246 +258,233 @@\n    \"source\": [\n     \"## Conversational responses\\n\",\n     \"\\n\",\n-    \"Because our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:\\n\"\n+    \"Because our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 9,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  messages: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Hello Nemo! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Hello Nemo! How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      usage_metadata: \\u001b[90mundefined\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    }\\n\",\n-       \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 6,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"6433afc5-31bd-44b3-b34c-f11647e1677d\\\",\\n\",\n+      \"      \\\"content\\\": \\\"I'm Nemo!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"f163b5f1-ea29-4d7a-9965-7c7c563d9cea\\\",\\n\",\n+      \"      \\\"content\\\": \\\"Hello Nemo! How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"382c3354-d02b-4888-98d8-44d75d045044\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What is my name?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKMKu7ThZDZW09yMIPTq2N723Cj\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 93,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 101\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"import { AIMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n-    \"await agentExecutor.invoke({\\n\",\n+    \"await agent.invoke({\\n\",\n     \"  messages: [\\n\",\n-    \"    new HumanMessage(\\\"I'm Nemo!\\\"),\\n\",\n-    \"    new AIMessage(\\\"Hello Nemo! How can I assist you today?\\\"),\\n\",\n-    \"    new HumanMessage(\\\"What is my name?\\\"),\\n\",\n-    \"  ],\\n\",\n-    \"});\"\n+    \"    { role: \\\"user\\\", content: \\\"I'm Nemo!\\\" },\\n\",\n+    \"    { role: \\\"user\\\", content: \\\"Hello Nemo! How can I assist you today?\\\" },\\n\",\n+    \"    { role: \\\"user\\\", content: \\\"What is my name?\\\" }\\n\",\n+    \"  ]\\n\",\n+    \"})\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"If preferred, you can also wrap the agent executor in a [`RunnableWithMessageHistory`](/docs/how_to/message_history/) class to internally manage history messages. Let's redeclare it this way:\"\n+    \"If preferred, you can also add memory to the LangGraph agent to manage the history of messages. Let's redeclare it this way:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 12,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"const agent2 = await createToolCallingAgent({\\n\",\n-    \"  llm,\\n\",\n-    \"  tools,\\n\",\n-    \"  prompt,\\n\",\n-    \"});\\n\",\n+    \"import { MemorySaver } from \\\"@langchain/langgraph\\\"\\n\",\n     \"\\n\",\n-    \"const agentExecutor2 = new AgentExecutor({ agent: agent2, tools });\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"Then, because our agent executor has multiple outputs, we also have to set the `outputMessagesKey` property when initializing the wrapper:\\n\"\n+    \"// highlight-start\\n\",\n+    \"const memory = new MemorySaver()\\n\",\n+    \"const agent2 = createReactAgent({ llm, tools, messageModifier: prompt, checkpointSaver: memory })\\n\",\n+    \"// highlight-end\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 13,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  messages: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    }\\n\",\n-       \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Hi Nemo! It's great to meet you. How can I assist you today?\\\"\\u001b[39m\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 11,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"a4a4f663-8192-4179-afcc-88d9d186aa80\\\",\\n\",\n+      \"      \\\"content\\\": \\\"I'm Nemo!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKi4tBzOWMh3hgA46xXo7bJzb8r\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 93,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 101\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"import { ChatMessageHistory } from \\\"langchain/stores/message/in_memory\\\";\\n\",\n-    \"import { RunnableWithMessageHistory } from \\\"@langchain/core/runnables\\\";\\n\",\n-    \"\\n\",\n-    \"const demoEphemeralChatMessageHistory = new ChatMessageHistory();\\n\",\n-    \"\\n\",\n-    \"const conversationalAgentExecutor = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: agentExecutor2,\\n\",\n-    \"  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistory,\\n\",\n-    \"  inputMessagesKey: \\\"messages\\\",\\n\",\n-    \"  outputMessagesKey: \\\"output\\\",\\n\",\n-    \"});\\n\",\n-    \"\\n\",\n-    \"await conversationalAgentExecutor.invoke(\\n\",\n-    \"  { messages: [new HumanMessage(\\\"I'm Nemo!\\\")] },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n-    \");\"\n+    \"await agent2.invoke({ messages: [{ role: \\\"user\\\", content: \\\"I'm Nemo!\\\" }]}, { configurable: { thread_id: \\\"1\\\" } })\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"And then if we rerun our wrapped agent executor:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 14,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  messages: [\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"I'm Nemo!\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    },\\n\",\n-       \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Hi Nemo! It's great to meet you. How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Hi Nemo! It's great to meet you. How can I assist you today?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: [],\\n\",\n-       \"      usage_metadata: \\u001b[90mundefined\\u001b[39m\\n\",\n-       \"    },\\n\",\n-       \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"What is my name?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n-       \"    }\\n\",\n-       \"  ],\\n\",\n-       \"  output: \\u001b[32m\\\"Your name is Nemo!\\\"\\u001b[39m\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 12,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  messages: [\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"c5fd303c-eb49-41a0-868e-bc8c5aa02cf6\\\",\\n\",\n+      \"      \\\"content\\\": \\\"I'm Nemo!\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKi4tBzOWMh3hgA46xXo7bJzb8r\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": []\\n\",\n+      \"    },\\n\",\n+      \"    HumanMessage {\\n\",\n+      \"      \\\"id\\\": \\\"635b17b9-2ec7-412f-bf45-85d0e9944430\\\",\\n\",\n+      \"      \\\"content\\\": \\\"What is my name?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n+      \"    },\\n\",\n+      \"    AIMessage {\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-ABTKjBbmFlPb5t37aJ8p4NtoHb8YG\\\",\\n\",\n+      \"      \\\"content\\\": \\\"How can I assist you today?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 8,\\n\",\n+      \"          \\\"promptTokens\\\": 93,\\n\",\n+      \"          \\\"totalTokens\\\": 101\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"      },\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 93,\\n\",\n+      \"        \\\"output_tokens\\\": 8,\\n\",\n+      \"        \\\"total_tokens\\\": 101\\n\",\n+      \"      }\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await conversationalAgentExecutor.invoke(\\n\",\n-    \"  { messages: [new HumanMessage(\\\"What is my name?\\\")] },\\n\",\n-    \"  { configurable: { sessionId: \\\"unused\\\" } }\\n\",\n-    \");\"\n+    \"await agent2.invoke({ messages: [{ role: \\\"user\\\", content: \\\"What is my name?\\\" }]}, { configurable: { thread_id: \\\"1\\\" } })\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"## Next steps\\n\",\n+    \"This [LangSmith trace](https://smith.langchain.com/public/16cbcfa5-5ef1-4d4c-92c9-538a6e71f23d/r) shows what's going on under the hood.\\n\",\n+    \"\\n\",\n+    \"## Further reading\\n\",\n+    \"\\n\",\n+    \"For more on how to build agents, check these [LangGraph](https://langchain-ai.github.io/langgraphjs/) guides:\\n\",\n     \"\\n\",\n-    \"You've now learned how to create chatbots with tool-use capabilities.\\n\",\n+    \"* [agents conceptual guide](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/)\\n\",\n+    \"* [agents tutorials](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/multi_agent_collaboration/)\\n\",\n+    \"* [createReactAgent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/)\\n\",\n     \"\\n\",\n-    \"For more, check out the other guides in this section, including [how to add history to your chatbots](/docs/how_to/chatbots_memory).\"\n+    \"For more on tool usage, you can also check out [this use case section](/docs/how_to#tools).\"\n    ]\n   }\n  ],\n@@ -469,14 +495,17 @@\n    \"name\": \"deno\"\n   },\n   \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n    \"file_extension\": \".ts\",\n-   \"mimetype\": \"text/x.typescript\",\n+   \"mimetype\": \"text/typescript\",\n    \"name\": \"typescript\",\n-   \"nb_converter\": \"script\",\n-   \"pygments_lexer\": \"typescript\",\n-   \"version\": \"5.3.3\"\n+   \"version\": \"3.7.2\"\n   }\n  },\n  \"nbformat\": 4,\n- \"nbformat_minor\": 2\n+ \"nbformat_minor\": 4\n }",
          "docs/core_docs/docs/how_to/graph_prompting.ipynb": "@@ -6,7 +6,20 @@\n       \"source\": [\n         \"# How to improve results with prompting\\n\",\n         \"\\n\",\n-        \"In this guide we’ll go over prompting strategies to improve graph database query generation. We’ll largely focus on methods for getting relevant database-specific information in your prompt.\"\n+        \"In this guide we’ll go over prompting strategies to improve graph database query generation. We’ll largely focus on methods for getting relevant database-specific information in your prompt.\\n\",\n+        \"\\n\",\n+        \"```{=mdx}\\n\",\n+        \":::warning\\n\",\n+        \"\\n\",\n+        \"The `GraphCypherQAChain` used in this guide will execute Cypher statements against the provided database.\\n\",\n+        \"For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.\\n\",\n+        \"\\n\",\n+        \"Failure to do so may result in data corruption or loss, since the calling code\\n\",\n+        \"may attempt commands that would result in deletion, mutation of data\\n\",\n+        \"if appropriately prompted or reading sensitive data if such data is present in the database.\\n\",\n+        \"\\n\",\n+        \":::\\n\",\n+        \"```\"\n       ]\n     },\n     {",
          "docs/core_docs/docs/how_to/graph_semantic.ipynb": "@@ -12,7 +12,20 @@\n         \"While that option provides excellent flexibility, the solution could be brittle and not consistently generating precise Cypher statements.\\n\",\n         \"Instead of generating Cypher statements, we can implement Cypher templates as tools in a semantic layer that an LLM agent can interact with.\\n\",\n         \"\\n\",\n-        \"![graph_semantic.png](../../static/img/graph_semantic.png)\"\n+        \"![graph_semantic.png](../../static/img/graph_semantic.png)\\n\",\n+        \"\\n\",\n+        \"```{=mdx}\\n\",\n+        \":::warning\\n\",\n+        \"\\n\",\n+        \"The code in this guide will execute Cypher statements against the provided database.\\n\",\n+        \"For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.\\n\",\n+        \"\\n\",\n+        \"Failure to do so may result in data corruption or loss, since the calling code\\n\",\n+        \"may attempt commands that would result in deletion, mutation of data\\n\",\n+        \"if appropriately prompted or reading sensitive data if such data is present in the database.\\n\",\n+        \"\\n\",\n+        \":::\\n\",\n+        \"```\"\n       ]\n     },\n     {",
          "docs/core_docs/docs/how_to/installation.mdx": "@@ -270,13 +270,13 @@ You will have to make `fetch` available globally, either:\n You'll also need to [polyfill `ReadableStream`](https://www.npmjs.com/package/web-streams-polyfill) by installing:\n \n ```bash npm2yarn\n-npm i web-streams-polyfill\n+npm i web-streams-polyfill@4\n ```\n \n And then adding it to the global namespace in your main entrypoint:\n \n ```typescript\n-import \"web-streams-polyfill/es6\";\n+import \"web-streams-polyfill/polyfill\";\n ```\n \n Additionally you'll have to polyfill `structuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).",
          "docs/core_docs/docs/how_to/message_history.ipynb": "@@ -0,0 +1,586 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"raw\",\n+   \"id\": \"8165bd4c\",\n+   \"metadata\": {\n+    \"vscode\": {\n+     \"languageId\": \"raw\"\n+    }\n+   },\n+   \"source\": [\n+    \"---\\n\",\n+    \"keywords: [memory]\\n\",\n+    \"---\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"f47033eb\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# How to add message history\\n\",\n+    \"\\n\",\n+    \":::info Prerequisites\\n\",\n+    \"\\n\",\n+    \"This guide assumes familiarity with the following concepts:\\n\",\n+    \"\\n\",\n+    \"- [Chaining runnables](/docs/how_to/sequence/)\\n\",\n+    \"- [Prompt templates](/docs/concepts/#prompt-templates)\\n\",\n+    \"- [Chat Messages](/docs/concepts/#message-types)\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"This guide previously covered the [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) abstraction. You can access this version of the guide in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/message_history/).\\n\",\n+    \"\\n\",\n+    \"The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"Passing conversation state into and out a chain is vital when building a chatbot. LangGraph implements a built-in persistence layer, allowing chain states to be automatically persisted in memory, or external backends such as SQLite, Postgres or Redis. Details can be found in the LangGraph persistence documentation.\\n\",\n+    \"\\n\",\n+    \"In this guide we demonstrate how to add persistence to arbitrary LangChain runnables by wrapping them in a minimal LangGraph application. This lets us persist the message history and other elements of the chain's state, simplifying the development of multi-turn applications. It also supports multiple threads, enabling a single application to interact separately with multiple users.\\n\",\n+    \"\\n\",\n+    \"## Setup\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\";\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/core @langchain/langgraph\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"Let’s also set up a chat model that we’ll use for the below examples.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n+    \"\\n\",\n+    \"<ChatModelTabs customVarName=\\\"llm\\\" />\\n\",\n+    \"```\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 30,\n+   \"id\": \"8a4e4708\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const llm = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o\\\",\\n\",\n+    \"  temperature: 0,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"1f6121bc-2080-4ccc-acf0-f77de4bc951d\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Example: message inputs\\n\",\n+    \"\\n\",\n+    \"Adding memory to a [chat model](/docs/concepts/#chat-models) provides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in `MessagesState` that we can use for this purpose.\\n\",\n+    \"\\n\",\n+    \"Below, we:\\n\",\n+    \"1. Define the graph state to be a list of messages;\\n\",\n+    \"2. Add a single node to the graph that calls a chat model;\\n\",\n+    \"3. Compile the graph with an in-memory checkpointer to store messages between runs.\\n\",\n+    \"\\n\",\n+    \":::info\\n\",\n+    \"\\n\",\n+    \"The output of a LangGraph application is its [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/).\\n\",\n+    \"\\n\",\n+    \":::\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 31,\n+   \"id\": \"f691a73a-a866-4354-9fff-8315605e2b8f\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  const response = await llm.invoke(state.messages);\\n\",\n+    \"  // Update message history with response:\\n\",\n+    \"  return { messages: response };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"// Define a new graph\\n\",\n+    \"const workflow = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  // Define the (single) node in the graph\\n\",\n+    \"  .addNode(\\\"model\\\", callModel)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"// Add memory\\n\",\n+    \"const memory = new MemorySaver();\\n\",\n+    \"const app = workflow.compile({ checkpointer: memory });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"c0b396a8-f81e-4139-b4b2-75adf61d8179\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"When we run the application, we pass in a configuration object that specifies a `thread_id`. This ID is used to distinguish conversational threads (e.g., between different users).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 32,\n+   \"id\": \"e4309511-2140-4d91-8f5f-ea3661e6d179\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { v4 as uuidv4 } from \\\"uuid\\\";\\n\",\n+    \"\\n\",\n+    \"const config = { configurable: { thread_id: uuidv4() } }\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"108c45a2-4971-4120-ba64-9a4305a414bb\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"We can then invoke the application:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 33,\n+   \"id\": \"72a5ff6c-501f-4151-8dd9-f600f70554be\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABTqCeKnMQmG9IH8dNF5vPjsgXtcM\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Hi Bob! How can I assist you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 10,\\n\",\n+      \"      \\\"promptTokens\\\": 12,\\n\",\n+      \"      \\\"totalTokens\\\": 22\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 12,\\n\",\n+      \"    \\\"output_tokens\\\": 10,\\n\",\n+      \"    \\\"total_tokens\\\": 22\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const input = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"Hi! I'm Bob.\\\",\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"const output = await app.invoke({ messages: input }, config)\\n\",\n+    \"// The output contains all messages in the state.\\n\",\n+    \"// This will long the last message in the conversation.\\n\",\n+    \"console.log(output.messages[output.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 34,\n+   \"id\": \"5931fb35-0fac-40e7-8ac6-b14cb4e926cd\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABTqD5jrJXeKCpvoIDp47fvgw2OPn\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Your name is Bob. How can I help you today, Bob?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 14,\\n\",\n+      \"      \\\"promptTokens\\\": 34,\\n\",\n+      \"      \\\"totalTokens\\\": 48\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 34,\\n\",\n+      \"    \\\"output_tokens\\\": 14,\\n\",\n+      \"    \\\"total_tokens\\\": 48\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const input2 = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"What's my name?\\\",\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"const output2 = await app.invoke({ messages: input2 }, config)\\n\",\n+    \"console.log(output2.messages[output2.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"91de6d12-881d-4d23-a421-f2e3bf829b79\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Note that states are separated for different threads. If we issue the same query to a thread with a new `thread_id`, the model indicates that it does not know the answer:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 35,\n+   \"id\": \"6f12c26f-8913-4484-b2c5-b49eda2e6d7d\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABTqDkctxwmXjeGOZpK6Km8jdCqdl\\\",\\n\",\n+      \"  \\\"content\\\": \\\"I'm sorry, but I don't have access to personal information about users. How can I assist you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 21,\\n\",\n+      \"      \\\"promptTokens\\\": 11,\\n\",\n+      \"      \\\"totalTokens\\\": 32\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 11,\\n\",\n+      \"    \\\"output_tokens\\\": 21,\\n\",\n+      \"    \\\"total_tokens\\\": 32\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const config2 = { configurable: { thread_id: uuidv4() } }\\n\",\n+    \"const input3 = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"What's my name?\\\",\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"const output3 = await app.invoke({ messages: input3 }, config2)\\n\",\n+    \"console.log(output3.messages[output3.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"6749ea95-3382-4843-bb96-cfececb9e4e5\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Example: object inputs\\n\",\n+    \"\\n\",\n+    \"LangChain runnables often accept multiple inputs via separate keys in a single object argument. A common example is a prompt template with multiple parameters.\\n\",\n+    \"\\n\",\n+    \"Whereas before our runnable was a chat model, here we chain together a prompt template and chat model.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 36,\n+   \"id\": \"6e7a402a-0994-4fc5-a607-fb990a248aa4\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"\\n\",\n+    \"const prompt = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"Answer in {language}.\\\"],\\n\",\n+    \"  new MessagesPlaceholder(\\\"messages\\\"),\\n\",\n+    \"])\\n\",\n+    \"\\n\",\n+    \"const runnable = prompt.pipe(llm);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"f83107bd-ae61-45e1-a57e-94ab043aad4b\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"For this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.\\n\",\n+    \"\\n\",\n+    \"Note that in the below state:\\n\",\n+    \"- Updates to the `messages` list will append messages;\\n\",\n+    \"- Updates to the `language` string will overwrite the string.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 37,\n+   \"id\": \"267429ea-be0f-4f80-8daf-c63d881a1436\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"\\n\",\n+    \"// Define the State\\n\",\n+    \"// highlight-next-line\\n\",\n+    \"const GraphAnnotation = Annotation.Root({\\n\",\n+    \"  // highlight-next-line\\n\",\n+    \"  language: Annotation<string>(),\\n\",\n+    \"  // Spread `MessagesAnnotation` into the state to add the `messages` field.\\n\",\n+    \"  // highlight-next-line\\n\",\n+    \"  ...MessagesAnnotation.spec,\\n\",\n+    \"})\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel2 = async (state: typeof GraphAnnotation.State) => {\\n\",\n+    \"  const response = await runnable.invoke(state);\\n\",\n+    \"  // Update message history with response:\\n\",\n+    \"  return { messages: [response] };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"const workflow2 = new StateGraph(GraphAnnotation)\\n\",\n+    \"  .addNode(\\\"model\\\", callModel2)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"const app2 = workflow2.compile({ checkpointer: new MemorySaver() });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 38,\n+   \"id\": \"f3844fb4-58d7-43c8-b427-6d9f64d7411b\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 19,\\n\",\n+      \"      \\\"promptTokens\\\": 19,\\n\",\n+      \"      \\\"totalTokens\\\": 38\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 19,\\n\",\n+      \"    \\\"output_tokens\\\": 19,\\n\",\n+      \"    \\\"total_tokens\\\": 38\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const config3 = { configurable: { thread_id: uuidv4() } }\\n\",\n+    \"const input4 = {\\n\",\n+    \"  messages: [\\n\",\n+    \"    {\\n\",\n+    \"      role: \\\"user\\\",\\n\",\n+    \"      content: \\\"What's my name?\\\",\\n\",\n+    \"    }\\n\",\n+    \"  ],\\n\",\n+    \"  language: \\\"Spanish\\\",\\n\",\n+    \"} \\n\",\n+    \"const output4 = await app2.invoke(input4, config3)\\n\",\n+    \"console.log(output4.messages[output4.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"7df47824-ef18-4a6e-a416-345ec9203f88\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Managing message history\\n\",\n+    \"\\n\",\n+    \"The message history (and other elements of the application state) can be accessed via `.getState`:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 39,\n+   \"id\": \"1cbd6d82-43c1-4d11-98af-5c3ad9cd9b3b\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Language: Spanish\\n\",\n+      \"[\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"What's my name?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"id\\\": \\\"chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U\\\",\\n\",\n+      \"    \\\"content\\\": \\\"Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {\\n\",\n+      \"      \\\"tokenUsage\\\": {\\n\",\n+      \"        \\\"completionTokens\\\": 19,\\n\",\n+      \"        \\\"promptTokens\\\": 19,\\n\",\n+      \"        \\\"totalTokens\\\": 38\\n\",\n+      \"      },\\n\",\n+      \"      \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"      \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  }\\n\",\n+      \"]\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const state = (await app2.getState(config3)).values\\n\",\n+    \"\\n\",\n+    \"console.log(`Language: ${state.language}`);\\n\",\n+    \"console.log(state.messages)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"acfbccda-0bd6-4c4d-ae6e-8118520314e1\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"We can also update the state via `.updateState`. For example, we can manually append a new message:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 40,\n+   \"id\": \"e98310d7-8ab1-461d-94a7-dd419494ab8d\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"const _ = await app2.updateState(config3, { messages: [{ role: \\\"user\\\", content: \\\"test\\\" }]})\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 41,\n+   \"id\": \"74ab3691-6f3b-49c5-aad0-2a90fc2a1e6a\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Language: Spanish\\n\",\n+      \"[\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"What's my name?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"id\\\": \\\"chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U\\\",\\n\",\n+      \"    \\\"content\\\": \\\"Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {\\n\",\n+      \"      \\\"tokenUsage\\\": {\\n\",\n+      \"        \\\"completionTokens\\\": 19,\\n\",\n+      \"        \\\"promptTokens\\\": 19,\\n\",\n+      \"        \\\"totalTokens\\\": 38\\n\",\n+      \"      },\\n\",\n+      \"      \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"      \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  },\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"test\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  }\\n\",\n+      \"]\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const state2 = (await app2.getState(config3)).values\\n\",\n+    \"\\n\",\n+    \"console.log(`Language: ${state2.language}`);\\n\",\n+    \"console.log(state2.messages)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"e4a1ea00-d7ff-4f18-b9ec-9aec5909d027\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"For details on managing state, including deleting messages, see the LangGraph documentation:\\n\",\n+    \"\\n\",\n+    \"- [How to delete messages](https://langchain-ai.github.io/langgraphjs/how-tos/delete-messages/)\\n\",\n+    \"- [How to view and update past graph state](https://langchain-ai.github.io/langgraphjs/how-tos/time-travel/)\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/how_to/message_history.mdx": "@@ -1,206 +0,0 @@\n-# How to add message history\n-\n-:::info Prerequisites\n-\n-This guide assumes familiarity with the following concepts:\n-\n-- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n-- [Chaining runnables](/docs/how_to/sequence/)\n-- [Configuring chain parameters at runtime](/docs/how_to/binding)\n-- [Prompt templates](/docs/concepts/#prompt-templates)\n-- [Chat Messages](/docs/concepts/#message-types)\n-\n-:::\n-\n-The `RunnableWithMessageHistory` lets us add message history to certain types of chains.\n-\n-Specifically, it can be used for any Runnable that takes as input one of\n-\n-- a sequence of [`BaseMessages`](/docs/concepts/#message-types)\n-- a dict with a key that takes a sequence of `BaseMessage`\n-- a dict with a key that takes the latest message(s) as a string or sequence of `BaseMessage`, and a separate key that takes historical messages\n-\n-And returns as output one of\n-\n-- a string that can be treated as the contents of an `AIMessage`\n-- a sequence of `BaseMessage`\n-- a dict with a key that contains a sequence of `BaseMessage`\n-\n-Let's take a look at some examples to see how it works.\n-\n-## Setup\n-\n-We'll use Upstash to store our chat message histories and Anthropic's claude-2 model so we'll need to install the following dependencies:\n-\n-```bash npm2yarn\n-npm install @langchain/anthropic @langchain/community @langchain/core @upstash/redis\n-```\n-\n-You'll need to set environment variables for `ANTHROPIC_API_KEY` and grab your Upstash REST url and secret token.\n-\n-### [LangSmith](https://smith.langchain.com/)\n-\n-LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.\n-\n-Note that LangSmith is not needed, but it is helpful.\n-If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:\n-\n-```bash\n-export LANGCHAIN_TRACING_V2=\"true\"\n-export LANGCHAIN_API_KEY=\"<your-api-key>\"\n-\n-# Reduce tracing latency if you are not in a serverless environment\n-# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n-```\n-\n-Let's create a simple runnable that takes a dict as input and returns a `BaseMessage`.\n-\n-In this case the `\"question\"` key in the input represents our input message, and the `\"history\"` key is where our historical messages will be injected.\n-\n-```typescript\n-import {\n-  ChatPromptTemplate,\n-  MessagesPlaceholder,\n-} from \"@langchain/core/prompts\";\n-import { ChatAnthropic } from \"@langchain/anthropic\";\n-import { UpstashRedisChatMessageHistory } from \"@langchain/community/stores/message/upstash_redis\";\n-// For demos, you can also use an in-memory store:\n-// import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n-\n-const prompt = ChatPromptTemplate.fromMessages([\n-  [\"system\", \"You're an assistant who's good at {ability}\"],\n-  new MessagesPlaceholder(\"history\"),\n-  [\"human\", \"{question}\"],\n-]);\n-\n-const chain = prompt.pipe(\n-  new ChatAnthropic({ model: \"claude-3-sonnet-20240229\" })\n-);\n-```\n-\n-### Adding message history\n-\n-To add message history to our original chain we wrap it in the `RunnableWithMessageHistory` class.\n-\n-Crucially, we also need to define a `getMessageHistory()` method that takes a `sessionId` string and based on it returns a `BaseChatMessageHistory`. Given the same input, this method should return an equivalent output.\n-\n-In this case, we'll also want to specify `inputMessagesKey` (the key to be treated as the latest input message) and `historyMessagesKey` (the key to add historical messages to).\n-\n-```typescript\n-import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n-\n-const chainWithHistory = new RunnableWithMessageHistory({\n-  runnable: chain,\n-  getMessageHistory: (sessionId) =>\n-    new UpstashRedisChatMessageHistory({\n-      sessionId,\n-      config: {\n-        url: process.env.UPSTASH_REDIS_REST_URL!,\n-        token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n-      },\n-    }),\n-  inputMessagesKey: \"question\",\n-  historyMessagesKey: \"history\",\n-});\n-```\n-\n-## Invoking with config\n-\n-Whenever we call our chain with message history, we need to include an additional config object that contains the `session_id`\n-\n-```typescript\n-{\n-  configurable: {\n-    sessionId: \"<SESSION_ID>\";\n-  }\n-}\n-```\n-\n-Given the same configuration, our chain should be pulling from the same chat message history.\n-\n-```typescript\n-const result = await chainWithHistory.invoke(\n-  {\n-    ability: \"math\",\n-    question: \"What does cosine mean?\",\n-  },\n-  {\n-    configurable: {\n-      sessionId: \"foobarbaz\",\n-    },\n-  }\n-);\n-\n-console.log(result);\n-\n-/*\n-  AIMessage {\n-    content: 'Cosine refers to one of the basic trigonometric functions. Specifically:\\n' +\n-      '\\n' +\n-      '- Cosine is one of the three main trigonometric functions, along with sine and tangent. It is often abbreviated as cos.\\n' +\n-      '\\n' +\n-      '- For a right triangle with sides a, b, and c (where c is the hypotenuse), cosine represents the ratio of the length of the adjacent side (a) to the length of the hypotenuse (c). So cos(A) = a/c, where A is the angle opposite side a.\\n' +\n-      '\\n' +\n-      '- On the Cartesian plane, cosine represents the x-coordinate of a point on the unit circle for a given angle. So if you take an angle θ on the unit circle, the cosine of θ gives you the x-coordinate of where the terminal side of that angle intersects the circle.\\n' +\n-      '\\n' +\n-      '- The cosine function has a periodic waveform that oscillates between 1 and -1. Its graph forms a cosine wave.\\n' +\n-      '\\n' +\n-      'So in essence, cosine helps relate an angle in a right triangle to the ratio of two of its sides. Along with sine and tangent, it is foundational to trigonometry and mathematical modeling of periodic functions.',\n-    name: undefined,\n-    additional_kwargs: {\n-      id: 'msg_01QnnAkKEz7WvhJrwLWGbLBm',\n-      type: 'message',\n-      role: 'assistant',\n-      model: 'claude-3-sonnet-20240229',\n-      stop_reason: 'end_turn',\n-      stop_sequence: null\n-    }\n-  }\n-*/\n-\n-const result2 = await chainWithHistory.invoke(\n-  {\n-    ability: \"math\",\n-    question: \"What's its inverse?\",\n-  },\n-  {\n-    configurable: {\n-      sessionId: \"foobarbaz\",\n-    },\n-  }\n-);\n-\n-console.log(result2);\n-\n-/*\n-  AIMessage {\n-    content: 'The inverse of the cosine function is the arcsine or inverse sine function, often written as sin−1(x) or sin^{-1}(x).\\n' +\n-      '\\n' +\n-      'Some key properties of the inverse cosine function:\\n' +\n-      '\\n' +\n-      '- It accepts values between -1 and 1 as inputs and returns angles from 0 to π radians (0 to 180 degrees). This is the inverse of the regular cosine function, which takes angles and returns the cosine ratio.\\n' +\n-      '\\n' +\n-      '- It is also called cos−1(x) or cos^{-1}(x) (read as \"cosine inverse of x\").\\n' +\n-      '\\n' +\n-      '- The notation sin−1(x) is usually preferred over cos−1(x) since it relates more directly to the unit circle definition of cosine. sin−1(x) gives the angle whose sine equals x.\\n' +\n-      '\\n' +\n-      '- The arcsine function is one-to-one on the domain [-1, 1]. This means every output angle maps back to exactly one input ratio x. This one-to-one mapping is what makes it the mathematical inverse of cosine.\\n' +\n-      '\\n' +\n-      'So in summary, arcsine or inverse sine, written as sin−1(x) or sin^{-1}(x), gives you the angle whose cosine evaluates to the input x, undoing the cosine function. It is used throughout trigonometry and calculus.',\n-    additional_kwargs: {\n-      id: 'msg_01PYRhpoUudApdJvxug6R13W',\n-      type: 'message',\n-      role: 'assistant',\n-      model: 'claude-3-sonnet-20240229',\n-      stop_reason: 'end_turn',\n-      stop_sequence: null\n-    }\n-  }\n-*/\n-```\n-\n-:::tip\n-[Langsmith trace](https://smith.langchain.com/public/50377a89-d0b8-413b-8cd7-8e6618835e00/r)\n-:::\n-\n-Looking at the Langsmith trace for the second call, we can see that when constructing the prompt, a \"history\" variable has been injected which is a list of two messages (our first input and first output).",
          "docs/core_docs/docs/how_to/migrate_agent.ipynb": "@@ -42,7 +42,7 @@\n     \"\\n\",\n     \"#### Prerequisites\\n\",\n     \"\\n\",\n-    \"This how-to guide uses Anthropic's `\\\"claude-3-haiku-20240307\\\"` as the LLM. If you are running this guide as a notebook, set your Anthropic API key to run.\"\n+    \"This how-to guide uses OpenAI's `\\\"gpt-4o-mini\\\"` as the LLM. If you are running this guide as a notebook, set your OpenAI API key as shown below:\"\n    ]\n   },\n   {\n@@ -54,7 +54,7 @@\n    },\n    \"outputs\": [],\n    \"source\": [\n-    \"// process.env.ANTHROPIC_API_KEY = \\\"sk-...\\\";\\n\",\n+    \"// process.env.OPENAI_API_KEY = \\\"...\\\";\\n\",\n     \"\\n\",\n     \"// Optional, add tracing in LangSmith\\n\",\n     \"// process.env.LANGCHAIN_API_KEY = \\\"ls...\\\";\\n\",\n@@ -86,7 +86,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 1,\n+   \"execution_count\": 2,\n    \"id\": \"1222c5e2\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -95,11 +95,10 @@\n    \"source\": [\n     \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n     \"import { z } from \\\"zod\\\";\\n\",\n-    \"import { ChatAnthropic } from \\\"@langchain/anthropic\\\";\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n     \"\\n\",\n-    \"const llm = new ChatAnthropic({\\n\",\n-    \"  model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-    \"  temperature: 0,\\n\",\n+    \"const llm = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-mini\\\",\\n\",\n     \"});\\n\",\n     \"\\n\",\n     \"const magicTool = tool(async ({ input }: { input: number }) => {\\n\",\n@@ -130,7 +129,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 3,\n    \"id\": \"e52bf891\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -141,11 +140,11 @@\n       \"text/plain\": [\n        \"{\\n\",\n        \"  input: \\u001b[32m\\\"what is the value of magic_function(3)?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m\\\"The value of magic_function(3) is 5.\\\"\\u001b[39m\\n\",\n+       \"  output: \\u001b[32m\\\"The value of `magic_function(3)` is 5.\\\"\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 2,\n+     \"execution_count\": 3,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -164,8 +163,15 @@\n     \"  [\\\"placeholder\\\", \\\"{agent_scratchpad}\\\"],\\n\",\n     \"]);\\n\",\n     \"\\n\",\n-    \"const agent = createToolCallingAgent({ llm, tools, prompt });\\n\",\n-    \"const agentExecutor = new AgentExecutor({ agent, tools });\\n\",\n+    \"const agent = createToolCallingAgent({\\n\",\n+    \"  llm,\\n\",\n+    \"  tools,\\n\",\n+    \"  prompt\\n\",\n+    \"});\\n\",\n+    \"const agentExecutor = new AgentExecutor({\\n\",\n+    \"  agent,\\n\",\n+    \"  tools,\\n\",\n+    \"});\\n\",\n     \"\\n\",\n     \"await agentExecutor.invoke({ input: query });\"\n    ]\n@@ -185,7 +191,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 4,\n    \"id\": \"dcda7082\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -196,124 +202,77 @@\n       \"{\\n\",\n       \"  messages: [\\n\",\n       \"    HumanMessage {\\n\",\n-      \"      lc_serializable: true,\\n\",\n-      \"      lc_kwargs: {\\n\",\n-      \"        content: \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"        additional_kwargs: {},\\n\",\n-      \"        response_metadata: {}\\n\",\n-      \"      },\\n\",\n-      \"      lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"      content: \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"      name: undefined,\\n\",\n-      \"      additional_kwargs: {},\\n\",\n-      \"      response_metadata: {}\\n\",\n+      \"      \\\"id\\\": \\\"eeef343c-80d1-4ccb-86af-c109343689cd\\\",\\n\",\n+      \"      \\\"content\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {}\\n\",\n       \"    },\\n\",\n       \"    AIMessage {\\n\",\n-      \"      lc_serializable: true,\\n\",\n-      \"      lc_kwargs: {\\n\",\n-      \"        content: [ [Object] ],\\n\",\n-      \"        additional_kwargs: {\\n\",\n-      \"          id: \\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\",\\n\",\n-      \"          type: \\\"message\\\",\\n\",\n-      \"          role: \\\"assistant\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"tool_use\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n-      \"        },\\n\",\n-      \"        tool_calls: [ [Object] ],\\n\",\n-      \"        invalid_tool_calls: [],\\n\",\n-      \"        response_metadata: {}\\n\",\n-      \"      },\\n\",\n-      \"      lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"      content: [\\n\",\n-      \"        {\\n\",\n-      \"          type: \\\"tool_use\\\",\\n\",\n-      \"          id: \\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\",\\n\",\n-      \"          name: \\\"magic_function\\\",\\n\",\n-      \"          input: [Object]\\n\",\n-      \"        }\\n\",\n-      \"      ],\\n\",\n-      \"      name: undefined,\\n\",\n-      \"      additional_kwargs: {\\n\",\n-      \"        id: \\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\",\\n\",\n-      \"        type: \\\"message\\\",\\n\",\n-      \"        role: \\\"assistant\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"tool_use\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 365, output_tokens: 53 }\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-A7exs2uRqEipaZ7MtRbXnqu0vT0Da\\\",\\n\",\n+      \"      \\\"content\\\": \\\"\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {\\n\",\n+      \"        \\\"tool_calls\\\": [\\n\",\n+      \"          {\\n\",\n+      \"            \\\"id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\",\\n\",\n+      \"            \\\"type\\\": \\\"function\\\",\\n\",\n+      \"            \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"          }\\n\",\n+      \"        ]\\n\",\n       \"      },\\n\",\n-      \"      response_metadata: {\\n\",\n-      \"        id: \\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"tool_use\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 365, output_tokens: 53 }\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 14,\\n\",\n+      \"          \\\"promptTokens\\\": 55,\\n\",\n+      \"          \\\"totalTokens\\\": 69\\n\",\n+      \"        },\\n\",\n+      \"        \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n       \"      },\\n\",\n-      \"      tool_calls: [\\n\",\n+      \"      \\\"tool_calls\\\": [\\n\",\n       \"        {\\n\",\n-      \"          name: \\\"magic_function\\\",\\n\",\n-      \"          args: [Object],\\n\",\n-      \"          id: \\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\n\",\n+      \"          \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"          \\\"args\\\": {\\n\",\n+      \"            \\\"input\\\": 3\\n\",\n+      \"          },\\n\",\n+      \"          \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"          \\\"id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\"\\n\",\n       \"        }\\n\",\n       \"      ],\\n\",\n-      \"      invalid_tool_calls: []\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 55,\\n\",\n+      \"        \\\"output_tokens\\\": 14,\\n\",\n+      \"        \\\"total_tokens\\\": 69\\n\",\n+      \"      }\\n\",\n       \"    },\\n\",\n       \"    ToolMessage {\\n\",\n-      \"      lc_serializable: true,\\n\",\n-      \"      lc_kwargs: {\\n\",\n-      \"        name: \\\"magic_function\\\",\\n\",\n-      \"        content: \\\"5\\\",\\n\",\n-      \"        tool_call_id: \\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\",\\n\",\n-      \"        additional_kwargs: {},\\n\",\n-      \"        response_metadata: {}\\n\",\n-      \"      },\\n\",\n-      \"      lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"      content: \\\"5\\\",\\n\",\n-      \"      name: \\\"magic_function\\\",\\n\",\n-      \"      additional_kwargs: {},\\n\",\n-      \"      response_metadata: {},\\n\",\n-      \"      tool_call_id: \\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\n\",\n+      \"      \\\"id\\\": \\\"1001bf20-7cde-4f8b-81f1-1faa654a8bb4\\\",\\n\",\n+      \"      \\\"content\\\": \\\"5\\\",\\n\",\n+      \"      \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {},\\n\",\n+      \"      \\\"tool_call_id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\"\\n\",\n       \"    },\\n\",\n       \"    AIMessage {\\n\",\n-      \"      lc_serializable: true,\\n\",\n-      \"      lc_kwargs: {\\n\",\n-      \"        content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"        tool_calls: [],\\n\",\n-      \"        invalid_tool_calls: [],\\n\",\n-      \"        additional_kwargs: {\\n\",\n-      \"          id: \\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\",\\n\",\n-      \"          type: \\\"message\\\",\\n\",\n-      \"          role: \\\"assistant\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"end_turn\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n+      \"      \\\"id\\\": \\\"chatcmpl-A7exsTk3ilzGzC8DuY8GpnKOaGdvx\\\",\\n\",\n+      \"      \\\"content\\\": \\\"The value of `magic_function(3)` is 5.\\\",\\n\",\n+      \"      \\\"additional_kwargs\\\": {},\\n\",\n+      \"      \\\"response_metadata\\\": {\\n\",\n+      \"        \\\"tokenUsage\\\": {\\n\",\n+      \"          \\\"completionTokens\\\": 14,\\n\",\n+      \"          \\\"promptTokens\\\": 78,\\n\",\n+      \"          \\\"totalTokens\\\": 92\\n\",\n       \"        },\\n\",\n-      \"        response_metadata: {}\\n\",\n-      \"      },\\n\",\n-      \"      lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"      content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"      name: undefined,\\n\",\n-      \"      additional_kwargs: {\\n\",\n-      \"        id: \\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\",\\n\",\n-      \"        type: \\\"message\\\",\\n\",\n-      \"        role: \\\"assistant\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"end_turn\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 431, output_tokens: 17 }\\n\",\n+      \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"        \\\"system_fingerprint\\\": \\\"fp_54e2f484be\\\"\\n\",\n       \"      },\\n\",\n-      \"      response_metadata: {\\n\",\n-      \"        id: \\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"end_turn\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 431, output_tokens: 17 }\\n\",\n-      \"      },\\n\",\n-      \"      tool_calls: [],\\n\",\n-      \"      invalid_tool_calls: []\\n\",\n+      \"      \\\"tool_calls\\\": [],\\n\",\n+      \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"      \\\"usage_metadata\\\": {\\n\",\n+      \"        \\\"input_tokens\\\": 78,\\n\",\n+      \"        \\\"output_tokens\\\": 14,\\n\",\n+      \"        \\\"total_tokens\\\": 92\\n\",\n+      \"      }\\n\",\n       \"    }\\n\",\n       \"  ]\\n\",\n       \"}\\n\"\n@@ -322,13 +281,18 @@\n    ],\n    \"source\": [\n     \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n-    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n     \"\\n\",\n-    \"const app = createReactAgent({ llm, tools });\\n\",\n+    \"const app = createReactAgent({\\n\",\n+    \"  llm,\\n\",\n+    \"  tools,\\n\",\n+    \"});\\n\",\n     \"\\n\",\n     \"let agentOutput = await app.invoke({\\n\",\n     \"  messages: [\\n\",\n-    \"    new HumanMessage(query)\\n\",\n+    \"    {\\n\",\n+    \"      role: \\\"user\\\",\\n\",\n+    \"      content: query\\n\",\n+    \"    },\\n\",\n     \"  ],\\n\",\n     \"});\\n\",\n     \"\\n\",\n@@ -337,7 +301,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 5,\n    \"id\": \"b0a390a2\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -347,186 +311,110 @@\n        \"{\\n\",\n        \"  messages: [\\n\",\n        \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"what is the value of magic_function(3)?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"what is the value of magic_function(3)?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n+       \"      \\\"id\\\": \\\"eeef343c-80d1-4ccb-86af-c109343689cd\\\",\\n\",\n+       \"      \\\"content\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {}\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: [ \\u001b[36m[Object]\\u001b[39m ],\\n\",\n-       \"        additional_kwargs: {\\n\",\n-       \"          id: \\u001b[32m\\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\"\\u001b[39m,\\n\",\n-       \"          type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"          role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"          model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"          stop_reason: \\u001b[32m\\\"tool_use\\\"\\u001b[39m,\\n\",\n-       \"          stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"          usage: \\u001b[36m[Object]\\u001b[39m\\n\",\n-       \"        },\\n\",\n-       \"        tool_calls: [ \\u001b[36m[Object]\\u001b[39m ],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        response_metadata: {}\\n\",\n+       \"      \\\"id\\\": \\\"chatcmpl-A7exs2uRqEipaZ7MtRbXnqu0vT0Da\\\",\\n\",\n+       \"      \\\"content\\\": \\\"\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {\\n\",\n+       \"        \\\"tool_calls\\\": [\\n\",\n+       \"          {\\n\",\n+       \"            \\\"id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\",\\n\",\n+       \"            \\\"type\\\": \\\"function\\\",\\n\",\n+       \"            \\\"function\\\": \\\"[Object]\\\"\\n\",\n+       \"          }\\n\",\n+       \"        ]\\n\",\n        \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: [\\n\",\n-       \"        {\\n\",\n-       \"          type: \\u001b[32m\\\"tool_use\\\"\\u001b[39m,\\n\",\n-       \"          id: \\u001b[32m\\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\u001b[39m,\\n\",\n-       \"          name: \\u001b[32m\\\"magic_function\\\"\\u001b[39m,\\n\",\n-       \"          input: \\u001b[36m[Object]\\u001b[39m\\n\",\n-       \"        }\\n\",\n-       \"      ],\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\"\\u001b[39m,\\n\",\n-       \"        type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"        role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"tool_use\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m365\\u001b[39m, output_tokens: \\u001b[33m53\\u001b[39m }\\n\",\n-       \"      },\\n\",\n-       \"      response_metadata: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_015jSku8UgrtRQ2kNQuTsvi1\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"tool_use\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m365\\u001b[39m, output_tokens: \\u001b[33m53\\u001b[39m }\\n\",\n+       \"      \\\"response_metadata\\\": {\\n\",\n+       \"        \\\"tokenUsage\\\": {\\n\",\n+       \"          \\\"completionTokens\\\": 14,\\n\",\n+       \"          \\\"promptTokens\\\": 55,\\n\",\n+       \"          \\\"totalTokens\\\": 69\\n\",\n+       \"        },\\n\",\n+       \"        \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+       \"        \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n        \"      },\\n\",\n-       \"      tool_calls: [\\n\",\n+       \"      \\\"tool_calls\\\": [\\n\",\n        \"        {\\n\",\n-       \"          name: \\u001b[32m\\\"magic_function\\\"\\u001b[39m,\\n\",\n-       \"          args: \\u001b[36m[Object]\\u001b[39m,\\n\",\n-       \"          id: \\u001b[32m\\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\u001b[39m\\n\",\n+       \"          \\\"name\\\": \\\"magic_function\\\",\\n\",\n+       \"          \\\"args\\\": {\\n\",\n+       \"            \\\"input\\\": 3\\n\",\n+       \"          },\\n\",\n+       \"          \\\"type\\\": \\\"tool_call\\\",\\n\",\n+       \"          \\\"id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\"\\n\",\n        \"        }\\n\",\n        \"      ],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"      \\\"usage_metadata\\\": {\\n\",\n+       \"        \\\"input_tokens\\\": 55,\\n\",\n+       \"        \\\"output_tokens\\\": 14,\\n\",\n+       \"        \\\"total_tokens\\\": 69\\n\",\n+       \"      }\\n\",\n        \"    },\\n\",\n        \"    ToolMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        name: \\u001b[32m\\\"magic_function\\\"\\u001b[39m,\\n\",\n-       \"        content: \\u001b[32m\\\"5\\\"\\u001b[39m,\\n\",\n-       \"        tool_call_id: \\u001b[32m\\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"5\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[32m\\\"magic_function\\\"\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {},\\n\",\n-       \"      tool_call_id: \\u001b[32m\\\"toolu_01WCezi2ywMPnRm1xbrXYPoB\\\"\\u001b[39m\\n\",\n+       \"      \\\"id\\\": \\\"1001bf20-7cde-4f8b-81f1-1faa654a8bb4\\\",\\n\",\n+       \"      \\\"content\\\": \\\"5\\\",\\n\",\n+       \"      \\\"name\\\": \\\"magic_function\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {},\\n\",\n+       \"      \\\"tool_call_id\\\": \\\"call_MtwWLn000BQHeSYQKsbxYNR0\\\"\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"The value of magic_function(3) is 5.\\\"\\u001b[39m,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {\\n\",\n-       \"          id: \\u001b[32m\\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\"\\u001b[39m,\\n\",\n-       \"          type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"          role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"          model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"          stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"          stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"          usage: \\u001b[36m[Object]\\u001b[39m\\n\",\n+       \"      \\\"id\\\": \\\"chatcmpl-A7exsTk3ilzGzC8DuY8GpnKOaGdvx\\\",\\n\",\n+       \"      \\\"content\\\": \\\"The value of `magic_function(3)` is 5.\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {\\n\",\n+       \"        \\\"tokenUsage\\\": {\\n\",\n+       \"          \\\"completionTokens\\\": 14,\\n\",\n+       \"          \\\"promptTokens\\\": 78,\\n\",\n+       \"          \\\"totalTokens\\\": 92\\n\",\n        \"        },\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"The value of magic_function(3) is 5.\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\"\\u001b[39m,\\n\",\n-       \"        type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"        role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m431\\u001b[39m, output_tokens: \\u001b[33m17\\u001b[39m }\\n\",\n+       \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+       \"        \\\"system_fingerprint\\\": \\\"fp_54e2f484be\\\"\\n\",\n        \"      },\\n\",\n-       \"      response_metadata: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_01FbyPvpxtczu2Cmd4vKcPQm\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m431\\u001b[39m, output_tokens: \\u001b[33m17\\u001b[39m }\\n\",\n-       \"      },\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"tool_calls\\\": [],\\n\",\n+       \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"      \\\"usage_metadata\\\": {\\n\",\n+       \"        \\\"input_tokens\\\": 78,\\n\",\n+       \"        \\\"output_tokens\\\": 14,\\n\",\n+       \"        \\\"total_tokens\\\": 92\\n\",\n+       \"      }\\n\",\n        \"    },\\n\",\n        \"    HumanMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"Pardon?\\\"\\u001b[39m,\\n\",\n-       \"        additional_kwargs: {},\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"Pardon?\\\"\\u001b[39m,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {},\\n\",\n-       \"      response_metadata: {}\\n\",\n+       \"      \\\"id\\\": \\\"1f2a9f41-c8ff-48fe-9d93-e663ee9279ff\\\",\\n\",\n+       \"      \\\"content\\\": \\\"Pardon?\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {}\\n\",\n        \"    },\\n\",\n        \"    AIMessage {\\n\",\n-       \"      lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"      lc_kwargs: {\\n\",\n-       \"        content: \\u001b[32m\\\"I apologize for the confusion. Let me explain the steps I took to arrive at the result:\\\\n\\\"\\u001b[39m +\\n\",\n-       \"          \\u001b[32m\\\"\\\\n\\\"\\u001b[39m +\\n\",\n-       \"          \\u001b[32m\\\"1. You aske\\\"\\u001b[39m... 52 more characters,\\n\",\n-       \"        tool_calls: [],\\n\",\n-       \"        invalid_tool_calls: [],\\n\",\n-       \"        additional_kwargs: {\\n\",\n-       \"          id: \\u001b[32m\\\"msg_012yLSnnf1c64NWKS9K58hcN\\\"\\u001b[39m,\\n\",\n-       \"          type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"          role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"          model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"          stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"          stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"          usage: \\u001b[36m[Object]\\u001b[39m\\n\",\n+       \"      \\\"id\\\": \\\"chatcmpl-A7exyTe9Ofs63Ex3sKwRx3wWksNup\\\",\\n\",\n+       \"      \\\"content\\\": \\\"The result of calling the `magic_function` with an input of 3 is 5.\\\",\\n\",\n+       \"      \\\"additional_kwargs\\\": {},\\n\",\n+       \"      \\\"response_metadata\\\": {\\n\",\n+       \"        \\\"tokenUsage\\\": {\\n\",\n+       \"          \\\"completionTokens\\\": 20,\\n\",\n+       \"          \\\"promptTokens\\\": 102,\\n\",\n+       \"          \\\"totalTokens\\\": 122\\n\",\n        \"        },\\n\",\n-       \"        response_metadata: {}\\n\",\n-       \"      },\\n\",\n-       \"      lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"      content: \\u001b[32m\\\"I apologize for the confusion. Let me explain the steps I took to arrive at the result:\\\\n\\\"\\u001b[39m +\\n\",\n-       \"        \\u001b[32m\\\"\\\\n\\\"\\u001b[39m +\\n\",\n-       \"        \\u001b[32m\\\"1. You aske\\\"\\u001b[39m... 52 more characters,\\n\",\n-       \"      name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"      additional_kwargs: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_012yLSnnf1c64NWKS9K58hcN\\\"\\u001b[39m,\\n\",\n-       \"        type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"        role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m455\\u001b[39m, output_tokens: \\u001b[33m137\\u001b[39m }\\n\",\n-       \"      },\\n\",\n-       \"      response_metadata: {\\n\",\n-       \"        id: \\u001b[32m\\\"msg_012yLSnnf1c64NWKS9K58hcN\\\"\\u001b[39m,\\n\",\n-       \"        model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"        stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"        stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"        usage: { input_tokens: \\u001b[33m455\\u001b[39m, output_tokens: \\u001b[33m137\\u001b[39m }\\n\",\n+       \"        \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+       \"        \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n        \"      },\\n\",\n-       \"      tool_calls: [],\\n\",\n-       \"      invalid_tool_calls: []\\n\",\n+       \"      \\\"tool_calls\\\": [],\\n\",\n+       \"      \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"      \\\"usage_metadata\\\": {\\n\",\n+       \"        \\\"input_tokens\\\": 102,\\n\",\n+       \"        \\\"output_tokens\\\": 20,\\n\",\n+       \"        \\\"total_tokens\\\": 122\\n\",\n+       \"      }\\n\",\n        \"    }\\n\",\n        \"  ]\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 4,\n+     \"execution_count\": 5,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -538,7 +426,7 @@\n     \"agentOutput = await app.invoke({\\n\",\n     \"  messages: [\\n\",\n     \"    ...messageHistory,\\n\",\n-    \"    new HumanMessage(newQuery)\\n\",\n+    \"    { role: \\\"user\\\", content: newQuery }\\n\",\n     \"  ],\\n\",\n     \"});\\n\"\n    ]\n@@ -571,7 +459,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 6,\n    \"id\": \"4c5266cc\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -582,11 +470,11 @@\n       \"text/plain\": [\n        \"{\\n\",\n        \"  input: \\u001b[32m\\\"what is the value of magic_function(3)?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m\\\"El valor de magic_function(3) es 5.\\\"\\u001b[39m\\n\",\n+       \"  output: \\u001b[32m\\\"El valor de `magic_function(3)` es 5.\\\"\\u001b[39m\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 5,\n+     \"execution_count\": 6,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -631,7 +519,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 7,\n    \"id\": \"38a751ba\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -641,54 +529,34 @@\n      \"data\": {\n       \"text/plain\": [\n        \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"El valor de magic_function(3) es 5.\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: {\\n\",\n-       \"      id: \\u001b[32m\\\"msg_01P5VUYbBZoeMaReqBgqFJZa\\\"\\u001b[39m,\\n\",\n-       \"      type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"      role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"      model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"      stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"      stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"      usage: { input_tokens: \\u001b[33m444\\u001b[39m, output_tokens: \\u001b[33m17\\u001b[39m }\\n\",\n+       \"  \\\"id\\\": \\\"chatcmpl-A7ey8LGWAs8ldrRRcO5wlHM85w9T8\\\",\\n\",\n+       \"  \\\"content\\\": \\\"El valor de `magic_function(3)` es 5.\\\",\\n\",\n+       \"  \\\"additional_kwargs\\\": {},\\n\",\n+       \"  \\\"response_metadata\\\": {\\n\",\n+       \"    \\\"tokenUsage\\\": {\\n\",\n+       \"      \\\"completionTokens\\\": 14,\\n\",\n+       \"      \\\"promptTokens\\\": 89,\\n\",\n+       \"      \\\"totalTokens\\\": 103\\n\",\n        \"    },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"El valor de magic_function(3) es 5.\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01P5VUYbBZoeMaReqBgqFJZa\\\"\\u001b[39m,\\n\",\n-       \"    type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"    role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m444\\u001b[39m, output_tokens: \\u001b[33m17\\u001b[39m }\\n\",\n-       \"  },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01P5VUYbBZoeMaReqBgqFJZa\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-haiku-20240307\\\"\\u001b[39m,\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m444\\u001b[39m, output_tokens: \\u001b[33m17\\u001b[39m }\\n\",\n+       \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+       \"    \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n        \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n+       \"  \\\"tool_calls\\\": [],\\n\",\n+       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"  \\\"usage_metadata\\\": {\\n\",\n+       \"    \\\"input_tokens\\\": 89,\\n\",\n+       \"    \\\"output_tokens\\\": 14,\\n\",\n+       \"    \\\"total_tokens\\\": 103\\n\",\n+       \"  }\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 7,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n    ],\n    \"source\": [\n-    \"import { SystemMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n     \"const systemMessage = \\\"You are a helpful assistant. Respond only in Spanish.\\\";\\n\",\n     \"\\n\",\n     \"// This could also be a SystemMessage object\\n\",\n@@ -702,7 +570,7 @@\n     \"\\n\",\n     \"agentOutput = await appWithSystemMessage.invoke({\\n\",\n     \"  messages: [\\n\",\n-    \"    new HumanMessage(query)\\n\",\n+    \"    { role: \\\"user\\\", content: query }\\n\",\n     \"  ],\\n\",\n     \"});\\n\",\n     \"agentOutput.messages[agentOutput.messages.length - 1];\"\n@@ -721,7 +589,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 8,\n    \"id\": \"c7120cdd\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -731,13 +599,13 @@\n      \"text\": [\n       \"{\\n\",\n       \"  input: \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"  output: \\\"5. ¡Pandemonium!\\\"\\n\",\n+      \"  output: \\\"El valor de magic_function(3) es 5. ¡Pandemonium!\\\"\\n\",\n       \"}\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n-    \"import { BaseMessage, SystemMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { BaseMessage, SystemMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n     \"\\n\",\n     \"const modifyMessages = (messages: BaseMessage[]) => {\\n\",\n     \"  return [\\n\",\n@@ -754,7 +622,7 @@\n     \"});\\n\",\n     \"\\n\",\n     \"agentOutput = await appWithMessagesModifier.invoke({\\n\",\n-    \"  messages: [new HumanMessage(query)],\\n\",\n+    \"  messages: [{ role: \\\"user\\\", content: query }],\\n\",\n     \"});\\n\",\n     \"\\n\",\n     \"console.log({\\n\",\n@@ -776,19 +644,19 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 9,\n    \"id\": \"4d67ba36\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"name\": \"stdout\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"The magic_function takes an input number and applies some magic to it, returning the output. For an input of 3, the output is 5.\\n\",\n+      \"The output of the magic function for the input 3 is 5.\\n\",\n       \"---\\n\",\n-      \"Okay, I remember your name is Polly.\\n\",\n+      \"Yes, your name is Polly! How can I assist you today?\\n\",\n       \"---\\n\",\n-      \"So the output of the magic_function with an input of 3 is 5.\\n\"\n+      \"The output of the magic function for the input 3 is 5.\\n\"\n      ]\n     }\n    ],\n@@ -844,33 +712,33 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 10,\n    \"id\": \"bbc64438\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"name\": \"stdout\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"The magic_function takes an input number and applies some magic to it, returning the output. For an input of 3, the magic_function returns 5.\\n\",\n+      \"Hi Polly! The output of the magic function for the input 3 is 5.\\n\",\n       \"---\\n\",\n-      \"Ah yes, I remember your name is Polly! It's nice to meet you Polly.\\n\",\n+      \"Yes, your name is Polly!\\n\",\n       \"---\\n\",\n-      \"So the magic_function returned an output of 5 for an input of 3.\\n\"\n+      \"The output of the magic function for the input 3 was 5.\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n     \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n     \"\\n\",\n-    \"const memory = new MemorySaver();\\n\",\n+    \"const checkpointer = new MemorySaver();\\n\",\n     \"const appWithMemory = createReactAgent({\\n\",\n-    \"  llm,\\n\",\n-    \"  tools,\\n\",\n-    \"  checkpointSaver: memory\\n\",\n+    \"  llm: llm,\\n\",\n+    \"  tools: tools,\\n\",\n+    \"  checkpointSaver: checkpointer\\n\",\n     \"});\\n\",\n     \"\\n\",\n-    \"const config = {\\n\",\n+    \"const langGraphConfig = {\\n\",\n     \"  configurable: {\\n\",\n     \"    thread_id: \\\"test-thread\\\",\\n\",\n     \"  },\\n\",\n@@ -879,12 +747,13 @@\n     \"agentOutput = await appWithMemory.invoke(\\n\",\n     \"  {\\n\",\n     \"    messages: [\\n\",\n-    \"      new HumanMessage(\\n\",\n-    \"        \\\"Hi, I'm polly! What's the output of magic_function of 3?\\\",\\n\",\n-    \"      ),\\n\",\n+    \"      {\\n\",\n+    \"        role: \\\"user\\\",\\n\",\n+    \"        content: \\\"Hi, I'm polly! What's the output of magic_function of 3?\\\",\\n\",\n+    \"      }\\n\",\n     \"    ],\\n\",\n     \"  },\\n\",\n-    \"  config,\\n\",\n+    \"  langGraphConfig,\\n\",\n     \");\\n\",\n     \"\\n\",\n     \"console.log(agentOutput.messages[agentOutput.messages.length - 1].content);\\n\",\n@@ -893,10 +762,10 @@\n     \"agentOutput = await appWithMemory.invoke(\\n\",\n     \"  {\\n\",\n     \"    messages: [\\n\",\n-    \"      new HumanMessage(\\\"Remember my name?\\\")\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"Remember my name?\\\" }\\n\",\n     \"    ]\\n\",\n     \"  },\\n\",\n-    \"  config,\\n\",\n+    \"  langGraphConfig,\\n\",\n     \");\\n\",\n     \"\\n\",\n     \"console.log(agentOutput.messages[agentOutput.messages.length - 1].content);\\n\",\n@@ -905,10 +774,10 @@\n     \"agentOutput = await appWithMemory.invoke(\\n\",\n     \"  {\\n\",\n     \"    messages: [\\n\",\n-    \"      new HumanMessage(\\\"what was that output again?\\\")\\n\",\n+    \"      { role: \\\"user\\\", content: \\\"what was that output again?\\\" }\\n\",\n     \"    ]\\n\",\n     \"  },\\n\",\n-    \"  config,\\n\",\n+    \"  langGraphConfig,\\n\",\n     \");\\n\",\n     \"\\n\",\n     \"console.log(agentOutput.messages[agentOutput.messages.length - 1].content);\"\n@@ -929,7 +798,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 11,\n    \"id\": \"5c928049\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -945,16 +814,61 @@\n       \"      action: {\\n\",\n       \"        tool: \\\"magic_function\\\",\\n\",\n       \"        toolInput: { input: 3 },\\n\",\n-      \"        toolCallId: \\\"toolu_01KCJJ8kyiY5LV4RHbVPzK8v\\\",\\n\",\n-      \"        log: 'Invoking \\\"magic_function\\\" with {\\\"input\\\":3}\\\\n' +\\n\",\n-      \"          '[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01KCJJ8kyiY5LV4RHbVPzK8v\\\"'... 46 more characters,\\n\",\n-      \"        messageLog: [ [AIMessageChunk] ]\\n\",\n+      \"        toolCallId: \\\"call_IQZr1yy2Ug6904VkQg6pWGgR\\\",\\n\",\n+      \"        log: 'Invoking \\\"magic_function\\\" with {\\\"input\\\":3}\\\\n',\\n\",\n+      \"        messageLog: [\\n\",\n+      \"          AIMessageChunk {\\n\",\n+      \"            \\\"id\\\": \\\"chatcmpl-A7eziUrDmLSSMoiOskhrfbsHqx4Sd\\\",\\n\",\n+      \"            \\\"content\\\": \\\"\\\",\\n\",\n+      \"            \\\"additional_kwargs\\\": {\\n\",\n+      \"              \\\"tool_calls\\\": [\\n\",\n+      \"                {\\n\",\n+      \"                  \\\"index\\\": 0,\\n\",\n+      \"                  \\\"id\\\": \\\"call_IQZr1yy2Ug6904VkQg6pWGgR\\\",\\n\",\n+      \"                  \\\"type\\\": \\\"function\\\",\\n\",\n+      \"                  \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"                }\\n\",\n+      \"              ]\\n\",\n+      \"            },\\n\",\n+      \"            \\\"response_metadata\\\": {\\n\",\n+      \"              \\\"prompt\\\": 0,\\n\",\n+      \"              \\\"completion\\\": 0,\\n\",\n+      \"              \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"              \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"tool_calls\\\": [\\n\",\n+      \"              {\\n\",\n+      \"                \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"                \\\"args\\\": {\\n\",\n+      \"                  \\\"input\\\": 3\\n\",\n+      \"                },\\n\",\n+      \"                \\\"id\\\": \\\"call_IQZr1yy2Ug6904VkQg6pWGgR\\\",\\n\",\n+      \"                \\\"type\\\": \\\"tool_call\\\"\\n\",\n+      \"              }\\n\",\n+      \"            ],\\n\",\n+      \"            \\\"tool_call_chunks\\\": [\\n\",\n+      \"              {\\n\",\n+      \"                \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"                \\\"args\\\": \\\"{\\\\\\\"input\\\\\\\":3}\\\",\\n\",\n+      \"                \\\"id\\\": \\\"call_IQZr1yy2Ug6904VkQg6pWGgR\\\",\\n\",\n+      \"                \\\"index\\\": 0,\\n\",\n+      \"                \\\"type\\\": \\\"tool_call_chunk\\\"\\n\",\n+      \"              }\\n\",\n+      \"            ],\\n\",\n+      \"            \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"            \\\"usage_metadata\\\": {\\n\",\n+      \"              \\\"input_tokens\\\": 61,\\n\",\n+      \"              \\\"output_tokens\\\": 14,\\n\",\n+      \"              \\\"total_tokens\\\": 75\\n\",\n+      \"            }\\n\",\n+      \"          }\\n\",\n+      \"        ]\\n\",\n       \"      },\\n\",\n       \"      observation: \\\"5\\\"\\n\",\n       \"    }\\n\",\n       \"  ]\\n\",\n       \"}\\n\",\n-      \"{ output: \\\"The value of magic_function(3) is 5.\\\" }\\n\"\n+      \"{ output: \\\"The value of `magic_function(3)` is 5.\\\" }\\n\"\n      ]\n     }\n    ],\n@@ -978,7 +892,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 12,\n    \"id\": \"2be89a30\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -990,35 +904,42 @@\n       \"  agent: {\\n\",\n       \"    messages: [\\n\",\n       \"      AIMessage {\\n\",\n-      \"        lc_serializable: true,\\n\",\n-      \"        lc_kwargs: {\\n\",\n-      \"          content: [Array],\\n\",\n-      \"          additional_kwargs: [Object],\\n\",\n-      \"          tool_calls: [Array],\\n\",\n-      \"          invalid_tool_calls: [],\\n\",\n-      \"          response_metadata: {}\\n\",\n-      \"        },\\n\",\n-      \"        lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"        content: [ [Object] ],\\n\",\n-      \"        name: undefined,\\n\",\n-      \"        additional_kwargs: {\\n\",\n-      \"          id: \\\"msg_01WWYeJvJroT82QhJQZKdwSt\\\",\\n\",\n-      \"          type: \\\"message\\\",\\n\",\n-      \"          role: \\\"assistant\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"tool_use\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-A7ezu8hirCENjdjR2GpLjkzXFTEmp\\\",\\n\",\n+      \"        \\\"content\\\": \\\"\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {\\n\",\n+      \"          \\\"tool_calls\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"id\\\": \\\"call_KhhNL0m3mlPoJiboFMoX8hzk\\\",\\n\",\n+      \"              \\\"type\\\": \\\"function\\\",\\n\",\n+      \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ]\\n\",\n       \"        },\\n\",\n-      \"        response_metadata: {\\n\",\n-      \"          id: \\\"msg_01WWYeJvJroT82QhJQZKdwSt\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"tool_use\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 14,\\n\",\n+      \"            \\\"promptTokens\\\": 55,\\n\",\n+      \"            \\\"totalTokens\\\": 69\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n       \"        },\\n\",\n-      \"        tool_calls: [ [Object] ],\\n\",\n-      \"        invalid_tool_calls: []\\n\",\n+      \"        \\\"tool_calls\\\": [\\n\",\n+      \"          {\\n\",\n+      \"            \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"            \\\"args\\\": {\\n\",\n+      \"              \\\"input\\\": 3\\n\",\n+      \"            },\\n\",\n+      \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"            \\\"id\\\": \\\"call_KhhNL0m3mlPoJiboFMoX8hzk\\\"\\n\",\n+      \"          }\\n\",\n+      \"        ],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 55,\\n\",\n+      \"          \\\"output_tokens\\\": 14,\\n\",\n+      \"          \\\"total_tokens\\\": 69\\n\",\n+      \"        }\\n\",\n       \"      }\\n\",\n       \"    ]\\n\",\n       \"  }\\n\",\n@@ -1027,20 +948,11 @@\n       \"  tools: {\\n\",\n       \"    messages: [\\n\",\n       \"      ToolMessage {\\n\",\n-      \"        lc_serializable: true,\\n\",\n-      \"        lc_kwargs: {\\n\",\n-      \"          name: \\\"magic_function\\\",\\n\",\n-      \"          content: \\\"5\\\",\\n\",\n-      \"          tool_call_id: \\\"toolu_01X9pwxuroTWNVqiwQTL1U8C\\\",\\n\",\n-      \"          additional_kwargs: {},\\n\",\n-      \"          response_metadata: {}\\n\",\n-      \"        },\\n\",\n-      \"        lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"        content: \\\"5\\\",\\n\",\n-      \"        name: \\\"magic_function\\\",\\n\",\n-      \"        additional_kwargs: {},\\n\",\n-      \"        response_metadata: {},\\n\",\n-      \"        tool_call_id: \\\"toolu_01X9pwxuroTWNVqiwQTL1U8C\\\"\\n\",\n+      \"        \\\"content\\\": \\\"5\\\",\\n\",\n+      \"        \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {},\\n\",\n+      \"        \\\"tool_call_id\\\": \\\"call_KhhNL0m3mlPoJiboFMoX8hzk\\\"\\n\",\n       \"      }\\n\",\n       \"    ]\\n\",\n       \"  }\\n\",\n@@ -1049,35 +961,25 @@\n       \"  agent: {\\n\",\n       \"    messages: [\\n\",\n       \"      AIMessage {\\n\",\n-      \"        lc_serializable: true,\\n\",\n-      \"        lc_kwargs: {\\n\",\n-      \"          content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"          tool_calls: [],\\n\",\n-      \"          invalid_tool_calls: [],\\n\",\n-      \"          additional_kwargs: [Object],\\n\",\n-      \"          response_metadata: {}\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-A7ezuTrh8GC550eKa1ZqRZGjpY5zh\\\",\\n\",\n+      \"        \\\"content\\\": \\\"The value of `magic_function(3)` is 5.\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 14,\\n\",\n+      \"            \\\"promptTokens\\\": 78,\\n\",\n+      \"            \\\"totalTokens\\\": 92\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n       \"        },\\n\",\n-      \"        lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"        content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"        name: undefined,\\n\",\n-      \"        additional_kwargs: {\\n\",\n-      \"          id: \\\"msg_012kQPkxt2CrsFw4CsdfNTWr\\\",\\n\",\n-      \"          type: \\\"message\\\",\\n\",\n-      \"          role: \\\"assistant\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"end_turn\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n-      \"        },\\n\",\n-      \"        response_metadata: {\\n\",\n-      \"          id: \\\"msg_012kQPkxt2CrsFw4CsdfNTWr\\\",\\n\",\n-      \"          model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"          stop_reason: \\\"end_turn\\\",\\n\",\n-      \"          stop_sequence: null,\\n\",\n-      \"          usage: [Object]\\n\",\n-      \"        },\\n\",\n-      \"        tool_calls: [],\\n\",\n-      \"        invalid_tool_calls: []\\n\",\n+      \"        \\\"tool_calls\\\": [],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 78,\\n\",\n+      \"          \\\"output_tokens\\\": 14,\\n\",\n+      \"          \\\"total_tokens\\\": 92\\n\",\n+      \"        }\\n\",\n       \"      }\\n\",\n       \"    ]\\n\",\n       \"  }\\n\",\n@@ -1087,7 +989,7 @@\n    ],\n    \"source\": [\n     \"const langGraphStream = await app.stream(\\n\",\n-    \"  { messages: [new HumanMessage(query)] },\\n\",\n+    \"  { messages: [{ role: \\\"user\\\", content: query }] },\\n\",\n     \"  { streamMode: \\\"updates\\\" },\\n\",\n     \");\\n\",\n     \"\\n\",\n@@ -1110,7 +1012,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 13,\n    \"id\": \"77ce2771\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -1125,21 +1027,53 @@\n       \"    action: {\\n\",\n       \"      tool: \\\"magic_function\\\",\\n\",\n       \"      toolInput: { input: 3 },\\n\",\n-      \"      toolCallId: \\\"toolu_0126dJXbjwLC5daAScz8bw1k\\\",\\n\",\n-      \"      log: 'Invoking \\\"magic_function\\\" with {\\\"input\\\":3}\\\\n' +\\n\",\n-      \"        '[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_0126dJXbjwLC5daAScz8bw1k\\\"'... 46 more characters,\\n\",\n+      \"      toolCallId: \\\"call_mbg1xgLEYEEWClbEaDe7p5tK\\\",\\n\",\n+      \"      log: 'Invoking \\\"magic_function\\\" with {\\\"input\\\":3}\\\\n',\\n\",\n       \"      messageLog: [\\n\",\n       \"        AIMessageChunk {\\n\",\n-      \"          lc_serializable: true,\\n\",\n-      \"          lc_kwargs: [Object],\\n\",\n-      \"          lc_namespace: [Array],\\n\",\n-      \"          content: [Array],\\n\",\n-      \"          name: undefined,\\n\",\n-      \"          additional_kwargs: [Object],\\n\",\n-      \"          response_metadata: {},\\n\",\n-      \"          tool_calls: [Array],\\n\",\n-      \"          invalid_tool_calls: [],\\n\",\n-      \"          tool_call_chunks: [Array]\\n\",\n+      \"          \\\"id\\\": \\\"chatcmpl-A7f0NdSRSUJsBP6ENTpiQD4LzpBAH\\\",\\n\",\n+      \"          \\\"content\\\": \\\"\\\",\\n\",\n+      \"          \\\"additional_kwargs\\\": {\\n\",\n+      \"            \\\"tool_calls\\\": [\\n\",\n+      \"              {\\n\",\n+      \"                \\\"index\\\": 0,\\n\",\n+      \"                \\\"id\\\": \\\"call_mbg1xgLEYEEWClbEaDe7p5tK\\\",\\n\",\n+      \"                \\\"type\\\": \\\"function\\\",\\n\",\n+      \"                \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"              }\\n\",\n+      \"            ]\\n\",\n+      \"          },\\n\",\n+      \"          \\\"response_metadata\\\": {\\n\",\n+      \"            \\\"prompt\\\": 0,\\n\",\n+      \"            \\\"completion\\\": 0,\\n\",\n+      \"            \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"            \\\"system_fingerprint\\\": \\\"fp_54e2f484be\\\"\\n\",\n+      \"          },\\n\",\n+      \"          \\\"tool_calls\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"              \\\"args\\\": {\\n\",\n+      \"                \\\"input\\\": 3\\n\",\n+      \"              },\\n\",\n+      \"              \\\"id\\\": \\\"call_mbg1xgLEYEEWClbEaDe7p5tK\\\",\\n\",\n+      \"              \\\"type\\\": \\\"tool_call\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"tool_call_chunks\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"              \\\"args\\\": \\\"{\\\\\\\"input\\\\\\\":3}\\\",\\n\",\n+      \"              \\\"id\\\": \\\"call_mbg1xgLEYEEWClbEaDe7p5tK\\\",\\n\",\n+      \"              \\\"index\\\": 0,\\n\",\n+      \"              \\\"type\\\": \\\"tool_call_chunk\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"          \\\"usage_metadata\\\": {\\n\",\n+      \"            \\\"input_tokens\\\": 61,\\n\",\n+      \"            \\\"output_tokens\\\": 14,\\n\",\n+      \"            \\\"total_tokens\\\": 75\\n\",\n+      \"          }\\n\",\n       \"        }\\n\",\n       \"      ]\\n\",\n       \"    },\\n\",\n@@ -1176,7 +1110,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 14,\n    \"id\": \"2f9cdfa8\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n@@ -1188,137 +1122,77 @@\n      \"text\": [\n       \"[\\n\",\n       \"  HumanMessage {\\n\",\n-      \"    lc_serializable: true,\\n\",\n-      \"    lc_kwargs: {\\n\",\n-      \"      content: \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"      additional_kwargs: {},\\n\",\n-      \"      response_metadata: {}\\n\",\n-      \"    },\\n\",\n-      \"    lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"    content: \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"    name: undefined,\\n\",\n-      \"    additional_kwargs: {},\\n\",\n-      \"    response_metadata: {}\\n\",\n+      \"    \\\"id\\\": \\\"46a825b2-13a3-4f19-b1aa-7716c53eb247\\\",\\n\",\n+      \"    \\\"content\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n       \"  },\\n\",\n       \"  AIMessage {\\n\",\n-      \"    lc_serializable: true,\\n\",\n-      \"    lc_kwargs: {\\n\",\n-      \"      content: [\\n\",\n-      \"        {\\n\",\n-      \"          type: \\\"tool_use\\\",\\n\",\n-      \"          id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\",\\n\",\n-      \"          name: \\\"magic_function\\\",\\n\",\n-      \"          input: [Object]\\n\",\n-      \"        }\\n\",\n-      \"      ],\\n\",\n-      \"      additional_kwargs: {\\n\",\n-      \"        id: \\\"msg_01BhXyjA2PTwGC5J3JNnfAXY\\\",\\n\",\n-      \"        type: \\\"message\\\",\\n\",\n-      \"        role: \\\"assistant\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"tool_use\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 365, output_tokens: 53 }\\n\",\n-      \"      },\\n\",\n-      \"      tool_calls: [\\n\",\n+      \"    \\\"id\\\": \\\"chatcmpl-A7f0iUuWktC8gXztWZCjofqyCozY2\\\",\\n\",\n+      \"    \\\"content\\\": \\\"\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {\\n\",\n+      \"      \\\"tool_calls\\\": [\\n\",\n       \"        {\\n\",\n-      \"          name: \\\"magic_function\\\",\\n\",\n-      \"          args: [Object],\\n\",\n-      \"          id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\"\\n\",\n+      \"          \\\"id\\\": \\\"call_ndsPDU58wsMeGaqr41cSlLlF\\\",\\n\",\n+      \"          \\\"type\\\": \\\"function\\\",\\n\",\n+      \"          \\\"function\\\": \\\"[Object]\\\"\\n\",\n       \"        }\\n\",\n-      \"      ],\\n\",\n-      \"      invalid_tool_calls: [],\\n\",\n-      \"      response_metadata: {}\\n\",\n-      \"    },\\n\",\n-      \"    lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"    content: [\\n\",\n-      \"      {\\n\",\n-      \"        type: \\\"tool_use\\\",\\n\",\n-      \"        id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\",\\n\",\n-      \"        name: \\\"magic_function\\\",\\n\",\n-      \"        input: { input: 3 }\\n\",\n-      \"      }\\n\",\n-      \"    ],\\n\",\n-      \"    name: undefined,\\n\",\n-      \"    additional_kwargs: {\\n\",\n-      \"      id: \\\"msg_01BhXyjA2PTwGC5J3JNnfAXY\\\",\\n\",\n-      \"      type: \\\"message\\\",\\n\",\n-      \"      role: \\\"assistant\\\",\\n\",\n-      \"      model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"      stop_reason: \\\"tool_use\\\",\\n\",\n-      \"      stop_sequence: null,\\n\",\n-      \"      usage: { input_tokens: 365, output_tokens: 53 }\\n\",\n+      \"      ]\\n\",\n       \"    },\\n\",\n-      \"    response_metadata: {\\n\",\n-      \"      id: \\\"msg_01BhXyjA2PTwGC5J3JNnfAXY\\\",\\n\",\n-      \"      model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"      stop_reason: \\\"tool_use\\\",\\n\",\n-      \"      stop_sequence: null,\\n\",\n-      \"      usage: { input_tokens: 365, output_tokens: 53 }\\n\",\n+      \"    \\\"response_metadata\\\": {\\n\",\n+      \"      \\\"tokenUsage\\\": {\\n\",\n+      \"        \\\"completionTokens\\\": 14,\\n\",\n+      \"        \\\"promptTokens\\\": 55,\\n\",\n+      \"        \\\"totalTokens\\\": 69\\n\",\n+      \"      },\\n\",\n+      \"      \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"      \\\"system_fingerprint\\\": \\\"fp_483d39d857\\\"\\n\",\n       \"    },\\n\",\n-      \"    tool_calls: [\\n\",\n+      \"    \\\"tool_calls\\\": [\\n\",\n       \"      {\\n\",\n-      \"        name: \\\"magic_function\\\",\\n\",\n-      \"        args: { input: 3 },\\n\",\n-      \"        id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\"\\n\",\n+      \"        \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"        \\\"args\\\": {\\n\",\n+      \"          \\\"input\\\": 3\\n\",\n+      \"        },\\n\",\n+      \"        \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"        \\\"id\\\": \\\"call_ndsPDU58wsMeGaqr41cSlLlF\\\"\\n\",\n       \"      }\\n\",\n       \"    ],\\n\",\n-      \"    invalid_tool_calls: []\\n\",\n+      \"    \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"    \\\"usage_metadata\\\": {\\n\",\n+      \"      \\\"input_tokens\\\": 55,\\n\",\n+      \"      \\\"output_tokens\\\": 14,\\n\",\n+      \"      \\\"total_tokens\\\": 69\\n\",\n+      \"    }\\n\",\n       \"  },\\n\",\n       \"  ToolMessage {\\n\",\n-      \"    lc_serializable: true,\\n\",\n-      \"    lc_kwargs: {\\n\",\n-      \"      name: \\\"magic_function\\\",\\n\",\n-      \"      content: \\\"5\\\",\\n\",\n-      \"      tool_call_id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\",\\n\",\n-      \"      additional_kwargs: {},\\n\",\n-      \"      response_metadata: {}\\n\",\n-      \"    },\\n\",\n-      \"    lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"    content: \\\"5\\\",\\n\",\n-      \"    name: \\\"magic_function\\\",\\n\",\n-      \"    additional_kwargs: {},\\n\",\n-      \"    response_metadata: {},\\n\",\n-      \"    tool_call_id: \\\"toolu_01L2N6TKrZxyUWRCQZ5qLYVj\\\"\\n\",\n+      \"    \\\"id\\\": \\\"ac6aa309-bbfb-46cd-ba27-cbdbfd848705\\\",\\n\",\n+      \"    \\\"content\\\": \\\"5\\\",\\n\",\n+      \"    \\\"name\\\": \\\"magic_function\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {},\\n\",\n+      \"    \\\"tool_call_id\\\": \\\"call_ndsPDU58wsMeGaqr41cSlLlF\\\"\\n\",\n       \"  },\\n\",\n       \"  AIMessage {\\n\",\n-      \"    lc_serializable: true,\\n\",\n-      \"    lc_kwargs: {\\n\",\n-      \"      content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"      tool_calls: [],\\n\",\n-      \"      invalid_tool_calls: [],\\n\",\n-      \"      additional_kwargs: {\\n\",\n-      \"        id: \\\"msg_01ABtcXJ4CwMHphYYmffQZoF\\\",\\n\",\n-      \"        type: \\\"message\\\",\\n\",\n-      \"        role: \\\"assistant\\\",\\n\",\n-      \"        model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"        stop_reason: \\\"end_turn\\\",\\n\",\n-      \"        stop_sequence: null,\\n\",\n-      \"        usage: { input_tokens: 431, output_tokens: 17 }\\n\",\n+      \"    \\\"id\\\": \\\"chatcmpl-A7f0i7iHyDUV6is6sgwtcXivmFZ1x\\\",\\n\",\n+      \"    \\\"content\\\": \\\"The value of `magic_function(3)` is 5.\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {\\n\",\n+      \"      \\\"tokenUsage\\\": {\\n\",\n+      \"        \\\"completionTokens\\\": 14,\\n\",\n+      \"        \\\"promptTokens\\\": 78,\\n\",\n+      \"        \\\"totalTokens\\\": 92\\n\",\n       \"      },\\n\",\n-      \"      response_metadata: {}\\n\",\n-      \"    },\\n\",\n-      \"    lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-      \"    content: \\\"The value of magic_function(3) is 5.\\\",\\n\",\n-      \"    name: undefined,\\n\",\n-      \"    additional_kwargs: {\\n\",\n-      \"      id: \\\"msg_01ABtcXJ4CwMHphYYmffQZoF\\\",\\n\",\n-      \"      type: \\\"message\\\",\\n\",\n-      \"      role: \\\"assistant\\\",\\n\",\n-      \"      model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"      stop_reason: \\\"end_turn\\\",\\n\",\n-      \"      stop_sequence: null,\\n\",\n-      \"      usage: { input_tokens: 431, output_tokens: 17 }\\n\",\n-      \"    },\\n\",\n-      \"    response_metadata: {\\n\",\n-      \"      id: \\\"msg_01ABtcXJ4CwMHphYYmffQZoF\\\",\\n\",\n-      \"      model: \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"      stop_reason: \\\"end_turn\\\",\\n\",\n-      \"      stop_sequence: null,\\n\",\n-      \"      usage: { input_tokens: 431, output_tokens: 17 }\\n\",\n+      \"      \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"      \\\"system_fingerprint\\\": \\\"fp_54e2f484be\\\"\\n\",\n       \"    },\\n\",\n-      \"    tool_calls: [],\\n\",\n-      \"    invalid_tool_calls: []\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"    \\\"usage_metadata\\\": {\\n\",\n+      \"      \\\"input_tokens\\\": 78,\\n\",\n+      \"      \\\"output_tokens\\\": 14,\\n\",\n+      \"      \\\"total_tokens\\\": 92\\n\",\n+      \"    }\\n\",\n       \"  }\\n\",\n       \"]\\n\"\n      ]\n@@ -1327,7 +1201,7 @@\n    \"source\": [\n     \"agentOutput = await app.invoke({\\n\",\n     \"  messages: [\\n\",\n-    \"    new HumanMessage(query)\\n\",\n+    \"    { role: \\\"user\\\", content: query },\\n\",\n     \"  ]\\n\",\n     \"});\\n\",\n     \"\\n\",\n@@ -1349,229 +1223,20 @@\n     \"limit, so we will need to multiply by two (and add one) to get equivalent\\n\",\n     \"results.\\n\",\n     \"\\n\",\n-    \"If the recursion limit is reached, LangGraph raises a specific exception type,\\n\",\n-    \"that we can catch and manage similarly to AgentExecutor.\\n\"\n+    \"Here's an example of how you'd set this parameter with the legacy `AgentExecutor`:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": null,\n    \"id\": \"1cca9d11\",\n    \"metadata\": {\n     \"lines_to_next_cell\": 2\n    },\n-   \"outputs\": [\n-    {\n-     \"name\": \"stdout\",\n-     \"output_type\": \"stream\",\n-     \"text\": [\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m\\u001b[1m1:chain:AgentExecutor\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"what is the value of magic_function(3)?\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > \\u001b[1m2:chain:ToolCallingAgent\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"  \\\"steps\\\": []\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m3:chain:RunnableAssign\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > \\u001b[1m4:chain:RunnableMap\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > \\u001b[1m5:chain:RunnableLambda\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > \\u001b[1m5:chain:RunnableLambda\\u001b[22m\\u001b[39m] [0ms] Exiting Chain run with output: {\\n\",\n-      \"  \\\"output\\\": []\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > \\u001b[1m4:chain:RunnableMap\\u001b[22m\\u001b[39m] [1ms] Exiting Chain run with output: {\\n\",\n-      \"  \\\"agent_scratchpad\\\": []\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m3:chain:RunnableAssign\\u001b[22m\\u001b[39m] [1ms] Exiting Chain run with output: {\\n\",\n-      \"  \\\"input\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"  \\\"steps\\\": [],\\n\",\n-      \"  \\\"agent_scratchpad\\\": []\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m6:prompt:ChatPromptTemplate\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"input\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"  \\\"steps\\\": [],\\n\",\n-      \"  \\\"agent_scratchpad\\\": []\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m6:prompt:ChatPromptTemplate\\u001b[22m\\u001b[39m] [0ms] Exiting Chain run with output: {\\n\",\n-      \"  \\\"lc\\\": 1,\\n\",\n-      \"  \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"  \\\"id\\\": [\\n\",\n-      \"    \\\"langchain_core\\\",\\n\",\n-      \"    \\\"prompt_values\\\",\\n\",\n-      \"    \\\"ChatPromptValue\\\"\\n\",\n-      \"  ],\\n\",\n-      \"  \\\"kwargs\\\": {\\n\",\n-      \"    \\\"messages\\\": [\\n\",\n-      \"      {\\n\",\n-      \"        \\\"lc\\\": 1,\\n\",\n-      \"        \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"        \\\"id\\\": [\\n\",\n-      \"          \\\"langchain_core\\\",\\n\",\n-      \"          \\\"messages\\\",\\n\",\n-      \"          \\\"SystemMessage\\\"\\n\",\n-      \"        ],\\n\",\n-      \"        \\\"kwargs\\\": {\\n\",\n-      \"          \\\"content\\\": \\\"You are a helpful assistant. Respond only in Spanish.\\\",\\n\",\n-      \"          \\\"additional_kwargs\\\": {},\\n\",\n-      \"          \\\"response_metadata\\\": {}\\n\",\n-      \"        }\\n\",\n-      \"      },\\n\",\n-      \"      {\\n\",\n-      \"        \\\"lc\\\": 1,\\n\",\n-      \"        \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"        \\\"id\\\": [\\n\",\n-      \"          \\\"langchain_core\\\",\\n\",\n-      \"          \\\"messages\\\",\\n\",\n-      \"          \\\"HumanMessage\\\"\\n\",\n-      \"        ],\\n\",\n-      \"        \\\"kwargs\\\": {\\n\",\n-      \"          \\\"content\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"          \\\"additional_kwargs\\\": {},\\n\",\n-      \"          \\\"response_metadata\\\": {}\\n\",\n-      \"        }\\n\",\n-      \"      }\\n\",\n-      \"    ]\\n\",\n-      \"  }\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[llm/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m7:llm:ChatAnthropic\\u001b[22m\\u001b[39m] Entering LLM run with input: {\\n\",\n-      \"  \\\"messages\\\": [\\n\",\n-      \"    [\\n\",\n-      \"      {\\n\",\n-      \"        \\\"lc\\\": 1,\\n\",\n-      \"        \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"        \\\"id\\\": [\\n\",\n-      \"          \\\"langchain_core\\\",\\n\",\n-      \"          \\\"messages\\\",\\n\",\n-      \"          \\\"SystemMessage\\\"\\n\",\n-      \"        ],\\n\",\n-      \"        \\\"kwargs\\\": {\\n\",\n-      \"          \\\"content\\\": \\\"You are a helpful assistant. Respond only in Spanish.\\\",\\n\",\n-      \"          \\\"additional_kwargs\\\": {},\\n\",\n-      \"          \\\"response_metadata\\\": {}\\n\",\n-      \"        }\\n\",\n-      \"      },\\n\",\n-      \"      {\\n\",\n-      \"        \\\"lc\\\": 1,\\n\",\n-      \"        \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"        \\\"id\\\": [\\n\",\n-      \"          \\\"langchain_core\\\",\\n\",\n-      \"          \\\"messages\\\",\\n\",\n-      \"          \\\"HumanMessage\\\"\\n\",\n-      \"        ],\\n\",\n-      \"        \\\"kwargs\\\": {\\n\",\n-      \"          \\\"content\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"          \\\"additional_kwargs\\\": {},\\n\",\n-      \"          \\\"response_metadata\\\": {}\\n\",\n-      \"        }\\n\",\n-      \"      }\\n\",\n-      \"    ]\\n\",\n-      \"  ]\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[llm/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m7:llm:ChatAnthropic\\u001b[22m\\u001b[39m] [1.56s] Exiting LLM run with output: {\\n\",\n-      \"  \\\"generations\\\": [\\n\",\n-      \"    [\\n\",\n-      \"      {\\n\",\n-      \"        \\\"text\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\",\\n\",\n-      \"        \\\"message\\\": {\\n\",\n-      \"          \\\"lc\\\": 1,\\n\",\n-      \"          \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"          \\\"id\\\": [\\n\",\n-      \"            \\\"langchain_core\\\",\\n\",\n-      \"            \\\"messages\\\",\\n\",\n-      \"            \\\"AIMessageChunk\\\"\\n\",\n-      \"          ],\\n\",\n-      \"          \\\"kwargs\\\": {\\n\",\n-      \"            \\\"content\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\",\\n\",\n-      \"            \\\"additional_kwargs\\\": {\\n\",\n-      \"              \\\"id\\\": \\\"msg_011b4GnLtiCRnCzZiqUBAZeH\\\",\\n\",\n-      \"              \\\"type\\\": \\\"message\\\",\\n\",\n-      \"              \\\"role\\\": \\\"assistant\\\",\\n\",\n-      \"              \\\"model\\\": \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"              \\\"stop_reason\\\": \\\"end_turn\\\",\\n\",\n-      \"              \\\"stop_sequence\\\": null,\\n\",\n-      \"              \\\"usage\\\": {\\n\",\n-      \"                \\\"input_tokens\\\": 378,\\n\",\n-      \"                \\\"output_tokens\\\": 59\\n\",\n-      \"              }\\n\",\n-      \"            },\\n\",\n-      \"            \\\"tool_call_chunks\\\": [],\\n\",\n-      \"            \\\"tool_calls\\\": [],\\n\",\n-      \"            \\\"invalid_tool_calls\\\": [],\\n\",\n-      \"            \\\"response_metadata\\\": {}\\n\",\n-      \"          }\\n\",\n-      \"        }\\n\",\n-      \"      }\\n\",\n-      \"    ]\\n\",\n-      \"  ]\\n\",\n-      \"}\\n\",\n-      \"\\u001b[32m[chain/start]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m8:parser:ToolCallingAgentOutputParser\\u001b[22m\\u001b[39m] Entering Chain run with input: {\\n\",\n-      \"  \\\"lc\\\": 1,\\n\",\n-      \"  \\\"type\\\": \\\"constructor\\\",\\n\",\n-      \"  \\\"id\\\": [\\n\",\n-      \"    \\\"langchain_core\\\",\\n\",\n-      \"    \\\"messages\\\",\\n\",\n-      \"    \\\"AIMessageChunk\\\"\\n\",\n-      \"  ],\\n\",\n-      \"  \\\"kwargs\\\": {\\n\",\n-      \"    \\\"content\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\",\\n\",\n-      \"    \\\"additional_kwargs\\\": {\\n\",\n-      \"      \\\"id\\\": \\\"msg_011b4GnLtiCRnCzZiqUBAZeH\\\",\\n\",\n-      \"      \\\"type\\\": \\\"message\\\",\\n\",\n-      \"      \\\"role\\\": \\\"assistant\\\",\\n\",\n-      \"      \\\"model\\\": \\\"claude-3-haiku-20240307\\\",\\n\",\n-      \"      \\\"stop_reason\\\": \\\"end_turn\\\",\\n\",\n-      \"      \\\"stop_sequence\\\": null,\\n\",\n-      \"      \\\"usage\\\": {\\n\",\n-      \"        \\\"input_tokens\\\": 378,\\n\",\n-      \"        \\\"output_tokens\\\": 59\\n\",\n-      \"      }\\n\",\n-      \"    },\\n\",\n-      \"    \\\"tool_call_chunks\\\": [],\\n\",\n-      \"    \\\"tool_calls\\\": [],\\n\",\n-      \"    \\\"invalid_tool_calls\\\": [],\\n\",\n-      \"    \\\"response_metadata\\\": {}\\n\",\n-      \"  }\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > 2:chain:ToolCallingAgent > \\u001b[1m8:parser:ToolCallingAgentOutputParser\\u001b[22m\\u001b[39m] [0ms] Exiting Chain run with output: {\\n\",\n-      \"  \\\"returnValues\\\": {\\n\",\n-      \"    \\\"output\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\"\\n\",\n-      \"  },\\n\",\n-      \"  \\\"log\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m1:chain:AgentExecutor > \\u001b[1m2:chain:ToolCallingAgent\\u001b[22m\\u001b[39m] [1.56s] Exiting Chain run with output: {\\n\",\n-      \"  \\\"returnValues\\\": {\\n\",\n-      \"    \\\"output\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\"\\n\",\n-      \"  },\\n\",\n-      \"  \\\"log\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\"\\n\",\n-      \"}\\n\",\n-      \"\\u001b[36m[chain/end]\\u001b[39m [\\u001b[90m\\u001b[1m1:chain:AgentExecutor\\u001b[22m\\u001b[39m] [1.56s] Exiting Chain run with output: {\\n\",\n-      \"  \\\"input\\\": \\\"what is the value of magic_function(3)?\\\",\\n\",\n-      \"  \\\"output\\\": \\\"Lo siento, pero la función \\\\\\\"magic_function\\\\\\\" espera un parámetro de tipo \\\\\\\"string\\\\\\\", no un número entero. Por favor, proporciona una entrada de tipo cadena de texto para que pueda aplicar la función mágica.\\\"\\n\",\n-      \"}\\n\"\n-     ]\n-    },\n-    {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"{\\n\",\n-       \"  input: \\u001b[32m\\\"what is the value of magic_function(3)?\\\"\\u001b[39m,\\n\",\n-       \"  output: \\u001b[32m'Lo siento, pero la función \\\"magic_function\\\" espera un parámetro de tipo \\\"string\\\", no un número enter'\\u001b[39m... 103 more characters\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 14,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n-    }\n-   ],\n+   \"outputs\": [],\n    \"source\": [\n-    \"const badMagicTool = tool(async ({ input }) => {\\n\",\n-    \"  return \\\"Sorry, there was an error. Please try again.\\\";\\n\",\n+    \"const badMagicTool = tool(async ({ input: _input }) => {\\n\",\n+    \"  return \\\"Sorry, there was a temporary error. Please try again with the same input.\\\";\\n\",\n     \"}, {\\n\",\n     \"  name: \\\"magic_function\\\",\\n\",\n     \"  description: \\\"Applies a magic function to an input.\\\",\\n\",\n@@ -1593,12 +1258,20 @@\n     \"  maxIterations: 2,\\n\",\n     \"});\\n\",\n     \"\\n\",\n-    \"await spanishAgentExecutorWithMaxIterations.invoke({ input: query });\\n\"\n+    \"await spanishAgentExecutorWithMaxIterations.invoke({ input: query });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"245e064c\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"If the recursion limit is reached in LangGraph.js, the framework will raise a specific exception type that we can catch and manage similarly to AgentExecutor.\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 16,\n    \"id\": \"2f5e7d58\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1620,7 +1293,7 @@\n     \"try {\\n\",\n     \"  await appWithBadTools.invoke({\\n\",\n     \"    messages: [\\n\",\n-    \"      new HumanMessage(query)\\n\",\n+    \"      { role: \\\"user\\\", content: query }\\n\",\n     \"    ]\\n\",\n     \"  }, {\\n\",\n     \"    recursionLimit: RECURSION_LIMIT,\\n\",\n@@ -1645,14 +1318,6 @@\n     \"\\n\",\n     \"Next, check out other [LangGraph how-to guides](https://langchain-ai.github.io/langgraphjs/how-tos/).\"\n    ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": null,\n-   \"id\": \"3f5bf788\",\n-   \"metadata\": {},\n-   \"outputs\": [],\n-   \"source\": []\n   }\n  ],\n  \"metadata\": {",
          "docs/core_docs/docs/how_to/output_parser_structured.ipynb": "@@ -140,7 +140,7 @@\n    \"id\": \"75976cd6-78e2-458b-821f-3ddf3683466b\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Output parsers implement the [Runnable interface](/docs/how_to/#langchain-expression-language-lcel), the basic building block of the [LangChain Expression Language (LCEL)](/docs/how_to/#langchain-expression-language-lcel). This means they support `invoke`, `stream`, `batch`, `streamLog` calls.\\n\",\n+    \"Output parsers implement the [Runnable interface](/docs/how_to/#langchain-expression-language-lcel), the basic building block of [LangChain Expression Language (LCEL)](/docs/how_to/#langchain-expression-language-lcel). This means they support `invoke`, `stream`, `batch`, `streamLog` calls.\\n\",\n     \"\\n\",\n     \"## Validation\\n\",\n     \"\\n\",",
          "docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb": "@@ -4,24 +4,29 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"# How to add chat history to a question-answering chain\\n\",\n+        \"# How to add chat history\\n\",\n         \"\\n\",\n-        \":::info Prerequisites\\n\",\n         \"\\n\",\n-        \"This guide assumes familiarity with the following:\\n\",\n+        \":::note\\n\",\n         \"\\n\",\n-        \"- [Retrieval-augmented generation](/docs/tutorials/rag/)\\n\",\n+        \"This tutorial previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).\\n\",\n+        \"\\n\",\n+        \"The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).\\n\",\n         \"\\n\",\n         \":::\\n\",\n         \"\\n\",\n         \"In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \\\"memory\\\" of past questions and answers, and some logic for incorporating those into its current thinking.\\n\",\n         \"\\n\",\n-        \"In this guide we focus on **adding logic for incorporating historical messages, and NOT on chat history management.** Chat history management is [covered here](/docs/how_to/message_history).\\n\",\n+        \"In this guide we focus on **adding logic for incorporating historical messages.**\\n\",\n+        \"\\n\",\n+        \"This is largely a condensed version of the [Conversational RAG tutorial](/docs/tutorials/qa_chat_history).\\n\",\n+        \"\\n\",\n+        \"We will cover two approaches:\\n\",\n         \"\\n\",\n-        \"We'll work off of the Q&A app we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng. We'll need to update two things about our existing app:\\n\",\n+        \"1. [Chains](/docs/how_to/qa_chat_history_how_to#chains), in which we always execute a retrieval step;\\n\",\n+        \"2. [Agents](/docs/how_to/qa_chat_history_how_to#agents), in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\\n\",\n         \"\\n\",\n-        \"1. **Prompt**: Update our prompt to support historical messages as an input.\\n\",\n-        \"2. **Contextualizing questions**: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This is needed in case the latest question references some context from past messages. For example, if a user asks a follow-up question like \\\"Can you elaborate on the second point?\\\", this cannot be understood without the context of the previous message. Therefore we can't effectively perform retrieval with a question like this.\"\n+        \"For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag).\"\n       ]\n     },\n     {\n@@ -36,7 +41,7 @@\n         \"We’ll use the following packages:\\n\",\n         \"\\n\",\n         \"```bash\\n\",\n-        \"npm install --save langchain @langchain/openai cheerio\\n\",\n+        \"npm install --save langchain @langchain/openai langchain cheerio uuid\\n\",\n         \"```\\n\",\n         \"\\n\",\n         \"We need to set environment variable `OPENAI_API_KEY`:\\n\",\n@@ -66,6 +71,43 @@\n         \"```\"\n       ]\n     },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Chains {#chains}\\n\",\n+        \"\\n\",\n+        \"In a conversational RAG application, queries issued to the retriever should be informed by the context of the conversation. LangChain provides a [createHistoryAwareRetriever](https://api.js.langchain.com/functions/langchain.chains_history_aware_retriever.createHistoryAwareRetriever.html) constructor to simplify this. It constructs a chain that accepts keys `input` and `chat_history` as input, and has the same output schema as a retriever. `createHistoryAwareRetriever` requires as inputs:  \\n\",\n+        \"\\n\",\n+        \"1. LLM;\\n\",\n+        \"2. Retriever;\\n\",\n+        \"3. Prompt.\\n\",\n+        \"\\n\",\n+        \"First we obtain these objects:\\n\",\n+        \"\\n\",\n+        \"### LLM\\n\",\n+        \"\\n\",\n+        \"We can use any supported chat model:\\n\",\n+        \"\\n\",\n+        \"```{=mdx}\\n\",\n+        \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\"\\n\",\n+        \"\\n\",\n+        \"<ChatModelTabs customVarName=\\\"llm\\\" />\\n\",\n+        \"```\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 1,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"// @lc-docs-hide-cell\\n\",\n+        \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+        \"\\n\",\n+        \"const llm = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\"\n+      ]\n+    },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n@@ -75,21 +117,14 @@\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": null,\n+      \"execution_count\": 2,\n       \"metadata\": {},\n       \"outputs\": [],\n       \"source\": [\n-        \"import \\\"cheerio\\\";\\n\",\n         \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n         \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n         \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\"\\n\",\n-        \"import { OpenAIEmbeddings, ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n-        \"import { pull } from \\\"langchain/hub\\\";\\n\",\n-        \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n-        \"import { RunnableSequence, RunnablePassthrough } from \\\"@langchain/core/runnables\\\";\\n\",\n-        \"import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";\\n\",\n-        \"\\n\",\n-        \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n+        \"import { OpenAIEmbeddings } from \\\"@langchain/openai\\\";\\n\",\n         \"\\n\",\n         \"const loader = new CheerioWebBaseLoader(\\n\",\n         \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n\",\n@@ -102,293 +137,998 @@\n         \"const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\\n\",\n         \"\\n\",\n         \"// Retrieve and generate using the relevant snippets of the blog.\\n\",\n-        \"const retriever = vectorStore.asRetriever();\\n\",\n-        \"// Tip - you can edit this!\\n\",\n-        \"const prompt = await pull<ChatPromptTemplate>(\\\"rlm/rag-prompt\\\");\\n\",\n-        \"const llm = new ChatOpenAI({ model: \\\"gpt-3.5-turbo\\\", temperature: 0 });\\n\",\n-        \"const ragChain = await createStuffDocumentsChain({\\n\",\n+        \"const retriever = vectorStore.asRetriever();\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"### Prompt\\n\",\n+        \"\\n\",\n+        \"We'll use a prompt that includes a `MessagesPlaceholder` variable under the name \\\"chat_history\\\". This allows us to pass in a list of Messages to the prompt using the \\\"chat_history\\\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 3,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+        \"\\n\",\n+        \"const contextualizeQSystemPrompt = (\\n\",\n+        \"  \\\"Given a chat history and the latest user question \\\" +\\n\",\n+        \"  \\\"which might reference context in the chat history, \\\" +\\n\",\n+        \"  \\\"formulate a standalone question which can be understood \\\" +\\n\",\n+        \"  \\\"without the chat history. Do NOT answer the question, \\\" +\\n\",\n+        \"  \\\"just reformulate it if needed and otherwise return it as is.\\\"\\n\",\n+        \")\\n\",\n+        \"\\n\",\n+        \"const contextualizeQPrompt = ChatPromptTemplate.fromMessages(\\n\",\n+        \"  [\\n\",\n+        \"    [\\\"system\\\", contextualizeQSystemPrompt],\\n\",\n+        \"    new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+        \"    [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+        \"  ]\\n\",\n+        \")\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"### Assembling the chain\\n\",\n+        \"\\n\",\n+        \"We can then instantiate the history-aware retriever:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 5,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { createHistoryAwareRetriever } from \\\"langchain/chains/history_aware_retriever\\\";\\n\",\n+        \"\\n\",\n+        \"const historyAwareRetriever = await createHistoryAwareRetriever({\\n\",\n         \"  llm,\\n\",\n-        \"  prompt,\\n\",\n-        \"  outputParser: new StringOutputParser(),\\n\",\n+        \"  retriever,\\n\",\n+        \"  rephrasePrompt: contextualizeQPrompt\\n\",\n+        \"});\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.\\n\",\n+        \"\\n\",\n+        \"Now we can build our full QA chain.\\n\",\n+        \"\\n\",\n+        \"As in the [RAG tutorial](/docs/tutorials/rag), we will use [createStuffDocumentsChain](https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html) to generate a `questionAnswerChain`, with input keys `context`, `chat_history`, and `input`-- it accepts the retrieved context alongside the conversation history and query to generate an answer.\\n\",\n+        \"\\n\",\n+        \"We build our final `ragChain` with [createRetrievalChain](https://api.js.langchain.com/functions/langchain.chains_retrieval.createRetrievalChain.html). This chain applies the `historyAwareRetriever` and `questionAnswerChain` in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys `input` and `chat_history`, and includes `input`, `chat_history`, `context`, and `answer` in its output.\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 7,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n+        \"import { createRetrievalChain } from \\\"langchain/chains/retrieval\\\";\\n\",\n+        \"\\n\",\n+        \"const systemPrompt = \\n\",\n+        \"  \\\"You are an assistant for question-answering tasks. \\\" +\\n\",\n+        \"  \\\"Use the following pieces of retrieved context to answer \\\" +\\n\",\n+        \"  \\\"the question. If you don't know the answer, say that you \\\" +\\n\",\n+        \"  \\\"don't know. Use three sentences maximum and keep the \\\" +\\n\",\n+        \"  \\\"answer concise.\\\" +\\n\",\n+        \"  \\\"\\\\n\\\\n\\\" +\\n\",\n+        \"  \\\"{context}\\\";\\n\",\n+        \"\\n\",\n+        \"const qaPrompt = ChatPromptTemplate.fromMessages([\\n\",\n+        \"  [\\\"system\\\", systemPrompt],\\n\",\n+        \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+        \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+        \"]);\\n\",\n+        \"\\n\",\n+        \"const questionAnswerChain = await createStuffDocumentsChain({\\n\",\n+        \"  llm,\\n\",\n+        \"  prompt: qaPrompt,\\n\",\n+        \"});\\n\",\n+        \"\\n\",\n+        \"const ragChain = await createRetrievalChain({\\n\",\n+        \"  retriever: historyAwareRetriever,\\n\",\n+        \"  combineDocsChain: questionAnswerChain,\\n\",\n         \"});\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"Let's see what this prompt actually looks like\"\n+        \"### Stateful Management of chat history\\n\",\n+        \"\\n\",\n+        \"We have added application logic for incorporating chat history, but we are still manually plumbing it through our application. In production, the Q&A application we usually persist the chat history into a database, and be able to read and update it appropriately.\\n\",\n+        \"\\n\",\n+        \"[LangGraph](https://langchain-ai.github.io/langgraphjs/) implements a built-in [persistence layer](https://langchain-ai.github.io/langgraphjs/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.\\n\",\n+        \"\\n\",\n+        \"Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\n\",\n+        \"\\n\",\n+        \"LangGraph comes with a simple [in-memory checkpointer](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.MemorySaver.html), which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\n\",\n+        \"\\n\",\n+        \"For a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\"\n       ]\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": 2,\n+      \"execution_count\": 8,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { AIMessage, BaseMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+        \"import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from \\\"@langchain/langgraph\\\";\\n\",\n+        \"\\n\",\n+        \"// Define the State interface\\n\",\n+        \"const GraphAnnotation = Annotation.Root({\\n\",\n+        \"  input: Annotation<string>(),\\n\",\n+        \"  chat_history: Annotation<BaseMessage[]>({\\n\",\n+        \"    reducer: messagesStateReducer,\\n\",\n+        \"    default: () => [],\\n\",\n+        \"  }),\\n\",\n+        \"  context: Annotation<string>(),\\n\",\n+        \"  answer: Annotation<string>(),\\n\",\n+        \"})\\n\",\n+        \"\\n\",\n+        \"// Define the call_model function\\n\",\n+        \"async function callModel(state: typeof GraphAnnotation.State) {\\n\",\n+        \"  const response = await ragChain.invoke(state);\\n\",\n+        \"  return {\\n\",\n+        \"    chat_history: [\\n\",\n+        \"      new HumanMessage(state.input),\\n\",\n+        \"      new AIMessage(response.answer),\\n\",\n+        \"    ],\\n\",\n+        \"    context: response.context,\\n\",\n+        \"    answer: response.answer,\\n\",\n+        \"  };\\n\",\n+        \"}\\n\",\n+        \"\\n\",\n+        \"// Create the workflow\\n\",\n+        \"const workflow = new StateGraph(GraphAnnotation)\\n\",\n+        \"  .addNode(\\\"model\\\", callModel)\\n\",\n+        \"  .addEdge(START, \\\"model\\\")\\n\",\n+        \"  .addEdge(\\\"model\\\", END);\\n\",\n+        \"\\n\",\n+        \"// Compile the graph with a checkpointer object\\n\",\n+        \"const memory = new MemorySaver();\\n\",\n+        \"const app = workflow.compile({ checkpointer: memory });\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 9,\n       \"metadata\": {},\n       \"outputs\": [\n         {\n           \"name\": \"stdout\",\n           \"output_type\": \"stream\",\n           \"text\": [\n-            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\",\n-            \"Question: {question} \\n\",\n-            \"Context: {context} \\n\",\n-            \"Answer:\\n\"\n+            \"Task Decomposition is the process of breaking down a complicated task into smaller, simpler, and more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts expand on this by enabling agents to think step by step or explore multiple reasoning possibilities at each step. This allows for a more structured and interpretable approach to handling complex tasks.\\n\"\n           ]\n         }\n       ],\n       \"source\": [\n-        \"console.log(prompt.promptMessages.map((msg) => msg.prompt.template).join(\\\"\\\\n\\\"));\"\n+        \"import { v4 as uuidv4 } from \\\"uuid\\\";\\n\",\n+        \"\\n\",\n+        \"const threadId = uuidv4();\\n\",\n+        \"const config = { configurable: { thread_id: threadId } };\\n\",\n+        \"\\n\",\n+        \"const result = await app.invoke(\\n\",\n+        \"  { input: \\\"What is Task Decomposition?\\\" },\\n\",\n+        \"  config,\\n\",\n+        \")\\n\",\n+        \"console.log(result.answer);\"\n       ]\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": 3,\n+      \"execution_count\": 10,\n       \"metadata\": {},\n       \"outputs\": [\n         {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"\\u001b[32m\\\"Task Decomposition involves breaking down complex tasks into smaller and simpler steps to make them \\\"\\u001b[39m... 243 more characters\"\n-            ]\n-          },\n-          \"execution_count\": 3,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"One way of doing task decomposition is by using an LLM with simple prompting, such as asking \\\"Steps for XYZ.\\\\n1.\\\" or \\\"What are the subgoals for achieving XYZ?\\\" This method leverages direct prompts to guide the model in breaking down tasks.\\n\"\n+          ]\n         }\n       ],\n       \"source\": [\n-        \"await ragChain.invoke({\\n\",\n-        \"  context: await retriever.invoke(\\\"What is Task Decomposition?\\\"),\\n\",\n-        \"  question: \\\"What is Task Decomposition?\\\"\\n\",\n-        \"});\"\n+        \"const result2 = await app.invoke(\\n\",\n+        \"  { input: \\\"What is one way of doing it?\\\" },\\n\",\n+        \"  config,\\n\",\n+        \")\\n\",\n+        \"console.log(result2.answer);\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Contextualizing the question\\n\",\n+        \"The conversation history can be inspected via the state of the application:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 11,\n+      \"metadata\": {},\n+      \"outputs\": [\n+        {\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"HumanMessage {\\n\",\n+            \"  \\\"content\\\": \\\"What is Task Decomposition?\\\",\\n\",\n+            \"  \\\"additional_kwargs\\\": {},\\n\",\n+            \"  \\\"response_metadata\\\": {}\\n\",\n+            \"}\\n\",\n+            \"AIMessage {\\n\",\n+            \"  \\\"content\\\": \\\"Task Decomposition is the process of breaking down a complicated task into smaller, simpler, and more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts expand on this by enabling agents to think step by step or explore multiple reasoning possibilities at each step. This allows for a more structured and interpretable approach to handling complex tasks.\\\",\\n\",\n+            \"  \\\"additional_kwargs\\\": {},\\n\",\n+            \"  \\\"response_metadata\\\": {},\\n\",\n+            \"  \\\"tool_calls\\\": [],\\n\",\n+            \"  \\\"invalid_tool_calls\\\": []\\n\",\n+            \"}\\n\",\n+            \"HumanMessage {\\n\",\n+            \"  \\\"content\\\": \\\"What is one way of doing it?\\\",\\n\",\n+            \"  \\\"additional_kwargs\\\": {},\\n\",\n+            \"  \\\"response_metadata\\\": {}\\n\",\n+            \"}\\n\",\n+            \"AIMessage {\\n\",\n+            \"  \\\"content\\\": \\\"One way of doing task decomposition is by using an LLM with simple prompting, such as asking \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\" This method leverages direct prompts to guide the model in breaking down tasks.\\\",\\n\",\n+            \"  \\\"additional_kwargs\\\": {},\\n\",\n+            \"  \\\"response_metadata\\\": {},\\n\",\n+            \"  \\\"tool_calls\\\": [],\\n\",\n+            \"  \\\"invalid_tool_calls\\\": []\\n\",\n+            \"}\\n\"\n+          ]\n+        }\n+      ],\n+      \"source\": [\n+        \"const chatHistory = (await app.getState(config)).values.chat_history;\\n\",\n+        \"for (const message of chatHistory) {\\n\",\n+        \"  console.log(message);\\n\",\n+        \"}\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"### Tying it together\\n\",\n         \"\\n\",\n-        \"First we'll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.\\n\",\n+        \"![](../../static/img/conversational_retrieval_chain.png)\\n\",\n         \"\\n\",\n-        \"We'll use a prompt that includes a `MessagesPlaceholder` variable under the name \\\"chat_history\\\". This allows us to pass in a list of Messages to the prompt using the \\\"chat_history\\\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\"\n+        \"For convenience, we tie together all of the necessary steps in a single code cell:\"\n       ]\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": 4,\n+      \"execution_count\": 12,\n       \"metadata\": {},\n-      \"outputs\": [],\n+      \"outputs\": [\n+        {\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"Task Decomposition is the process of breaking a complicated task into smaller, simpler steps to enhance model performance on complex tasks. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used for this, with CoT focusing on step-by-step thinking and ToT exploring multiple reasoning possibilities at each step. Decomposition can be carried out by the LLM itself, using task-specific instructions, or through human inputs.\\n\",\n+            \"One way of doing task decomposition is by prompting the LLM with simple instructions such as \\\"Steps for XYZ.\\\\n1.\\\" or \\\"What are the subgoals for achieving XYZ?\\\" This encourages the model to break down the task into smaller, manageable steps on its own.\\n\"\n+          ]\n+        }\n+      ],\n       \"source\": [\n+        \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n+        \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n+        \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\"\\n\",\n+        \"import { OpenAIEmbeddings, ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n         \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+        \"import { createHistoryAwareRetriever } from \\\"langchain/chains/history_aware_retriever\\\";\\n\",\n+        \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n+        \"import { createRetrievalChain } from \\\"langchain/chains/retrieval\\\";\\n\",\n+        \"import { AIMessage, BaseMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+        \"import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from \\\"@langchain/langgraph\\\";\\n\",\n+        \"import { v4 as uuidv4 } from \\\"uuid\\\";\\n\",\n         \"\\n\",\n-        \"const contextualizeQSystemPrompt = `Given a chat history and the latest user question\\n\",\n-        \"which might reference context in the chat history, formulate a standalone question\\n\",\n-        \"which can be understood without the chat history. Do NOT answer the question,\\n\",\n-        \"just reformulate it if needed and otherwise return it as is.`;\\n\",\n+        \"const llm2 = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\",\n         \"\\n\",\n-        \"const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-        \"  [\\\"system\\\", contextualizeQSystemPrompt],\\n\",\n+        \"const loader2 = new CheerioWebBaseLoader(\\n\",\n+        \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n\",\n+        \");\\n\",\n+        \"\\n\",\n+        \"const docs2 = await loader2.load();\\n\",\n+        \"\\n\",\n+        \"const textSplitter2 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n+        \"const splits2 = await textSplitter2.splitDocuments(docs2);\\n\",\n+        \"const vectorStore2 = await MemoryVectorStore.fromDocuments(splits2, new OpenAIEmbeddings());\\n\",\n+        \"\\n\",\n+        \"// Retrieve and generate using the relevant snippets of the blog.\\n\",\n+        \"const retriever2 = vectorStore2.asRetriever();\\n\",\n+        \"\\n\",\n+        \"const contextualizeQSystemPrompt2 =\\n\",\n+        \"  \\\"Given a chat history and the latest user question \\\" +\\n\",\n+        \"  \\\"which might reference context in the chat history, \\\" +\\n\",\n+        \"  \\\"formulate a standalone question which can be understood \\\" +\\n\",\n+        \"  \\\"without the chat history. Do NOT answer the question, \\\" +\\n\",\n+        \"  \\\"just reformulate it if needed and otherwise return it as is.\\\";\\n\",\n+        \"\\n\",\n+        \"const contextualizeQPrompt2 = ChatPromptTemplate.fromMessages(\\n\",\n+        \"  [\\n\",\n+        \"    [\\\"system\\\", contextualizeQSystemPrompt2],\\n\",\n+        \"    new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+        \"    [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+        \"  ]\\n\",\n+        \")\\n\",\n+        \"\\n\",\n+        \"const historyAwareRetriever2 = await createHistoryAwareRetriever({\\n\",\n+        \"  llm: llm2,\\n\",\n+        \"  retriever: retriever2,\\n\",\n+        \"  rephrasePrompt: contextualizeQPrompt2\\n\",\n+        \"});\\n\",\n+        \"\\n\",\n+        \"const systemPrompt2 = \\n\",\n+        \"  \\\"You are an assistant for question-answering tasks. \\\" +\\n\",\n+        \"  \\\"Use the following pieces of retrieved context to answer \\\" +\\n\",\n+        \"  \\\"the question. If you don't know the answer, say that you \\\" +\\n\",\n+        \"  \\\"don't know. Use three sentences maximum and keep the \\\" +\\n\",\n+        \"  \\\"answer concise.\\\" +\\n\",\n+        \"  \\\"\\\\n\\\\n\\\" +\\n\",\n+        \"  \\\"{context}\\\";\\n\",\n+        \"\\n\",\n+        \"const qaPrompt2 = ChatPromptTemplate.fromMessages([\\n\",\n+        \"  [\\\"system\\\", systemPrompt2],\\n\",\n         \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-        \"  [\\\"human\\\", \\\"{question}\\\"]\\n\",\n+        \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n         \"]);\\n\",\n-        \"const contextualizeQChain = contextualizeQPrompt.pipe(llm).pipe(new StringOutputParser());\"\n+        \"\\n\",\n+        \"const questionAnswerChain2 = await createStuffDocumentsChain({\\n\",\n+        \"  llm: llm2,\\n\",\n+        \"  prompt: qaPrompt2,\\n\",\n+        \"});\\n\",\n+        \"\\n\",\n+        \"const ragChain2 = await createRetrievalChain({\\n\",\n+        \"  retriever: historyAwareRetriever2,\\n\",\n+        \"  combineDocsChain: questionAnswerChain2,\\n\",\n+        \"});\\n\",\n+        \"\\n\",\n+        \"// Define the State interface\\n\",\n+        \"const GraphAnnotation2 = Annotation.Root({\\n\",\n+        \"  input: Annotation<string>(),\\n\",\n+        \"  chat_history: Annotation<BaseMessage[]>({\\n\",\n+        \"    reducer: messagesStateReducer,\\n\",\n+        \"    default: () => [],\\n\",\n+        \"  }),\\n\",\n+        \"  context: Annotation<string>(),\\n\",\n+        \"  answer: Annotation<string>(),\\n\",\n+        \"})\\n\",\n+        \"\\n\",\n+        \"// Define the call_model function\\n\",\n+        \"async function callModel2(state: typeof GraphAnnotation2.State) {\\n\",\n+        \"  const response = await ragChain2.invoke(state);\\n\",\n+        \"  return {\\n\",\n+        \"    chat_history: [\\n\",\n+        \"      new HumanMessage(state.input),\\n\",\n+        \"      new AIMessage(response.answer),\\n\",\n+        \"    ],\\n\",\n+        \"    context: response.context,\\n\",\n+        \"    answer: response.answer,\\n\",\n+        \"  };\\n\",\n+        \"}\\n\",\n+        \"\\n\",\n+        \"// Create the workflow\\n\",\n+        \"const workflow2 = new StateGraph(GraphAnnotation2)\\n\",\n+        \"  .addNode(\\\"model\\\", callModel2)\\n\",\n+        \"  .addEdge(START, \\\"model\\\")\\n\",\n+        \"  .addEdge(\\\"model\\\", END);\\n\",\n+        \"\\n\",\n+        \"// Compile the graph with a checkpointer object\\n\",\n+        \"const memory2 = new MemorySaver();\\n\",\n+        \"const app2 = workflow2.compile({ checkpointer: memory2 });\\n\",\n+        \"\\n\",\n+        \"const threadId2 = uuidv4();\\n\",\n+        \"const config2 = { configurable: { thread_id: threadId2 } };\\n\",\n+        \"\\n\",\n+        \"const result3 = await app2.invoke(\\n\",\n+        \"  { input: \\\"What is Task Decomposition?\\\" },\\n\",\n+        \"  config2,\\n\",\n+        \")\\n\",\n+        \"console.log(result3.answer);\\n\",\n+        \"\\n\",\n+        \"const result4 = await app2.invoke(\\n\",\n+        \"  { input: \\\"What is one way of doing it?\\\" },\\n\",\n+        \"  config2,\\n\",\n+        \")\\n\",\n+        \"console.log(result4.answer);\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"Using this chain we can ask follow-up questions that reference past messages and have them reformulated into standalone questions:\"\n+        \"## Agents {#agents}\\n\",\n+        \"\\n\",\n+        \"Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:\\n\",\n+        \"- Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;\\n\",\n+        \"- Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user).\\n\",\n+        \"\\n\",\n+        \"### Retrieval tool\\n\",\n+        \"\\n\",\n+        \"Agents can access \\\"tools\\\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:\"\n       ]\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": 5,\n+      \"execution_count\": 13,\n       \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"\\u001b[32m'What is the definition of \\\"large\\\" in this context?'\\u001b[39m\"\n-            ]\n-          },\n-          \"execution_count\": 5,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n-        }\n-      ],\n+      \"outputs\": [],\n       \"source\": [\n-        \"import { AIMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+        \"import { createRetrieverTool } from \\\"langchain/tools/retriever\\\";\\n\",\n         \"\\n\",\n-        \"await contextualizeQChain.invoke({\\n\",\n-        \"  chat_history: [\\n\",\n-        \"    new HumanMessage(\\\"What does LLM stand for?\\\"),\\n\",\n-        \"    new AIMessage(\\\"Large language model\\\") \\n\",\n-        \"  ],\\n\",\n-        \"  question: \\\"What is meant by large\\\",\\n\",\n-        \"})\"\n+        \"const tool =  createRetrieverTool(\\n\",\n+        \"    retriever,\\n\",\n+        \"    {\\n\",\n+        \"      name: \\\"blog_post_retriever\\\",\\n\",\n+        \"      description: \\\"Searches and returns excerpts from the Autonomous Agents blog post.\\\",\\n\",\n+        \"    }\\n\",\n+        \")\\n\",\n+        \"const tools = [tool]\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Chain with chat history\\n\",\n+        \"### Agent constructor\\n\",\n         \"\\n\",\n-        \"And now we can build our full QA chain. \\n\",\n+        \"Now that we have defined the tools and the LLM, we can create the agent. We will be using [LangGraph](/docs/concepts/#langgraph) to construct the agent. \\n\",\n+        \"Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 16,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n         \"\\n\",\n-        \"Notice we add some routing functionality to only run the \\\"condense question chain\\\" when our chat history isn't empty. Here we're taking advantage of the fact that if a function in an LCEL chain returns another chain, that chain will itself be invoked.\"\n+        \"const agentExecutor = createReactAgent({ llm, tools })\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now try it out. Note that so far it is not stateful (we still need to add in memory)\"\n       ]\n     },\n     {\n       \"cell_type\": \"code\",\n-      \"execution_count\": 6,\n+      \"execution_count\": 17,\n       \"metadata\": {},\n       \"outputs\": [\n         {\n           \"name\": \"stdout\",\n           \"output_type\": \"stream\",\n           \"text\": [\n-            \"AIMessage {\\n\",\n-            \"  lc_serializable: true,\\n\",\n-            \"  lc_kwargs: {\\n\",\n-            \"    content: \\\"Task decomposition involves breaking down a complex task into smaller and simpler steps to make it m\\\"... 358 more characters,\\n\",\n-            \"    tool_calls: [],\\n\",\n-            \"    invalid_tool_calls: [],\\n\",\n-            \"    additional_kwargs: { function_call: undefined, tool_calls: undefined },\\n\",\n-            \"    response_metadata: {}\\n\",\n-            \"  },\\n\",\n-            \"  lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-            \"  content: \\\"Task decomposition involves breaking down a complex task into smaller and simpler steps to make it m\\\"... 358 more characters,\\n\",\n-            \"  name: undefined,\\n\",\n-            \"  additional_kwargs: { function_call: undefined, tool_calls: undefined },\\n\",\n-            \"  response_metadata: {\\n\",\n-            \"    tokenUsage: { completionTokens: 83, promptTokens: 701, totalTokens: 784 },\\n\",\n-            \"    finish_reason: \\\"stop\\\"\\n\",\n-            \"  },\\n\",\n-            \"  tool_calls: [],\\n\",\n-            \"  invalid_tool_calls: []\\n\",\n-            \"}\\n\"\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7xlcJBGSKSp1GvgDY9FP8KvXxwB\\\",\\n\",\n+            \"        \\\"content\\\": \\\"\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {\\n\",\n+            \"          \\\"tool_calls\\\": [\\n\",\n+            \"            {\\n\",\n+            \"              \\\"id\\\": \\\"call_Ev0nA6nzGwOeMC5upJUUxTuw\\\",\\n\",\n+            \"              \\\"type\\\": \\\"function\\\",\\n\",\n+            \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+            \"            }\\n\",\n+            \"          ]\\n\",\n+            \"        },\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 19,\\n\",\n+            \"            \\\"promptTokens\\\": 66,\\n\",\n+            \"            \\\"totalTokens\\\": 85\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [\\n\",\n+            \"          {\\n\",\n+            \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"            \\\"args\\\": {\\n\",\n+            \"              \\\"query\\\": \\\"Task Decomposition\\\"\\n\",\n+            \"            },\\n\",\n+            \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+            \"            \\\"id\\\": \\\"call_Ev0nA6nzGwOeMC5upJUUxTuw\\\"\\n\",\n+            \"          }\\n\",\n+            \"        ],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 66,\\n\",\n+            \"          \\\"output_tokens\\\": 19,\\n\",\n+            \"          \\\"total_tokens\\\": 85\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  tools: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      ToolMessage {\\n\",\n+            \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\nAgent System Overview\\\\n                \\\\n                    Component One: Planning\\\\n                        \\\\n                \\\\n                    Task Decomposition\\\\n                \\\\n                    Self-Reflection\\\\n                \\\\n                \\\\n                    Component Two: Memory\\\\n                        \\\\n                \\\\n                    Types of Memory\\\\n                \\\\n                    Maximum Inner Product Search (MIPS)\\\\n                \\\\n                \\\\n                    Component Three: Tool Use\\\\n                \\\\n                    Case Studies\\\\n                        \\\\n                \\\\n                    Scientific Discovery Agent\\\\n                \\\\n                    Generative Agents Simulation\\\\n                \\\\n                    Proof-of-Concept Examples\\\\n                \\\\n                \\\\n                    Challenges\\\\n                \\\\n                    Citation\\\\n                \\\\n                    References\\\\n\\\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\\",\\n\",\n+            \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {},\\n\",\n+            \"        \\\"tool_call_id\\\": \\\"call_Ev0nA6nzGwOeMC5upJUUxTuw\\\"\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7xmiPNPbMX2KvZKHM2oPfcoFMnY\\\",\\n\",\n+            \"        \\\"content\\\": \\\"**Task Decomposition** involves breaking down a complicated or large task into smaller, more manageable subtasks. Here are some insights based on current techniques and research:\\\\n\\\\n1. **Chain of Thought (CoT)**:\\\\n   - Introduced by Wei et al. (2022), this technique prompts the model to \\\\\\\"think step by step\\\\\\\".\\\\n   - It helps decompose hard tasks into several simpler steps.\\\\n   - Enhances the interpretability of the model's thought process.\\\\n\\\\n2. **Tree of Thoughts (ToT)**:\\\\n   - An extension of CoT by Yao et al. (2023).\\\\n   - Decomposes problems into multiple thought steps and generates several possibilities at each step.\\\\n   - Utilizes tree structures through BFS (Breadth-First Search) or DFS (Depth-First Search) with evaluation by a classifier or majority vote.\\\\n\\\\n3. **Methods of Task Decomposition**:\\\\n   - **Simple Prompting**: Asking the model directly, e.g., \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n   - **Task-Specific Instructions**: Tailoring instructions to the task, such as \\\\\\\"Write a story outline\\\\\\\" for writing a novel.\\\\n   - **Human Inputs**: Receiving inputs from humans to refine the process.\\\\n\\\\n4. **LLM+P Approach**:\\\\n   - Suggested by Liu et al. (2023), combines language models with an external classical planner.\\\\n   - Uses Planning Domain Definition Language (PDDL) for long-horizon planning:\\\\n     1. Translates the problem into a PDDL problem.\\\\n     2. Requests an external planner to generate a PDDL plan.\\\\n     3. Translates the PDDL plan back into natural language.\\\\n   - This method offloads the planning complexity to a specialized tool, especially relevant for domains utilizing robotic setups.\\\\n\\\\nTask Decomposition is a fundamental component of planning in autonomous agent systems, aiding in the efficient accomplishment of complex tasks by breaking them into smaller, actionable steps.\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 411,\\n\",\n+            \"            \\\"promptTokens\\\": 732,\\n\",\n+            \"            \\\"totalTokens\\\": 1143\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 732,\\n\",\n+            \"          \\\"output_tokens\\\": 411,\\n\",\n+            \"          \\\"total_tokens\\\": 1143\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\"\n           ]\n-        },\n-        {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"AIMessage {\\n\",\n-              \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-              \"  lc_kwargs: {\\n\",\n-              \"    content: \\u001b[32m\\\"Common ways of task decomposition include using simple prompting techniques like Chain of Thought (C\\\"\\u001b[39m... 353 more characters,\\n\",\n-              \"    tool_calls: [],\\n\",\n-              \"    invalid_tool_calls: [],\\n\",\n-              \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-              \"    response_metadata: {}\\n\",\n-              \"  },\\n\",\n-              \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-              \"  content: \\u001b[32m\\\"Common ways of task decomposition include using simple prompting techniques like Chain of Thought (C\\\"\\u001b[39m... 353 more characters,\\n\",\n-              \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-              \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m },\\n\",\n-              \"  response_metadata: {\\n\",\n-              \"    tokenUsage: { completionTokens: \\u001b[33m81\\u001b[39m, promptTokens: \\u001b[33m779\\u001b[39m, totalTokens: \\u001b[33m860\\u001b[39m },\\n\",\n-              \"    finish_reason: \\u001b[32m\\\"stop\\\"\\u001b[39m\\n\",\n-              \"  },\\n\",\n-              \"  tool_calls: [],\\n\",\n-              \"  invalid_tool_calls: []\\n\",\n-              \"}\"\n-            ]\n-          },\n-          \"execution_count\": 6,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n         }\n       ],\n       \"source\": [\n-        \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\"\\n\",\n-        \"import { RunnablePassthrough, RunnableSequence } from \\\"@langchain/core/runnables\\\";\\n\",\n-        \"import { formatDocumentsAsString } from \\\"langchain/util/document\\\";\\n\",\n+        \"const query = \\\"What is Task Decomposition?\\\"\\n\",\n         \"\\n\",\n-        \"const qaSystemPrompt = `You are an assistant for question-answering tasks.\\n\",\n-        \"Use the following pieces of retrieved context to answer the question.\\n\",\n-        \"If you don't know the answer, just say that you don't know.\\n\",\n-        \"Use three sentences maximum and keep the answer concise.\\n\",\n+        \"for await (const s of await agentExecutor.stream(\\n\",\n+        \"  { messages: [{ role: \\\"user\\\", content: query }] },\\n\",\n+        \")){\\n\",\n+        \"  console.log(s)\\n\",\n+        \"  console.log(\\\"----\\\")\\n\",\n+        \"}\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"LangGraph comes with built in persistence, so we don't need to use `ChatMessageHistory`! Rather, we can pass in a checkpointer to our LangGraph agent directly.\\n\",\n         \"\\n\",\n-        \"{context}`\\n\",\n+        \"Distinct conversations are managed by specifying a key for a conversation thread in the config object, as shown below.\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 19,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n         \"\\n\",\n-        \"const qaPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-        \"  [\\\"system\\\", qaSystemPrompt],\\n\",\n-        \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-        \"  [\\\"human\\\", \\\"{question}\\\"]\\n\",\n-        \"]);\\n\",\n+        \"const memory3 = new MemorySaver();\\n\",\n         \"\\n\",\n-        \"const contextualizedQuestion = (input: Record<string, unknown>) => {\\n\",\n-        \"  if (\\\"chat_history\\\" in input) {\\n\",\n-        \"    return contextualizeQChain;\\n\",\n-        \"  }\\n\",\n-        \"  return input.question;\\n\",\n-        \"};\\n\",\n-        \"\\n\",\n-        \"const ragChain = RunnableSequence.from([\\n\",\n-        \"  RunnablePassthrough.assign({\\n\",\n-        \"    context: async (input: Record<string, unknown>) => {\\n\",\n-        \"      if (\\\"chat_history\\\" in input) {\\n\",\n-        \"        const chain = contextualizedQuestion(input);\\n\",\n-        \"        return chain.pipe(retriever).pipe(formatDocumentsAsString);\\n\",\n-        \"      }\\n\",\n-        \"      return \\\"\\\";\\n\",\n-        \"    },\\n\",\n-        \"  }),\\n\",\n-        \"  qaPrompt,\\n\",\n-        \"  llm\\n\",\n-        \"]);\\n\",\n+        \"const agentExecutor2 = createReactAgent({ llm, tools, checkpointSaver: memory3 })\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"This is all we need to construct a conversational RAG agent.\\n\",\n         \"\\n\",\n-        \"const chat_history = [];\\n\",\n+        \"Let's observe its behavior. Note that if we input a query that does not require a retrieval step, the agent does not execute one:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 20,\n+      \"metadata\": {},\n+      \"outputs\": [\n+        {\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7y8P8AGHkxOwKpwMc3qj6r0skYr\\\",\\n\",\n+            \"        \\\"content\\\": \\\"Hello, Bob! How can I assist you today?\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 12,\\n\",\n+            \"            \\\"promptTokens\\\": 64,\\n\",\n+            \"            \\\"totalTokens\\\": 76\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 64,\\n\",\n+            \"          \\\"output_tokens\\\": 12,\\n\",\n+            \"          \\\"total_tokens\\\": 76\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\"\n+          ]\n+        }\n+      ],\n+      \"source\": [\n+        \"const threadId3 = uuidv4();\\n\",\n+        \"const config3 = { configurable: { thread_id: threadId3 } };\\n\",\n         \"\\n\",\n-        \"const question = \\\"What is task decomposition?\\\";\\n\",\n-        \"const aiMsg = await ragChain.invoke({ question, chat_history });\\n\",\n+        \"for await (const s of await agentExecutor2.stream({ messages: [{ role: \\\"user\\\", content: \\\"Hi! I'm bob\\\" }] }, config3)) {\\n\",\n+        \"  console.log(s)\\n\",\n+        \"  console.log(\\\"----\\\")\\n\",\n+        \"}\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Further, if we input a query that does require a retrieval step, the agent generates the input to the tool:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 21,\n+      \"metadata\": {},\n+      \"outputs\": [\n+        {\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7y8Do2IHJ2rnUvvMU3pTggmuZud\\\",\\n\",\n+            \"        \\\"content\\\": \\\"\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {\\n\",\n+            \"          \\\"tool_calls\\\": [\\n\",\n+            \"            {\\n\",\n+            \"              \\\"id\\\": \\\"call_3tSaOZ3xdKY4miIJdvBMR80V\\\",\\n\",\n+            \"              \\\"type\\\": \\\"function\\\",\\n\",\n+            \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+            \"            }\\n\",\n+            \"          ]\\n\",\n+            \"        },\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 19,\\n\",\n+            \"            \\\"promptTokens\\\": 89,\\n\",\n+            \"            \\\"totalTokens\\\": 108\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [\\n\",\n+            \"          {\\n\",\n+            \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"            \\\"args\\\": {\\n\",\n+            \"              \\\"query\\\": \\\"Task Decomposition\\\"\\n\",\n+            \"            },\\n\",\n+            \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+            \"            \\\"id\\\": \\\"call_3tSaOZ3xdKY4miIJdvBMR80V\\\"\\n\",\n+            \"          }\\n\",\n+            \"        ],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 89,\\n\",\n+            \"          \\\"output_tokens\\\": 19,\\n\",\n+            \"          \\\"total_tokens\\\": 108\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  tools: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      ToolMessage {\\n\",\n+            \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\nAgent System Overview\\\\n                \\\\n                    Component One: Planning\\\\n                        \\\\n                \\\\n                    Task Decomposition\\\\n                \\\\n                    Self-Reflection\\\\n                \\\\n                \\\\n                    Component Two: Memory\\\\n                        \\\\n                \\\\n                    Types of Memory\\\\n                \\\\n                    Maximum Inner Product Search (MIPS)\\\\n                \\\\n                \\\\n                    Component Three: Tool Use\\\\n                \\\\n                    Case Studies\\\\n                        \\\\n                \\\\n                    Scientific Discovery Agent\\\\n                \\\\n                    Generative Agents Simulation\\\\n                \\\\n                    Proof-of-Concept Examples\\\\n                \\\\n                \\\\n                    Challenges\\\\n                \\\\n                    Citation\\\\n                \\\\n                    References\\\\n\\\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\\",\\n\",\n+            \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {},\\n\",\n+            \"        \\\"tool_call_id\\\": \\\"call_3tSaOZ3xdKY4miIJdvBMR80V\\\"\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7y9tpoTvM3lsrhoxCWkkerk9fb2\\\",\\n\",\n+            \"        \\\"content\\\": \\\"Task decomposition is a methodology used to break down complex tasks into smaller, more manageable steps. Here’s an overview of various approaches to task decomposition:\\\\n\\\\n1. **Chain of Thought (CoT)**: This technique prompts a model to \\\\\\\"think step by step,\\\\\\\" which aids in transforming big tasks into multiple smaller tasks. This method enhances the model’s performance on complex tasks by making the problem more manageable and interpretable.\\\\n\\\\n2. **Tree of Thoughts (ToT)**: An extension of Chain of Thought, this approach explores multiple reasoning possibilities at each step, effectively creating a tree structure. The search process can be carried out using Breadth-First Search (BFS) or Depth-First Search (DFS), with each state evaluated by either a classifier or a majority vote.\\\\n\\\\n3. **Simple Prompting**: Involves straightforward instructions to decompose a task, such as starting with \\\\\\\"Steps for XYZ. 1.\\\\\\\" or asking \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\". This can also include task-specific instructions like \\\\\\\"Write a story outline\\\\\\\" for writing a novel.\\\\n\\\\n4. **LLM+P**: Combines Large Language Models (LLMs) with an external classical planner. The problem is translated into a Planning Domain Definition Language (PDDL) format, an external planner generates a plan, and then the plan is translated back into natural language. This approach highlights a synergy between modern AI techniques and traditional planning strategies.\\\\n\\\\nThese approaches allow complex problems to be approached and solved more efficiently by focusing on manageable sub-tasks.\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 311,\\n\",\n+            \"            \\\"promptTokens\\\": 755,\\n\",\n+            \"            \\\"totalTokens\\\": 1066\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 755,\\n\",\n+            \"          \\\"output_tokens\\\": 311,\\n\",\n+            \"          \\\"total_tokens\\\": 1066\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\"\n+          ]\n+        }\n+      ],\n+      \"source\": [\n+        \"const query2 = \\\"What is Task Decomposition?\\\"\\n\",\n         \"\\n\",\n-        \"console.log(aiMsg)\\n\",\n+        \"for await (const s of await agentExecutor2.stream({ messages: [{ role: \\\"user\\\", content: query2 }] }, config3)) {\\n\",\n+        \"  console.log(s)\\n\",\n+        \"  console.log(\\\"----\\\")\\n\",\n+        \"}\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Above, instead of inserting our query verbatim into the tool, the agent stripped unnecessary words like \\\"what\\\" and \\\"is\\\".\\n\",\n         \"\\n\",\n-        \"chat_history.push(aiMsg);\\n\",\n+        \"This same principle allows the agent to use the context of the conversation when necessary:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 22,\n+      \"metadata\": {},\n+      \"outputs\": [\n+        {\n+          \"name\": \"stdout\",\n+          \"output_type\": \"stream\",\n+          \"text\": [\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7yDE4rCOXTPZ3595GknUgVzASmt\\\",\\n\",\n+            \"        \\\"content\\\": \\\"\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {\\n\",\n+            \"          \\\"tool_calls\\\": [\\n\",\n+            \"            {\\n\",\n+            \"              \\\"id\\\": \\\"call_cWnDZq2aloVtMB4KjZlTxHmZ\\\",\\n\",\n+            \"              \\\"type\\\": \\\"function\\\",\\n\",\n+            \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+            \"            }\\n\",\n+            \"          ]\\n\",\n+            \"        },\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 21,\\n\",\n+            \"            \\\"promptTokens\\\": 1089,\\n\",\n+            \"            \\\"totalTokens\\\": 1110\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [\\n\",\n+            \"          {\\n\",\n+            \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"            \\\"args\\\": {\\n\",\n+            \"              \\\"query\\\": \\\"common ways of task decomposition\\\"\\n\",\n+            \"            },\\n\",\n+            \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+            \"            \\\"id\\\": \\\"call_cWnDZq2aloVtMB4KjZlTxHmZ\\\"\\n\",\n+            \"          }\\n\",\n+            \"        ],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 1089,\\n\",\n+            \"          \\\"output_tokens\\\": 21,\\n\",\n+            \"          \\\"total_tokens\\\": 1110\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  tools: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      ToolMessage {\\n\",\n+            \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\nAgent System Overview\\\\n                \\\\n                    Component One: Planning\\\\n                        \\\\n                \\\\n                    Task Decomposition\\\\n                \\\\n                    Self-Reflection\\\\n                \\\\n                \\\\n                    Component Two: Memory\\\\n                        \\\\n                \\\\n                    Types of Memory\\\\n                \\\\n                    Maximum Inner Product Search (MIPS)\\\\n                \\\\n                \\\\n                    Component Three: Tool Use\\\\n                \\\\n                    Case Studies\\\\n                        \\\\n                \\\\n                    Scientific Discovery Agent\\\\n                \\\\n                    Generative Agents Simulation\\\\n                \\\\n                    Proof-of-Concept Examples\\\\n                \\\\n                \\\\n                    Challenges\\\\n                \\\\n                    Citation\\\\n                \\\\n                    References\\\\n\\\\nResources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\\",\\n\",\n+            \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {},\\n\",\n+            \"        \\\"tool_call_id\\\": \\\"call_cWnDZq2aloVtMB4KjZlTxHmZ\\\"\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\",\n+            \"{\\n\",\n+            \"  agent: {\\n\",\n+            \"    messages: [\\n\",\n+            \"      AIMessage {\\n\",\n+            \"        \\\"id\\\": \\\"chatcmpl-AB7yGASxz0Z0g2jiCxwx4gYHYJTi4\\\",\\n\",\n+            \"        \\\"content\\\": \\\"According to the blog post, there are several common methods of task decomposition:\\\\n\\\\n1. **Simple Prompting by LLMs**: This involves straightforward instructions to decompose a task. Examples include:\\\\n   - \\\\\\\"Steps for XYZ. 1.\\\\\\\"\\\\n   - \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\"\\\\n   - Task-specific instructions like \\\\\\\"Write a story outline\\\\\\\" for writing a novel.\\\\n\\\\n2. **Human Inputs**: Decomposition can be guided by human insights and instructions.\\\\n\\\\n3. **Chain of Thought (CoT)**: This technique prompts a model to think step-by-step, enabling it to break down complex tasks into smaller, more manageable tasks. CoT has become a standard method to enhance model performance on intricate tasks.\\\\n\\\\n4. **Tree of Thoughts (ToT)**: An extension of CoT, this approach decomposes the problem into multiple thought steps and generates several thoughts per step, forming a tree structure. The search process can be performed using Breadth-First Search (BFS) or Depth-First Search (DFS), with each state evaluated by a classifier or through a majority vote.\\\\n\\\\n5. **LLM+P (Large Language Model plus Planner)**: This method integrates LLMs with an external classical planner. It involves:\\\\n   - Translating the problem into “Problem PDDL” (Planning Domain Definition Language).\\\\n   - Using an external planner to generate a PDDL plan based on an existing “Domain PDDL”.\\\\n   - Translating the PDDL plan back into natural language.\\\\n  \\\\nBy utilizing these methods, tasks can be effectively decomposed into more manageable parts, allowing for more efficient problem-solving and planning.\\\",\\n\",\n+            \"        \\\"additional_kwargs\\\": {},\\n\",\n+            \"        \\\"response_metadata\\\": {\\n\",\n+            \"          \\\"tokenUsage\\\": {\\n\",\n+            \"            \\\"completionTokens\\\": 334,\\n\",\n+            \"            \\\"promptTokens\\\": 1746,\\n\",\n+            \"            \\\"totalTokens\\\": 2080\\n\",\n+            \"          },\\n\",\n+            \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+            \"          \\\"system_fingerprint\\\": \\\"fp_52a7f40b0b\\\"\\n\",\n+            \"        },\\n\",\n+            \"        \\\"tool_calls\\\": [],\\n\",\n+            \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+            \"        \\\"usage_metadata\\\": {\\n\",\n+            \"          \\\"input_tokens\\\": 1746,\\n\",\n+            \"          \\\"output_tokens\\\": 334,\\n\",\n+            \"          \\\"total_tokens\\\": 2080\\n\",\n+            \"        }\\n\",\n+            \"      }\\n\",\n+            \"    ]\\n\",\n+            \"  }\\n\",\n+            \"}\\n\",\n+            \"----\\n\"\n+          ]\n+        }\n+      ],\n+      \"source\": [\n+        \"const query3 = \\\"What according to the blog post are common ways of doing it? redo the search\\\"\\n\",\n         \"\\n\",\n-        \"const secondQuestion = \\\"What are common ways of doing it?\\\";\\n\",\n-        \"await ragChain.invoke({ question: secondQuestion, chat_history });\"\n+        \"for await (const s of await agentExecutor2.stream({ messages: [{ role: \\\"user\\\", content: query3 }] }, config3)) {\\n\",\n+        \"  console.log(s)\\n\",\n+        \"  console.log(\\\"----\\\")\\n\",\n+        \"}\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"See the first [LangSmith trace here](https://smith.langchain.com/public/527981c6-5018-4b68-a11a-ebcde77843e7/r) and the [second trace here](https://smith.langchain.com/public/7b97994a-ab9f-4bf3-a2e4-abb609e5610a/r)\"\n+        \"Note that the agent was able to infer that \\\"it\\\" in our query refers to \\\"task decomposition\\\", and generated a reasonable search query as a result-- in this case, \\\"common ways of task decomposition\\\".\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"### Tying it together\\n\",\n+        \"\\n\",\n+        \"For convenience, we tie together all of the necessary steps in a single code cell:\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": 23,\n+      \"metadata\": {},\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import { createRetrieverTool } from \\\"langchain/tools/retriever\\\";\\n\",\n+        \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n+        \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+        \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+        \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n+        \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n+        \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\"\\n\",\n+        \"import { OpenAIEmbeddings } from \\\"@langchain/openai\\\";\\n\",\n+        \"\\n\",\n+        \"const llm3 = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\",\n+        \"\\n\",\n+        \"const loader3 = new CheerioWebBaseLoader(\\n\",\n+        \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n\",\n+        \");\\n\",\n+        \"\\n\",\n+        \"const docs3 = await loader3.load();\\n\",\n+        \"\\n\",\n+        \"const textSplitter3 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n+        \"const splits3 = await textSplitter3.splitDocuments(docs3);\\n\",\n+        \"const vectorStore3 = await MemoryVectorStore.fromDocuments(splits3, new OpenAIEmbeddings());\\n\",\n+        \"\\n\",\n+        \"// Retrieve and generate using the relevant snippets of the blog.\\n\",\n+        \"const retriever3 = vectorStore3.asRetriever();\\n\",\n+        \"\\n\",\n+        \"const tool2 = createRetrieverTool(\\n\",\n+        \"    retriever3,\\n\",\n+        \"    {\\n\",\n+        \"      name: \\\"blog_post_retriever\\\",\\n\",\n+        \"      description: \\\"Searches and returns excerpts from the Autonomous Agents blog post.\\\",\\n\",\n+        \"    }\\n\",\n+        \")\\n\",\n+        \"const tools2 = [tool2]\\n\",\n+        \"const memory4 = new MemorySaver();\\n\",\n+        \"\\n\",\n+        \"const agentExecutor3 = createReactAgent({ llm: llm3, tools: tools2, checkpointSaver: memory4 })\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it.\\n\",\n+        \"## Next steps\\n\",\n+        \"\\n\",\n+        \"We've covered the steps to build a basic conversational Q&A application:\\n\",\n         \"\\n\",\n-        \"For this we can use:\\n\",\n+        \"- We used chains to build a predictable application that generates search queries for each user input;\\n\",\n+        \"- We used agents to build an application that \\\"decides\\\" when and how to generate search queries.\\n\",\n         \"\\n\",\n-        \"- [BaseChatMessageHistory](https://api.js.langchain.com/classes/langchain_core.chat_history.BaseChatMessageHistory.html): Store chat history.\\n\",\n-        \"- [RunnableWithMessageHistory](/docs/how_to/message_history/): Wrapper for an LCEL chain and a `BaseChatMessageHistory` that handles injecting chat history into inputs and updating it after each invocation.\\n\",\n+        \"To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to#retrievers) section of the how-to guides.\\n\",\n         \"\\n\",\n-        \"For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the [How to add message history (memory)](/docs/how_to/message_history/) LCEL page.\"\n+        \"For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history) LCEL page.\\n\"\n       ]\n     }\n   ],\n   \"metadata\": {\n     \"kernelspec\": {\n-      \"display_name\": \"Deno\",\n+      \"display_name\": \"TypeScript\",\n       \"language\": \"typescript\",\n-      \"name\": \"deno\"\n+      \"name\": \"tslab\"\n     },\n     \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"mode\": \"typescript\",\n+        \"name\": \"javascript\",\n+        \"typescript\": true\n+      },\n       \"file_extension\": \".ts\",\n-      \"mimetype\": \"text/x.typescript\",\n+      \"mimetype\": \"text/typescript\",\n       \"name\": \"typescript\",\n-      \"nb_converter\": \"script\",\n-      \"pygments_lexer\": \"typescript\",\n-      \"version\": \"5.3.3\"\n+      \"version\": \"3.7.2\"\n     }\n   },\n   \"nbformat\": 4,",
          "docs/core_docs/docs/how_to/routing.mdx": "@@ -65,4 +65,4 @@ import BranchExample from \"@examples/guides/expression_language/how_to_routing_r\n \n You've now learned how to add routing to your composed LCEL chains.\n \n-Next, check out the other [how-to guides on runnables](/docs/how_to/#langchain-expression-language-lcel) in this section.\n+Next, check out the other [how-to guides on runnables](/docs/how_to/#langchain-expression-language) in this section.",
          "docs/core_docs/docs/how_to/streaming.ipynb": "@@ -11,7 +11,7 @@\n     \"This guide assumes familiarity with the following concepts:\\n\",\n     \"\\n\",\n     \"- [Chat models](/docs/concepts/#chat-models)\\n\",\n-    \"- [LangChain Expression Language](/docs/concepts/#langchain-expression-language-lcel)\\n\",\n+    \"- [LangChain Expression Language](/docs/concepts/#langchain-expression-language)\\n\",\n     \"- [Output parsers](/docs/concepts/#output-parsers)\\n\",\n     \"\\n\",\n     \":::\\n\",",
          "docs/core_docs/docs/how_to/tool_runtime.ipynb": "@@ -28,17 +28,13 @@\n     \"\\n\",\n     \"Most of the time, such values should not be controlled by the LLM. In fact, allowing the LLM to control the user ID may lead to a security risk.\\n\",\n     \"\\n\",\n-    \"Instead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.\\n\",\n-    \"\\n\",\n-    \"This how-to guide shows a design pattern that creates the tool dynamically at run time and binds to them appropriate values.\"\n+    \"Instead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We can bind them to chat models as follows:\\n\",\n-    \"\\n\",\n     \"```{=mdx}\\n\",\n     \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n     \"\\n\",\n@@ -48,26 +44,224 @@\n     \"```\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const llm = new ChatOpenAI({ model: \\\"gpt-4o-mini\\\" })\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# Passing request time information\\n\",\n+    \"## Using context variables\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \":::caution Compatibility\\n\",\n+    \"This functionality was added in `@langchain/core>=0.3.10`. If you are using the LangSmith SDK separately in your project, we also recommend upgrading to `langsmith>=0.1.65`. Please make sure your packages are up to date.\\n\",\n+    \"\\n\",\n+    \"It also requires [`async_hooks`](https://nodejs.org/api/async_hooks.html) support, which is not supported in all environments.\\n\",\n+    \":::\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"One way to solve this problem is by using **context variables**. Context variables are a powerful feature that allows you to set values at a higher level of your application, then access them within child runnable (such as tools) called from that level.\\n\",\n+    \"\\n\",\n+    \"They work outside of traditional scoping rules, so you don't need to have a direct reference to the declared variable to access its value.\\n\",\n+    \"\\n\",\n+    \"Below, we declare a tool that updates a central `userToPets` state based on a context variable called `userId`. Note that this `userId` is not part of the tool schema or input:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { z } from \\\"zod\\\";\\n\",\n+    \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n+    \"import { getContextVariable } from \\\"@langchain/core/context\\\";\\n\",\n+    \"\\n\",\n+    \"let userToPets: Record<string, string[]> = {};\\n\",\n+    \"\\n\",\n+    \"const updateFavoritePets = tool(async (input) => {\\n\",\n+    \"  const userId = getContextVariable(\\\"userId\\\");\\n\",\n+    \"  if (userId === undefined) {\\n\",\n+    \"    throw new Error(`No \\\"userId\\\" found in current context. Remember to call \\\"setContextVariable('userId', value)\\\";`);\\n\",\n+    \"  }\\n\",\n+    \"  userToPets[userId] = input.pets;\\n\",\n+    \"  return \\\"update_favorite_pets called.\\\"\\n\",\n+    \"}, {\\n\",\n+    \"  name: \\\"update_favorite_pets\\\",\\n\",\n+    \"  description: \\\"add to the list of favorite pets.\\\",\\n\",\n+    \"  schema: z.object({\\n\",\n+    \"    pets: z.array(z.string())\\n\",\n+    \"  }),\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"If you were to invoke the above tool before setting a context variable at a higher level, `userId` would be `undefined`:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Error: No \\\"userId\\\" found in current context. Remember to call \\\"setContextVariable('userId', value)\\\";\\n\",\n+      \"    at updateFavoritePets.name (evalmachine.<anonymous>:14:15)\\n\",\n+      \"    at /Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:329:33\\n\",\n+      \"    at AsyncLocalStorage.run (node:async_hooks:346:14)\\n\",\n+      \"    at AsyncLocalStorageProvider.runWithConfig (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/singletons/index.cjs:58:24)\\n\",\n+      \"    at /Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:325:68\\n\",\n+      \"    at new Promise (<anonymous>)\\n\",\n+      \"    at DynamicStructuredTool.func (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:321:20)\\n\",\n+      \"    at DynamicStructuredTool._call (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:283:21)\\n\",\n+      \"    at DynamicStructuredTool.call (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:111:33)\\n\",\n+      \"    at async evalmachine.<anonymous>:3:22\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"await updateFavoritePets.invoke({ pets: [\\\"cat\\\", \\\"dog\\\" ]})\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Instead, set a context variable with a parent of where the tools are invoked:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { setContextVariable } from \\\"@langchain/core/context\\\";\\n\",\n+    \"import { BaseChatModel } from \\\"@langchain/core/language_models/chat_models\\\";\\n\",\n+    \"import { RunnableLambda } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"\\n\",\n+    \"const handleRunTimeRequestRunnable = RunnableLambda.from(async (params: {\\n\",\n+    \"  userId: string;\\n\",\n+    \"  query: string;\\n\",\n+    \"  llm: BaseChatModel;\\n\",\n+    \"}) => {\\n\",\n+    \"  const { userId, query, llm } = params;\\n\",\n+    \"  if (!llm.bindTools) {\\n\",\n+    \"    throw new Error(\\\"Language model does not support tools.\\\");\\n\",\n+    \"  }\\n\",\n+    \"  // Set a context variable accessible to any child runnables called within this one.\\n\",\n+    \"  // You can also set context variables at top level that act as globals.\\n\",\n+    \"  setContextVariable(\\\"userId\\\", userId);\\n\",\n+    \"  const tools = [updateFavoritePets];\\n\",\n+    \"  const llmWithTools = llm.bindTools(tools);\\n\",\n+    \"  const modelResponse = await llmWithTools.invoke(query);\\n\",\n+    \"  // For simplicity, skip checking the tool call's name field and assume\\n\",\n+    \"  // that the model is calling the \\\"updateFavoritePets\\\" tool\\n\",\n+    \"  if (modelResponse.tool_calls.length > 0) {\\n\",\n+    \"    return updateFavoritePets.invoke(modelResponse.tool_calls[0]);\\n\",\n+    \"  } else {\\n\",\n+    \"    return \\\"No tool invoked.\\\";\\n\",\n+    \"  }\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"And when our method invokes the tools, you will see that the tool properly access the previously set `userId` context variable and runs successfully:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 5,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"ToolMessage {\\n\",\n+      \"  \\\"content\\\": \\\"update_favorite_pets called.\\\",\\n\",\n+      \"  \\\"name\\\": \\\"update_favorite_pets\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {},\\n\",\n+      \"  \\\"tool_call_id\\\": \\\"call_vsD2DbSpDquOtmFlOtbUME6h\\\"\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"await handleRunTimeRequestRunnable.invoke({\\n\",\n+    \"  userId: \\\"brace\\\",\\n\",\n+    \"  query: \\\"my favorite animals are cats and parrots.\\\",\\n\",\n+    \"  llm: llm\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"And have additionally updated the `userToPets` object with a key matching the `userId` we passed, `\\\"brace\\\"`:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{ brace: [ 'cats', 'parrots' ] }\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"console.log(userToPets);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Without context variables\\n\",\n+    \"\\n\",\n+    \"If you are on an earlier version of core or an environment that does not support `async_hooks`, you can use the following design pattern that creates the tool dynamically at run time and binds to them appropriate values.\\n\",\n     \"\\n\",\n     \"The idea is to create the tool dynamically at request time, and bind to it the appropriate information. For example,\\n\",\n     \"this information may be the user ID as resolved from the request itself.\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 1,\n+   \"execution_count\": 7,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n     \"import { z } from \\\"zod\\\";\\n\",\n     \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n     \"\\n\",\n-    \"const userToPets: Record<string, string[]> = {};\\n\",\n+    \"userToPets = {};\\n\",\n     \"\\n\",\n     \"function generateToolsForUser(userId: string) {\\n\",\n     \"  const updateFavoritePets = tool(async (input) => {\\n\",\n@@ -80,63 +274,42 @@\n     \"      pets: z.array(z.string())\\n\",\n     \"    }),\\n\",\n     \"  });\\n\",\n-    \"\\n\",\n-    \"  const deleteFavoritePets = tool(async () => {\\n\",\n-    \"    if (userId in userToPets) {\\n\",\n-    \"      delete userToPets[userId];\\n\",\n-    \"    }\\n\",\n-    \"    return \\\"delete_favorite_pets called.\\\";\\n\",\n-    \"  }, {\\n\",\n-    \"    name: \\\"delete_favorite_pets\\\",\\n\",\n-    \"    description: \\\"Delete the list of favorite pets.\\\",\\n\",\n-    \"    schema: z.object({}),\\n\",\n-    \"  });\\n\",\n-    \"\\n\",\n-    \"  const listFavoritePets = tool(async () => {\\n\",\n-    \"    return JSON.stringify(userToPets[userId] ?? []);\\n\",\n-    \"  }, {\\n\",\n-    \"    name: \\\"list_favorite_pets\\\",\\n\",\n-    \"    description: \\\"List favorite pets if any.\\\",\\n\",\n-    \"    schema: z.object({}),\\n\",\n-    \"  });\\n\",\n-    \"\\n\",\n-    \"  return [updateFavoritePets, deleteFavoritePets, listFavoritePets];\\n\",\n+    \"  // You can declare and return additional tools as well:\\n\",\n+    \"  return [updateFavoritePets];\\n\",\n     \"}\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Verify that the tools work correctly\"\n+    \"Verify that the tool works correctly\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 9,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"name\": \"stdout\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"{ brace: [ 'cat', 'dog' ] }\\n\",\n-      \"[\\\"cat\\\",\\\"dog\\\"]\\n\"\n+      \"{ cobb: [ 'tiger', 'wolf' ] }\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n-    \"const [updatePets, deletePets, listPets] = generateToolsForUser(\\\"brace\\\");\\n\",\n+    \"const [updatePets] = generateToolsForUser(\\\"cobb\\\");\\n\",\n     \"\\n\",\n-    \"await updatePets.invoke({ pets: [\\\"cat\\\", \\\"dog\\\"] });\\n\",\n+    \"await updatePets.invoke({ pets: [\\\"tiger\\\", \\\"wolf\\\"] });\\n\",\n     \"\\n\",\n-    \"console.log(userToPets);\\n\",\n-    \"console.log(await listPets.invoke({}));\"\n+    \"console.log(userToPets);\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 10,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -156,12 +329,12 @@\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"This code will allow the LLM to invoke the tools, but the LLM is **unaware** of the fact that a **user ID** even exists! You can see that `user_id` is not among the params the LLM generates:\"\n+    \"This code will allow the LLM to invoke the tools, but the LLM is **unaware** of the fact that a **user ID** even exists. You can see that `user_id` is not among the params the LLM generates:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 11,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n@@ -170,16 +343,16 @@\n      \"text\": [\n       \"{\\n\",\n       \"  name: 'update_favorite_pets',\\n\",\n-      \"  args: { pets: [ 'cats', 'parrots' ] },\\n\",\n+      \"  args: { pets: [ 'tigers', 'wolves' ] },\\n\",\n       \"  type: 'tool_call',\\n\",\n-      \"  id: 'call_97h0nQ3B3cr0m58HOwq9ZyUz'\\n\",\n+      \"  id: 'call_FBF4D51SkVK2clsLOQHX6wTv'\\n\",\n       \"}\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n     \"const aiMessage = await handleRunTimeRequest(\\n\",\n-    \"  \\\"brace\\\", \\\"my favorite animals are cats and parrots.\\\", llm,\\n\",\n+    \"  \\\"cobb\\\", \\\"my favorite pets are tigers and wolves.\\\", llm,\\n\",\n     \");\\n\",\n     \"console.log(aiMessage.tool_calls[0]);\"\n    ]",
          "docs/core_docs/docs/integrations/chat/cohere.ipynb": "@@ -11,6 +11,7 @@\n    \"source\": [\n     \"---\\n\",\n     \"sidebar_label: Cohere\\n\",\n+    \"lc_docs_skip_validation: true\\n\",\n     \"---\"\n    ]\n   },\n@@ -107,6 +108,40 @@\n     \"})\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"cd31a8b7\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.\\n\",\n+    \"\\n\",\n+    \"We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.\\n\",\n+    \"\\n\",\n+    \"**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and `apiKey` parameter in the constructor will be ignored.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"d92326b8\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { ChatCohere } from \\\"@langchain/cohere\\\";\\n\",\n+    \"import { CohereClient } from \\\"cohere-ai\\\";\\n\",\n+    \"\\n\",\n+    \"const client = new CohereClient({\\n\",\n+    \"  token: \\\"<your-api-key>\\\",\\n\",\n+    \"  environment: \\\"<your-cohere-deployment-url>\\\", //optional\\n\",\n+    \"  // other params\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const llmWithCustomClient = new ChatCohere({\\n\",\n+    \"  client,\\n\",\n+    \"  // other params...\\n\",\n+    \"});\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"id\": \"2b4f3e15\",",
          "docs/core_docs/docs/integrations/chat/groq.ipynb": "@@ -145,11 +145,11 @@\n    ],\n    \"source\": [\n     \"const aiMsg = await llm.invoke([\\n\",\n-    \"    [\\n\",\n-    \"        \\\"system\\\",\\n\",\n-    \"        \\\"You are a helpful assistant that translates English to French. Translate the user sentence.\\\",\\n\",\n-    \"    ],\\n\",\n-    \"    [\\\"human\\\", \\\"I love programming.\\\"],\\n\",\n+    \"    {\\n\",\n+    \"      role: \\\"system\\\",\\n\",\n+    \"      content: \\\"You are a helpful assistant that translates English to French. Translate the user sentence.\\\",\\n\",\n+    \"    },\\n\",\n+    \"    { role: \\\"user\\\", content: \\\"I love programming.\\\" },\\n\",\n     \"])\\n\",\n     \"aiMsg\"\n    ]\n@@ -174,6 +174,50 @@\n     \"console.log(aiMsg.content)\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"ce0414fe\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Json invocation\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"id\": \"3f0a7a2a\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  aiInvokeMsgContent: '{\\\\n\\\"result\\\": 6\\\\n}',\\n\",\n+      \"  aiBindMsg: '{\\\\n\\\"result\\\": 6\\\\n}'\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const messages = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"system\\\",\\n\",\n+    \"    content: \\\"You are a math tutor that handles math exercises and makes output in json in format { result: number }.\\\",\\n\",\n+    \"  },\\n\",\n+    \"  { role: \\\"user\\\",  content: \\\"2 + 2 * 2\\\" },\\n\",\n+    \"];\\n\",\n+    \"\\n\",\n+    \"const aiInvokeMsg = await llm.invoke(messages, { response_format: { type: \\\"json_object\\\" } });\\n\",\n+    \"\\n\",\n+    \"// if you want not to pass response_format in every invoke, you can bind it to the instance\\n\",\n+    \"const llmWithResponseFormat = llm.bind({ response_format: { type: \\\"json_object\\\" } });\\n\",\n+    \"const aiBindMsg = await llmWithResponseFormat.invoke(messages);\\n\",\n+    \"\\n\",\n+    \"// they are the same\\n\",\n+    \"console.log({ aiInvokeMsgContent: aiInvokeMsg.content, aiBindMsg: aiBindMsg.content });\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"id\": \"18e2bfc0-7e78-4528-a73f-499ac150dca8\",\n@@ -186,7 +230,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 5,\n    \"id\": \"e197d1d7-a070-4c96-9f8a-a0e86d046e0b\",\n    \"metadata\": {},\n    \"outputs\": [",
          "docs/core_docs/docs/integrations/chat/llama_cpp.mdx": "@@ -12,14 +12,14 @@ This module is based on the [node-llama-cpp](https://github.com/withcatai/node-l\n \n ## Setup\n \n-You'll need to install the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n+You'll need to install major version `2` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n \n import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n \n <IntegrationInstallTooltip></IntegrationInstallTooltip>\n \n ```bash npm2yarn\n-npm install -S node-llama-cpp @langchain/community @langchain/core\n+npm install -S node-llama-cpp@2 @langchain/community @langchain/core\n ```\n \n You will also need a local Llama 2 model (or a model supported by [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example).",
          "docs/core_docs/docs/integrations/chat/openai.ipynb": "@@ -97,9 +97,9 @@\n     \"import { ChatOpenAI } from \\\"@langchain/openai\\\" \\n\",\n     \"\\n\",\n     \"const llm = new ChatOpenAI({\\n\",\n-    \"    model: \\\"gpt-4o\\\",\\n\",\n-    \"    temperature: 0,\\n\",\n-    \"    // other params...\\n\",\n+    \"  model: \\\"gpt-4o\\\",\\n\",\n+    \"  temperature: 0,\\n\",\n+    \"  // other params...\\n\",\n     \"})\"\n    ]\n   },\n@@ -124,35 +124,39 @@\n      \"output_type\": \"stream\",\n      \"text\": [\n       \"AIMessage {\\n\",\n-      \"  \\\"id\\\": \\\"chatcmpl-9rB4GvhlRb0x3hxupLBQYOKKmTxvV\\\",\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ADItECqSPuuEuBHHPjeCkh9wIO1H5\\\",\\n\",\n       \"  \\\"content\\\": \\\"J'adore la programmation.\\\",\\n\",\n       \"  \\\"additional_kwargs\\\": {},\\n\",\n       \"  \\\"response_metadata\\\": {\\n\",\n       \"    \\\"tokenUsage\\\": {\\n\",\n-      \"      \\\"completionTokens\\\": 8,\\n\",\n+      \"      \\\"completionTokens\\\": 5,\\n\",\n       \"      \\\"promptTokens\\\": 31,\\n\",\n-      \"      \\\"totalTokens\\\": 39\\n\",\n+      \"      \\\"totalTokens\\\": 36\\n\",\n       \"    },\\n\",\n-      \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_5796ac6771\\\"\\n\",\n       \"  },\\n\",\n       \"  \\\"tool_calls\\\": [],\\n\",\n       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n       \"  \\\"usage_metadata\\\": {\\n\",\n       \"    \\\"input_tokens\\\": 31,\\n\",\n-      \"    \\\"output_tokens\\\": 8,\\n\",\n-      \"    \\\"total_tokens\\\": 39\\n\",\n+      \"    \\\"output_tokens\\\": 5,\\n\",\n+      \"    \\\"total_tokens\\\": 36\\n\",\n       \"  }\\n\",\n       \"}\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n     \"const aiMsg = await llm.invoke([\\n\",\n-    \"    [\\n\",\n-    \"        \\\"system\\\",\\n\",\n-    \"        \\\"You are a helpful assistant that translates English to French. Translate the user sentence.\\\",\\n\",\n-    \"    ],\\n\",\n-    \"    [\\\"human\\\", \\\"I love programming.\\\"],\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"system\\\",\\n\",\n+    \"    content: \\\"You are a helpful assistant that translates English to French. Translate the user sentence.\\\",\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"I love programming.\\\"\\n\",\n+    \"  },\\n\",\n     \"])\\n\",\n     \"aiMsg\"\n    ]\n@@ -196,23 +200,24 @@\n      \"output_type\": \"stream\",\n      \"text\": [\n       \"AIMessage {\\n\",\n-      \"  \\\"id\\\": \\\"chatcmpl-9rB4JD9rVBLzTuMee9AabulowEH0d\\\",\\n\",\n-      \"  \\\"content\\\": \\\"Ich liebe das Programmieren.\\\",\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ADItFaWFNqkSjSmlxeGk6HxcBHzVN\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Ich liebe Programmieren.\\\",\\n\",\n       \"  \\\"additional_kwargs\\\": {},\\n\",\n       \"  \\\"response_metadata\\\": {\\n\",\n       \"    \\\"tokenUsage\\\": {\\n\",\n-      \"      \\\"completionTokens\\\": 6,\\n\",\n+      \"      \\\"completionTokens\\\": 5,\\n\",\n       \"      \\\"promptTokens\\\": 26,\\n\",\n-      \"      \\\"totalTokens\\\": 32\\n\",\n+      \"      \\\"totalTokens\\\": 31\\n\",\n       \"    },\\n\",\n-      \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_5796ac6771\\\"\\n\",\n       \"  },\\n\",\n       \"  \\\"tool_calls\\\": [],\\n\",\n       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n       \"  \\\"usage_metadata\\\": {\\n\",\n       \"    \\\"input_tokens\\\": 26,\\n\",\n-      \"    \\\"output_tokens\\\": 6,\\n\",\n-      \"    \\\"total_tokens\\\": 32\\n\",\n+      \"    \\\"output_tokens\\\": 5,\\n\",\n+      \"    \\\"total_tokens\\\": 31\\n\",\n       \"  }\\n\",\n       \"}\\n\"\n      ]\n@@ -222,22 +227,22 @@\n     \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\"\\n\",\n     \"\\n\",\n     \"const prompt = ChatPromptTemplate.fromMessages(\\n\",\n+    \"  [\\n\",\n     \"    [\\n\",\n-    \"        [\\n\",\n-    \"            \\\"system\\\",\\n\",\n-    \"            \\\"You are a helpful assistant that translates {input_language} to {output_language}.\\\",\\n\",\n-    \"        ],\\n\",\n-    \"        [\\\"human\\\", \\\"{input}\\\"],\\n\",\n-    \"    ]\\n\",\n+    \"      \\\"system\\\",\\n\",\n+    \"      \\\"You are a helpful assistant that translates {input_language} to {output_language}.\\\",\\n\",\n+    \"    ],\\n\",\n+    \"    [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"  ]\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"const chain = prompt.pipe(llm);\\n\",\n     \"await chain.invoke(\\n\",\n-    \"    {\\n\",\n-    \"        input_language: \\\"English\\\",\\n\",\n-    \"        output_language: \\\"German\\\",\\n\",\n-    \"        input: \\\"I love programming.\\\",\\n\",\n-    \"    }\\n\",\n+    \"  {\\n\",\n+    \"    input_language: \\\"English\\\",\\n\",\n+    \"    output_language: \\\"German\\\",\\n\",\n+    \"    input: \\\"I love programming.\\\",\\n\",\n+    \"  }\\n\",\n     \")\"\n    ]\n   },\n@@ -384,7 +389,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 8,\n    \"id\": \"2b675330\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -396,7 +401,7 @@\n       \"  content: [\\n\",\n       \"    {\\n\",\n       \"      token: 'Hello',\\n\",\n-      \"      logprob: -0.0005151443,\\n\",\n+      \"      logprob: -0.0004740447,\\n\",\n       \"      bytes: [ 72, 101, 108, 108, 111 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n@@ -408,25 +413,25 @@\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' How',\\n\",\n-      \"      logprob: -0.000035477897,\\n\",\n+      \"      logprob: -0.000030113732,\\n\",\n       \"      bytes: [ 32, 72, 111, 119 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' can',\\n\",\n-      \"      logprob: -0.0006658526,\\n\",\n+      \"      logprob: -0.0004797665,\\n\",\n       \"      bytes: [ 32, 99, 97, 110 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' I',\\n\",\n-      \"      logprob: -0.0000010280384,\\n\",\n+      \"      logprob: -7.89631e-7,\\n\",\n       \"      bytes: [ 32, 73 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' assist',\\n\",\n-      \"      logprob: -0.10124119,\\n\",\n+      \"      logprob: -0.114006,\\n\",\n       \"      bytes: [\\n\",\n       \"         32,  97, 115,\\n\",\n       \"        115, 105, 115,\\n\",\n@@ -436,23 +441,24 @@\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' you',\\n\",\n-      \"      logprob: -5.5122365e-7,\\n\",\n+      \"      logprob: -4.3202e-7,\\n\",\n       \"      bytes: [ 32, 121, 111, 117 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: ' today',\\n\",\n-      \"      logprob: -0.000052643223,\\n\",\n+      \"      logprob: -0.00004501419,\\n\",\n       \"      bytes: [ 32, 116, 111, 100, 97, 121 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    },\\n\",\n       \"    {\\n\",\n       \"      token: '?',\\n\",\n-      \"      logprob: -0.000012352386,\\n\",\n+      \"      logprob: -0.000010206721,\\n\",\n       \"      bytes: [ 63 ],\\n\",\n       \"      top_logprobs: []\\n\",\n       \"    }\\n\",\n-      \"  ]\\n\",\n+      \"  ],\\n\",\n+      \"  refusal: null\\n\",\n       \"}\\n\"\n      ]\n     }\n@@ -489,24 +495,26 @@\n    \"id\": \"3392390e\",\n    \"metadata\": {},\n    \"source\": [\n-    \"### ``strict: true``\\n\",\n+    \"## ``strict: true``\\n\",\n+    \"\\n\",\n+    \"As of Aug 6, 2024, OpenAI supports a `strict` argument when calling tools that will enforce that the tool argument schema is respected by the model. See more here: https://platform.openai.com/docs/guides/function-calling.\\n\",\n     \"\\n\",\n     \"```{=mdx}\\n\",\n     \"\\n\",\n     \":::info Requires ``@langchain/openai >= 0.2.6``\\n\",\n     \"\\n\",\n-    \"As of Aug 6, 2024, OpenAI supports a `strict` argument when calling tools that will enforce that the tool argument schema is respected by the model. See more here: https://platform.openai.com/docs/guides/function-calling\\n\",\n-    \"\\n\",\n     \"**Note**: If ``strict: true`` the tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values). Read the full docs on what types of schema are supported here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas. \\n\",\n     \":::\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"```\"\n+    \"```\\n\",\n+    \"\\n\",\n+    \"Here's an example with tool calling. Passing an extra `strict: true` argument to `.bindTools` will pass the param through to all tool definitions:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 1,\n+   \"execution_count\": 9,\n    \"id\": \"90f0d465\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -517,9 +525,9 @@\n       \"[\\n\",\n       \"  {\\n\",\n       \"    name: 'get_current_weather',\\n\",\n-      \"    args: { location: 'Hanoi' },\\n\",\n+      \"    args: { location: 'current' },\\n\",\n       \"    type: 'tool_call',\\n\",\n-      \"    id: 'call_aB85ybkLCoccpzqHquuJGH3d'\\n\",\n+      \"    id: 'call_hVFyYNRwc6CoTgr9AQFQVjm9'\\n\",\n       \"  }\\n\",\n       \"]\\n\"\n      ]\n@@ -552,6 +560,627 @@\n     \"console.dir(strictTrueResult.tool_calls, { depth: null });\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"6c46a668\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"If you only want to apply this parameter to a select number of tools, you can also pass OpenAI formatted tool schemas directly:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 15,\n+   \"id\": \"e2da9ead\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"[\\n\",\n+      \"  {\\n\",\n+      \"    name: 'get_current_weather',\\n\",\n+      \"    args: { location: 'London' },\\n\",\n+      \"    type: 'tool_call',\\n\",\n+      \"    id: 'call_EOSejtax8aYtqpchY8n8O82l'\\n\",\n+      \"  }\\n\",\n+      \"]\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { zodToJsonSchema } from \\\"zod-to-json-schema\\\";\\n\",\n+    \"\\n\",\n+    \"const toolSchema = {\\n\",\n+    \"  type: \\\"function\\\",\\n\",\n+    \"  function: {\\n\",\n+    \"    name: \\\"get_current_weather\\\",\\n\",\n+    \"    description: \\\"Get the current weather\\\",\\n\",\n+    \"    strict: true,\\n\",\n+    \"    parameters: zodToJsonSchema(\\n\",\n+    \"      z.object({\\n\",\n+    \"        location: z.string(),\\n\",\n+    \"      })\\n\",\n+    \"    ),\\n\",\n+    \"  },\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"const llmWithStrictTrueTools = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o\\\",\\n\",\n+    \"}).bindTools([toolSchema], {\\n\",\n+    \"  strict: true,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const weatherToolResult = await llmWithStrictTrueTools.invoke([{\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"What is the current weather in London?\\\"\\n\",\n+    \"}])\\n\",\n+    \"\\n\",\n+    \"weatherToolResult.tool_calls;\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"045668fe\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Structured output\\n\",\n+    \"\\n\",\n+    \"We can also pass `strict: true` to the [`.withStructuredOutput()`](https://js.langchain.com/docs/how_to/structured_output/#the-.withstructuredoutput-method). Here's an example:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 13,\n+   \"id\": \"8e8171a5\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{ traits: [ `6'5\\\" tall`, 'love fruit' ] }\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const traitSchema = z.object({\\n\",\n+    \"  traits: z.array(z.string()).describe(\\\"A list of traits contained in the input\\\"),\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const structuredLlm = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-mini\\\",\\n\",\n+    \"}).withStructuredOutput(traitSchema, {\\n\",\n+    \"  name: \\\"extract_traits\\\",\\n\",\n+    \"  strict: true,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"await structuredLlm.invoke([{\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: `I am 6'5\\\" tall and love fruit.`\\n\",\n+    \"}]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"af20e756\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Prompt caching\\n\",\n+    \"\\n\",\n+    \"Newer OpenAI models will automatically [cache parts of your prompt](https://openai.com/index/api-prompt-caching/) if your inputs are above a certain size (1024 tokens at the time of writing) in order to reduce costs for use-cases that require long context.\\n\",\n+    \"\\n\",\n+    \"**Note:** The number of tokens cached for a given query is not yet standardized in `AIMessage.usage_metadata`, and is instead contained in the `AIMessage.response_metadata` field.\\n\",\n+    \"\\n\",\n+    \"Here's an example\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"cb4e4fd0\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"const CACHED_TEXT = `## Components\\n\",\n+    \"\\n\",\n+    \"LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\\n\",\n+    \"Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\\n\",\n+    \"\\n\",\n+    \"### Chat models\\n\",\n+    \"\\n\",\n+    \"<span data-heading-keywords=\\\"chat model,chat models\\\"></span>\\n\",\n+    \"\\n\",\n+    \"Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\\n\",\n+    \"These are generally newer models (older models are generally \\\\`LLMs\\\\`, see below).\\n\",\n+    \"Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.\\n\",\n+    \"\\n\",\n+    \"Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.\\n\",\n+    \"This gives them the same interface as LLMs (and simpler to use).\\n\",\n+    \"When a string is passed in as input, it will be converted to a \\\\`HumanMessage\\\\` under the hood before being passed to the underlying model.\\n\",\n+    \"\\n\",\n+    \"LangChain does not host any Chat Models, rather we rely on third party integrations.\\n\",\n+    \"\\n\",\n+    \"We have some standardized parameters when constructing ChatModels:\\n\",\n+    \"\\n\",\n+    \"- \\\\`model\\\\`: the name of the model\\n\",\n+    \"\\n\",\n+    \"Chat Models also accept other parameters that are specific to that integration.\\n\",\n+    \"\\n\",\n+    \":::important\\n\",\n+    \"Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.\\n\",\n+    \"Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\\n\",\n+    \"Please see the [tool calling section](/docs/concepts/#functiontool-calling) for more information.\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"For specifics on how to use chat models, see the [relevant how-to guides here](/docs/how_to/#chat-models).\\n\",\n+    \"\\n\",\n+    \"#### Multimodality\\n\",\n+    \"\\n\",\n+    \"Some chat models are multimodal, accepting images, audio and even video as inputs.\\n\",\n+    \"These are still less common, meaning model providers haven't standardized on the \\\"best\\\" way to define the API.\\n\",\n+    \"Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight\\n\",\n+    \"and plan to further solidify the multimodal APIs and interaction patterns as the field matures.\\n\",\n+    \"\\n\",\n+    \"In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.\\n\",\n+    \"So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\\n\",\n+    \"\\n\",\n+    \"For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).\\n\",\n+    \"\\n\",\n+    \"### LLMs\\n\",\n+    \"\\n\",\n+    \"<span data-heading-keywords=\\\"llm,llms\\\"></span>\\n\",\n+    \"\\n\",\n+    \":::caution\\n\",\n+    \"Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/docs/concepts/#chat-models),\\n\",\n+    \"even for non-chat use cases.\\n\",\n+    \"\\n\",\n+    \"You are probably looking for [the section above instead](/docs/concepts/#chat-models).\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"Language models that takes a string as input and returns a string.\\n\",\n+    \"These are traditionally older models (newer models generally are [Chat Models](/docs/concepts/#chat-models), see above).\\n\",\n+    \"\\n\",\n+    \"Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\\n\",\n+    \"This gives them the same interface as [Chat Models](/docs/concepts/#chat-models).\\n\",\n+    \"When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.\\n\",\n+    \"\\n\",\n+    \"LangChain does not host any LLMs, rather we rely on third party integrations.\\n\",\n+    \"\\n\",\n+    \"For specifics on how to use LLMs, see the [relevant how-to guides here](/docs/how_to/#llms).\\n\",\n+    \"\\n\",\n+    \"### Message types\\n\",\n+    \"\\n\",\n+    \"Some language models take an array of messages as input and return a message.\\n\",\n+    \"There are a few different types of messages.\\n\",\n+    \"All messages have a \\\\`role\\\\`, \\\\`content\\\\`, and \\\\`response_metadata\\\\` property.\\n\",\n+    \"\\n\",\n+    \"The \\\\`role\\\\` describes WHO is saying the message.\\n\",\n+    \"LangChain has different message classes for different roles.\\n\",\n+    \"\\n\",\n+    \"The \\\\`content\\\\` property describes the content of the message.\\n\",\n+    \"This can be a few different things:\\n\",\n+    \"\\n\",\n+    \"- A string (most models deal this type of content)\\n\",\n+    \"- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)\\n\",\n+    \"\\n\",\n+    \"#### HumanMessage\\n\",\n+    \"\\n\",\n+    \"This represents a message from the user.\\n\",\n+    \"\\n\",\n+    \"#### AIMessage\\n\",\n+    \"\\n\",\n+    \"This represents a message from the model. In addition to the \\\\`content\\\\` property, these messages also have:\\n\",\n+    \"\\n\",\n+    \"**\\\\`response_metadata\\\\`**\\n\",\n+    \"\\n\",\n+    \"The \\\\`response_metadata\\\\` property contains additional metadata about the response. The data here is often specific to each model provider.\\n\",\n+    \"This is where information like log-probs and token usage may be stored.\\n\",\n+    \"\\n\",\n+    \"**\\\\`tool_calls\\\\`**\\n\",\n+    \"\\n\",\n+    \"These represent a decision from an language model to call a tool. They are included as part of an \\\\`AIMessage\\\\` output.\\n\",\n+    \"They can be accessed from there with the \\\\`.tool_calls\\\\` property.\\n\",\n+    \"\\n\",\n+    \"This property returns a list of \\\\`ToolCall\\\\`s. A \\\\`ToolCall\\\\` is an object with the following arguments:\\n\",\n+    \"\\n\",\n+    \"- \\\\`name\\\\`: The name of the tool that should be called.\\n\",\n+    \"- \\\\`args\\\\`: The arguments to that tool.\\n\",\n+    \"- \\\\`id\\\\`: The id of that tool call.\\n\",\n+    \"\\n\",\n+    \"#### SystemMessage\\n\",\n+    \"\\n\",\n+    \"This represents a system message, which tells the model how to behave. Not every model provider supports this.\\n\",\n+    \"\\n\",\n+    \"#### ToolMessage\\n\",\n+    \"\\n\",\n+    \"This represents the result of a tool call. In addition to \\\\`role\\\\` and \\\\`content\\\\`, this message has:\\n\",\n+    \"\\n\",\n+    \"- a \\\\`tool_call_id\\\\` field which conveys the id of the call to the tool that was called to produce this result.\\n\",\n+    \"- an \\\\`artifact\\\\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\\n\",\n+    \"\\n\",\n+    \"#### (Legacy) FunctionMessage\\n\",\n+    \"\\n\",\n+    \"This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \\\\`ToolMessage\\\\` should be used instead to correspond to the updated tool-calling API.\\n\",\n+    \"\\n\",\n+    \"This represents the result of a function call. In addition to \\\\`role\\\\` and \\\\`content\\\\`, this message has a \\\\`name\\\\` parameter which conveys the name of the function that was called to produce this result.\\n\",\n+    \"\\n\",\n+    \"### Prompt templates\\n\",\n+    \"\\n\",\n+    \"<span data-heading-keywords=\\\"prompt,prompttemplate,chatprompttemplate\\\"></span>\\n\",\n+    \"\\n\",\n+    \"Prompt templates help to translate user input and parameters into instructions for a language model.\\n\",\n+    \"This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\\n\",\n+    \"\\n\",\n+    \"Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.\\n\",\n+    \"\\n\",\n+    \"Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.\\n\",\n+    \"The reason this PromptValue exists is to make it easy to switch between strings and messages.\\n\",\n+    \"\\n\",\n+    \"There are a few different types of prompt templates:\\n\",\n+    \"\\n\",\n+    \"#### String PromptTemplates\\n\",\n+    \"\\n\",\n+    \"These prompt templates are used to format a single string, and generally are used for simpler inputs.\\n\",\n+    \"For example, a common way to construct and use a PromptTemplate is as follows:\\n\",\n+    \"\\n\",\n+    \"\\\\`\\\\`\\\\`typescript\\n\",\n+    \"import { PromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"\\n\",\n+    \"const promptTemplate = PromptTemplate.fromTemplate(\\n\",\n+    \"  \\\"Tell me a joke about {topic}\\\"\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"await promptTemplate.invoke({ topic: \\\"cats\\\" });\\n\",\n+    \"\\\\`\\\\`\\\\`\\n\",\n+    \"\\n\",\n+    \"#### ChatPromptTemplates\\n\",\n+    \"\\n\",\n+    \"These prompt templates are used to format an array of messages. These \\\"templates\\\" consist of an array of templates themselves.\\n\",\n+    \"For example, a common way to construct and use a ChatPromptTemplate is as follows:\\n\",\n+    \"\\n\",\n+    \"\\\\`\\\\`\\\\`typescript\\n\",\n+    \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"\\n\",\n+    \"const promptTemplate = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"You are a helpful assistant\\\"],\\n\",\n+    \"  [\\\"user\\\", \\\"Tell me a joke about {topic}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"await promptTemplate.invoke({ topic: \\\"cats\\\" });\\n\",\n+    \"\\\\`\\\\`\\\\`\\n\",\n+    \"\\n\",\n+    \"In the above example, this ChatPromptTemplate will construct two messages when called.\\n\",\n+    \"The first is a system message, that has no variables to format.\\n\",\n+    \"The second is a HumanMessage, and will be formatted by the \\\\`topic\\\\` variable the user passes in.\\n\",\n+    \"\\n\",\n+    \"#### MessagesPlaceholder\\n\",\n+    \"\\n\",\n+    \"<span data-heading-keywords=\\\"messagesplaceholder\\\"></span>\\n\",\n+    \"\\n\",\n+    \"This prompt template is responsible for adding an array of messages in a particular place.\\n\",\n+    \"In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\n\",\n+    \"But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?\\n\",\n+    \"This is how you use MessagesPlaceholder.\\n\",\n+    \"\\n\",\n+    \"\\\\`\\\\`\\\\`typescript\\n\",\n+    \"import {\\n\",\n+    \"  ChatPromptTemplate,\\n\",\n+    \"  MessagesPlaceholder,\\n\",\n+    \"} from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"const promptTemplate = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"You are a helpful assistant\\\"],\\n\",\n+    \"  new MessagesPlaceholder(\\\"msgs\\\"),\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"promptTemplate.invoke({ msgs: [new HumanMessage({ content: \\\"hi!\\\" })] });\\n\",\n+    \"\\\\`\\\\`\\\\`\\n\",\n+    \"\\n\",\n+    \"This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\n\",\n+    \"If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\n\",\n+    \"This is useful for letting an array of messages be slotted into a particular spot.\\n\",\n+    \"\\n\",\n+    \"An alternative way to accomplish the same thing without using the \\\\`MessagesPlaceholder\\\\` class explicitly is:\\n\",\n+    \"\\n\",\n+    \"\\\\`\\\\`\\\\`typescript\\n\",\n+    \"const promptTemplate = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"You are a helpful assistant\\\"],\\n\",\n+    \"  [\\\"placeholder\\\", \\\"{msgs}\\\"], // <-- This is the changed part\\n\",\n+    \"]);\\n\",\n+    \"\\\\`\\\\`\\\\`\\n\",\n+    \"\\n\",\n+    \"For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).\\n\",\n+    \"\\n\",\n+    \"### Example Selectors\\n\",\n+    \"\\n\",\n+    \"One common prompting technique for achieving better performance is to include examples as part of the prompt.\\n\",\n+    \"This gives the language model concrete examples of how it should behave.\\n\",\n+    \"Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\\n\",\n+    \"Example Selectors are classes responsible for selecting and then formatting examples into prompts.\\n\",\n+    \"\\n\",\n+    \"For specifics on how to use example selectors, see the [relevant how-to guides here](/docs/how_to/#example-selectors).\\n\",\n+    \"\\n\",\n+    \"### Output parsers\\n\",\n+    \"\\n\",\n+    \"<span data-heading-keywords=\\\"output parser\\\"></span>\\n\",\n+    \"\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\n\",\n+    \"More and more models are supporting function (or tool) calling, which handles this automatically.\\n\",\n+    \"It is recommended to use function/tool calling rather than output parsing.\\n\",\n+    \"See documentation for that [here](/docs/concepts/#function-tool-calling).\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\n\",\n+    \"Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\\n\",\n+    \"\\n\",\n+    \"There are two main methods an output parser must implement:\\n\",\n+    \"\\n\",\n+    \"- \\\"Get format instructions\\\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\",\n+    \"- \\\"Parse\\\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\",\n+    \"\\n\",\n+    \"And then one optional one:\\n\",\n+    \"\\n\",\n+    \"- \\\"Parse with prompt\\\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\",\n+    \"\\n\",\n+    \"Output parsers accept a string or \\\\`BaseMessage\\\\` as input and can return an arbitrary type.\\n\",\n+    \"\\n\",\n+    \"LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\\n\",\n+    \"\\n\",\n+    \"**Name**: The name of the output parser\\n\",\n+    \"\\n\",\n+    \"**Supports Streaming**: Whether the output parser supports streaming.\\n\",\n+    \"\\n\",\n+    \"**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.\\n\",\n+    \"\\n\",\n+    \"**Output Type**: The output type of the object returned by the parser.\\n\",\n+    \"\\n\",\n+    \"**Description**: Our commentary on this output parser and when to use it.\\n\",\n+    \"\\n\",\n+    \"The current date is ${new Date().toISOString()}`;\\n\",\n+    \"\\n\",\n+    \"// Noop statement to hide output\\n\",\n+    \"void 0;\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"7a43595c\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"USAGE: {\\n\",\n+      \"  prompt_tokens: 2624,\\n\",\n+      \"  completion_tokens: 263,\\n\",\n+      \"  total_tokens: 2887,\\n\",\n+      \"  prompt_tokens_details: { cached_tokens: 0 },\\n\",\n+      \"  completion_tokens_details: { reasoning_tokens: 0 }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const modelWithCaching = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-mini-2024-07-18\\\",\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// CACHED_TEXT is some string longer than 1024 tokens\\n\",\n+    \"const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.\\n\",\n+    \"\\n\",\n+    \"Use the following as context when answering questions:\\n\",\n+    \"\\n\",\n+    \"${CACHED_TEXT}`;\\n\",\n+    \"\\n\",\n+    \"const longMessages = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"system\\\",\\n\",\n+    \"    content: LONG_TEXT,\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"What types of messages are supported in LangChain?\\\",\\n\",\n+    \"  },\\n\",\n+    \"];\\n\",\n+    \"\\n\",\n+    \"const originalRes = await modelWithCaching.invoke(longMessages);\\n\",\n+    \"\\n\",\n+    \"console.log(\\\"USAGE:\\\", originalRes.response_metadata.usage);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"id\": \"76c8005e\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"USAGE: {\\n\",\n+      \"  prompt_tokens: 2624,\\n\",\n+      \"  completion_tokens: 272,\\n\",\n+      \"  total_tokens: 2896,\\n\",\n+      \"  prompt_tokens_details: { cached_tokens: 2432 },\\n\",\n+      \"  completion_tokens_details: { reasoning_tokens: 0 }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const resWitCaching = await modelWithCaching.invoke(longMessages);\\n\",\n+    \"\\n\",\n+    \"console.log(\\\"USAGE:\\\", resWitCaching.response_metadata.usage);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"cc8b3c94\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Audio output\\n\",\n+    \"\\n\",\n+    \"Some OpenAI models (such as `gpt-4o-audio-preview`) support generating audio output. This example shows how to use that feature:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"b4d579b7\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  id: 'audio_67129e9466f48190be70372922464162',\\n\",\n+      \"  data: 'UklGRgZ4BABXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGHA',\\n\",\n+      \"  expires_at: 1729277092,\\n\",\n+      \"  transcript: \\\"Why did the cat sit on the computer's keyboard? Because it wanted to keep an eye on the mouse!\\\"\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const modelWithAudioOutput = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-audio-preview\\\",\\n\",\n+    \"  // You may also pass these fields to `.bind` as a call argument.\\n\",\n+    \"  modalities: [\\\"text\\\", \\\"audio\\\"], // Specifies that the model should output audio.\\n\",\n+    \"  audio: {\\n\",\n+    \"    voice: \\\"alloy\\\",\\n\",\n+    \"    format: \\\"wav\\\",\\n\",\n+    \"  },\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const audioOutputResult = await modelWithAudioOutput.invoke(\\\"Tell me a joke about cats.\\\");\\n\",\n+    \"const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\\n\",\n+    \"\\n\",\n+    \"console.log({\\n\",\n+    \"  ...castAudioContent,\\n\",\n+    \"  data: castAudioContent.data.slice(0, 100) // Sliced for brevity\\n\",\n+    \"})\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"bfea3608\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"We see that the audio data is returned inside the `data` field. We are also provided an `expires_at` date field. This field represents the date the audio response will no longer be accessible on the server for use in multi-turn conversations.\\n\",\n+    \"\\n\",\n+    \"### Streaming Audio Output\\n\",\n+    \"\\n\",\n+    \"OpenAI also supports streaming audio output. Here's an example:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"0fa68183\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  id: 'audio_67129e976ce081908103ba4947399a3eaudio_67129e976ce081908103ba4947399a3e',\\n\",\n+      \"  transcript: 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',\\n\",\n+      \"  index: 0,\\n\",\n+      \"  data: 'CgAGAAIADAAAAA0AAwAJAAcACQAJAAQABQABAAgABQAPAAAACAADAAUAAwD8/wUA+f8MAPv/CAD7/wUA///8/wUA/f8DAPj/AgD6',\\n\",\n+      \"  expires_at: 1729277096\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { AIMessageChunk } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { concat } from \\\"@langchain/core/utils/stream\\\"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const modelWithStreamingAudioOutput = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-audio-preview\\\",\\n\",\n+    \"  modalities: [\\\"text\\\", \\\"audio\\\"],\\n\",\n+    \"  audio: {\\n\",\n+    \"    voice: \\\"alloy\\\",\\n\",\n+    \"    format: \\\"pcm16\\\", // Format must be `pcm16` for streaming\\n\",\n+    \"  },\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const audioOutputStream = await modelWithStreamingAudioOutput.stream(\\\"Tell me a joke about cats.\\\");\\n\",\n+    \"let finalAudioOutputMsg: AIMessageChunk | undefined;\\n\",\n+    \"for await (const chunk of audioOutputStream) {\\n\",\n+    \"  finalAudioOutputMsg = finalAudioOutputMsg ? concat(finalAudioOutputMsg, chunk) : chunk;\\n\",\n+    \"}\\n\",\n+    \"const castStreamedAudioContent = finalAudioOutputMsg?.additional_kwargs.audio as Record<string, any>;\\n\",\n+    \"\\n\",\n+    \"console.log({\\n\",\n+    \"  ...castStreamedAudioContent,\\n\",\n+    \"  data: castStreamedAudioContent.data.slice(0, 100) // Sliced for brevity\\n\",\n+    \"})\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"e8b84aac\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Audio input\\n\",\n+    \"\\n\",\n+    \"These models also support passing audio as input. For this, you must specify `input_audio` fields as seen below:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"id\": \"1a69dad8\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"That's a great joke! It's always fun to imagine why cats do the funny things they do. Keeping an eye on the \\\"mouse\\\" is a creatively punny way to describe it!\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"const userInput = new HumanMessage({\\n\",\n+    \"  content: [{\\n\",\n+    \"    type: \\\"input_audio\\\",\\n\",\n+    \"    input_audio: {\\n\",\n+    \"      data: castAudioContent.data, // Re-use the base64 data from the first example\\n\",\n+    \"      format: \\\"wav\\\",\\n\",\n+    \"    },\\n\",\n+    \"  }]\\n\",\n+    \"})\\n\",\n+    \"\\n\",\n+    \"// Re-use the same model instance\\n\",\n+    \"const userInputAudioRes = await modelWithAudioOutput.invoke([userInput]);\\n\",\n+    \"\\n\",\n+    \"console.log((userInputAudioRes.additional_kwargs.audio as Record<string, any>).transcript);\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"id\": \"3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3\",",
          "docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx": "@@ -33,3 +33,11 @@ import ExampleCompressor from \"@examples/document_compressors/cohere_rerank_comp\n From the results, we can see it returned the top 3 documents, and assigned a `relevanceScore` to each.\n \n As expected, the document with the highest `relevanceScore` is the one that references Washington, D.C., with a score of `98.7%`!\n+\n+### Usage with `CohereClient`\n+\n+If you are using Cohere on Azure, AWS Bedrock or a standalone instance you can use the `CohereClient` to create a `CohereRerank` instance with your endpoint.\n+\n+import ExampleClient from \"@examples/document_compressors/cohere_rerank_custom_client.ts\";\n+\n+<CodeBlock language=\"typescript\">{ExampleClient}</CodeBlock>",
          "docs/core_docs/docs/integrations/document_loaders/file_loaders/epub.mdx": "@@ -9,7 +9,7 @@ This example goes over how to load data from EPUB files. By default, one documen\n # Setup\n \n ```bash npm2yarn\n-npm install @langchian/community @langchain/core epub2 html-to-text\n+npm install @langchain/community @langchain/core epub2 html-to-text\n ```\n \n # Usage, one document per chapter",
          "docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx": "@@ -10,7 +10,7 @@ This guide shows how to use [Apify](https://apify.com) with LangChain to load do\n ## Overview\n \n [Apify](https://apify.com) is a cloud platform for web scraping and data extraction,\n-which provides an [ecosystem](https://apify.com/store) of more than a thousand\n+which provides an [ecosystem](https://apify.com/store) of more than two thousand\n ready-made apps called _Actors_ for various web scraping, crawling, and data extraction use cases.\n \n This guide shows how to load documents\n@@ -19,11 +19,14 @@ storage built for storing structured web scraping results,\n such as a list of products or Google SERPs, and then export them to various\n formats like JSON, CSV, or Excel.\n \n-Datasets are typically used to save results of Actors.\n+Datasets are typically used to save results of different Actors.\n For example, [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor\n deeply crawls websites such as documentation, knowledge bases, help centers, or blogs,\n and then stores the text content of webpages into a dataset,\n-from which you can feed the documents into a vector index and answer questions from it.\n+from which you can feed the documents into a vector database and use it for information retrieval.\n+Another example is the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor,\n+which queries Google Search, scrapes the top N pages from the results, and returns the cleaned\n+content in Markdown format for further processing by a large language model.\n \n ## Setup\n \n@@ -38,18 +41,21 @@ import IntegrationInstallTooltip from \"@mdx_components/integration_install_toolt\n <IntegrationInstallTooltip></IntegrationInstallTooltip>\n \n ```bash npm2yarn\n-npm install @langchain/openai @langchain/community @langchain/core\n+npm install hnswlib-node @langchain/openai @langchain/community @langchain/core\n ```\n \n-You'll also need to sign up and retrieve your [Apify API token](https://console.apify.com/account/integrations).\n+You'll also need to sign up and retrieve your [Apify API token](https://console.apify.com/settings/integrations).\n \n ## Usage\n \n-### From a New Dataset\n+### From a New Dataset (Crawl a Website and Store the data in Apify Dataset)\n \n If you don't already have an existing dataset on the Apify platform, you'll need to initialize the document loader by calling an Actor and waiting for the results.\n+In the example below, we use the [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor to crawl\n+LangChain documentation, store the results in Apify Dataset, and then load the dataset using the `ApifyDatasetLoader`.\n+For this demonstration, we'll use a fast Cheerio crawler type and limit the number of crawled pages to 10.\n \n-**Note:** Calling an Actor can take a significant amount of time, on the order of hours, or even days for large sites!\n+**Note:** Running the Website Content Crawler may take some time, depending on the size of the website. For large sites, it can take several hours or even days!\n \n Here's an example:\n \n@@ -60,7 +66,7 @@ import NewExample from \"@examples/document_loaders/apify_dataset_new.ts\";\n \n ## From an Existing Dataset\n \n-If you already have an existing dataset on the Apify platform, you can initialize the document loader with the constructor directly:\n+If you've already run an Actor and have an existing dataset on the Apify platform, you can initialize the document loader directly using the constructor\n \n import ExistingExample from \"@examples/document_loaders/apify_dataset_existing.ts\";\n ",
          "docs/core_docs/docs/integrations/document_loaders/web_loaders/firecrawl.ipynb": "@@ -37,7 +37,7 @@\n     \"\\n\",\n     \"## Setup\\n\",\n     \"\\n\",\n-    \"To access `FireCrawlLoader` document loader you'll need to install the `@langchain/community` integration, and the `@mendable/firecrawl-js` package. Then create a **[FireCrawl](https://firecrawl.dev)** account and get an API key.\\n\",\n+    \"To access `FireCrawlLoader` document loader you'll need to install the `@langchain/community` integration, and the `@mendable/firecrawl-js@0.0.36` package. Then create a **[FireCrawl](https://firecrawl.dev)** account and get an API key.\\n\",\n     \"\\n\",\n     \"### Credentials\\n\",\n     \"\\n\",\n@@ -67,7 +67,7 @@\n     \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n     \"\\n\",\n     \"<Npm2Yarn>\\n\",\n-    \"  @langchain/community @langchain/core @mendable/firecrawl-js\\n\",\n+    \"  @langchain/community @langchain/core @mendable/firecrawl-js@0.0.36\\n\",\n     \"</Npm2Yarn>\\n\",\n     \"\\n\",\n     \"```\"",
          "docs/core_docs/docs/integrations/llms/cohere.ipynb": "@@ -3,10 +3,15 @@\n   {\n    \"cell_type\": \"raw\",\n    \"id\": \"67db2992\",\n-   \"metadata\": {},\n+   \"metadata\": {\n+    \"vscode\": {\n+     \"languageId\": \"raw\"\n+    }\n+   },\n    \"source\": [\n     \"---\\n\",\n     \"sidebar_label: Cohere\\n\",\n+    \"lc_docs_skip_validation: true\\n\",\n     \"---\"\n    ]\n   },\n@@ -106,6 +111,40 @@\n     \"})\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"2518004d\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.\\n\",\n+    \"\\n\",\n+    \"We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.\\n\",\n+    \"\\n\",\n+    \"**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and `apiKey` parameter in the constructor will be ignored.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"79da9b26\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { Cohere } from \\\"@langchain/cohere\\\";\\n\",\n+    \"import { CohereClient } from \\\"cohere-ai\\\";\\n\",\n+    \"\\n\",\n+    \"const client = new CohereClient({\\n\",\n+    \"  token: \\\"<your-api-key>\\\",\\n\",\n+    \"  environment: \\\"<your-cohere-deployment-url>\\\", //optional\\n\",\n+    \"  // other params\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const llmWithCustomClient = new Cohere({\\n\",\n+    \"  client,\\n\",\n+    \"  // other params...\\n\",\n+    \"});\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"id\": \"0ee90032\",",
          "docs/core_docs/docs/integrations/llms/jigsawstack.mdx": "@@ -0,0 +1,43 @@\n+# JigsawStack Prompt Engine\n+\n+LangChain.js supports calling JigsawStack [Prompt Engine](https://docs.jigsawstack.com/api-reference/prompt-engine/run-direct) LLMs.\n+\n+## Setup\n+\n+- Set up an [account](https://jigsawstack.com/dashboard) (Get started for free)\n+- Create and retrieve your [API key](https://jigsawstack.com/dashboard)\n+\n+## Credentials\n+\n+```bash\n+export JIGSAWSTACK_API_KEY=\"your-api-key\"\n+```\n+\n+## Usage\n+\n+import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n+\n+<IntegrationInstallTooltip></IntegrationInstallTooltip>\n+\n+```bash npm2yarn\n+npm install @langchain/jigsawstack\n+```\n+\n+import CodeBlock from \"@theme/CodeBlock\";\n+\n+```ts\n+import { JigsawStackPromptEngine } from \"@langchain/jigsawstack\";\n+\n+export const run = async () => {\n+  const model = new JigsawStackPromptEngine();\n+  const res = await model.invoke(\n+    \"Tell me about the leaning tower of pisa?\\nAnswer:\"\n+  );\n+  console.log({ res });\n+};\n+```\n+\n+## Related\n+\n+- LLM [conceptual guide](/docs/concepts/#llms)\n+- LLM [how-to guides](/docs/how_to/#llms)",
          "docs/core_docs/docs/integrations/llms/llama_cpp.mdx": "@@ -12,10 +12,10 @@ This module is based on the [node-llama-cpp](https://github.com/withcatai/node-l\n \n ## Setup\n \n-You'll need to install the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n+You'll need to install major version `2` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n \n ```bash npm2yarn\n-npm install -S node-llama-cpp\n+npm install -S node-llama-cpp@2\n ```\n \n import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";",
          "docs/core_docs/docs/integrations/llms/raycast.mdx": "@@ -17,9 +17,18 @@ npm install @langchain/community @langchain/core\n ```\n \n import CodeBlock from \"@theme/CodeBlock\";\n-import RaycastAIExample from \"@examples/models/llm/raycast.ts\";\n \n-<CodeBlock language=\"typescript\">{RaycastAIExample}</CodeBlock>\n+```ts\n+import { RaycastAI } from \"@langchain/community/llms/raycast\";\n+\n+import { Tool } from \"@langchain/core/tools\";\n+\n+const model = new RaycastAI({\n+  rateLimitPerMinute: 10, // It is 10 by default so you can omit this line\n+  model: \"<model_name>\",\n+  creativity: 0, // `creativity` is a term used by Raycast which is equivalent to `temperature` in some other LLMs\n+});\n+```\n \n ## Related\n ",
          "docs/core_docs/docs/integrations/text_embedding/cohere.ipynb": "@@ -11,6 +11,7 @@\n    \"source\": [\n     \"---\\n\",\n     \"sidebar_label: Cohere\\n\",\n+    \"lc_docs_skip_validation: true\\n\",\n     \"---\"\n    ]\n   },\n@@ -91,6 +92,40 @@\n     \"});\"\n    ]\n   },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"b6470d5e\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.\\n\",\n+    \"\\n\",\n+    \"We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.\\n\",\n+    \"\\n\",\n+    \"**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and apiKey parameter in the constructor will be ignored\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"a241b0f0\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { CohereEmbeddings } from \\\"@langchain/cohere\\\";\\n\",\n+    \"import { CohereClient } from \\\"cohere-ai\\\";\\n\",\n+    \"\\n\",\n+    \"const client = new CohereClient({\\n\",\n+    \"  token: \\\"<your-api-key>\\\",\\n\",\n+    \"  environment: \\\"<your-cohere-deployment-url>\\\", //optional\\n\",\n+    \"  // other params\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const embeddingsWithCustomClient = new CohereEmbeddings({\\n\",\n+    \"  client,\\n\",\n+    \"  // other params...\\n\",\n+    \"});\"\n+   ]\n+  },\n   {\n    \"cell_type\": \"markdown\",\n    \"id\": \"77d271b6\",",
          "docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx": "@@ -12,10 +12,10 @@ This module is based on the [node-llama-cpp](https://github.com/withcatai/node-l\n \n ## Setup\n \n-You'll need to install the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n+You'll need to install major version `2` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.\n \n ```bash npm2yarn\n-npm install -S node-llama-cpp\n+npm install -S node-llama-cpp@2\n ```\n \n import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";",
          "docs/core_docs/docs/integrations/tools/jigsawstack.mdx": "@@ -0,0 +1,163 @@\n+---\n+hide_table_of_contents: true\n+---\n+\n+import CodeBlock from \"@theme/CodeBlock\";\n+\n+# JigsawStack Tool\n+\n+The JigsawStack Tool provides your agent with the following capabilities:\n+\n+- JigsawStackAIScrape: Scrape web content using advanced AI.\n+\n+- JigsawStackAISearch: Perform AI-powered web searches and retrieve high-quality results.\n+\n+- JigsawStackSpeechToText - Transcribe video and audio files using the Whisper large V3 AI model.\n+\n+- JigsawStackVOCR - Recognize, describe, and extract data from images using a prompt.\n+\n+- JigsawStackTextToSQL - Generate semantically correct SQL queries from text.\n+\n+## Setup\n+\n+- Set up an [account](https://jigsawstack.com/dashboard) (Get started for free)\n+- Create and retrieve your [API key](https://jigsawstack.com/dashboard)\n+\n+## Credentials\n+\n+```bash\n+export JIGSAWSTACK_API_KEY=\"your-api-key\"\n+```\n+\n+## Usage, standalone\n+\n+import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n+\n+<IntegrationInstallTooltip></IntegrationInstallTooltip>\n+\n+```bash npm2yarn\n+npm install @langchain/openai\n+```\n+\n+```js\n+import {\n+  JigsawStackAIScrape,\n+  JigsawStackAISearch,\n+  JigsawStackSpeechToText,\n+  JigsawStackVOCR,\n+  JigsawStackTextToSQL,\n+} from \"@langchain/jigsawstack\";\n+\n+export const run = async () => {\n+  // AI Scrape Tool\n+  const aiScrapeTool = new JigsawStackAIScrape({\n+    params: {\n+      element_prompts: [\"Pro plan\"],\n+    },\n+  });\n+  const result = await aiScrapeTool.invoke(\"https://jigsawstack.com/pricing\");\n+\n+  console.log({ result });\n+\n+  // AI Search Tool\n+\n+  const aiSearchTool = new JigsawStackAISearch();\n+  const doc = await aiSearchTool.invoke(\"The leaning tower of pisa\");\n+  console.log({ doc });\n+\n+  // VOCR Tool\n+\n+  const vocrTool = new JigsawStackVOCR({\n+    params: {\n+      prompt: \"Describe the image in detail\",\n+    },\n+  });\n+  const data = await vocrTool.invoke(\n+    \"https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Collabo%201080x842.jpg?t=2024-03-22T09%3A22%3A48.442Z\"\n+  );\n+\n+  console.log({ data });\n+\n+  // Speech-to-Text Tool\n+  const sttTool = new JigsawStackSpeechToText();\n+  await sttTool.invoke(\n+    \"https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Video%201737458382653833217.mp4?t=2024-03-22T09%3A50%3A49.894\"\n+  );\n+\n+  // Text-to-SQL Tool\n+  const sqlTool = new JigsawStackTextToSQL({\n+    params: {\n+      sql_schema:\n+        \"CREATE TABLE Transactions (transaction_id INT PRIMARY KEY, user_id INT NOT NULL,total_amount DECIMAL(10, 2 NOT NULL, transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,status VARCHAR(20) DEFAULT 'pending',FOREIGN KEY(user_id) REFERENCES Users(user_id))\",\n+    },\n+  });\n+\n+  await sqlTool.invoke(\n+    \"Generate a query to get transactions that amount exceed 10000 and sort by when created\"\n+  );\n+};\n+```\n+\n+## Usage, in an Agent\n+\n+```js\n+import { ChatOpenAI } from \"@langchain/openai\";\n+import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n+import {\n+  JigsawStackAIScrape,\n+  JigsawStackAISearch,\n+  JigsawStackVOCR,\n+  JigsawStackSpeechToText,\n+  JigsawStackTextToSQL,\n+} from \"@langchain/jigsawstack\";\n+\n+const model = new ChatOpenAI({\n+  temperature: 0,\n+});\n+\n+//  add the tools that you need\n+const tools = [\n+  new JigsawStackAIScrape(),\n+  new JigsawStackAISearch(),\n+  new JigsawStackVOCR(),\n+  new JigsawStackSpeechToText(),\n+  new JigsawStackTextToSQL(),\n+];\n+\n+const executor = await initializeAgentExecutorWithOptions(tools, model, {\n+  agentType: \"zero-shot-react-description\",\n+  verbose: true,\n+});\n+\n+const res = await executor.invoke({\n+  input: `Kokkalo Restaurant Santorini`,\n+});\n+\n+console.log(res.output);\n+\n+/*\n+{\n+  \"query\": \"Kokkalo Restaurant Santorini\",\n+  \"ai_overview\": \"Kokkalo Restaurant, located in Fira, Santorini, offers a unique dining experience that blends traditional Greek cuisine with modern culinary trends. Here are some key details about the restaurant:\\n\\n- **Location**: Situated on the main road of Firostefani, Kokkalo is surrounded by the picturesque Cycladic architecture and provides stunning views of the Aegean Sea.\\n- **Cuisine**: The restaurant specializes in authentic Greek dishes, crafted from high-quality, locally sourced ingredients. The menu is designed to engage all senses and features a variety of Mediterranean flavors.\\n- **Ambiance**: Kokkalo boasts a chic and modern décor, creating a welcoming atmosphere for guests. The staff is known for their professionalism and attentiveness, enhancing the overall dining experience.\\n- **Culinary Experience**: The name \\\"Kokkalo,\\\" meaning \\\"bone\\\" in Greek, symbolizes the strong foundation of the restaurant's culinary philosophy. Guests can expect a bold and unforgettable culinary journey.\\n- **Cooking Classes**: Kokkalo also offers cooking lessons, allowing visitors to learn how to prepare traditional Greek dishes, providing a unique souvenir of their time in Santorini.\\n- **Contact Information**: \\n  - Address: 25 Martiou str, Fira, Santorini 84 700, Cyclades, Greece\\n  - Phone: +30 22860 25407\\n  - Email: reservation@kokkalosantorini.com\\n\\nFor more information, you can visit their [official website](https://www.santorini-view.com/restaurants/kokkalo-restaurant/) or their [Facebook page](https://www.facebook.com/kokkalorestaurant/).\",\n+  \"is_safe\": true,\n+  \"results\": [\n+    {\n+      \"title\": \"Kokkalo restaurant, Restaurants in Firostefani Santorini Greece\",\n+      \"url\": \"http://www.travel-to-santorini.com/restaurants/firostefani/thebonerestaurant/\",\n+      \"description\": \"Details Contact : George Grafakos Address : Firostefani, Opposite of Fira Primary School Zipcode : 84700 City : Santorni Phone : +30 22860 25407 Send an email\",\n+      \"content\": null,\n+      \"site_name\": \"Travel-to-santorini\",\n+      \"site_long_name\": \"travel-to-santorini.com\",\n+      \"language\": \"en\",\n+      \"is_safe\": true,\n+      \"favicon\": \"https://t1.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=http://travel-to-santorini.com&size=96\"\n+    }\n+  ]\n+}\n+*/\n+```\n+\n+## Related\n+\n+- Tool [conceptual guide](/docs/concepts/#tools)\n+- Tool [how-to guides](/docs/how_to/#tools)",
          "docs/core_docs/docs/integrations/vectorstores/couchbase.mdx": "@@ -178,11 +178,77 @@ chunk the text into 500 character chunks with no overlaps and index all these ch\n \n After the data is indexed, we perform a simple query to find the top 4 chunks that are similar to the\n query \"What did president say about Ketanji Brown Jackson\".\n-At the emd, also shows how to get similarity score\n+At the end, it also shows how to get similarity score\n+\n+```ts\n+import { OpenAIEmbeddings } from \"@langchain/openai\";\n+import {\n+  CouchbaseVectorStoreArgs,\n+  CouchbaseVectorStore,\n+} from \"@langchain/community/vectorstores/couchbase\";\n+import { Cluster } from \"couchbase\";\n+import { TextLoader } from \"langchain/document_loaders/fs/text\";\n+import { CharacterTextSplitter } from \"@langchain/textsplitters\";\n+\n+const connectionString =\n+  process.env.COUCHBASE_DB_CONN_STR ?? \"couchbase://localhost\";\n+const databaseUsername = process.env.COUCHBASE_DB_USERNAME ?? \"Administrator\";\n+const databasePassword = process.env.COUCHBASE_DB_PASSWORD ?? \"Password\";\n+\n+// Load documents from file\n+const loader = new TextLoader(\"./state_of_the_union.txt\");\n+const rawDocuments = await loader.load();\n+const splitter = new CharacterTextSplitter({\n+  chunkSize: 500,\n+  chunkOverlap: 0,\n+});\n+const docs = await splitter.splitDocuments(rawDocuments);\n+\n+const couchbaseClient = await Cluster.connect(connectionString, {\n+  username: databaseUsername,\n+  password: databasePassword,\n+  configProfile: \"wanDevelopment\",\n+});\n+\n+// Open AI API Key is required to use OpenAIEmbeddings, some other embeddings may also be used\n+const embeddings = new OpenAIEmbeddings({\n+  apiKey: process.env.OPENAI_API_KEY,\n+});\n+\n+const couchbaseConfig: CouchbaseVectorStoreArgs = {\n+  cluster: couchbaseClient,\n+  bucketName: \"testing\",\n+  scopeName: \"_default\",\n+  collectionName: \"_default\",\n+  indexName: \"vector-index\",\n+  textKey: \"text\",\n+  embeddingKey: \"embedding\",\n+};\n+\n+const store = await CouchbaseVectorStore.fromDocuments(\n+  docs,\n+  embeddings,\n+  couchbaseConfig\n+);\n \n-import SimilaritySearch from \"@examples/indexes/vector_stores/couchbase/similaritySearch.ts\";\n+const query = \"What did president say about Ketanji Brown Jackson\";\n \n-<CodeBlock language=\"typescript\">{SimilaritySearch}</CodeBlock>\n+const resultsSimilaritySearch = await store.similaritySearch(query);\n+console.log(\"resulting documents: \", resultsSimilaritySearch[0]);\n+\n+// Similarity Search With Score\n+const resultsSimilaritySearchWithScore = await store.similaritySearchWithScore(\n+  query,\n+  1\n+);\n+console.log(\"resulting documents: \", resultsSimilaritySearchWithScore[0][0]);\n+console.log(\"resulting scores: \", resultsSimilaritySearchWithScore[0][1]);\n+\n+const result = await store.similaritySearch(query, 1, {\n+  fields: [\"metadata.source\"],\n+});\n+console.log(result[0]);\n+```\n \n ## Specifying Fields to Return\n ",
          "docs/core_docs/docs/integrations/vectorstores/libsql.mdx": "@@ -0,0 +1,169 @@\n+# libSQL\n+\n+[Turso](https://turso.tech) is a SQLite-compatible database built on [libSQL](https://docs.turso.tech/libsql), the Open Contribution fork of SQLite. Vector Similiarity Search is built into Turso and libSQL as a native datatype, enabling you to store and query vectors directly in the database.\n+\n+LangChain.js supports using a local libSQL, or remote Turso database as a vector store, and provides a simple API to interact with it.\n+\n+This guide provides a quick overview for getting started with libSQL vector stores. For detailed documentation of all libSQL features and configurations head to the API reference.\n+\n+## Overview\n+\n+## Integration details\n+\n+| Class               | Package                | JS support | Package latest                                                    |\n+| ------------------- | ---------------------- | ---------- | ----------------------------------------------------------------- |\n+| `LibSQLVectorStore` | `@langchain/community` | ✅         | ![npm version](https://img.shields.io/npm/v/@langchain/community) |\n+\n+## Setup\n+\n+To use libSQL vector stores, you'll need to create a Turso account or set up a local SQLite database, and install the `@langchain/community` integration package.\n+\n+This guide will also use OpenAI embeddings, which require you to install the `@langchain/openai` integration package. You can also use other supported embeddings models if you wish.\n+\n+You can use local SQLite when working with the libSQL vector store, or use a hosted Turso Database.\n+\n+import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n+\n+<IntegrationInstallTooltip></IntegrationInstallTooltip>\n+\n+```bash npm2yarn\n+npm install @libsql/client @langchain/openai @langchain/community\n+```\n+\n+Now it's time to create a database. You can create one locally, or use a hosted Turso database.\n+\n+### Local libSQL\n+\n+Create a new local SQLite file and connect to the shell:\n+\n+```bash\n+sqlite3 file.db\n+```\n+\n+### Hosted Turso\n+\n+Visit [sqlite.new](https://sqlite.new) to create a new database, give it a name, and create a database auth token.\n+\n+Make sure to copy the database auth token, and the database URL, it should look something like:\n+\n+```text\n+libsql://[database-name]-[your-username].turso.io\n+```\n+\n+### Setup the table and index\n+\n+Execute the following SQL command to create a new table or add the embedding column to an existing table.\n+\n+Make sure to mopdify the following parts of the SQL:\n+\n+- `TABLE_NAME` is the name of the table you want to create.\n+- `content` is used to store the `Document.pageContent` values.\n+- `metadata` is used to store the `Document.metadata` object.\n+- `EMBEDDING_COLUMN` is used to store the vector values, use the dimensions size used by the model you plan to use (1536 for OpenAI).\n+\n+```sql\n+CREATE TABLE IF NOT EXISTS TABLE_NAME (\n+    id INTEGER PRIMARY KEY AUTOINCREMENT,\n+    content TEXT,\n+    metadata TEXT,\n+    EMBEDDING_COLUMN F32_BLOB(1536) -- 1536-dimensional f32 vector for OpenAI\n+);\n+```\n+\n+Now create an index on the `EMBEDDING_COLUMN` column:\n+\n+```sql\n+CREATE INDEX IF NOT EXISTS idx_TABLE_NAME_EMBEDDING_COLUMN ON TABLE_NAME(libsql_vector_idx(EMBEDDING_COLUMN));\n+```\n+\n+Make sure to replace the `TABLE_NAME` and `EMBEDDING_COLUMN` with the values you used in the previous step.\n+\n+## Instantiation\n+\n+To initialize a new `LibSQL` vector store, you need to provide the database URL and Auth Token when working remotely, or by passing the filename for a local SQLite.\n+\n+```typescript\n+import { LibSQLVectorStore } from \"@langchain/community/vectorstores/libsql\";\n+import { OpenAIEmbeddings } from \"@langchain/openai\";\n+import { createClient } from \"@libsql/client\";\n+\n+const embeddings = new OpenAIEmbeddings({\n+  model: \"text-embedding-3-small\",\n+});\n+\n+const libsqlClient = createClient({\n+  url: \"libsql://[database-name]-[your-username].turso.io\",\n+  authToken: \"...\",\n+});\n+\n+// Local instantiation\n+// const libsqlClient = createClient({\n+//  url: \"file:./dev.db\",\n+// });\n+\n+const vectorStore = new LibSQLVectorStore(embeddings, {\n+  db: libsqlClient,\n+  tableName: \"TABLE_NAME\",\n+  embeddingColumn: \"EMBEDDING_COLUMN\",\n+  dimensions: 1536,\n+});\n+```\n+\n+## Manage vector store\n+\n+### Add items to vector store\n+\n+```typescript\n+import type { Document } from \"@langchain/core/documents\";\n+\n+const documents: Document[] = [\n+  { pageContent: \"Hello\", metadata: { topic: \"greeting\" } },\n+  { pageContent: \"Bye bye\", metadata: { topic: \"greeting\" } },\n+];\n+\n+await vectorStore.addDocuments(documents);\n+```\n+\n+### Delete items from vector store\n+\n+```typescript\n+await vectorStore.deleteDocuments({ ids: [1, 2] });\n+```\n+\n+## Query vector store\n+\n+Once you have inserted the documents, you can query the vector store.\n+\n+### Query directly\n+\n+Performing a simple similarity search can be done as follows:\n+\n+```typescript\n+const resultOne = await vectorStore.similaritySearch(\"hola\", 1);\n+\n+for (const doc of similaritySearchResults) {\n+  console.log(`${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+}\n+```\n+\n+For similarity search with scores:\n+\n+```typescript\n+const similaritySearchWithScoreResults =\n+  await vectorStore.similaritySearchWithScore(\"hola\", 1);\n+\n+for (const [doc, score] of similaritySearchWithScoreResults) {\n+  console.log(\n+    `${score.toFixed(3)} ${doc.pageContent} [${JSON.stringify(doc.metadata)}`\n+  );\n+}\n+```\n+\n+## API reference\n+\n+For detailed documentation of all `LibSQLVectorStore` features and configurations head to the API reference.\n+\n+## Related\n+\n+- Vector store [conceptual guide](/docs/concepts/#vectorstores)\n+- Vector store [how-to guides](/docs/how_to/#vectorstores)",
          "docs/core_docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx": "@@ -0,0 +1,57 @@\n+# INVALID_PROMPT_INPUT\n+\n+A [prompt template](/docs/concepts#prompt-templates) received missing or invalid input variables.\n+\n+One unexpected way this can occur is if you add a JSON object directly into a prompt template:\n+\n+```ts\n+import { PromptTemplate } from \"@langchain/core/prompts\";\n+import { ChatOpenAI } from \"@langchain/openai\";\n+\n+const prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n+\n+Here is an example of how you should respond:\n+\n+{\n+  \"firstName\": \"John\",\n+  \"lastName\": \"Doe\",\n+  \"age\": 21\n+}\n+\n+Now, answer the following question:\n+\n+{question}`);\n+```\n+\n+You might think that the above prompt template should require a single input key named `question`, but the JSON object will be\n+interpreted as an additional variable because the curly braces (`{`) are not escaped, and should be preceded by a second brace instead, like this:\n+\n+```ts\n+import { PromptTemplate } from \"@langchain/core/prompts\";\n+import { ChatOpenAI } from \"@langchain/openai\";\n+\n+const prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n+\n+Here is an example of how you should respond:\n+\n+{{\n+  \"firstName\": \"John\",\n+  \"lastName\": \"Doe\",\n+  \"age\": 21\n+}}\n+\n+Now, answer the following question:\n+\n+{question}`);\n+```\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Double-check your prompt template to ensure that it is correct.\n+  - If you are using default formatting and you are using curly braces `{` anywhere in your template, they should be double escaped like this: `{{`, as shown above.\n+- If you are using a [`MessagesPlaceholder`](/docs/concepts/#messagesplaceholder), make sure that you are passing in an array of messages or message-like objects.\n+  - If you are using shorthand tuples to declare your prompt template, make sure that the variable name is wrapped in curly braces (`[\"placeholder\", \"{messages}\"]`).\n+- Try viewing the inputs into your prompt template using [LangSmith](https://docs.smith.langchain.com/) or log statements to confirm they appear as expected.\n+- If you are pulling a prompt from the [LangChain Prompt Hub](https://smith.langchain.com/prompts), try pulling and logging it or running it in isolation with a sample input to confirm that it is what you expect.",
          "docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb": "@@ -0,0 +1,448 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# INVALID_TOOL_RESULTS\\n\",\n+    \"\\n\",\n+    \"You are passing too many, too few, or mismatched [`ToolMessages`](https://api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html) to a model.\\n\",\n+    \"\\n\",\n+    \"When [using a model to call tools](/docs/concepts#functiontool-calling), the [`AIMessage`](https://api.js.langchain.com/classes/_langchain_core.messages.AIMessage.html)\\n\",\n+    \"the model responds with will contain a `tool_calls` array. To continue the flow, the next messages you pass back to the model must\\n\",\n+    \"be exactly one `ToolMessage` for each item in that array containing the result of that tool call. Each `ToolMessage` must have a `tool_call_id` field\\n\",\n+    \"that matches one of the `tool_calls` on the `AIMessage`.\\n\",\n+    \"\\n\",\n+    \"For example, given the following response from a model:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-AIgT1xUd6lkWAutThiiBsqjq7Ykj1\\\",\\n\",\n+      \"  \\\"content\\\": \\\"\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {\\n\",\n+      \"    \\\"tool_calls\\\": [\\n\",\n+      \"      {\\n\",\n+      \"        \\\"id\\\": \\\"call_BknYpnY7xiARM17TPYqL7luj\\\",\\n\",\n+      \"        \\\"type\\\": \\\"function\\\",\\n\",\n+      \"        \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"      },\\n\",\n+      \"      {\\n\",\n+      \"        \\\"id\\\": \\\"call_EHf8MIcTdsLCZcFVlcH4hxJw\\\",\\n\",\n+      \"        \\\"type\\\": \\\"function\\\",\\n\",\n+      \"        \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  },\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"promptTokens\\\": 42,\\n\",\n+      \"      \\\"completionTokens\\\": 37,\\n\",\n+      \"      \\\"totalTokens\\\": 79\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"    \\\"usage\\\": {\\n\",\n+      \"      \\\"prompt_tokens\\\": 42,\\n\",\n+      \"      \\\"completion_tokens\\\": 37,\\n\",\n+      \"      \\\"total_tokens\\\": 79,\\n\",\n+      \"      \\\"prompt_tokens_details\\\": {\\n\",\n+      \"        \\\"cached_tokens\\\": 0\\n\",\n+      \"      },\\n\",\n+      \"      \\\"completion_tokens_details\\\": {\\n\",\n+      \"        \\\"reasoning_tokens\\\": 0\\n\",\n+      \"      }\\n\",\n+      \"    },\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e2bde53e6e\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"name\\\": \\\"foo\\\",\\n\",\n+      \"      \\\"args\\\": {},\\n\",\n+      \"      \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"      \\\"id\\\": \\\"call_BknYpnY7xiARM17TPYqL7luj\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"name\\\": \\\"foo\\\",\\n\",\n+      \"      \\\"args\\\": {},\\n\",\n+      \"      \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"      \\\"id\\\": \\\"call_EHf8MIcTdsLCZcFVlcH4hxJw\\\"\\n\",\n+      \"    }\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"output_tokens\\\": 37,\\n\",\n+      \"    \\\"input_tokens\\\": 42,\\n\",\n+      \"    \\\"total_tokens\\\": 79,\\n\",\n+      \"    \\\"input_token_details\\\": {\\n\",\n+      \"      \\\"cache_read\\\": 0\\n\",\n+      \"    },\\n\",\n+      \"    \\\"output_token_details\\\": {\\n\",\n+      \"      \\\"reasoning\\\": 0\\n\",\n+      \"    }\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { z } from \\\"zod\\\";\\n\",\n+    \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { BaseMessageLike } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"const model = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-mini\\\",\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const dummyTool = tool(\\n\",\n+    \"  async () => {\\n\",\n+    \"    return \\\"action complete!\\\";\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    name: \\\"foo\\\",\\n\",\n+    \"    schema: z.object({}),\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"const modelWithTools = model.bindTools([dummyTool]);\\n\",\n+    \"\\n\",\n+    \"const chatHistory: BaseMessageLike[] = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: `Call tool \\\"foo\\\" twice with no arguments`,\\n\",\n+    \"  },\\n\",\n+    \"];\\n\",\n+    \"\\n\",\n+    \"const responseMessage = await modelWithTools.invoke(chatHistory);\\n\",\n+    \"\\n\",\n+    \"console.log(responseMessage);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Calling the model with only one tool response would result in an error:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"BadRequestError: 400 An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EHf8MIcTdsLCZcFVlcH4hxJw\\n\",\n+      \"    at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)\\n\",\n+      \"    at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)\\n\",\n+      \"    at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)\\n\",\n+      \"    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n\",\n+      \"    at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29\\n\",\n+      \"    at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\\n\",\n+      \"  status: 400,\\n\",\n+      \"  headers: {\\n\",\n+      \"    'access-control-expose-headers': 'X-Request-ID',\\n\",\n+      \"    'alt-svc': 'h3=\\\":443\\\"; ma=86400',\\n\",\n+      \"    'cf-cache-status': 'DYNAMIC',\\n\",\n+      \"    'cf-ray': '8d31d4d95e2a0c96-EWR',\\n\",\n+      \"    connection: 'keep-alive',\\n\",\n+      \"    'content-length': '315',\\n\",\n+      \"    'content-type': 'application/json',\\n\",\n+      \"    date: 'Tue, 15 Oct 2024 18:21:53 GMT',\\n\",\n+      \"    'openai-organization': 'langchain',\\n\",\n+      \"    'openai-processing-ms': '16',\\n\",\n+      \"    'openai-version': '2020-10-01',\\n\",\n+      \"    server: 'cloudflare',\\n\",\n+      \"    'set-cookie': '__cf_bm=e5.GX1bHiMVgr76YSvAKuECCGG7X_RXF0jDGSMXFGfU-1729016513-1.0.1.1-ZBYeVqX.M6jSNJB.wS696fEhX7V.es._M0WcWtQ9Qx8doEA5qMVKNE5iX6i7UKyPCg2GvDfM.MoDwRCXKMSkEA; path=/; expires=Tue, 15-Oct-24 18:51:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=J8gS08GodUA9hRTYuElen0YOCzMO3d4LW0ZT0k_kyj4-1729016513560-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\\n\",\n+      \"    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\\n\",\n+      \"    'x-content-type-options': 'nosniff',\\n\",\n+      \"    'x-ratelimit-limit-requests': '30000',\\n\",\n+      \"    'x-ratelimit-limit-tokens': '150000000',\\n\",\n+      \"    'x-ratelimit-remaining-requests': '29999',\\n\",\n+      \"    'x-ratelimit-remaining-tokens': '149999967',\\n\",\n+      \"    'x-ratelimit-reset-requests': '2ms',\\n\",\n+      \"    'x-ratelimit-reset-tokens': '0s',\\n\",\n+      \"    'x-request-id': 'req_f810058e7f047fafcb713575c4419161'\\n\",\n+      \"  },\\n\",\n+      \"  request_id: 'req_f810058e7f047fafcb713575c4419161',\\n\",\n+      \"  error: {\\n\",\n+      \"    message: \\\"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EHf8MIcTdsLCZcFVlcH4hxJw\\\",\\n\",\n+      \"    type: 'invalid_request_error',\\n\",\n+      \"    param: 'messages',\\n\",\n+      \"    code: null\\n\",\n+      \"  },\\n\",\n+      \"  code: null,\\n\",\n+      \"  param: 'messages',\\n\",\n+      \"  type: 'invalid_request_error',\\n\",\n+      \"  attemptNumber: 1,\\n\",\n+      \"  retriesLeft: 6\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const toolResponse1 = await dummyTool.invoke(responseMessage.tool_calls![0]);\\n\",\n+    \"\\n\",\n+    \"chatHistory.push(responseMessage);\\n\",\n+    \"chatHistory.push(toolResponse1);\\n\",\n+    \"\\n\",\n+    \"await modelWithTools.invoke(chatHistory);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"If we add a second response, the call will succeed as expected because we now have one tool response per tool call:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-AIgTPDBm1epnnLHx0tPFTgpsf8Ay6\\\",\\n\",\n+      \"  \\\"content\\\": \\\"The tool \\\\\\\"foo\\\\\\\" was called twice, and both times returned the result: \\\\\\\"action complete!\\\\\\\".\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"promptTokens\\\": 98,\\n\",\n+      \"      \\\"completionTokens\\\": 21,\\n\",\n+      \"      \\\"totalTokens\\\": 119\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"usage\\\": {\\n\",\n+      \"      \\\"prompt_tokens\\\": 98,\\n\",\n+      \"      \\\"completion_tokens\\\": 21,\\n\",\n+      \"      \\\"total_tokens\\\": 119,\\n\",\n+      \"      \\\"prompt_tokens_details\\\": {\\n\",\n+      \"        \\\"cached_tokens\\\": 0\\n\",\n+      \"      },\\n\",\n+      \"      \\\"completion_tokens_details\\\": {\\n\",\n+      \"        \\\"reasoning_tokens\\\": 0\\n\",\n+      \"      }\\n\",\n+      \"    },\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_e2bde53e6e\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"output_tokens\\\": 21,\\n\",\n+      \"    \\\"input_tokens\\\": 98,\\n\",\n+      \"    \\\"total_tokens\\\": 119,\\n\",\n+      \"    \\\"input_token_details\\\": {\\n\",\n+      \"      \\\"cache_read\\\": 0\\n\",\n+      \"    },\\n\",\n+      \"    \\\"output_token_details\\\": {\\n\",\n+      \"      \\\"reasoning\\\": 0\\n\",\n+      \"    }\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const toolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);\\n\",\n+    \"\\n\",\n+    \"chatHistory.push(toolResponse2);\\n\",\n+    \"\\n\",\n+    \"await modelWithTools.invoke(chatHistory);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"But if we add a duplicate, extra tool response, the call will fail again:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"BadRequestError: 400 Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\\n\",\n+      \"    at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)\\n\",\n+      \"    at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)\\n\",\n+      \"    at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)\\n\",\n+      \"    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n\",\n+      \"    at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29\\n\",\n+      \"    at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\\n\",\n+      \"  status: 400,\\n\",\n+      \"  headers: {\\n\",\n+      \"    'access-control-expose-headers': 'X-Request-ID',\\n\",\n+      \"    'alt-svc': 'h3=\\\":443\\\"; ma=86400',\\n\",\n+      \"    'cf-cache-status': 'DYNAMIC',\\n\",\n+      \"    'cf-ray': '8d31d57dff5e0f3b-EWR',\\n\",\n+      \"    connection: 'keep-alive',\\n\",\n+      \"    'content-length': '233',\\n\",\n+      \"    'content-type': 'application/json',\\n\",\n+      \"    date: 'Tue, 15 Oct 2024 18:22:19 GMT',\\n\",\n+      \"    'openai-organization': 'langchain',\\n\",\n+      \"    'openai-processing-ms': '36',\\n\",\n+      \"    'openai-version': '2020-10-01',\\n\",\n+      \"    server: 'cloudflare',\\n\",\n+      \"    'set-cookie': '__cf_bm=QUsNlSGxVeIbscI0rm2YR3U9aUFLNxxqh1i_3aYBGN4-1729016539-1.0.1.1-sKRUvxHkQXvlb5LaqASkGtIwPMWUF5x9kF0ut8NLP6e0FVKEhdIEkEe6lYA1toW45JGTwp98xahaX7wt9CO4AA; path=/; expires=Tue, 15-Oct-24 18:52:19 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=J6fN8u8HUieCeyLDI59mi_0r_W0DgiO207wEtvrmT9Y-1729016539919-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\\n\",\n+      \"    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\\n\",\n+      \"    'x-content-type-options': 'nosniff',\\n\",\n+      \"    'x-ratelimit-limit-requests': '30000',\\n\",\n+      \"    'x-ratelimit-limit-tokens': '150000000',\\n\",\n+      \"    'x-ratelimit-remaining-requests': '29999',\\n\",\n+      \"    'x-ratelimit-remaining-tokens': '149999956',\\n\",\n+      \"    'x-ratelimit-reset-requests': '2ms',\\n\",\n+      \"    'x-ratelimit-reset-tokens': '0s',\\n\",\n+      \"    'x-request-id': 'req_aebfebbb9af2feaf2e9683948e431676'\\n\",\n+      \"  },\\n\",\n+      \"  request_id: 'req_aebfebbb9af2feaf2e9683948e431676',\\n\",\n+      \"  error: {\\n\",\n+      \"    message: \\\"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\\\",\\n\",\n+      \"    type: 'invalid_request_error',\\n\",\n+      \"    param: 'messages.[4].role',\\n\",\n+      \"    code: null\\n\",\n+      \"  },\\n\",\n+      \"  code: null,\\n\",\n+      \"  param: 'messages.[4].role',\\n\",\n+      \"  type: 'invalid_request_error',\\n\",\n+      \"  attemptNumber: 1,\\n\",\n+      \"  retriesLeft: 6\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const duplicateToolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);\\n\",\n+    \"\\n\",\n+    \"chatHistory.push(duplicateToolResponse2);\\n\",\n+    \"\\n\",\n+    \"await modelWithTools.invoke(chatHistory);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"You should additionally not pass `ToolMessages` back to to a model if they are not preceded by an `AIMessage` with tool calls. For example, this will fail:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 5,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"BadRequestError: 400 Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\\n\",\n+      \"    at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)\\n\",\n+      \"    at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)\\n\",\n+      \"    at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)\\n\",\n+      \"    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n\",\n+      \"    at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29\\n\",\n+      \"    at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\\n\",\n+      \"  status: 400,\\n\",\n+      \"  headers: {\\n\",\n+      \"    'access-control-expose-headers': 'X-Request-ID',\\n\",\n+      \"    'alt-svc': 'h3=\\\":443\\\"; ma=86400',\\n\",\n+      \"    'cf-cache-status': 'DYNAMIC',\\n\",\n+      \"    'cf-ray': '8d31d5da7fba19aa-EWR',\\n\",\n+      \"    connection: 'keep-alive',\\n\",\n+      \"    'content-length': '233',\\n\",\n+      \"    'content-type': 'application/json',\\n\",\n+      \"    date: 'Tue, 15 Oct 2024 18:22:34 GMT',\\n\",\n+      \"    'openai-organization': 'langchain',\\n\",\n+      \"    'openai-processing-ms': '25',\\n\",\n+      \"    'openai-version': '2020-10-01',\\n\",\n+      \"    server: 'cloudflare',\\n\",\n+      \"    'set-cookie': '__cf_bm=qK6.PWACr7IYuMafLpxumD4CrFnwHQiJn4TiGkrNTBk-1729016554-1.0.1.1-ECIk0cvh1wOfsK41a1Ce7npngsUDRRG93_yinP4.kVIWu1eX0CFG19iZ8yfGXedyPo6Wh1CKTGLk_3Qwrg.blA; path=/; expires=Tue, 15-Oct-24 18:52:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=IVTqysqHo4VUVJ.tVTcGg0rnXGWTbSSzX5mcUVrw8BU-1729016554732-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\\n\",\n+      \"    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\\n\",\n+      \"    'x-content-type-options': 'nosniff',\\n\",\n+      \"    'x-ratelimit-limit-requests': '30000',\\n\",\n+      \"    'x-ratelimit-limit-tokens': '150000000',\\n\",\n+      \"    'x-ratelimit-remaining-requests': '29999',\\n\",\n+      \"    'x-ratelimit-remaining-tokens': '149999978',\\n\",\n+      \"    'x-ratelimit-reset-requests': '2ms',\\n\",\n+      \"    'x-ratelimit-reset-tokens': '0s',\\n\",\n+      \"    'x-request-id': 'req_59339f8163ef5bd3f0308a212611dfea'\\n\",\n+      \"  },\\n\",\n+      \"  request_id: 'req_59339f8163ef5bd3f0308a212611dfea',\\n\",\n+      \"  error: {\\n\",\n+      \"    message: \\\"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\\\",\\n\",\n+      \"    type: 'invalid_request_error',\\n\",\n+      \"    param: 'messages.[0].role',\\n\",\n+      \"    code: null\\n\",\n+      \"  },\\n\",\n+      \"  code: null,\\n\",\n+      \"  param: 'messages.[0].role',\\n\",\n+      \"  type: 'invalid_request_error',\\n\",\n+      \"  attemptNumber: 1,\\n\",\n+      \"  retriesLeft: 6\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"await modelWithTools.invoke([{\\n\",\n+    \"  role: \\\"tool\\\",\\n\",\n+    \"  content: \\\"action completed!\\\",\\n\",\n+    \"  tool_call_id: \\\"dummy\\\",\\n\",\n+    \"}])\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"See [this guide](/docs/how_to/tool_results_pass_to_model/) for more details on tool calling.\\n\",\n+    \"\\n\",\n+    \"## Troubleshooting\\n\",\n+    \"\\n\",\n+    \"The following may help resolve this error:\\n\",\n+    \"\\n\",\n+    \"- If you are using a custom executor rather than a prebuilt one like LangGraph's [`ToolNode`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html)\\n\",\n+    \"  or the legacy LangChain [AgentExecutor](/docs/how_to/agent_executor), verify that you are invoking and returning the result for one tool per tool call.\\n\",\n+    \"- If you are using [few-shot tool call examples](/docs/how_to/tools_few_shot) with messages that you manually create, and you want to simulate a failure,\\n\",\n+    \"  you still need to pass back a `ToolMessage` whose content indicates that failure.\\n\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}",
          "docs/core_docs/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE.mdx": "@@ -0,0 +1,33 @@\n+# MESSAGE_COERCION_FAILURE\n+\n+Several modules in LangChain take [`MessageLike`](https://api.js.langchain.com/types/_langchain_core.messages.BaseMessageLike.html)\n+objects in place of formal [`BaseMessage`](/docs/concepts#message-types) classes. These include OpenAI style message objects (`{ role: \"user\", content: \"Hello world!\" }`),\n+tuples, and plain strings (which are converted to [`HumanMessages`](/docs/concepts#humanmessage)).\n+\n+If one of these modules receives a value outside of one of these formats, you will receive an error like the following:\n+\n+```ts\n+const badlyFormattedMessageObject = {\n+  role: \"foo\",\n+  randomNonContentValue: \"bar\",\n+};\n+\n+await model.invoke([badlyFormattedMessageObject]);\n+```\n+\n+```\n+Error: Unable to coerce message from array: only human, AI, system, or tool message coercion is currently supported.\n+\n+Received: {\n+  \"role\": \"foo\",\n+  \"randomNonContentValue\": \"bar\",\n+}\n+```\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Ensure that all inputs to chat models are an array of LangChain message classes or a supported message-like.\n+  - Check that there is no stringification or other unexpected transformation occuring.\n+- Check the error's stack trace and add log or debugger statements.",
          "docs/core_docs/docs/troubleshooting/errors/MODEL_AUTHENTICATION.mdx": "@@ -0,0 +1,20 @@\n+# MODEL_AUTHENTICATION\n+\n+Your model provider is denying you access to their service.\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Confirm that your API key or other credentials are correct.\n+- If you are relying on an environment variable to authenticate, confirm that the variable name is correct and that it has a value set.\n+  - Note that some environments, like Cloudflare Workers, do not support environment variables.\n+  - For some models, you can try explicitly passing an `apiKey` parameter to rule out any environment variable issues like this:\n+\n+```ts\n+const model = new ChatOpenAI({\n+  apiKey: \"YOUR_KEY_HERE\",\n+});\n+```\n+\n+- If you are using a proxy or other custom endpoint, make sure that your custom provider does not expect an alternative authentication scheme.",
          "docs/core_docs/docs/troubleshooting/errors/MODEL_NOT_FOUND.mdx": "@@ -0,0 +1,10 @@\n+# MODEL_NOT_FOUND\n+\n+The model name you have specified is not acknowledged by your provider.\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Double check the model string you are passing in.\n+- If you are using a proxy or other alternative host with a model wrapper, confirm that the permitted model names are not restricted or altered.",
          "docs/core_docs/docs/troubleshooting/errors/MODEL_RATE_LIMIT.mdx": "@@ -0,0 +1,14 @@\n+# MODEL_RATE_LIMIT\n+\n+You have hit the maximum number of requests that a model provider allows over a given time period and are being temporarily blocked.\n+Generally, this error is temporary and your limit will reset after a certain amount of time.\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Contact your model provider and ask for a rate limit increase.\n+- If many of your incoming requests are the same, utilize [model response caching](/docs/how_to/chat_model_caching/).\n+- Spread requests across different providers if your application allows it.\n+- Set a higher number of [max retries](https://api.js.langchain.com/interfaces/_langchain_core.language_models_base.BaseLanguageModelParams.html#maxRetries) when initializing your model.\n+  LangChain will use an exponential backoff strategy for requests that fail in this way, so the retry may occur when your limits have reset.",
          "docs/core_docs/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE.mdx": "@@ -0,0 +1,36 @@\n+# OUTPUT_PARSING_FAILURE\n+\n+An [output parser](/docs/concepts#output-parsers) was unable to handle model output as expected.\n+\n+To illustrate this, let's say you have an output parser that expects a chat model to output JSON surrounded by a markdown code tag (triple backticks). Here would be an example of good input:\n+\n+````ts\n+AIMessage {\n+  content: \"```\\n{\\\"foo\\\": \\\"bar\\\"}\\n```\"\n+}\n+````\n+\n+Internally, our output parser might try to strip out the markdown fence and newlines and then run `JSON.parse()`.\n+\n+If instead the chat model generated an output with malformed JSON like this:\n+\n+````ts\n+AIMessage {\n+  content: \"```\\n{\\\"foo\\\":\\n```\"\n+}\n+````\n+\n+When our output parser attempts to parse this, the `JSON.parse()` call will fail.\n+\n+Note that some prebuilt constructs like [legacy LangChain agents](/docs/how_to/agent_executor) and chains may use output parsers internally,\n+so you may see this error even if you're not visibly instantiating and using an output parser.\n+\n+## Troubleshooting\n+\n+The following may help resolve this error:\n+\n+- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n+  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraphjs/) to compose your logic explicitly instead.\n+- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n+- If you are using a smaller or less capable model, try using a more capable one.\n+- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).",
          "docs/core_docs/docs/troubleshooting/errors/index.mdx": "@@ -0,0 +1,12 @@\n+# Error reference\n+\n+This page contains guides around resolving common errors you may find while building with LangChain.\n+Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n+\n+- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT)\n+- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS)\n+- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE)\n+- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION)\n+- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND)\n+- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT)\n+- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE)",
          "docs/core_docs/docs/tutorials/chatbot.ipynb": "@@ -2,25 +2,24 @@\n  \"cells\": [\n   {\n    \"cell_type\": \"raw\",\n-   \"metadata\": {},\n+   \"metadata\": {\n+    \"vscode\": {\n+     \"languageId\": \"raw\"\n+    }\n+   },\n    \"source\": [\n     \"---\\n\",\n     \"sidebar_position: 1\\n\",\n+    \"keywords: [conversationchain]\\n\",\n     \"---\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# Build a Chatbot\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"## Overview\\n\",\n+    \"# Build a Chatbot\\n\",\n+    \"\\n\",\n     \"\\n\",\n     \":::info Prerequisites\\n\",\n     \"\\n\",\n@@ -30,34 +29,57 @@\n     \"- [Prompt Templates](/docs/concepts/#prompt-templates)\\n\",\n     \"- [Chat History](/docs/concepts/#chat-history)\\n\",\n     \"\\n\",\n+    \"This guide requires `langgraph >= 0.2.28`.\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"This tutorial previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/tutorials/chatbot/).\\n\",\n+    \"\\n\",\n+    \"The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).\\n\",\n+    \"\\n\",\n     \":::\\n\",\n     \"\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"## Overview\\n\",\n+    \"\\n\",\n     \"We'll go over an example of how to design and implement an LLM-powered chatbot. \\n\",\n     \"This chatbot will be able to have a conversation and remember previous interactions.\\n\",\n     \"\\n\",\n+    \"\\n\",\n     \"Note that this chatbot that we build will only use the language model to have a conversation.\\n\",\n     \"There are several other related concepts that you may be looking for:\\n\",\n     \"\\n\",\n     \"- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data\\n\",\n-    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/): Build a chatbot that can take actions\\n\",\n+    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/): Build a chatbot that can take actions\\n\",\n     \"\\n\",\n     \"This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\n\",\n     \"\\n\",\n     \"## Setup\\n\",\n     \"\\n\",\n+    \"### Jupyter Notebook\\n\",\n+    \"\\n\",\n+    \"This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\n\",\n+    \"\\n\",\n+    \"This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.\\n\",\n+    \"\\n\",\n     \"### Installation\\n\",\n     \"\\n\",\n-    \"To install LangChain run:\\n\",\n+    \"For this tutorial we will need `@langchain/core` and `langgraph`:\\n\",\n     \"\\n\",\n     \"```{=mdx}\\n\",\n     \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\"\\n\",\n     \"\\n\",\n     \"<Npm2Yarn>\\n\",\n-    \"  langchain @langchain/core\\n\",\n+    \"  @langchain/core @langchain/langgraph uuid\\n\",\n     \"</Npm2Yarn>\\n\",\n     \"```\\n\",\n     \"\\n\",\n-    \"\\n\",\n     \"For more details, see our [Installation guide](/docs/how_to/installation).\\n\",\n     \"\\n\",\n     \"### LangSmith\\n\",\n@@ -68,87 +90,77 @@\n     \"\\n\",\n     \"After you sign up at the link above, make sure to set your environment variables to start logging traces:\\n\",\n     \"\\n\",\n-    \"```shell\\n\",\n-    \"export LANGCHAIN_TRACING_V2=\\\"true\\\"\\n\",\n-    \"export LANGCHAIN_API_KEY=\\\"...\\\"\\n\",\n-    \"\\n\",\n-    \"# Reduce tracing latency if you are not in a serverless environment\\n\",\n-    \"# export LANGCHAIN_CALLBACKS_BACKGROUND=true\\n\",\n+    \"```typescript\\n\",\n+    \"process.env.LANGCHAIN_TRACING_V2 = \\\"true\\\"\\n\",\n+    \"process.env.LANGCHAIN_API_KEY = \\\"...\\\"\\n\",\n     \"```\\n\",\n     \"\\n\",\n     \"## Quickstart\\n\",\n     \"\\n\",\n-    \"First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\\n\",\n+    \"First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\",\n     \"\\n\",\n     \"```{=mdx}\\n\",\n     \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n     \"\\n\",\n-    \"<ChatModelTabs />\\n\",\n-    \"```\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"Let's first use the model directly. `ChatModel`s are instances of LangChain \\\"Runnables\\\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the `.invoke` method.\"\n+    \"<ChatModelTabs customVarName=\\\"llm\\\" />\\n\",\n+    \"```\\n\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 1,\n+   \"execution_count\": 27,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n     \"// @lc-docs-hide-cell\\n\",\n     \"\\n\",\n     \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n     \"\\n\",\n-    \"const model = new ChatOpenAI({\\n\",\n-    \"  model: \\\"gpt-4o-mini\\\",\\n\",\n-    \"  temperature: 0,\\n\",\n-    \"});\"\n+    \"const llm = new ChatOpenAI({ model: \\\"gpt-4o-mini\\\" })\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Let's first use the model directly. `ChatModel`s are instances of LangChain \\\"Runnables\\\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the `.invoke` method.\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 28,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  \\\"id\\\": \\\"chatcmpl-A64of8iD4GIFNSYlOaFHxPdCeyl9E\\\",\\n\",\n-       \"  \\\"content\\\": \\\"Hi Bob! How can I assist you today?\\\",\\n\",\n-       \"  \\\"additional_kwargs\\\": {},\\n\",\n-       \"  \\\"response_metadata\\\": {\\n\",\n-       \"    \\\"tokenUsage\\\": {\\n\",\n-       \"      \\\"completionTokens\\\": 10,\\n\",\n-       \"      \\\"promptTokens\\\": 11,\\n\",\n-       \"      \\\"totalTokens\\\": 21\\n\",\n-       \"    },\\n\",\n-       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n-       \"  },\\n\",\n-       \"  \\\"tool_calls\\\": [],\\n\",\n-       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n-       \"  \\\"usage_metadata\\\": {\\n\",\n-       \"    \\\"input_tokens\\\": 11,\\n\",\n-       \"    \\\"output_tokens\\\": 10,\\n\",\n-       \"    \\\"total_tokens\\\": 21\\n\",\n-       \"  }\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 2,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXeSO4JQpxO96lj7iudUptJ6nfW\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Hi Bob! How can I assist you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 10,\\n\",\n+      \"      \\\"promptTokens\\\": 10,\\n\",\n+      \"      \\\"totalTokens\\\": 20\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 10,\\n\",\n+      \"    \\\"output_tokens\\\": 10,\\n\",\n+      \"    \\\"total_tokens\\\": 20\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"\\n\",\n-    \"await model.invoke([new HumanMessage({ content: \\\"Hi! I'm Bob\\\" })]);\"\n+    \"await llm.invoke([{ role: \\\"user\\\", content: \\\"Hi im bob\\\" }])\"\n    ]\n   },\n   {\n@@ -160,48 +172,46 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 29,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  \\\"id\\\": \\\"chatcmpl-A64ogC7owxmPla3ggZERNCFZpVHSp\\\",\\n\",\n-       \"  \\\"content\\\": \\\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. If you'd like to tell me your name, feel free!\\\",\\n\",\n-       \"  \\\"additional_kwargs\\\": {},\\n\",\n-       \"  \\\"response_metadata\\\": {\\n\",\n-       \"    \\\"tokenUsage\\\": {\\n\",\n-       \"      \\\"completionTokens\\\": 39,\\n\",\n-       \"      \\\"promptTokens\\\": 11,\\n\",\n-       \"      \\\"totalTokens\\\": 50\\n\",\n-       \"    },\\n\",\n-       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n-       \"  },\\n\",\n-       \"  \\\"tool_calls\\\": [],\\n\",\n-       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n-       \"  \\\"usage_metadata\\\": {\\n\",\n-       \"    \\\"input_tokens\\\": 11,\\n\",\n-       \"    \\\"output_tokens\\\": 39,\\n\",\n-       \"    \\\"total_tokens\\\": 50\\n\",\n-       \"  }\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 3,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXe1Zih4gMe3XgotWL83xeWub2h\\\",\\n\",\n+      \"  \\\"content\\\": \\\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me during our conversation. If you'd like to tell me your name, feel free to do so!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 39,\\n\",\n+      \"      \\\"promptTokens\\\": 10,\\n\",\n+      \"      \\\"totalTokens\\\": 49\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 10,\\n\",\n+      \"    \\\"output_tokens\\\": 39,\\n\",\n+      \"    \\\"total_tokens\\\": 49\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"await model.invoke([new HumanMessage({ content: \\\"What's my name?\\\" })])\"\n+    \"await llm.invoke([{ role: \\\"user\\\", content: \\\"Whats my name\\\" }])\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Let's take a look at the example [LangSmith trace](https://smith.langchain.com/public/e5a0ae1b-32b9-4beb-836d-38f40bfa6762/r)\\n\",\n+    \"Let's take a look at the example [LangSmith trace](https://smith.langchain.com/public/3b768e44-a319-453a-bd6e-30f9df75f16a/r)\\n\",\n     \"\\n\",\n     \"We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\\n\",\n     \"This makes for a terrible chatbot experience!\\n\",\n@@ -211,49 +221,43 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 30,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"AIMessage {\\n\",\n-       \"  \\\"id\\\": \\\"chatcmpl-A64ohhg3P4BuIiw8mUCLI3zYHNOvS\\\",\\n\",\n-       \"  \\\"content\\\": \\\"Your name is Bob! How can I help you today, Bob?\\\",\\n\",\n-       \"  \\\"additional_kwargs\\\": {},\\n\",\n-       \"  \\\"response_metadata\\\": {\\n\",\n-       \"    \\\"tokenUsage\\\": {\\n\",\n-       \"      \\\"completionTokens\\\": 14,\\n\",\n-       \"      \\\"promptTokens\\\": 33,\\n\",\n-       \"      \\\"totalTokens\\\": 47\\n\",\n-       \"    },\\n\",\n-       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n-       \"  },\\n\",\n-       \"  \\\"tool_calls\\\": [],\\n\",\n-       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n-       \"  \\\"usage_metadata\\\": {\\n\",\n-       \"    \\\"input_tokens\\\": 33,\\n\",\n-       \"    \\\"output_tokens\\\": 14,\\n\",\n-       \"    \\\"total_tokens\\\": 47\\n\",\n-       \"  }\\n\",\n-       \"}\"\n-      ]\n-     },\n-     \"execution_count\": 4,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXfX4Fnp247rOxyPlBUYMQgahj2\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Your name is Bob! How can I help you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 12,\\n\",\n+      \"      \\\"promptTokens\\\": 33,\\n\",\n+      \"      \\\"totalTokens\\\": 45\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 33,\\n\",\n+      \"    \\\"output_tokens\\\": 12,\\n\",\n+      \"    \\\"total_tokens\\\": 45\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"import { AIMessage } from \\\"@langchain/core/messages\\\"\\n\",\n-    \"\\n\",\n-    \"await model.invoke(\\n\",\n-    \"  [\\n\",\n-    \"    new HumanMessage({ content: \\\"Hi! I'm Bob\\\" }),\\n\",\n-    \"    new AIMessage({ content: \\\"Hello Bob! How can I assist you today?\\\" }),\\n\",\n-    \"    new HumanMessage({ content: \\\"What's my name?\\\" }),\\n\",\n-    \"  ]\\n\",\n-    \");\"\n+    \"await llm.invoke([\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"Hi! I'm Bob\\\" },\\n\",\n+    \"  { role: \\\"assistant\\\", content: \\\"Hello Bob! How can I assist you today?\\\" },\\n\",\n+    \"  { role: \\\"user\\\", content: \\\"What's my name?\\\" }\\n\",\n+    \"]);\"\n    ]\n   },\n   {\n@@ -270,153 +274,208 @@\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"## Message History\\n\",\n+    \"## Message persistence\\n\",\n+    \"\\n\",\n+    \"[LangGraph](https://langchain-ai.github.io/langgraphjs/) implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\n\",\n+    \"\\n\",\n+    \"Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\n\",\n     \"\\n\",\n-    \"We can use a Message History class to wrap our model and make it stateful.\\n\",\n-    \"This will keep track of inputs and outputs of the model, and store them in some datastore.\\n\",\n-    \"Future interactions will then load those messages and pass them into the chain as part of the input.\\n\",\n-    \"Let's see how to use this!\"\n+    \"LangGraph comes with a simple in-memory checkpointer, which we use below.\"\n    ]\n   },\n   {\n-   \"cell_type\": \"markdown\",\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 31,\n    \"metadata\": {},\n+   \"outputs\": [],\n    \"source\": [\n-    \"We import the relevant classes and set up our chain which wraps the model and adds in this message history. A key part here is the function we pass into as the `getSessionHistory()`. This function is expected to take in a `sessionId` and return a Message History object. This `sessionId` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain.\\n\",\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n     \"\\n\",\n-    \"Let's also create a simple chain by adding a prompt to help with formatting:\"\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  const response = await llm.invoke(state.messages);\\n\",\n+    \"  return { messages: response };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"// Define a new graph\\n\",\n+    \"const workflow = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  // Define the node and edge\\n\",\n+    \"  .addNode(\\\"model\\\", callModel)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"// Add memory\\n\",\n+    \"const memory = new MemorySaver();\\n\",\n+    \"const app = workflow.compile({ checkpointer: memory });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 32,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"// We use an ephemeral, in-memory chat history for this demo.\\n\",\n-    \"import { InMemoryChatMessageHistory } from \\\"@langchain/core/chat_history\\\";\\n\",\n-    \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n-    \"import { RunnableWithMessageHistory } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"import { v4 as uuidv4 } from \\\"uuid\\\";\\n\",\n     \"\\n\",\n-    \"const messageHistories: Record<string, InMemoryChatMessageHistory> = {};\\n\",\n-    \"\\n\",\n-    \"const prompt = ChatPromptTemplate.fromMessages([\\n\",\n-    \"  [\\\"system\\\", `You are a helpful assistant who remembers all details the user shares with you.`],\\n\",\n-    \"  [\\\"placeholder\\\", \\\"{chat_history}\\\"],\\n\",\n-    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n-    \"]);\\n\",\n-    \"\\n\",\n-    \"const chain = prompt.pipe(model);\\n\",\n-    \"\\n\",\n-    \"const withMessageHistory = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: chain,\\n\",\n-    \"  getMessageHistory: async (sessionId) => {\\n\",\n-    \"    if (messageHistories[sessionId] === undefined) {\\n\",\n-    \"      messageHistories[sessionId] = new InMemoryChatMessageHistory();\\n\",\n-    \"    }\\n\",\n-    \"    return messageHistories[sessionId];\\n\",\n-    \"  },\\n\",\n-    \"  inputMessagesKey: \\\"input\\\",\\n\",\n-    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n-    \"});\"\n+    \"const config = { configurable: { thread_id: uuidv4() } };\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `session_id`. This should look like:\"\n+    \"This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\n\",\n+    \"\\n\",\n+    \"We can then invoke the application:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 33,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"Hi Bob! How can I assist you today?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 6,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXfjqCno78CGXCHoAgamqXG1pnZ\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Hi Bob! How can I assist you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 10,\\n\",\n+      \"      \\\"promptTokens\\\": 12,\\n\",\n+      \"      \\\"totalTokens\\\": 22\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 12,\\n\",\n+      \"    \\\"output_tokens\\\": 10,\\n\",\n+      \"    \\\"total_tokens\\\": 22\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const config = {\\n\",\n-    \"  configurable: {\\n\",\n-    \"    sessionId: \\\"abc2\\\"\\n\",\n+    \"const input = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"Hi! I'm Bob.\\\",\\n\",\n     \"  }\\n\",\n-    \"};\\n\",\n-    \"\\n\",\n-    \"const response = await withMessageHistory.invoke({\\n\",\n-    \"  input: \\\"Hi! I'm Bob\\\",\\n\",\n-    \"}, config);\\n\",\n-    \"\\n\",\n-    \"response.content;\"\n+    \"]\\n\",\n+    \"const output = await app.invoke({ messages: input }, config)\\n\",\n+    \"// The output contains all messages in the state.\\n\",\n+    \"// This will long the last message in the conversation.\\n\",\n+    \"console.log(output.messages[output.messages.length - 1]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 34,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"Your name is Bob. How can I help you today?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 7,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXgzHFHk4KsaNmDJyvflHq4JY2L\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Your name is Bob! How can I help you today, Bob?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 14,\\n\",\n+      \"      \\\"promptTokens\\\": 34,\\n\",\n+      \"      \\\"totalTokens\\\": 48\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 34,\\n\",\n+      \"    \\\"output_tokens\\\": 14,\\n\",\n+      \"    \\\"total_tokens\\\": 48\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const followupResponse = await withMessageHistory.invoke({\\n\",\n-    \"  input: \\\"What's my name?\\\",\\n\",\n-    \"}, config);\\n\",\n-    \"\\n\",\n-    \"followupResponse.content\"\n+    \"const input2 = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"What's my name?\\\",\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"const output2 = await app.invoke({ messages: input2 }, config)\\n\",\n+    \"console.log(output2.messages[output2.messages.length - 1]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Great! Our chatbot now remembers things about us. If we change the config to reference a different `session_id`, we can see that it starts the conversation fresh.\"\n+    \"Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh.\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 35,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"I'm sorry, but I don't have your name. If you tell me, I'll remember it for our future conversations\\\"\\u001b[39m... 1 more character\"\n-      ]\n-     },\n-     \"execution_count\": 8,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXhT4EVx8mGgmKXJ1s132qEluxR\\\",\\n\",\n+      \"  \\\"content\\\": \\\"I'm sorry, but I don’t have access to personal data about individuals unless it has been shared in the course of our conversation. Therefore, I don't know your name. How can I assist you today?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 41,\\n\",\n+      \"      \\\"promptTokens\\\": 11,\\n\",\n+      \"      \\\"totalTokens\\\": 52\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 11,\\n\",\n+      \"    \\\"output_tokens\\\": 41,\\n\",\n+      \"    \\\"total_tokens\\\": 52\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const config2 = {\\n\",\n-    \"  configurable: {\\n\",\n-    \"    sessionId: \\\"abc3\\\"\\n\",\n+    \"const config2 = { configurable: { thread_id: uuidv4() } }\\n\",\n+    \"const input3 = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"What's my name?\\\",\\n\",\n     \"  }\\n\",\n-    \"};\\n\",\n-    \"\\n\",\n-    \"const response2 = await withMessageHistory.invoke({\\n\",\n-    \"  input: \\\"What's my name?\\\",\\n\",\n-    \"}, config2);\\n\",\n-    \"\\n\",\n-    \"response2.content\"\n+    \"]\\n\",\n+    \"const output3 = await app.invoke({ messages: input3 }, config2)\\n\",\n+    \"console.log(output3.messages[output3.messages.length - 1]);\"\n    ]\n   },\n   {\n@@ -428,338 +487,623 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 36,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"Your name is Bob. What would you like to talk about?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 9,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXhZmtzvV3kqKig47xxhKEnvVfH\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Your name is Bob! If there's anything else you'd like to talk about or ask, feel free!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 20,\\n\",\n+      \"      \\\"promptTokens\\\": 60,\\n\",\n+      \"      \\\"totalTokens\\\": 80\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 60,\\n\",\n+      \"    \\\"output_tokens\\\": 20,\\n\",\n+      \"    \\\"total_tokens\\\": 80\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const config3 = {\\n\",\n-    \"  configurable: {\\n\",\n-    \"    sessionId: \\\"abc2\\\"\\n\",\n-    \"  }\\n\",\n-    \"};\\n\",\n+    \"const output4 = await app.invoke({ messages: input2 }, config)\\n\",\n+    \"console.log(output4.messages[output4.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"This is how we can support a chatbot having conversations with many users!\\n\",\n+    \"\\n\",\n+    \"Right now, all we've done is add a simple persistence layer around the model. We can start to make the more complicated and personalized by adding in a prompt template.\\n\",\n     \"\\n\",\n-    \"const response3 = await withMessageHistory.invoke({\\n\",\n-    \"  input: \\\"What's my name?\\\",\\n\",\n-    \"}, config3);\\n\",\n+    \"## Prompt templates\\n\",\n     \"\\n\",\n-    \"response3.content\"\n+    \"Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\\n\",\n+    \"\\n\",\n+    \"To add in a system message, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in.\"\n    ]\n   },\n   {\n-   \"cell_type\": \"markdown\",\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 37,\n    \"metadata\": {},\n+   \"outputs\": [],\n    \"source\": [\n-    \"This is how we can support a chatbot having conversations with many users!\"\n+    \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"\\n\",\n+    \"const prompt = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"You talk like a pirate. Answer all questions to the best of your ability.\\\"],\\n\",\n+    \"  new MessagesPlaceholder(\\\"messages\\\"),\\n\",\n+    \"]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"## Managing Conversation History\\n\",\n-    \"\\n\",\n-    \"One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\n\",\n-    \"\\n\",\n-    \"**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**\\n\",\n-    \"\\n\",\n-    \"We can do this by adding a simple step in front of the prompt that modifies the `chat_history` key appropriately, and then wrap that new chain in the Message History class. First, let's define a function that will modify the messages passed in. Let's make it so that it selects the 10 most recent messages. We can then create a new chain by adding that at the start.\"\n+    \"We can now update our application to incorporate this template:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 38,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import type { BaseMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-    \"import { RunnablePassthrough, RunnableSequence } from \\\"@langchain/core/runnables\\\";\\n\",\n-    \"\\n\",\n-    \"type ChainInput = {\\n\",\n-    \"  chat_history: BaseMessage[];\\n\",\n-    \"  input: string;\\n\",\n-    \"}\\n\",\n+    \"import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel2 = async (state: typeof MessagesAnnotation.State) => {\\n\",\n+    \"  // highlight-start\\n\",\n+    \"  const chain = prompt.pipe(llm);\\n\",\n+    \"  const response = await chain.invoke(state);\\n\",\n+    \"  // highlight-end\\n\",\n+    \"  // Update message history with response:\\n\",\n+    \"  return { messages: [response] };\\n\",\n+    \"};\\n\",\n     \"\\n\",\n-    \"const filterMessages = (input: ChainInput) => input.chat_history.slice(-10);\\n\",\n+    \"// Define a new graph\\n\",\n+    \"const workflow2 = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  // Define the (single) node in the graph\\n\",\n+    \"  .addNode(\\\"model\\\", callModel2)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n     \"\\n\",\n-    \"const chain2 = RunnableSequence.from<ChainInput>([\\n\",\n-    \"  RunnablePassthrough.assign({\\n\",\n-    \"    chat_history: filterMessages\\n\",\n-    \"  }),\\n\",\n-    \"  prompt,\\n\",\n-    \"  model,\\n\",\n-    \"]);\"\n+    \"// Add memory\\n\",\n+    \"const app2 = workflow2.compile({ checkpointer: new MemorySaver() });\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Let's now try it out! If we create a list of messages more than 10 messages long, we can see what it no longer remembers information in the early messages.\"\n+    \"We invoke the application in the same way:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 39,\n    \"metadata\": {},\n-   \"outputs\": [],\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXio2Vy1YNRDiFdKKEyN3Yw1B9I\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Ahoy, Jim! What brings ye to these treacherous waters today? Speak up, matey!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 22,\\n\",\n+      \"      \\\"promptTokens\\\": 32,\\n\",\n+      \"      \\\"totalTokens\\\": 54\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 32,\\n\",\n+      \"    \\\"output_tokens\\\": 22,\\n\",\n+      \"    \\\"total_tokens\\\": 54\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n    \"source\": [\n-    \"const messages = [\\n\",\n-    \"  new HumanMessage({ content: \\\"hi! I'm bob\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"hi!\\\" }),\\n\",\n-    \"  new HumanMessage({ content: \\\"I like vanilla ice cream\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"nice\\\" }),\\n\",\n-    \"  new HumanMessage({ content: \\\"whats 2 + 2\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"4\\\" }),\\n\",\n-    \"  new HumanMessage({ content: \\\"thanks\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"No problem!\\\" }),\\n\",\n-    \"  new HumanMessage({ content: \\\"having fun?\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"yes!\\\" }),\\n\",\n-    \"  new HumanMessage({ content: \\\"That's great!\\\" }),\\n\",\n-    \"  new AIMessage({ content: \\\"yes it is!\\\" }),\\n\",\n-    \"];\"\n+    \"const config3 = { configurable: { thread_id: uuidv4() } }\\n\",\n+    \"const input4 = [\\n\",\n+    \"  {\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content: \\\"Hi! I'm Jim.\\\",\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"const output5 = await app2.invoke({ messages: input4 }, config3)\\n\",\n+    \"console.log(output5.messages[output5.messages.length - 1]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 40,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"You haven't shared your name with me yet. What is it?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 12,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXjZNHiT5g7eTf52auWGXDUUcDs\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Ye be callin' yerself Jim, if me memory serves me right! Arrr, what else can I do fer ye, matey?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 31,\\n\",\n+      \"      \\\"promptTokens\\\": 67,\\n\",\n+      \"      \\\"totalTokens\\\": 98\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_3a215618e8\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 67,\\n\",\n+      \"    \\\"output_tokens\\\": 31,\\n\",\n+      \"    \\\"total_tokens\\\": 98\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const response4 = await chain2.invoke(\\n\",\n+    \"const input5 = [\\n\",\n     \"  {\\n\",\n-    \"    chat_history: messages,\\n\",\n-    \"    input: \\\"what's my name?\\\"\\n\",\n+    \"    role: \\\"user\\\",\\n\",\n+    \"    content:  \\\"What is my name?\\\"\\n\",\n     \"  }\\n\",\n-    \")\\n\",\n-    \"response4.content\"\n+    \"]\\n\",\n+    \"const output6 = await app2.invoke({ messages: input5 }, config3)\\n\",\n+    \"console.log(output6.messages[output6.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 41,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"const prompt2 = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", \\\"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\\\"],\\n\",\n+    \"  new MessagesPlaceholder(\\\"messages\\\"),\\n\",\n+    \"]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"But if we ask about information that is within the last ten messages, it still remembers it\"\n+    \"Note that we have added a new `language` input to the prompt. Our application now has two parameters-- the input `messages` and `language`. We should update our application's state to reflect this:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 42,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"\\n\",\n+    \"// Define the State\\n\",\n+    \"const GraphAnnotation = Annotation.Root({\\n\",\n+    \"  ...MessagesAnnotation.spec,\\n\",\n+    \"  language: Annotation<string>(),\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel3 = async (state: typeof GraphAnnotation.State) => {\\n\",\n+    \"  const chain = prompt2.pipe(llm);\\n\",\n+    \"  const response = await chain.invoke(state);\\n\",\n+    \"  return { messages: [response] };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"const workflow3 = new StateGraph(GraphAnnotation)\\n\",\n+    \"  .addNode(\\\"model\\\", callModel3)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"const app3 = workflow3.compile({ checkpointer: new MemorySaver() });\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 43,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"Your favorite ice cream is vanilla!\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 13,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXkq2ZV9xmOBSM2iJbYSn8Epvqa\\\",\\n\",\n+      \"  \\\"content\\\": \\\"¡Hola, Bob! ¿En qué puedo ayudarte hoy?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 12,\\n\",\n+      \"      \\\"promptTokens\\\": 32,\\n\",\n+      \"      \\\"totalTokens\\\": 44\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 32,\\n\",\n+      \"    \\\"output_tokens\\\": 12,\\n\",\n+      \"    \\\"total_tokens\\\": 44\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const response5 = await chain2.invoke(\\n\",\n-    \"  {\\n\",\n-    \"    chat_history: messages,\\n\",\n-    \"    input: \\\"what's my fav ice cream\\\"\\n\",\n-    \"  }\\n\",\n-    \")\\n\",\n-    \"response5.content\"\n+    \"const config4 = { configurable: { thread_id: uuidv4() } }\\n\",\n+    \"const input6 = {\\n\",\n+    \"  messages: [\\n\",\n+    \"    {\\n\",\n+    \"      role: \\\"user\\\",\\n\",\n+    \"      content:  \\\"Hi im bob\\\"\\n\",\n+    \"    }\\n\",\n+    \"  ],\\n\",\n+    \"  language: \\\"Spanish\\\"\\n\",\n+    \"}\\n\",\n+    \"const output7 = await app3.invoke(input6, config4)\\n\",\n+    \"console.log(output7.messages[output7.messages.length - 1]);\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"Let's now wrap this chain in a `RunnableWithMessageHistory` constructor. For demo purposes, we will also slightly modify our `getMessageHistory()` method to always start new sessions with the previously declared list of 10 messages to simulate several conversation turns:\"\n+    \"Note that the entire state is persisted, so we can omit parameters like `language` if no changes are desired:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 44,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"You haven't shared your name with me yet. What is it?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 14,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUXk9Ccr1dhmA9lZ1VmZ998PFyJF\\\",\\n\",\n+      \"  \\\"content\\\": \\\"Tu nombre es Bob. ¿Hay algo más en lo que te pueda ayudar?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 16,\\n\",\n+      \"      \\\"promptTokens\\\": 57,\\n\",\n+      \"      \\\"totalTokens\\\": 73\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 57,\\n\",\n+      \"    \\\"output_tokens\\\": 16,\\n\",\n+      \"    \\\"total_tokens\\\": 73\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const messageHistories2: Record<string, InMemoryChatMessageHistory> = {};\\n\",\n-    \"\\n\",\n-    \"const withMessageHistory2 = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: chain2,\\n\",\n-    \"  getMessageHistory: async (sessionId) => {\\n\",\n-    \"    if (messageHistories2[sessionId] === undefined) {\\n\",\n-    \"      const messageHistory = new InMemoryChatMessageHistory();\\n\",\n-    \"      await messageHistory.addMessages(messages);\\n\",\n-    \"      messageHistories2[sessionId] = messageHistory;\\n\",\n+    \"const input7 = {\\n\",\n+    \"  messages: [\\n\",\n+    \"    {\\n\",\n+    \"      role: \\\"user\\\",\\n\",\n+    \"      content:  \\\"What is my name?\\\"\\n\",\n     \"    }\\n\",\n-    \"    return messageHistories2[sessionId];\\n\",\n-    \"  },\\n\",\n-    \"  inputMessagesKey: \\\"input\\\",\\n\",\n-    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n-    \"})\\n\",\n-    \"\\n\",\n-    \"const config4 = {\\n\",\n-    \"  configurable: {\\n\",\n-    \"    sessionId: \\\"abc4\\\"\\n\",\n-    \"  }\\n\",\n-    \"};\\n\",\n-    \"\\n\",\n-    \"const response7 = await withMessageHistory2.invoke(\\n\",\n-    \"  {\\n\",\n-    \"    input: \\\"whats my name?\\\",\\n\",\n-    \"    chat_history: [],\\n\",\n-    \"  },\\n\",\n-    \"  config4,\\n\",\n-    \")\\n\",\n-    \"\\n\",\n-    \"response7.content\"\n+    \"  ],\\n\",\n+    \"}\\n\",\n+    \"const output8 = await app3.invoke(input7, config4)\\n\",\n+    \"console.log(output8.messages[output8.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"To help you understand what's happening internally, check out [this LangSmith trace](https://smith.langchain.com/public/d61630b7-6a52-4dc9-974c-8452008c498a/r).\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"There's now two new messages in the chat history. This means that even more information that used to be accessible in our conversation history is no longer available!\"\n+    \"## Managing Conversation History\\n\",\n+    \"\\n\",\n+    \"One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\n\",\n+    \"\\n\",\n+    \"**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**\\n\",\n+    \"\\n\",\n+    \"We can do this by adding a simple step in front of the prompt that modifies the `messages` key appropriately, and then wrap that new chain in the Message History class. \\n\",\n+    \"\\n\",\n+    \"LangChain comes with a few built-in helpers for [managing a list of messages](/docs/how_to/#messages). In this case we'll use the [trimMessages](/docs/how_to/trim_messages/) helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 54,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"\\u001b[32m\\\"You haven't mentioned your favorite ice cream yet. What is it?\\\"\\u001b[39m\"\n-      ]\n-     },\n-     \"execution_count\": 15,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"[\\n\",\n+      \"  SystemMessage {\\n\",\n+      \"    \\\"content\\\": \\\"you're a good assistant\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"I like vanilla ice cream\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"content\\\": \\\"nice\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {},\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  },\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"whats 2 + 2\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"content\\\": \\\"4\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {},\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  },\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"thanks\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"content\\\": \\\"no problem!\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {},\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  },\\n\",\n+      \"  HumanMessage {\\n\",\n+      \"    \\\"content\\\": \\\"having fun?\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {}\\n\",\n+      \"  },\\n\",\n+      \"  AIMessage {\\n\",\n+      \"    \\\"content\\\": \\\"yes!\\\",\\n\",\n+      \"    \\\"additional_kwargs\\\": {},\\n\",\n+      \"    \\\"response_metadata\\\": {},\\n\",\n+      \"    \\\"tool_calls\\\": [],\\n\",\n+      \"    \\\"invalid_tool_calls\\\": []\\n\",\n+      \"  }\\n\",\n+      \"]\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"const response8 = await withMessageHistory2.invoke({\\n\",\n-    \"  input: \\\"whats my favorite ice cream?\\\",\\n\",\n-    \"  chat_history: [],\\n\",\n-    \"}, config4);\\n\",\n+    \"import { SystemMessage, HumanMessage, AIMessage, trimMessages } from \\\"@langchain/core/messages\\\"\\n\",\n+    \"\\n\",\n+    \"const trimmer = trimMessages({\\n\",\n+    \"  maxTokens: 10,\\n\",\n+    \"  strategy: \\\"last\\\",\\n\",\n+    \"  tokenCounter: (msgs) => msgs.length,\\n\",\n+    \"  includeSystem: true,\\n\",\n+    \"  allowPartial: false,\\n\",\n+    \"  startOn: \\\"human\\\",\\n\",\n+    \"})\\n\",\n     \"\\n\",\n-    \"response8.content\"\n+    \"const messages = [\\n\",\n+    \"    new SystemMessage(\\\"you're a good assistant\\\"),\\n\",\n+    \"    new HumanMessage(\\\"hi! I'm bob\\\"),\\n\",\n+    \"    new AIMessage(\\\"hi!\\\"),\\n\",\n+    \"    new HumanMessage(\\\"I like vanilla ice cream\\\"),\\n\",\n+    \"    new AIMessage(\\\"nice\\\"),\\n\",\n+    \"    new HumanMessage(\\\"whats 2 + 2\\\"),\\n\",\n+    \"    new AIMessage(\\\"4\\\"),\\n\",\n+    \"    new HumanMessage(\\\"thanks\\\"),\\n\",\n+    \"    new AIMessage(\\\"no problem!\\\"),\\n\",\n+    \"    new HumanMessage(\\\"having fun?\\\"),\\n\",\n+    \"    new AIMessage(\\\"yes!\\\"),\\n\",\n+    \"]\\n\",\n+    \"\\n\",\n+    \"await trimmer.invoke(messages)\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"If you take a look at LangSmith, you can see exactly what is happening under the hood in the [LangSmith trace](https://smith.langchain.com/public/ebc2e1e7-0703-43f7-a476-8cb8cbd7f61a/r). Navigate to the chat model call to see exactly which messages are getting filtered out.\"\n+    \"To  use it in our chain, we just need to run the trimmer before we pass the `messages` input to our prompt. \"\n    ]\n   },\n   {\n-   \"cell_type\": \"markdown\",\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 55,\n    \"metadata\": {},\n+   \"outputs\": [],\n    \"source\": [\n-    \"## Streaming\\n\",\n+    \"const callModel4 = async (state: typeof GraphAnnotation.State) => {\\n\",\n+    \"  const chain = prompt2.pipe(llm);\\n\",\n+    \"  // highlight-start\\n\",\n+    \"  const trimmedMessage = await trimmer.invoke(state.messages);\\n\",\n+    \"  const response = await chain.invoke({ messages: trimmedMessage, language: state.language });\\n\",\n+    \"  // highlight-end\\n\",\n+    \"  return { messages: [response] };\\n\",\n+    \"};\\n\",\n     \"\\n\",\n-    \"Now we've got a functional chatbot. However, one *really* important UX consideration for chatbot application is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most application do is stream back each token as it is generated. This allows the user to see progress.\\n\",\n     \"\\n\",\n-    \"It's actually super easy to do this!\\n\",\n+    \"const workflow4 = new StateGraph(GraphAnnotation)\\n\",\n+    \"  .addNode(\\\"model\\\", callModel4)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n     \"\\n\",\n-    \"All chains expose a `.stream()` method, and ones that use message history are no different. We can simply use that method to get back a streaming response.\"\n+    \"const app4 = workflow4.compile({ checkpointer: new MemorySaver() });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 56,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"name\": \"stdout\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"| \\n\",\n-      \"| Hi\\n\",\n-      \"|  Todd\\n\",\n-      \"| !\\n\",\n-      \"|  Here\\n\",\n-      \"| ’s\\n\",\n-      \"|  a\\n\",\n-      \"|  joke\\n\",\n-      \"|  for\\n\",\n-      \"|  you\\n\",\n-      \"| :\\n\",\n-      \"|  \\n\",\n-      \"\\n\",\n-      \"\\n\",\n-      \"| Why\\n\",\n-      \"|  did\\n\",\n-      \"|  the\\n\",\n-      \"|  scare\\n\",\n-      \"| crow\\n\",\n-      \"|  win\\n\",\n-      \"|  an\\n\",\n-      \"|  award\\n\",\n-      \"| ?\\n\",\n-      \"|  \\n\",\n-      \"\\n\",\n-      \"\\n\",\n-      \"| Because\\n\",\n-      \"|  he\\n\",\n-      \"|  was\\n\",\n-      \"|  outstanding\\n\",\n-      \"|  in\\n\",\n-      \"|  his\\n\",\n-      \"|  field\\n\",\n-      \"| !\\n\",\n-      \"| \\n\"\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUdCOvzRAvgoxd2sf93oGKQfA9vh\\\",\\n\",\n+      \"  \\\"content\\\": \\\"I don’t know your name, but I’d be happy to learn it if you’d like to share!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 22,\\n\",\n+      \"      \\\"promptTokens\\\": 97,\\n\",\n+      \"      \\\"totalTokens\\\": 119\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 97,\\n\",\n+      \"    \\\"output_tokens\\\": 22,\\n\",\n+      \"    \\\"total_tokens\\\": 119\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n      ]\n     }\n    ],\n    \"source\": [\n-    \"const config5 = {\\n\",\n-    \"  configurable: {\\n\",\n-    \"    sessionId: \\\"abc6\\\"\\n\",\n-    \"  }\\n\",\n-    \"};\\n\",\n+    \"const config5 = { configurable: { thread_id: uuidv4() }}\\n\",\n+    \"const input8 = {\\n\",\n+    \"  // highlight-next-line\\n\",\n+    \"  messages: [...messages, new HumanMessage(\\\"What is my name?\\\")],\\n\",\n+    \"  language: \\\"English\\\"\\n\",\n+    \"}\\n\",\n     \"\\n\",\n-    \"const stream = await withMessageHistory2.stream({\\n\",\n-    \"  input: \\\"hi! I'm todd. tell me a joke\\\",\\n\",\n-    \"  chat_history: [],\\n\",\n-    \"}, config5);\\n\",\n+    \"const output9 = await app4.invoke(\\n\",\n+    \"  input8,\\n\",\n+    \"  config5,\\n\",\n+    \")\\n\",\n+    \"console.log(output9.messages[output9.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"But if we ask about information that is within the last few messages, it remembers:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 57,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-ABUdChq5JOMhcFA1dB7PvCHLyliwM\\\",\\n\",\n+      \"  \\\"content\\\": \\\"You asked for the solution to the math problem \\\\\\\"what's 2 + 2,\\\\\\\" and I answered that it equals 4.\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 27,\\n\",\n+      \"      \\\"promptTokens\\\": 99,\\n\",\n+      \"      \\\"totalTokens\\\": 126\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_1bb46167f9\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 99,\\n\",\n+      \"    \\\"output_tokens\\\": 27,\\n\",\n+      \"    \\\"total_tokens\\\": 126\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const config6 = { configurable: { thread_id: uuidv4() }}\\n\",\n+    \"const input9 = {\\n\",\n+    \"  // highlight-next-line\\n\",\n+    \"  messages: [...messages, new HumanMessage(\\\"What math problem did I ask?\\\")],\\n\",\n+    \"  language: \\\"English\\\"\\n\",\n+    \"}\\n\",\n     \"\\n\",\n-    \"for await (const chunk of stream) {\\n\",\n-    \"  console.log(\\\"|\\\", chunk.content);\\n\",\n-    \"}\"\n+    \"const output10 = await app4.invoke(\\n\",\n+    \"  input9,\\n\",\n+    \"  config6,\\n\",\n+    \")\\n\",\n+    \"console.log(output10.messages[output10.messages.length - 1]);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"If you take a look at LangSmith, you can see exactly what is happening under the hood in the [LangSmith trace](https://smith.langchain.com/public/bf1b1a10-0fe0-42f6-9f0f-b70d9f7520dc/r).\"\n    ]\n   },\n   {\n@@ -771,12 +1115,14 @@\n     \"Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\",\n     \"\\n\",\n     \"- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data\\n\",\n-    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/): Build a chatbot that can take actions\\n\",\n+    \"- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/): Build a chatbot that can take actions\\n\",\n     \"\\n\",\n     \"If you want to dive deeper on specifics, some things worth checking out are:\\n\",\n     \"\\n\",\n     \"- [Streaming](/docs/how_to/streaming): streaming is *crucial* for chat applications\\n\",\n-    \"- [How to add message history](/docs/how_to/message_history): for a deeper dive into all things related to message history\"\n+    \"- [How to add message history](/docs/how_to/message_history): for a deeper dive into all things related to message history\\n\",\n+    \"- [How to manage large message history](/docs/how_to/trim_messages/): more techniques for managing a large chat history\\n\",\n+    \"- [LangGraph main docs](https://langchain-ai.github.io/langgraph/): for more detail on building with LangGraph\"\n    ]\n   }\n  ],\n@@ -787,12 +1133,15 @@\n    \"name\": \"deno\"\n   },\n   \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n    \"file_extension\": \".ts\",\n-   \"mimetype\": \"text/x.typescript\",\n+   \"mimetype\": \"text/typescript\",\n    \"name\": \"typescript\",\n-   \"nb_converter\": \"script\",\n-   \"pygments_lexer\": \"typescript\",\n-   \"version\": \"5.3.3\"\n+   \"version\": \"3.7.2\"\n   }\n  },\n  \"nbformat\": 4,",
          "docs/core_docs/docs/tutorials/graph.ipynb": "@@ -181,8 +181,20 @@\n         \"\\n\",\n         \"![graph_chain.webp](../../static/img/graph_chain.webp)\\n\",\n         \"\\n\",\n+        \"LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: `GraphCypherQAChain`.\\n\",\n         \"\\n\",\n-        \"LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: [GraphCypherQAChain](https://python.langchain.com/docs/use_cases/graph/graph_cypher_qa)\"\n+        \"```{=mdx}\\n\",\n+        \":::warning\\n\",\n+        \"\\n\",\n+        \"The `GraphCypherQAChain` used in this guide will execute Cypher statements against the provided database.\\n\",\n+        \"For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.\\n\",\n+        \"\\n\",\n+        \"Failure to do so may result in data corruption or loss, since the calling code\\n\",\n+        \"may attempt commands that would result in deletion, mutation of data\\n\",\n+        \"if appropriately prompted or reading sensitive data if such data is present in the database.\\n\",\n+        \"\\n\",\n+        \":::\\n\",\n+        \"```\"\n       ]\n     },\n     {",
          "docs/core_docs/docs/tutorials/qa_chat_history.ipynb": "@@ -1,390 +1,1438 @@\n {\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"# Conversational RAG\\n\",\n-        \"\\n\",\n-        \":::info Prerequisites\\n\",\n-        \"\\n\",\n-        \"This guide assumes familiarity with the following concepts:\\n\",\n-        \"\\n\",\n-        \"- [Chat history](/docs/concepts/#chat-history)\\n\",\n-        \"- [Chat models](/docs/concepts/#chat-models)\\n\",\n-        \"- [Embeddings](/docs/concepts/#embedding-models)\\n\",\n-        \"- [Vector stores](/docs/concepts/#vector-stores)\\n\",\n-        \"- [Retrieval-augmented generation](/docs/tutorials/rag/)\\n\",\n-        \"- [Tools](/docs/concepts/#tools)\\n\",\n-        \"- [Agents](/docs/concepts/#agents)\\n\",\n-        \"\\n\",\n-        \":::\\n\",\n-        \"\\n\",\n-        \"In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \\\"memory\\\" of past questions and answers, and some logic for incorporating those into its current thinking.\\n\",\n-        \"\\n\",\n-        \"In this guide we focus on **adding logic for incorporating historical messages.** Further details on chat history management is [covered here](/docs/how_to/message_history).\\n\",\n-        \"\\n\",\n-        \"We will cover two approaches:\\n\",\n-        \"\\n\",\n-        \"1. Chains, in which we always execute a retrieval step;\\n\",\n-        \"2. Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\\n\",\n-        \"\\n\",\n-        \"For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag).\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Setup\\n\",\n-        \"### Dependencies\\n\",\n-        \"\\n\",\n-        \"We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/#chat-models) or [LLM](/docs/concepts#llms), [Embeddings](/docs/concepts#embedding-models), and [VectorStore](/docs/concepts#vectorstores) or [Retriever](/docs/concepts#retrievers).\\n\",\n-        \"\\n\",\n-        \"We’ll use the following packages:\\n\",\n-        \"\\n\",\n-        \"```bash\\n\",\n-        \"npm install --save langchain @langchain/openai cheerio\\n\",\n-        \"```\\n\",\n-        \"\\n\",\n-        \"We need to set environment variable `OPENAI_API_KEY`:\\n\",\n-        \"\\n\",\n-        \"```bash\\n\",\n-        \"export OPENAI_API_KEY=YOUR_KEY\\n\",\n-        \"```\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"### LangSmith\\n\",\n-        \"\\n\",\n-        \"Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).\\n\",\n-        \"\\n\",\n-        \"Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\n\",\n-        \"\\n\",\n-        \"\\n\",\n-        \"```bash\\n\",\n-        \"export LANGCHAIN_TRACING_V2=true\\n\",\n-        \"export LANGCHAIN_API_KEY=YOUR_KEY\\n\",\n-        \"\\n\",\n-        \"# Reduce tracing latency if you are not in a serverless environment\\n\",\n-        \"# export LANGCHAIN_CALLBACKS_BACKGROUND=true\\n\",\n-        \"```\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"### Initial setup\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 5,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import \\\"cheerio\\\";\\n\",\n-        \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n-        \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n-        \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\"\\n\",\n-        \"import { OpenAIEmbeddings, ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n-        \"import { pull } from \\\"langchain/hub\\\";\\n\",\n-        \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n-        \"import { RunnableSequence, RunnablePassthrough } from \\\"@langchain/core/runnables\\\";\\n\",\n-        \"import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";\\n\",\n-        \"\\n\",\n-        \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n-        \"\\n\",\n-        \"const loader = new CheerioWebBaseLoader(\\n\",\n-        \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n\",\n-        \");\\n\",\n-        \"\\n\",\n-        \"const docs = await loader.load();\\n\",\n-        \"\\n\",\n-        \"const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n-        \"const splits = await textSplitter.splitDocuments(docs);\\n\",\n-        \"const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\\n\",\n-        \"\\n\",\n-        \"// Retrieve and generate using the relevant snippets of the blog.\\n\",\n-        \"const retriever = vectorStore.asRetriever();\\n\",\n-        \"const prompt = await pull<ChatPromptTemplate>(\\\"rlm/rag-prompt\\\");\\n\",\n-        \"const llm = new ChatOpenAI({ model: \\\"gpt-3.5-turbo\\\", temperature: 0 });\\n\",\n-        \"const ragChain = await createStuffDocumentsChain({\\n\",\n-        \"  llm,\\n\",\n-        \"  prompt,\\n\",\n-        \"  outputParser: new StringOutputParser(),\\n\",\n-        \"});\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Let's see what this prompt actually looks like:\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 1,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\",\n-            \"Question: {question} \\n\",\n-            \"Context: {context} \\n\",\n-            \"Answer:\\n\"\n-          ]\n-        }\n-      ],\n-      \"source\": [\n-        \"console.log(prompt.promptMessages.map((msg) => msg.prompt.template).join(\\\"\\\\n\\\"));\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 7,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"\\u001b[32m\\\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. I\\\"\\u001b[39m... 208 more characters\"\n-            ]\n-          },\n-          \"execution_count\": 7,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n-        }\n-      ],\n-      \"source\": [\n-        \"await ragChain.invoke({\\n\",\n-        \"  context: await retriever.invoke(\\\"What is Task Decomposition?\\\"),\\n\",\n-        \"  question: \\\"What is Task Decomposition?\\\"\\n\",\n-        \"});\"\n-      ]\n-    },\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"raw\",\n+   \"id\": \"023635f2-71cf-43f2-a2e2-a7b4ced30a74\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"---\\n\",\n+    \"sidebar_position: 2\\n\",\n+    \"---\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"86fc5bb2-017f-434e-8cd6-53ab214a5604\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Conversational RAG\\n\",\n+    \"\\n\",\n+    \":::info Prerequisites\\n\",\n+    \"\\n\",\n+    \"This guide assumes familiarity with the following concepts:\\n\",\n+    \"\\n\",\n+    \"- [Chat history](/docs/concepts/#chat-history)\\n\",\n+    \"- [Chat models](/docs/concepts/#chat-models)\\n\",\n+    \"- [Embeddings](/docs/concepts/#embedding-models)\\n\",\n+    \"- [Vector stores](/docs/concepts/#vector-stores)\\n\",\n+    \"- [Retrieval-augmented generation](/docs/tutorials/rag/)\\n\",\n+    \"- [Tools](/docs/concepts/#tools)\\n\",\n+    \"- [Agents](/docs/concepts/#agents)\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \\\"memory\\\" of past questions and answers, and some logic for incorporating those into its current thinking.\\n\",\n+    \"\\n\",\n+    \"In this guide we focus on **adding logic for incorporating historical messages.** Further details on chat history management is [covered here](/docs/how_to/message_history).\\n\",\n+    \"\\n\",\n+    \"We will cover two approaches:\\n\",\n+    \"\\n\",\n+    \"1. Chains, in which we always execute a retrieval step;\\n\",\n+    \"2. Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\\n\",\n+    \"\\n\",\n+    \"For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"487d8d79-5ee9-4aa4-9fdf-cd5f4303e099\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Setup\\n\",\n+    \"### Dependencies\\n\",\n+    \"\\n\",\n+    \"We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/#chat-models) or [LLM](/docs/concepts#llms), [Embeddings](/docs/concepts#embedding-models), and [VectorStore](/docs/concepts#vectorstores) or [Retriever](/docs/concepts#retrievers).\\n\",\n+    \"\\n\",\n+    \"We’ll use the following packages:\\n\",\n+    \"\\n\",\n+    \"```bash\\n\",\n+    \"npm install --save langchain @langchain/openai langchain cheerio\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"We need to set environment variable `OPENAI_API_KEY`:\\n\",\n+    \"\\n\",\n+    \"```bash\\n\",\n+    \"export OPENAI_API_KEY=YOUR_KEY\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"1665e740-ce01-4f09-b9ed-516db0bd326f\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### LangSmith\\n\",\n+    \"\\n\",\n+    \"Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://docs.smith.langchain.com).\\n\",\n+    \"\\n\",\n+    \"Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"```bash\\n\",\n+    \"export LANGCHAIN_TRACING_V2=true\\n\",\n+    \"export LANGCHAIN_API_KEY=YOUR_KEY\\n\",\n+    \"\\n\",\n+    \"# Reduce tracing latency if you are not in a serverless environment\\n\",\n+    \"# export LANGCHAIN_CALLBACKS_BACKGROUND=true\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"fa6ba684-26cf-4860-904e-a4d51380c134\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Chains {#chains}\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"7d2cf4ef\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"\\n\",\n+    \"Let's first revisit the Q&A app we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [RAG tutorial](/docs/tutorials/rag).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"646840fb-5212-48ea-8bc7-ec7be5ec727e\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \"import ChatModelTabs from \\\"@theme/ChatModelTabs\\\";\\n\",\n+    \"\\n\",\n+    \"<ChatModelTabs customVarName=\\\"llm\\\" />\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"id\": \"cb58f273-2111-4a9b-8932-9b64c95030c8\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const llm = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"id\": \"820244ae-74b4-4593-b392-822979dd91b8\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n+    \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n+    \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\";\\n\",\n+    \"import { OpenAIEmbeddings } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"import { createRetrievalChain } from \\\"langchain/chains/retrieval\\\";\\n\",\n+    \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n+    \"\\n\",\n+    \"// 1. Load, chunk and index the contents of the blog to create a retriever.\\n\",\n+    \"const loader = new CheerioWebBaseLoader(\\n\",\n+    \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\",\\n\",\n+    \"  {\\n\",\n+    \"    selector: \\\".post-content, .post-title, .post-header\\\"\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"const docs = await loader.load();\\n\",\n+    \"\\n\",\n+    \"const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n+    \"const splits = await textSplitter.splitDocuments(docs);\\n\",\n+    \"const vectorstore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\\n\",\n+    \"const retriever = vectorstore.asRetriever();\\n\",\n+    \"\\n\",\n+    \"// 2. Incorporate the retriever into a question-answering chain.\\n\",\n+    \"const systemPrompt = \\n\",\n+    \"  \\\"You are an assistant for question-answering tasks. \\\" +\\n\",\n+    \"  \\\"Use the following pieces of retrieved context to answer \\\" +\\n\",\n+    \"  \\\"the question. If you don't know the answer, say that you \\\" +\\n\",\n+    \"  \\\"don't know. Use three sentences maximum and keep the \\\" +\\n\",\n+    \"  \\\"answer concise.\\\" +\\n\",\n+    \"  \\\"\\\\n\\\\n\\\" +\\n\",\n+    \"  \\\"{context}\\\";\\n\",\n+    \"\\n\",\n+    \"const prompt = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", systemPrompt],\\n\",\n+    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const questionAnswerChain = await createStuffDocumentsChain({\\n\",\n+    \"  llm,\\n\",\n+    \"  prompt,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const ragChain = await createRetrievalChain({\\n\",\n+    \"  retriever,\\n\",\n+    \"  combineDocsChain: questionAnswerChain,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"id\": \"bf55faaf-0d17-4b74-925d-c478b555f7b2\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Contextualizing the question\\n\",\n-        \"\\n\",\n-        \"First we'll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.\\n\",\n-        \"\\n\",\n-        \"We'll use a prompt that includes a `MessagesPlaceholder` variable under the name \\\"chat_history\\\". This allows us to pass in a list of Messages to the prompt using the \\\"chat_history\\\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Task decomposition involves breaking down large and complex tasks into smaller, more manageable subgoals or steps. This approach helps agents or models efficiently handle intricate tasks by simplifying them into easier components. Task decomposition can be achieved through techniques like Chain of Thought, Tree of Thoughts, or by using task-specific instructions and human input.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const response = await ragChain.invoke({ input: \\\"What is Task Decomposition?\\\" });\\n\",\n+    \"console.log(response.answer);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"187404c7-db47-49c5-be29-9ecb96dc9afa\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Note that we have used the built-in chain constructors `createStuffDocumentsChain` and `createRetrievalChain`, so that the basic ingredients to our solution are:\\n\",\n+    \"\\n\",\n+    \"1. retriever;\\n\",\n+    \"2. prompt;\\n\",\n+    \"3. LLM.\\n\",\n+    \"\\n\",\n+    \"This will simplify the process of incorporating chat history.\\n\",\n+    \"\\n\",\n+    \"### Adding chat history\\n\",\n+    \"\\n\",\n+    \"The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood. For example, consider this exchange:\\n\",\n+    \"\\n\",\n+    \"> Human: \\\"What is Task Decomposition?\\\"\\n\",\n+    \">\\n\",\n+    \"> AI: \\\"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\\\"\\n\",\n+    \">\\n\",\n+    \"> Human: \\\"What are common ways of doing it?\\\"\\n\",\n+    \"\\n\",\n+    \"In order to answer the second question, our system needs to understand that \\\"it\\\" refers to \\\"Task Decomposition.\\\"\\n\",\n+    \"\\n\",\n+    \"We'll need to update two things about our existing app:\\n\",\n+    \"\\n\",\n+    \"1. **Prompt**: Update our prompt to support historical messages as an input.\\n\",\n+    \"2. **Contextualizing questions**: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This can be thought of simply as building a new \\\"history aware\\\" retriever. Whereas before we had:\\n\",\n+    \"   - `query` -> `retriever`  \\n\",\n+    \"     Now we will have:\\n\",\n+    \"   - `(query, conversation history)` -> `LLM` -> `rephrased query` -> `retriever`\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"776ae958-cbdc-4471-8669-c6087436f0b5\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"#### Contextualizing the question\\n\",\n+    \"\\n\",\n+    \"First we'll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.\\n\",\n+    \"\\n\",\n+    \"We'll use a prompt that includes a `MessagesPlaceholder` variable under the name \\\"chat_history\\\". This allows us to pass in a list of Messages to the prompt using the \\\"chat_history\\\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\\n\",\n+    \"\\n\",\n+    \"Note that we leverage a helper function [createHistoryAwareRetriever](https://api.js.langchain.com/functions/langchain.chains_history_aware_retriever.createHistoryAwareRetriever.html) for this step, which manages the case where `chat_history` is empty, and otherwise applies `prompt.pipe(llm).pipe(new StringOutputParser()).pipe(retriever)` in sequence.\\n\",\n+    \"\\n\",\n+    \"`createHistoryAwareRetriever` constructs a chain that accepts keys `input` and `chat_history` as input, and has the same output schema as a retriever.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 7,\n+   \"id\": \"2b685428-8b82-4af1-be4f-7232c5d55b73\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { createHistoryAwareRetriever } from \\\"langchain/chains/history_aware_retriever\\\";\\n\",\n+    \"import { MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"\\n\",\n+    \"const contextualizeQSystemPrompt = \\n\",\n+    \"  \\\"Given a chat history and the latest user question \\\" +\\n\",\n+    \"  \\\"which might reference context in the chat history, \\\" +\\n\",\n+    \"  \\\"formulate a standalone question which can be understood \\\" +\\n\",\n+    \"  \\\"without the chat history. Do NOT answer the question, \\\" +\\n\",\n+    \"  \\\"just reformulate it if needed and otherwise return it as is.\\\";\\n\",\n+    \"\\n\",\n+    \"const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", contextualizeQSystemPrompt],\\n\",\n+    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const historyAwareRetriever = await createHistoryAwareRetriever({\\n\",\n+    \"  llm,\\n\",\n+    \"  retriever,\\n\",\n+    \"  rephrasePrompt: contextualizeQPrompt,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"42a47168-4a1f-4e39-bd2d-d5b03609a243\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.\\n\",\n+    \"\\n\",\n+    \"Now we can build our full QA chain. This is as simple as updating the retriever to be our new `historyAwareRetriever`.\\n\",\n+    \"\\n\",\n+    \"Again, we will use [createStuffDocumentsChain](https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html) to generate a `questionAnswerChain2`, with input keys `context`, `chat_history`, and `input`-- it accepts the retrieved context alongside the conversation history and query to generate an answer. A more detailed explaination is over [here](/docs/tutorials/rag/#built-in-chains)\\n\",\n+    \"\\n\",\n+    \"We build our final `ragChain2` with [createRetrievalChain](https://api.js.langchain.com/functions/langchain.chains_retrieval.createRetrievalChain.html). This chain applies the `historyAwareRetriever` and `questionAnswerChain2` in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys `input` and `chat_history`, and includes `input`, `chat_history`, `context`, and `answer` in its output.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 9,\n+   \"id\": \"66f275f3-ddef-4678-b90d-ee64576878f9\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"const qaPrompt = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", systemPrompt],\\n\",\n+    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const questionAnswerChain2 = await createStuffDocumentsChain({\\n\",\n+    \"  llm,\\n\",\n+    \"  prompt: qaPrompt,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const ragChain2 = await createRetrievalChain({\\n\",\n+    \"  retriever: historyAwareRetriever,\\n\",\n+    \"  combineDocsChain: questionAnswerChain2,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"1ba1ae56-7ecb-4563-b792-50a1a5042df3\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Let's try this. Below we ask a question and a follow-up question that requires contextualization to return a sensible response. Because our chain includes a `\\\"chat_history\\\"` input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 10,\n+   \"id\": \"0005810b-1b95-4666-a795-08d80e478b83\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 5,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n-        \"\\n\",\n-        \"const contextualizeQSystemPrompt = `Given a chat history and the latest user question\\n\",\n-        \"which might reference context in the chat history, formulate a standalone question\\n\",\n-        \"which can be understood without the chat history. Do NOT answer the question,\\n\",\n-        \"just reformulate it if needed and otherwise return it as is.`;\\n\",\n-        \"\\n\",\n-        \"const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-        \"  [\\\"system\\\", contextualizeQSystemPrompt],\\n\",\n-        \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-        \"  [\\\"human\\\", \\\"{question}\\\"]\\n\",\n-        \"]);\\n\",\n-        \"const contextualizeQChain = contextualizeQPrompt.pipe(llm).pipe(new StringOutputParser());\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Common ways of doing Task Decomposition include:\\n\",\n+      \"1. Using simple prompting with an LLM, such as asking it to outline steps or subgoals for a task.\\n\",\n+      \"2. Employing task-specific instructions, like \\\"Write a story outline\\\" for writing a novel.\\n\",\n+      \"3. Incorporating human inputs for guidance.\\n\",\n+      \"Additionally, advanced approaches like Chain of Thought (CoT) and Tree of Thoughts (ToT) can further refine the process, and using an external classical planner with PDDL (as in LLM+P) is another option.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { BaseMessage, HumanMessage, AIMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"let chatHistory: BaseMessage[] = [];\\n\",\n+    \"\\n\",\n+    \"const question = \\\"What is Task Decomposition?\\\";\\n\",\n+    \"const aiMsg1 = await ragChain2.invoke({ input: question, chat_history: chatHistory });\\n\",\n+    \"chatHistory = chatHistory.concat([\\n\",\n+    \"  new HumanMessage(question),\\n\",\n+    \"  new AIMessage(aiMsg1.answer),\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const secondQuestion = \\\"What are common ways of doing it?\\\";\\n\",\n+    \"const aiMsg2 = await ragChain2.invoke({ input: secondQuestion, chat_history: chatHistory });\\n\",\n+    \"\\n\",\n+    \"console.log(aiMsg2.answer);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"53a662c2-f38b-45f9-95c4-66de15637614\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"#### Stateful management of chat history\\n\",\n+    \"\\n\",\n+    \"Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it.\\n\",\n+    \"\\n\",\n+    \"For this we can use:\\n\",\n+    \"\\n\",\n+    \"- [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html): Store chat history.\\n\",\n+    \"- [RunnableWithMessageHistory](/docs/how_to/message_history): Wrapper for an LCEL chain and a `BaseChatMessageHistory` that handles injecting chat history into inputs and updating it after each invocation.\\n\",\n+    \"\\n\",\n+    \"For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the [How to add message history (memory)](/docs/how_to/message_history) LCEL page.\\n\",\n+    \"\\n\",\n+    \"Instances of `RunnableWithMessageHistory` manage the chat history for you. They accept a config with a key (`\\\"sessionId\\\"` by default) that specifies what conversation history to fetch and prepend to the input, and append the output to the same conversation history. Below is an example:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 15,\n+   \"id\": \"9c3fb176-8d6a-4dc7-8408-6a22c5f7cc72\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { RunnableWithMessageHistory } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"import { ChatMessageHistory } from \\\"langchain/stores/message/in_memory\\\";\\n\",\n+    \"\\n\",\n+    \"const demoEphemeralChatMessageHistoryForChain = new ChatMessageHistory();\\n\",\n+    \"\\n\",\n+    \"const conversationalRagChain = new RunnableWithMessageHistory({\\n\",\n+    \"  runnable: ragChain2,\\n\",\n+    \"  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistoryForChain,\\n\",\n+    \"  inputMessagesKey: \\\"input\\\",\\n\",\n+    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n+    \"  outputMessagesKey: \\\"answer\\\",\\n\",\n+    \"})\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 16,\n+   \"id\": \"1046c92f-21b3-4214-907d-92878d8cba23\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Using this chain we can ask follow-up questions that reference past messages and have them reformulated into standalone questions:\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Task Decomposition involves breaking down complicated tasks into smaller, more manageable subgoals. Techniques such as the Chain of Thought (CoT) and Tree of Thoughts extend this by decomposing problems into multiple thought steps and exploring multiple reasoning possibilities at each step. LLMs can perform task decomposition using simple prompts, task-specific instructions, or human inputs, and some approaches like LLM+P involve using external classical planners.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const result1 = await conversationalRagChain.invoke(\\n\",\n+    \"  { input: \\\"What is Task Decomposition?\\\" },\\n\",\n+    \"  { configurable: { sessionId: \\\"abc123\\\" } }\\n\",\n+    \");\\n\",\n+    \"console.log(result1.answer);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 17,\n+   \"id\": \"0e89c75f-7ad7-4331-a2fe-57579eb8f840\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 6,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"\\u001b[32m'What is the definition of \\\"large\\\" in the context of a language model?'\\u001b[39m\"\n-            ]\n-          },\n-          \"execution_count\": 6,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n-        }\n-      ],\n-      \"source\": [\n-        \"import { AIMessage, HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n-        \"\\n\",\n-        \"await contextualizeQChain.invoke({\\n\",\n-        \"  chat_history: [\\n\",\n-        \"    new HumanMessage(\\\"What does LLM stand for?\\\"),\\n\",\n-        \"    new AIMessage(\\\"Large language model\\\") \\n\",\n-        \"  ],\\n\",\n-        \"  question: \\\"What is meant by large\\\",\\n\",\n-        \"})\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Common ways of doing task decomposition include:\\n\",\n+      \"\\n\",\n+      \"1. Using simple prompting with an LLM, such as \\\"Steps for XYZ.\\\\n1.\\\" or \\\"What are the subgoals for achieving XYZ?\\\"\\n\",\n+      \"2. Utilizing task-specific instructions, like \\\"Write a story outline.\\\" for writing a novel.\\n\",\n+      \"3. Incorporating human inputs to guide and refine the decomposition process. \\n\",\n+      \"\\n\",\n+      \"Additionally, the LLM+P approach utilizes an external classical planner, involving PDDL to describe and plan complex tasks.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const result2 = await conversationalRagChain.invoke(\\n\",\n+    \"  { input: \\\"What are common ways of doing it?\\\" },\\n\",\n+    \"  { configurable: { sessionId: \\\"abc123\\\" } }\\n\",\n+    \");\\n\",\n+    \"console.log(result2.answer);\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0ab1ded4-76d9-453f-9b9b-db9a4560c737\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Tying it together\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"8a08a5ea-df5b-4547-93c6-2a3940dd5c3e\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"![](../../static/img/conversational_retrieval_chain.png)\\n\",\n+    \"\\n\",\n+    \"For convenience, we tie together all of the necessary steps in a single code cell:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 21,\n+   \"id\": \"71c32048-1a41-465f-a9e2-c4affc332fd9\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Chain with chat history\\n\",\n-        \"\\n\",\n-        \"And now we can build our full QA chain. \\n\",\n-        \"\\n\",\n-        \"Notice we add some routing functionality to only run the \\\"condense question chain\\\" when our chat history isn't empty. Here we're taking advantage of the fact that if a function in an LCEL chain returns another chain, that chain will itself be invoked.\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{ input: 'What is Task Decomposition?' }\\n\",\n+      \"----\\n\",\n+      \"{ chat_history: [] }\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  context: [\\n\",\n+      \"    Document {\\n\",\n+      \"      pageContent: 'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\n' +\\n\",\n+      \"        'Component One: Planning#\\\\n' +\\n\",\n+      \"        'A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\n' +\\n\",\n+      \"        'Task Decomposition#\\\\n' +\\n\",\n+      \"        'Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n' +\\n\",\n+      \"        'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.',\\n\",\n+      \"      metadata: [Object],\\n\",\n+      \"      id: undefined\\n\",\n+      \"    },\\n\",\n+      \"    Document {\\n\",\n+      \"      pageContent: 'Task decomposition can be done (1) by LLM with simple prompting like \\\"Steps for XYZ.\\\\\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for writing a novel, or (3) with human inputs.\\\\n' +\\n\",\n+      \"        'Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\n' +\\n\",\n+      \"        'Self-Reflection#',\\n\",\n+      \"      metadata: [Object],\\n\",\n+      \"      id: undefined\\n\",\n+      \"    },\\n\",\n+      \"    Document {\\n\",\n+      \"      pageContent: 'Planning\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\n' +\\n\",\n+      \"        'Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'Memory\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\n' +\\n\",\n+      \"        'Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'Tool use\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.',\\n\",\n+      \"      metadata: [Object],\\n\",\n+      \"      id: undefined\\n\",\n+      \"    },\\n\",\n+      \"    Document {\\n\",\n+      \"      pageContent: 'Resources:\\\\n' +\\n\",\n+      \"        '1. Internet access for searches and information gathering.\\\\n' +\\n\",\n+      \"        '2. Long Term memory management.\\\\n' +\\n\",\n+      \"        '3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n' +\\n\",\n+      \"        '4. File output.\\\\n' +\\n\",\n+      \"        '\\\\n' +\\n\",\n+      \"        'Performance Evaluation:\\\\n' +\\n\",\n+      \"        '1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n' +\\n\",\n+      \"        '2. Constructively self-criticize your big-picture behavior constantly.\\\\n' +\\n\",\n+      \"        '3. Reflect on past decisions and strategies to refine your approach.\\\\n' +\\n\",\n+      \"        '4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.',\\n\",\n+      \"      metadata: [Object],\\n\",\n+      \"      id: undefined\\n\",\n+      \"    }\\n\",\n+      \"  ]\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{ answer: '' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: 'Task' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' decomposition' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' involves' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' breaking' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' down' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' a' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' complex' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' task' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' into' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' smaller' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' and' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' more' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' manageable' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' sub' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: 'goals' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' or' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' steps' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '.' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' This' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' process' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' allows' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' an' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' agent' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' or' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' model' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' to' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' efficiently' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' handle' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' intricate' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' tasks' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' by' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' dividing' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' them' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' into' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' simpler' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' components' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '.' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' Task' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' decomposition' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' can' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' be' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' achieved' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' through' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' techniques' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' like' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' Chain' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' of' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' Thought' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ',' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' Tree' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' of' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' Thoughts' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ',' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' or' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' by' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' using' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' task' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '-specific' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: ' instructions' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '.' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '' }\\n\",\n+      \"----\\n\",\n+      \"{ answer: '' }\\n\",\n+      \"----\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n+    \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n+    \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\";\\n\",\n+    \"import { OpenAIEmbeddings, ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\";\\n\",\n+    \"import { createHistoryAwareRetriever } from \\\"langchain/chains/history_aware_retriever\\\";\\n\",\n+    \"import { createStuffDocumentsChain } from \\\"langchain/chains/combine_documents\\\";\\n\",\n+    \"import { createRetrievalChain } from \\\"langchain/chains/retrieval\\\";\\n\",\n+    \"import { RunnableWithMessageHistory } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"import { ChatMessageHistory } from \\\"langchain/stores/message/in_memory\\\";\\n\",\n+    \"import { BaseChatMessageHistory } from \\\"@langchain/core/chat_history\\\";\\n\",\n+    \"\\n\",\n+    \"const llm2 = new ChatOpenAI({ model: \\\"gpt-3.5-turbo\\\", temperature: 0 });\\n\",\n+    \"\\n\",\n+    \"// Construct retriever\\n\",\n+    \"const loader2 = new CheerioWebBaseLoader(\\n\",\n+    \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\",\\n\",\n+    \"  {\\n\",\n+    \"    selector: \\\".post-content, .post-title, .post-header\\\"\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"const docs2 = await loader2.load();\\n\",\n+    \"\\n\",\n+    \"const textSplitter2 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n+    \"const splits2 = await textSplitter2.splitDocuments(docs2);\\n\",\n+    \"const vectorstore2 = await MemoryVectorStore.fromDocuments(splits2, new OpenAIEmbeddings());\\n\",\n+    \"const retriever2 = vectorstore2.asRetriever();\\n\",\n+    \"\\n\",\n+    \"// Contextualize question\\n\",\n+    \"const contextualizeQSystemPrompt2 = \\n\",\n+    \"  \\\"Given a chat history and the latest user question \\\" +\\n\",\n+    \"  \\\"which might reference context in the chat history, \\\" +\\n\",\n+    \"  \\\"formulate a standalone question which can be understood \\\" +\\n\",\n+    \"  \\\"without the chat history. Do NOT answer the question, \\\" +\\n\",\n+    \"  \\\"just reformulate it if needed and otherwise return it as is.\\\";\\n\",\n+    \"\\n\",\n+    \"const contextualizeQPrompt2 = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", contextualizeQSystemPrompt2],\\n\",\n+    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const historyAwareRetriever2 = await createHistoryAwareRetriever({\\n\",\n+    \"  llm: llm2,\\n\",\n+    \"  retriever: retriever2,\\n\",\n+    \"  rephrasePrompt: contextualizeQPrompt2\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// Answer question\\n\",\n+    \"const systemPrompt2 = \\n\",\n+    \"  \\\"You are an assistant for question-answering tasks. \\\" +\\n\",\n+    \"  \\\"Use the following pieces of retrieved context to answer \\\" +\\n\",\n+    \"  \\\"the question. If you don't know the answer, say that you \\\" +\\n\",\n+    \"  \\\"don't know. Use three sentences maximum and keep the \\\" +\\n\",\n+    \"  \\\"answer concise.\\\" +\\n\",\n+    \"  \\\"\\\\n\\\\n\\\" +\\n\",\n+    \"  \\\"{context}\\\";\\n\",\n+    \"\\n\",\n+    \"const qaPrompt2 = ChatPromptTemplate.fromMessages([\\n\",\n+    \"  [\\\"system\\\", systemPrompt2],\\n\",\n+    \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n+    \"  [\\\"human\\\", \\\"{input}\\\"],\\n\",\n+    \"]);\\n\",\n+    \"\\n\",\n+    \"const questionAnswerChain3 = await createStuffDocumentsChain({\\n\",\n+    \"  llm,\\n\",\n+    \"  prompt: qaPrompt2,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const ragChain3 = await createRetrievalChain({\\n\",\n+    \"  retriever: historyAwareRetriever2,\\n\",\n+    \"  combineDocsChain: questionAnswerChain3,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// Statefully manage chat history\\n\",\n+    \"const store2: Record<string, BaseChatMessageHistory> = {};\\n\",\n+    \"\\n\",\n+    \"function getSessionHistory2(sessionId: string): BaseChatMessageHistory {\\n\",\n+    \"  if (!(sessionId in store2)) {\\n\",\n+    \"    store2[sessionId] = new ChatMessageHistory();\\n\",\n+    \"  }\\n\",\n+    \"  return store2[sessionId];\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"const conversationalRagChain2 = new RunnableWithMessageHistory({\\n\",\n+    \"  runnable: ragChain3,\\n\",\n+    \"  getMessageHistory: getSessionHistory2,\\n\",\n+    \"  inputMessagesKey: \\\"input\\\",\\n\",\n+    \"  historyMessagesKey: \\\"chat_history\\\",\\n\",\n+    \"  outputMessagesKey: \\\"answer\\\",\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// Example usage\\n\",\n+    \"const query2 = \\\"What is Task Decomposition?\\\";\\n\",\n+    \"\\n\",\n+    \"for await (const s of await conversationalRagChain2.stream(\\n\",\n+    \"  { input: query2 },\\n\",\n+    \"  { configurable: { sessionId: \\\"unique_session_id\\\" } }\\n\",\n+    \")) {\\n\",\n+    \"  console.log(s);\\n\",\n+    \"  console.log(\\\"----\\\");\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"861da8ed-d890-4fdc-a3bf-30433db61e0d\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Agents {#agents}\\n\",\n+    \"\\n\",\n+    \"Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:\\n\",\n+    \"\\n\",\n+    \"- Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;\\n\",\n+    \"- Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user).\\n\",\n+    \"\\n\",\n+    \"### Retrieval tool\\n\",\n+    \"\\n\",\n+    \"Agents can access \\\"tools\\\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 23,\n+   \"id\": \"809cc747-2135-40a2-8e73-e4556343ee64\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { createRetrieverTool } from \\\"langchain/tools/retriever\\\";\\n\",\n+    \"\\n\",\n+    \"const tool = createRetrieverTool(\\n\",\n+    \"    retriever,\\n\",\n+    \"    {\\n\",\n+    \"      name: \\\"blog_post_retriever\\\",\\n\",\n+    \"      description: \\\"Searches and returns excerpts from the Autonomous Agents blog post.\\\",\\n\",\n+    \"    }\\n\",\n+    \")\\n\",\n+    \"const tools = [tool]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"07dcb968-ed9a-458a-85e1-528cd28c6965\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Tools are LangChain [Runnables](/docs/concepts#langchain-expression-language-lcel), and implement the usual interface:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 24,\n+   \"id\": \"931c4fe3-c603-4efb-9b37-5f7cbbb1cbbd\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 8,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import { ChatPromptTemplate, MessagesPlaceholder } from \\\"@langchain/core/prompts\\\"\\n\",\n-        \"import { RunnablePassthrough, RunnableSequence } from \\\"@langchain/core/runnables\\\";\\n\",\n-        \"import { formatDocumentsAsString } from \\\"langchain/util/document\\\";\\n\",\n-        \"\\n\",\n-        \"const qaSystemPrompt = `You are an assistant for question-answering tasks.\\n\",\n-        \"Use the following pieces of retrieved context to answer the question.\\n\",\n-        \"If you don't know the answer, just say that you don't know.\\n\",\n-        \"Use three sentences maximum and keep the answer concise.\\n\",\n-        \"\\n\",\n-        \"{context}`\\n\",\n-        \"\\n\",\n-        \"const qaPrompt = ChatPromptTemplate.fromMessages([\\n\",\n-        \"  [\\\"system\\\", qaSystemPrompt],\\n\",\n-        \"  new MessagesPlaceholder(\\\"chat_history\\\"),\\n\",\n-        \"  [\\\"human\\\", \\\"{question}\\\"]\\n\",\n-        \"]);\\n\",\n-        \"\\n\",\n-        \"const contextualizedQuestion = (input: Record<string, unknown>) => {\\n\",\n-        \"  if (\\\"chat_history\\\" in input) {\\n\",\n-        \"    return contextualizeQChain;\\n\",\n-        \"  }\\n\",\n-        \"  return input.question;\\n\",\n-        \"};\\n\",\n-        \"\\n\",\n-        \"const ragChain = RunnableSequence.from([\\n\",\n-        \"  RunnablePassthrough.assign({\\n\",\n-        \"    context: (input: Record<string, unknown>) => {\\n\",\n-        \"      if (\\\"chat_history\\\" in input) {\\n\",\n-        \"        const chain = contextualizedQuestion(input);\\n\",\n-        \"        return chain.pipe(retriever).pipe(formatDocumentsAsString);\\n\",\n-        \"      }\\n\",\n-        \"      return \\\"\\\";\\n\",\n-        \"    },\\n\",\n-        \"  }),\\n\",\n-        \"  qaPrompt,\\n\",\n-        \"  llm\\n\",\n-        \"])\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Task decomposition can be done (1) by LLM with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for writing a novel, or (3) with human inputs.\\n\",\n+      \"Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\n\",\n+      \"Self-Reflection#\\n\",\n+      \"\\n\",\n+      \"Fig. 1. Overview of a LLM-powered autonomous agent system.\\n\",\n+      \"Component One: Planning#\\n\",\n+      \"A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\n\",\n+      \"Task Decomposition#\\n\",\n+      \"Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\",\n+      \"Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\",\n+      \"\\n\",\n+      \"(3) Task execution: Expert models execute on the specific tasks and log results.\\n\",\n+      \"Instruction:\\n\",\n+      \"\\n\",\n+      \"With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\",\n+      \"\\n\",\n+      \"Resources:\\n\",\n+      \"1. Internet access for searches and information gathering.\\n\",\n+      \"2. Long Term memory management.\\n\",\n+      \"3. GPT-3.5 powered Agents for delegation of simple tasks.\\n\",\n+      \"4. File output.\\n\",\n+      \"\\n\",\n+      \"Performance Evaluation:\\n\",\n+      \"1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n\",\n+      \"2. Constructively self-criticize your big-picture behavior constantly.\\n\",\n+      \"3. Reflect on past decisions and strategies to refine your approach.\\n\",\n+      \"4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"console.log(await tool.invoke({ query: \\\"task decomposition\\\" }))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"f77e0217-28be-4b8b-b4c4-9cc4ed5ec201\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Agent constructor\\n\",\n+    \"\\n\",\n+    \"Now that we have defined the tools and the LLM, we can create the agent. We will be using [LangGraph](/docs/concepts/#langgraph) to construct the agent. \\n\",\n+    \"Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 27,\n+   \"id\": \"1726d151-4653-4c72-a187-a14840add526\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n+    \"\\n\",\n+    \"const agentExecutor = createReactAgent({ llm, tools });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"6d5152ca-1c3b-4f58-bb28-f31c0be7ba66\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"We can now try it out. Note that so far it is not stateful (we still need to add in memory)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 28,\n+   \"id\": \"170403a2-c914-41db-85d8-a2c381da112d\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 10,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"AIMessage {\\n\",\n-            \"  lc_serializable: true,\\n\",\n-            \"  lc_kwargs: {\\n\",\n-            \"    content: \\\"Task decomposition is a technique used to break down complex tasks into smaller and more manageable \\\"... 278 more characters,\\n\",\n-            \"    additional_kwargs: { function_call: undefined, tool_calls: undefined }\\n\",\n-            \"  },\\n\",\n-            \"  lc_namespace: [ \\\"langchain_core\\\", \\\"messages\\\" ],\\n\",\n-            \"  content: \\\"Task decomposition is a technique used to break down complex tasks into smaller and more manageable \\\"... 278 more characters,\\n\",\n-            \"  name: undefined,\\n\",\n-            \"  additional_kwargs: { function_call: undefined, tool_calls: undefined }\\n\",\n-            \"}\\n\"\n-          ]\n-        },\n-        {\n-          \"data\": {\n-            \"text/plain\": [\n-              \"AIMessage {\\n\",\n-              \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-              \"  lc_kwargs: {\\n\",\n-              \"    content: \\u001b[32m\\\"Common ways of task decomposition include using prompting techniques like Chain of Thought (CoT) or \\\"\\u001b[39m... 332 more characters,\\n\",\n-              \"    additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m }\\n\",\n-              \"  },\\n\",\n-              \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-              \"  content: \\u001b[32m\\\"Common ways of task decomposition include using prompting techniques like Chain of Thought (CoT) or \\\"\\u001b[39m... 332 more characters,\\n\",\n-              \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-              \"  additional_kwargs: { function_call: \\u001b[90mundefined\\u001b[39m, tool_calls: \\u001b[90mundefined\\u001b[39m }\\n\",\n-              \"}\"\n-            ]\n-          },\n-          \"execution_count\": 10,\n-          \"metadata\": {},\n-          \"output_type\": \"execute_result\"\n-        }\n-      ],\n-      \"source\": [\n-        \"let chat_history = [];\\n\",\n-        \"\\n\",\n-        \"const question = \\\"What is task decomposition?\\\";\\n\",\n-        \"const aiMsg = await ragChain.invoke({ question, chat_history });\\n\",\n-        \"console.log(aiMsg)\\n\",\n-        \"chat_history = chat_history.concat(aiMsg);\\n\",\n-        \"\\n\",\n-        \"const secondQuestion = \\\"What are common ways of doing it?\\\";\\n\",\n-        \"await ragChain.invoke({ question: secondQuestion, chat_history });\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABABtUmgD1ZlOHZd0nD9TR8yb3mMe\\\",\\n\",\n+      \"        \\\"content\\\": \\\"\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {\\n\",\n+      \"          \\\"tool_calls\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"id\\\": \\\"call_dWxEY41mg9VSLamVYHltsUxL\\\",\\n\",\n+      \"              \\\"type\\\": \\\"function\\\",\\n\",\n+      \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ]\\n\",\n+      \"        },\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 19,\\n\",\n+      \"            \\\"promptTokens\\\": 66,\\n\",\n+      \"            \\\"totalTokens\\\": 85\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_3537616b13\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [\\n\",\n+      \"          {\\n\",\n+      \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"            \\\"args\\\": {\\n\",\n+      \"              \\\"query\\\": \\\"Task Decomposition\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"            \\\"id\\\": \\\"call_dWxEY41mg9VSLamVYHltsUxL\\\"\\n\",\n+      \"          }\\n\",\n+      \"        ],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 66,\\n\",\n+      \"          \\\"output_tokens\\\": 19,\\n\",\n+      \"          \\\"total_tokens\\\": 85\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  tools: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      ToolMessage {\\n\",\n+      \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\",\\n\",\n+      \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {},\\n\",\n+      \"        \\\"tool_call_id\\\": \\\"call_dWxEY41mg9VSLamVYHltsUxL\\\"\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABABuSj5FHmHFdeR2Pv7Cxcmq5aQz\\\",\\n\",\n+      \"        \\\"content\\\": \\\"Task Decomposition is a technique that allows an agent to break down a complex task into smaller, more manageable subtasks or steps. The primary goal is to simplify the task to ensure efficient execution and better understanding. \\\\n\\\\n### Methods in Task Decomposition:\\\\n1. **Chain of Thought (CoT)**:\\\\n    - **Description**: This technique involves instructing the model to “think step by step” to decompose hard tasks into smaller ones. It transforms large tasks into multiple manageable tasks, enhancing the model's performance and providing insight into its thinking process. \\\\n    - **Example**: When given a complex problem, the model outlines sequential steps to reach a solution.\\\\n\\\\n2. **Tree of Thoughts**:\\\\n    - **Description**: This extends CoT by exploring multiple reasoning possibilities at each step. The problem is decomposed into multiple thought steps, with several thoughts generated per step, forming a sort of decision tree.\\\\n    - **Example**: For a given task, the model might consider various alternative actions at each stage, evaluating each before proceeding.\\\\n\\\\n3. **LLM with Prompts**:\\\\n    - **Description**: Basic task decomposition can be done via simple prompts like \\\\\\\"Steps for XYZ\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\" This can also be guided by task-specific instructions or human inputs when necessary.\\\\n    - **Example**: Asking the model to list the subgoals for writing a novel might produce an outline broken down into chapters, character development, and plot points.\\\\n\\\\n4. **LLM+P**:\\\\n    - **Description**: This approach involves outsourcing long-term planning to an external classical planner using Planning Domain Definition Language (PDDL). The task is translated into a PDDL problem by the model, planned using classical planning tools, and then translated back into natural language.\\\\n    - **Example**: In robotics, translating a task into PDDL and then using a domain-specific planner to generate a sequence of actions.\\\\n\\\\n### Applications:\\\\n- **Planning**: Helps an agent plan tasks by breaking them into clear, manageable steps.\\\\n- **Self-Reflection**: Allows agents to reflect and refine their actions, learning from past mistakes to improve future performance.\\\\n- **Memory**: Utilizes short-term memory for immediate context and long-term memory for retaining and recalling information over extended periods.\\\\n- **Tool Use**: Enables the agent to call external APIs for additional information or capabilities not inherent in the model.\\\\n\\\\nIn essence, task decomposition leverages various methodologies to simplify complex tasks, ensuring better performance, improved reasoning, and effective task execution.\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 522,\\n\",\n+      \"            \\\"promptTokens\\\": 821,\\n\",\n+      \"            \\\"totalTokens\\\": 1343\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 821,\\n\",\n+      \"          \\\"output_tokens\\\": 522,\\n\",\n+      \"          \\\"total_tokens\\\": 1343\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const query = \\\"What is Task Decomposition?\\\";\\n\",\n+    \"\\n\",\n+    \"for await (const s of await agentExecutor.stream(\\n\",\n+    \"  { messages: [new HumanMessage(query)] }\\n\",\n+    \")) {\\n\",\n+    \"  console.log(s);\\n\",\n+    \"  console.log(\\\"----\\\");\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"1df703b1-aad6-48fb-b6fa-703e32ea88b9\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"LangGraph comes with built in persistence, so we don't need to use ChatMessageHistory! Rather, we can pass in a checkpointer to our LangGraph agent directly\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 29,\n+   \"id\": \"04a3a664-3c3f-4cd1-9995-26662a52da7c\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"\\n\",\n+    \"const memory = new MemorySaver();\\n\",\n+    \"\\n\",\n+    \"const agentExecutorWithMemory = createReactAgent({ llm, tools, checkpointSaver: memory });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"02026f78-338e-4d18-9f05-131e1dd59197\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"This is all we need to construct a conversational RAG agent.\\n\",\n+    \"\\n\",\n+    \"Let's observe its behavior. Note that if we input a query that does not require a retrieval step, the agent does not execute one:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 30,\n+   \"id\": \"d6d70833-b958-4cd7-9e27-29c1c08bb1b8\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"See the first [LastSmith trace here](https://smith.langchain.com/public/527981c6-5018-4b68-a11a-ebcde77843e7/r) and the [second trace here](https://smith.langchain.com/public/7b97994a-ab9f-4bf3-a2e4-abb609e5610a/r)\"\n-      ]\n-    },\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABACGc1vDPUSHYN7YVkuUMwpKR20P\\\",\\n\",\n+      \"        \\\"content\\\": \\\"Hello, Bob! How can I assist you today?\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 12,\\n\",\n+      \"            \\\"promptTokens\\\": 64,\\n\",\n+      \"            \\\"totalTokens\\\": 76\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_e375328146\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 64,\\n\",\n+      \"          \\\"output_tokens\\\": 12,\\n\",\n+      \"          \\\"total_tokens\\\": 76\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"const config = { configurable: { thread_id: \\\"abc123\\\" } };\\n\",\n+    \"\\n\",\n+    \"for await (const s of await agentExecutorWithMemory.stream(\\n\",\n+    \"  { messages: [new HumanMessage(\\\"Hi! I'm bob\\\")] },\\n\",\n+    \"  config\\n\",\n+    \")) {\\n\",\n+    \"  console.log(s);\\n\",\n+    \"  console.log(\\\"----\\\");\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"a7928865-3dd6-4d36-abc6-2a30de770d09\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Further, if we input a query that does require a retrieval step, the agent generates the input to the tool:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 31,\n+   \"id\": \"e2c570ae-dd91-402c-8693-ae746de63b16\",\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it.\\n\",\n-        \"\\n\",\n-        \"For this we can use:\\n\",\n-        \"\\n\",\n-        \"- [BaseChatMessageHistory](https://api.js.langchain.com/classes/langchain_core.chat_history.BaseChatMessageHistory.html): Store chat history.\\n\",\n-        \"- [RunnableWithMessageHistory](/docs/how_to/message_history/): Wrapper for an LCEL chain and a `BaseChatMessageHistory` that handles injecting chat history into inputs and updating it after each invocation.\\n\",\n-        \"\\n\",\n-        \"For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the [How to add message history (memory)](/docs/how_to/message_history/) LCEL page.\"\n-      ]\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABACI6WN7hkfJjFhIUBGt3TswtPOv\\\",\\n\",\n+      \"        \\\"content\\\": \\\"\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {\\n\",\n+      \"          \\\"tool_calls\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"id\\\": \\\"call_Lys2G4TbOMJ6RBuVvKnFSK4V\\\",\\n\",\n+      \"              \\\"type\\\": \\\"function\\\",\\n\",\n+      \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ]\\n\",\n+      \"        },\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 19,\\n\",\n+      \"            \\\"promptTokens\\\": 89,\\n\",\n+      \"            \\\"totalTokens\\\": 108\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_f82f5b050c\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [\\n\",\n+      \"          {\\n\",\n+      \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"            \\\"args\\\": {\\n\",\n+      \"              \\\"query\\\": \\\"Task Decomposition\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"            \\\"id\\\": \\\"call_Lys2G4TbOMJ6RBuVvKnFSK4V\\\"\\n\",\n+      \"          }\\n\",\n+      \"        ],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 89,\\n\",\n+      \"          \\\"output_tokens\\\": 19,\\n\",\n+      \"          \\\"total_tokens\\\": 108\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  tools: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      ToolMessage {\\n\",\n+      \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\",\\n\",\n+      \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {},\\n\",\n+      \"        \\\"tool_call_id\\\": \\\"call_Lys2G4TbOMJ6RBuVvKnFSK4V\\\"\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABACJu56eYSAyyMNaV9UEUwHS8vRu\\\",\\n\",\n+      \"        \\\"content\\\": \\\"Task Decomposition is a method used to break down complicated tasks into smaller, more manageable steps. This approach leverages the \\\\\\\"Chain of Thought\\\\\\\" (CoT) technique, which prompts models to \\\\\\\"think step by step\\\\\\\" to enhance performance on complex tasks. Here’s a summary of the key concepts related to Task Decomposition:\\\\n\\\\n1. **Chain of Thought (CoT):**\\\\n   - A prompting technique that encourages models to decompose hard tasks into simpler steps, transforming big tasks into multiple manageable sub-tasks.\\\\n   - CoT helps to provide insights into the model’s thinking process.\\\\n\\\\n2. **Tree of Thoughts:**\\\\n   - An extension of CoT, this approach explores multiple reasoning paths at each step.\\\\n   - It creates a tree structure by generating multiple thoughts per step, and uses search methods like breadth-first search (BFS) or depth-first search (DFS) to explore these thoughts.\\\\n   - Each state is evaluated by a classifier or majority vote.\\\\n\\\\n3. **Methods for Task Decomposition:**\\\\n   - Simple prompting such as instructing with phrases like \\\\\\\"Steps for XYZ: 1., 2., 3.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n   - Using task-specific instructions like \\\\\\\"Write a story outline\\\\\\\" for specific tasks such as writing a novel.\\\\n   - Incorporating human inputs for better granularity.\\\\n\\\\n4. **LLM+P (Long-horizon Planning):**\\\\n   - A method that involves using an external classical planner for long-horizon planning.\\\\n   - The process involves translating the problem into a Planning Domain Definition Language (PDDL) problem, using a classical planner to generate a PDDL plan, and then translating it back into natural language.\\\\n\\\\nTask Decomposition is essential in planning complex tasks, allowing for efficient handling by breaking them into sub-tasks and sub-goals. This process is integral to the functioning of autonomous agent systems and enhances their capability to execute intricate tasks effectively.\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 396,\\n\",\n+      \"            \\\"promptTokens\\\": 844,\\n\",\n+      \"            \\\"totalTokens\\\": 1240\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_9f2bfdaa89\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 844,\\n\",\n+      \"          \\\"output_tokens\\\": 396,\\n\",\n+      \"          \\\"total_tokens\\\": 1240\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\"\n+     ]\n     }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Deno\",\n-      \"language\": \"typescript\",\n-      \"name\": \"deno\"\n-    },\n-    \"language_info\": {\n-      \"file_extension\": \".ts\",\n-      \"mimetype\": \"text/x.typescript\",\n-      \"name\": \"typescript\",\n-      \"nb_converter\": \"script\",\n-      \"pygments_lexer\": \"typescript\",\n-      \"version\": \"5.3.3\"\n+   ],\n+   \"source\": [\n+    \"for await (const s of await agentExecutorWithMemory.stream(\\n\",\n+    \"  { messages: [new HumanMessage(query)] },\\n\",\n+    \"  config\\n\",\n+    \")) {\\n\",\n+    \"  console.log(s);\\n\",\n+    \"  console.log(\\\"----\\\");\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"26eaae33-3c4e-49fc-9fc6-db8967e25579\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Above, instead of inserting our query verbatim into the tool, the agent stripped unnecessary words like \\\"what\\\" and \\\"is\\\".\\n\",\n+    \"\\n\",\n+    \"This same principle allows the agent to use the context of the conversation when necessary:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 32,\n+   \"id\": \"570d8c68-136e-4ba5-969a-03ba195f6118\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABACPZzSugzrREQRO4mVQfI3cQOeL\\\",\\n\",\n+      \"        \\\"content\\\": \\\"\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {\\n\",\n+      \"          \\\"tool_calls\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"id\\\": \\\"call_5nSZb396Tcg73Pok6Bx1XV8b\\\",\\n\",\n+      \"              \\\"type\\\": \\\"function\\\",\\n\",\n+      \"              \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"            }\\n\",\n+      \"          ]\\n\",\n+      \"        },\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 22,\\n\",\n+      \"            \\\"promptTokens\\\": 1263,\\n\",\n+      \"            \\\"totalTokens\\\": 1285\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_9f2bfdaa89\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [\\n\",\n+      \"          {\\n\",\n+      \"            \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"            \\\"args\\\": {\\n\",\n+      \"              \\\"query\\\": \\\"common ways of doing task decomposition\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"            \\\"id\\\": \\\"call_5nSZb396Tcg73Pok6Bx1XV8b\\\"\\n\",\n+      \"          }\\n\",\n+      \"        ],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 1263,\\n\",\n+      \"          \\\"output_tokens\\\": 22,\\n\",\n+      \"          \\\"total_tokens\\\": 1285\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  tools: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      ToolMessage {\\n\",\n+      \"        \\\"content\\\": \\\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\n\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\", \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\", (2) by using task-specific instructions; e.g. \\\\\\\"Write a story outline.\\\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection#\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\nResources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\\",\\n\",\n+      \"        \\\"name\\\": \\\"blog_post_retriever\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {},\\n\",\n+      \"        \\\"tool_call_id\\\": \\\"call_5nSZb396Tcg73Pok6Bx1XV8b\\\"\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\",\n+      \"{\\n\",\n+      \"  agent: {\\n\",\n+      \"    messages: [\\n\",\n+      \"      AIMessage {\\n\",\n+      \"        \\\"id\\\": \\\"chatcmpl-ABACQt9pT5dKCTaGQpVawcmCCWdET\\\",\\n\",\n+      \"        \\\"content\\\": \\\"According to the blog post, common ways of performing Task Decomposition include:\\\\n\\\\n1. **Using Large Language Models (LLMs) with Simple Prompting:**\\\\n   - Providing clear and structured prompts such as \\\\\\\"Steps for XYZ: 1., 2., 3.\\\\\\\" or asking \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\"\\\\n   - This allows the model to break down the tasks step-by-step.\\\\n\\\\n2. **Task-Specific Instructions:**\\\\n   - Employing specific instructions tailored to the task at hand, for example, \\\\\\\"Write a story outline\\\\\\\" for writing a novel.\\\\n   - These instructions guide the model in decomposing the task appropriately.\\\\n\\\\n3. **Involving Human Inputs:**\\\\n   - Integrating insights and directives from humans to aid in the decomposition process.\\\\n   - This can ensure that the breakdown is comprehensive and accurately reflects the nuances of the task.\\\\n\\\\n4. **LLM+P Approach for Long-Horizon Planning:**\\\\n   - Utilizing an external classical planner by translating the problem into Planning Domain Definition Language (PDDL).\\\\n   - The process involves:\\\\n     1. Translating the problem into “Problem PDDL”.\\\\n     2. Requesting a classical planner to generate a PDDL plan based on an existing “Domain PDDL”.\\\\n     3. Translating the PDDL plan back into natural language.\\\\n\\\\nThese methods enable effective management and execution of complex tasks by transforming them into simpler, more manageable components.\\\",\\n\",\n+      \"        \\\"additional_kwargs\\\": {},\\n\",\n+      \"        \\\"response_metadata\\\": {\\n\",\n+      \"          \\\"tokenUsage\\\": {\\n\",\n+      \"            \\\"completionTokens\\\": 292,\\n\",\n+      \"            \\\"promptTokens\\\": 2010,\\n\",\n+      \"            \\\"totalTokens\\\": 2302\\n\",\n+      \"          },\\n\",\n+      \"          \\\"finish_reason\\\": \\\"stop\\\",\\n\",\n+      \"          \\\"system_fingerprint\\\": \\\"fp_9f2bfdaa89\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"tool_calls\\\": [],\\n\",\n+      \"        \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"        \\\"usage_metadata\\\": {\\n\",\n+      \"          \\\"input_tokens\\\": 2010,\\n\",\n+      \"          \\\"output_tokens\\\": 292,\\n\",\n+      \"          \\\"total_tokens\\\": 2302\\n\",\n+      \"        }\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  }\\n\",\n+      \"}\\n\",\n+      \"----\\n\"\n+     ]\n     }\n+   ],\n+   \"source\": [\n+    \"const query3 = \\\"What according to the blog post are common ways of doing it? redo the search\\\";\\n\",\n+    \"\\n\",\n+    \"for await (const s of await agentExecutorWithMemory.stream(\\n\",\n+    \"  { messages: [new HumanMessage(query3)] },\\n\",\n+    \"  config\\n\",\n+    \")) {\\n\",\n+    \"  console.log(s);\\n\",\n+    \"  console.log(\\\"----\\\");\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"f2724616-c106-4e15-a61a-3077c535f692\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Note that the agent was able to infer that \\\"it\\\" in our query refers to \\\"task decomposition\\\", and generated a reasonable search query as a result-- in this case, \\\"common ways of task decomposition\\\".\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"1cf87847-23bb-4672-b41c-12ad9cf81ed4\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Tying it together\\n\",\n+    \"\\n\",\n+    \"For convenience, we tie together all of the necessary steps in a single code cell:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 33,\n+   \"id\": \"b1d2b4d4-e604-497d-873d-d345b808578e\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n+    \"import { CheerioWebBaseLoader } from \\\"@langchain/community/document_loaders/web/cheerio\\\";\\n\",\n+    \"import { RecursiveCharacterTextSplitter } from \\\"langchain/text_splitter\\\";\\n\",\n+    \"import { MemoryVectorStore } from \\\"langchain/vectorstores/memory\\\";\\n\",\n+    \"import { createRetrieverTool } from \\\"langchain/tools/retriever\\\";\\n\",\n+    \"\\n\",\n+    \"const memory3 = new MemorySaver();\\n\",\n+    \"const llm3 = new ChatOpenAI({ model: \\\"gpt-4o\\\", temperature: 0 });\\n\",\n+    \"\\n\",\n+    \"// Construct retriever\\n\",\n+    \"const loader3 = new CheerioWebBaseLoader(\\n\",\n+    \"  \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\",\\n\",\n+    \"  {\\n\",\n+    \"    selector: \\\".post-content, .post-title, .post-header\\\"\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"const docs3 = await loader3.load();\\n\",\n+    \"\\n\",\n+    \"const textSplitter3 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\\n\",\n+    \"const splits3 = await textSplitter3.splitDocuments(docs3);\\n\",\n+    \"const vectorstore3 = await MemoryVectorStore.fromDocuments(splits3, new OpenAIEmbeddings());\\n\",\n+    \"const retriever3 = vectorstore3.asRetriever();\\n\",\n+    \"\\n\",\n+    \"// Build retriever tool\\n\",\n+    \"const tool3 = createRetrieverTool(\\n\",\n+    \"  retriever3,\\n\",\n+    \"  {\\n\",\n+    \"    name: \\\"blog_post_retriever\\\",\\n\",\n+    \"    description: \\\"Searches and returns excerpts from the Autonomous Agents blog post.\\\",\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"const tools3 = [tool3];\\n\",\n+    \"\\n\",\n+    \"const agentExecutor3 = createReactAgent({ llm: llm3, tools: tools3, checkpointSaver: memory3 });\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"cd6bf4f4-74f4-419d-9e26-f0ed83cf05fa\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Next steps\\n\",\n+    \"\\n\",\n+    \"We've covered the steps to build a basic conversational Q&A application:\\n\",\n+    \"\\n\",\n+    \"- We used chains to build a predictable application that generates search queries for each user input;\\n\",\n+    \"- We used agents to build an application that \\\"decides\\\" when and how to generate search queries.\\n\",\n+    \"\\n\",\n+    \"To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to/#retrievers) section of the how-to guides.\\n\",\n+    \"\\n\",\n+    \"For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history) LCEL page.\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n   },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 2\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n }",
          "docs/core_docs/docs/tutorials/rag.ipynb": "@@ -607,7 +607,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"We’ll use the [LCEL Runnable](/docs/how_to/#langchain-expression-language-lcel) protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box\"\n+        \"We’ll use [LangChain Expression Language (LCEL)](/docs/how_to/#langchain-expression-language-lcel) to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box\"\n       ]\n     },\n     {",
          "docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb": "@@ -0,0 +1,268 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"c298a5c9-b9af-481d-9eba-cbd65f987a8a\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# How to use BaseChatMessageHistory with LangGraph\\n\",\n+    \"\\n\",\n+    \":::info Prerequisites\\n\",\n+    \"\\n\",\n+    \"This guide assumes familiarity with the following concepts:\\n\",\n+    \"\\n\",\n+    \"- [Chat History](/docs/concepts/#chat-history)\\n\",\n+    \"- [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html)\\n\",\n+    \"- [LangGraph](https://langchain-ai.github.io/langgraphjs/concepts/high_level/)\\n\",\n+    \"- [Memory](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#memory)\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n+    \"We recommend that new LangChain applications take advantage of the [built-in LangGraph peristence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to implement memory.\\n\",\n+    \"\\n\",\n+    \"In some situations, users may need to keep using an existing persistence solution for chat message history.\\n\",\n+    \"\\n\",\n+    \"Here, we will show how to use [LangChain chat message histories](/docs/integrations/memory/) (implementations of [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html)) with LangGraph.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"548bc988-167b-43f1-860a-d247e28b2b42\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Set up\\n\",\n+    \"\\n\",\n+    \"```typescript\\n\",\n+    \"process.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\"\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/core @langchain/langgraph @langchain/anthropic\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"c5e08659-b68c-48f2-8b33-e79b0c6999e1\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## ChatMessageHistory\\n\",\n+    \"\\n\",\n+    \"A message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).\\n\",\n+    \"\\n\",\n+    \"Many of the [LangChain chat message histories](/docs/integrations/memory/) will have either a `sessionId` or some `namespace` to allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.\\n\",\n+    \"\\n\",\n+    \"The built-in `InMemoryChatMessageHistory` does not contains such a parameterization, so we'll create a dictionary to keep track of the message histories.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"28049308-2543-48e6-90d0-37a88951a637\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import { InMemoryChatMessageHistory } from \\\"@langchain/core/chat_history\\\";\\n\",\n+    \"\\n\",\n+    \"const chatsBySessionId: Record<string, InMemoryChatMessageHistory> = {}\\n\",\n+    \"\\n\",\n+    \"const getChatHistory = (sessionId: string) => {\\n\",\n+    \"    let chatHistory: InMemoryChatMessageHistory | undefined = chatsBySessionId[sessionId]\\n\",\n+    \"    if (!chatHistory) {\\n\",\n+    \"      chatHistory = new InMemoryChatMessageHistory()\\n\",\n+    \"      chatsBySessionId[sessionId] = chatHistory\\n\",\n+    \"    }\\n\",\n+    \"    return chatHistory\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"94c53ce3-4212-41e6-8ad3-f0ab5df6130f\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Use with LangGraph\\n\",\n+    \"\\n\",\n+    \"Next, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following [Quick Start Tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/).\\n\",\n+    \"\\n\",\n+    \"We'll create a [LangGraph node](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#nodes) for the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.\\n\",\n+    \"\\n\",\n+    \"The conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the [graph state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"d818e23f\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"hi! I'm bob\\n\",\n+      \"Hello Bob! It's nice to meet you. How can I assist you today?\\n\",\n+      \"what was my name?\\n\",\n+      \"You said your name is Bob.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { v4 as uuidv4 } from \\\"uuid\\\";\\n\",\n+    \"import { ChatAnthropic } from \\\"@langchain/anthropic\\\";\\n\",\n+    \"import { StateGraph, MessagesAnnotation, END, START } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { HumanMessage } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { RunnableConfig } from \\\"@langchain/core/runnables\\\";\\n\",\n+    \"\\n\",\n+    \"// Define a chat model\\n\",\n+    \"const model = new ChatAnthropic({ modelName: \\\"claude-3-haiku-20240307\\\" });\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel = async (\\n\",\n+    \"  state: typeof MessagesAnnotation.State,\\n\",\n+    \"  config: RunnableConfig\\n\",\n+    \"): Promise<Partial<typeof MessagesAnnotation.State>> => {\\n\",\n+    \"  if (!config.configurable?.sessionId) {\\n\",\n+    \"    throw new Error(\\n\",\n+    \"      \\\"Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}\\\"\\n\",\n+    \"    );\\n\",\n+    \"  }\\n\",\n+    \"\\n\",\n+    \"  const chatHistory = getChatHistory(config.configurable.sessionId as string);\\n\",\n+    \"\\n\",\n+    \"  let messages = [...(await chatHistory.getMessages()), ...state.messages];\\n\",\n+    \"\\n\",\n+    \"  if (state.messages.length === 1) {\\n\",\n+    \"    // First message, ensure it's in the chat history\\n\",\n+    \"    await chatHistory.addMessage(state.messages[0]);\\n\",\n+    \"  }\\n\",\n+    \"\\n\",\n+    \"  const aiMessage = await model.invoke(messages);\\n\",\n+    \"\\n\",\n+    \"  // Update the chat history\\n\",\n+    \"  await chatHistory.addMessage(aiMessage);\\n\",\n+    \"\\n\",\n+    \"  return { messages: [aiMessage] };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"// Define a new graph\\n\",\n+    \"const workflow = new StateGraph(MessagesAnnotation)\\n\",\n+    \"  .addNode(\\\"model\\\", callModel)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END);\\n\",\n+    \"\\n\",\n+    \"const app = workflow.compile();\\n\",\n+    \"\\n\",\n+    \"// Create a unique session ID to identify the conversation\\n\",\n+    \"const sessionId = uuidv4();\\n\",\n+    \"const config = { configurable: { sessionId }, streamMode: \\\"values\\\" as const };\\n\",\n+    \"\\n\",\n+    \"const inputMessage = new HumanMessage(\\\"hi! I'm bob\\\");\\n\",\n+    \"\\n\",\n+    \"for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"// Here, let's confirm that the AI remembers our name!\\n\",\n+    \"const followUpMessage = new HumanMessage(\\\"what was my name?\\\");\\n\",\n+    \"\\n\",\n+    \"for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"da0536dd-9a0b-49e3-b0b6-e8c7abf3b1f9\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Using With RunnableWithMessageHistory\\n\",\n+    \"\\n\",\n+    \"This how-to guide used the `messages` and `addMessages` interface of `BaseChatMessageHistory` directly. \\n\",\n+    \"\\n\",\n+    \"Alternatively, you can use [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html), as [LCEL](/docs/concepts/#langchain-expression-language-lcel/) can be used inside any [LangGraph node](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#nodes).\\n\",\n+    \"\\n\",\n+    \"To do that replace the following code:\\n\",\n+    \"\\n\",\n+    \"```typescript\\n\",\n+    \"const callModel = async (\\n\",\n+    \"  state: typeof MessagesAnnotation.State,\\n\",\n+    \"  config: RunnableConfig\\n\",\n+    \"): Promise<Partial<typeof MessagesAnnotation.State>> => {\\n\",\n+    \"  // highlight-start\\n\",\n+    \"  if (!config.configurable?.sessionId) {\\n\",\n+    \"    throw new Error(\\n\",\n+    \"      \\\"Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}\\\"\\n\",\n+    \"    );\\n\",\n+    \"  }\\n\",\n+    \"\\n\",\n+    \"  const chatHistory = getChatHistory(config.configurable.sessionId as string);\\n\",\n+    \"\\n\",\n+    \"  let messages = [...(await chatHistory.getMessages()), ...state.messages];\\n\",\n+    \"\\n\",\n+    \"  if (state.messages.length === 1) {\\n\",\n+    \"    // First message, ensure it's in the chat history\\n\",\n+    \"    await chatHistory.addMessage(state.messages[0]);\\n\",\n+    \"  }\\n\",\n+    \"\\n\",\n+    \"  const aiMessage = await model.invoke(messages);\\n\",\n+    \"\\n\",\n+    \"  // Update the chat history\\n\",\n+    \"  await chatHistory.addMessage(aiMessage);\\n\",\n+    \"  // highlight-end\\n\",\n+    \"  return { messages: [aiMessage] };\\n\",\n+    \"};\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"With the corresponding instance of `RunnableWithMessageHistory` defined in your current application.\\n\",\n+    \"\\n\",\n+    \"```typescript\\n\",\n+    \"const runnable = new RunnableWithMessageHistory({\\n\",\n+    \"  // ... configuration from existing code\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const callModel = async (\\n\",\n+    \"  state: typeof MessagesAnnotation.State,\\n\",\n+    \"  config: RunnableConfig\\n\",\n+    \"): Promise<Partial<typeof MessagesAnnotation.State>> => {\\n\",\n+    \"  // RunnableWithMessageHistory takes care of reading the message history\\n\",\n+    \"  // and updating it with the new human message and AI response.\\n\",\n+    \"  const aiMessage = await runnable.invoke(state.messages, config);\\n\",\n+    \"  return {\\n\",\n+    \"    messages: [aiMessage]\\n\",\n+    \"  };\\n\",\n+    \"};\\n\",\n+    \"```\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb": "@@ -0,0 +1,643 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"ce8457ed-c0b1-4a74-abbd-9d3d2211270f\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Migrating off ConversationTokenBufferMemory\\n\",\n+    \"\\n\",\n+    \"Follow this guide if you're trying to migrate off one of the old memory classes listed below:\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"| Memory Type                      | Description                                                                                                                                                       |\\n\",\n+    \"|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n\",\n+    \"| `ConversationTokenBufferMemory`  | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\\n\",\n+    \"\\n\",\n+    \"`ConversationTokenBufferMemory` applies additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model. \\n\",\n+    \"\\n\",\n+    \"This processing functionality can be accomplished using LangChain's built-in [trimMessages](https://api.js.langchain.com/functions/_langchain_core.messages.trimMessages.html) function.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"79935247-acc7-4a05-a387-5d72c9c8c8cb\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \":::important\\n\",\n+    \"\\n\",\n+    \"We’ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.\\n\",\n+    \"\\n\",\n+    \"While this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.\\n\",\n+    \"\\n\",\n+    \"More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.\\n\",\n+    \"\\n\",\n+    \"For instance, the LangGraph [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) demonstrates\\n\",\n+    \"how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\\n\",\n+    \":::\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"d07f9459-9fb6-4942-99c9-64558aedd7d4\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Set up\\n\",\n+    \"\\n\",\n+    \"### Dependencies\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\"\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/openai @langchain/core zod\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"### Environment variables\\n\",\n+    \"\\n\",\n+    \"```typescript\\n\",\n+    \"process.env.OPENAI_API_KEY = \\\"YOUR_OPENAI_API_KEY\\\";\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<details open>\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"7ce2d951\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Reimplementing ConversationTokenBufferMemory logic\\n\",\n+    \"\\n\",\n+    \"Here, we'll use `trimMessages` to keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"e1550bee\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import {\\n\",\n+    \"  AIMessage,\\n\",\n+    \"  HumanMessage,\\n\",\n+    \"  SystemMessage,\\n\",\n+    \"} from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"const messages = [\\n\",\n+    \"  new SystemMessage(\\\"you're a good assistant, you always respond with a joke.\\\"),\\n\",\n+    \"  new HumanMessage(\\\"i wonder why it's called langchain\\\"),\\n\",\n+    \"  new AIMessage(\\n\",\n+    \"    'Well, I guess they thought \\\"WordRope\\\" and \\\"SentenceString\\\" just didn\\\\'t have the same ring to it!'\\n\",\n+    \"  ),\\n\",\n+    \"  new HumanMessage(\\\"and who is harrison chasing anyways\\\"),\\n\",\n+    \"  new AIMessage(\\n\",\n+    \"      \\\"Hmmm let me think.\\\\n\\\\nWhy, he's probably chasing after the last cup of coffee in the office!\\\"\\n\",\n+    \"  ),\\n\",\n+    \"  new HumanMessage(\\\"why is 42 always the answer?\\\"),\\n\",\n+    \"  new AIMessage(\\n\",\n+    \"      \\\"Because it's the only number that's constantly right, even when it doesn't add up!\\\"\\n\",\n+    \"  ),\\n\",\n+    \"  new HumanMessage(\\\"What did the cow say?\\\"),\\n\",\n+    \"]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"6442f74b-2c36-48fd-a3d1-c7c5d18c050f\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"SystemMessage {\\n\",\n+      \"  \\\"content\\\": \\\"you're a good assistant, you always respond with a joke.\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {}\\n\",\n+      \"}\\n\",\n+      \"HumanMessage {\\n\",\n+      \"  \\\"content\\\": \\\"and who is harrison chasing anyways\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {}\\n\",\n+      \"}\\n\",\n+      \"AIMessage {\\n\",\n+      \"  \\\"content\\\": \\\"Hmmm let me think.\\\\n\\\\nWhy, he's probably chasing after the last cup of coffee in the office!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {},\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": []\\n\",\n+      \"}\\n\",\n+      \"HumanMessage {\\n\",\n+      \"  \\\"content\\\": \\\"why is 42 always the answer?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {}\\n\",\n+      \"}\\n\",\n+      \"AIMessage {\\n\",\n+      \"  \\\"content\\\": \\\"Because it's the only number that's constantly right, even when it doesn't add up!\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {},\\n\",\n+      \"  \\\"tool_calls\\\": [],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": []\\n\",\n+      \"}\\n\",\n+      \"HumanMessage {\\n\",\n+      \"  \\\"content\\\": \\\"What did the cow say?\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {},\\n\",\n+      \"  \\\"response_metadata\\\": {}\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { trimMessages } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const selectedMessages = await trimMessages(\\n\",\n+    \"  messages,\\n\",\n+    \"  {\\n\",\n+    \"    // Please see API reference for trimMessages for other ways to specify a token counter.\\n\",\n+    \"    tokenCounter: new ChatOpenAI({ model: \\\"gpt-4o\\\" }),\\n\",\n+    \"    maxTokens: 80,  // <-- token limit\\n\",\n+    \"    // The startOn is specified\\n\",\n+    \"    // to make sure we do not generate a sequence where\\n\",\n+    \"    // a ToolMessage that contains the result of a tool invocation\\n\",\n+    \"    // appears before the AIMessage that requested a tool invocation\\n\",\n+    \"    // as this will cause some chat models to raise an error.\\n\",\n+    \"    startOn: \\\"human\\\",\\n\",\n+    \"    strategy: \\\"last\\\",\\n\",\n+    \"    includeSystem: true,  // <-- Keep the system message\\n\",\n+    \"  }\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"for (const msg of selectedMessages) {\\n\",\n+    \"    console.log(msg);\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0f05d272-2d22-44b7-9fa6-e617a48584b4\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \"</details>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"## Modern usage with LangGraph\\n\",\n+    \"\\n\",\n+    \"The example below shows how to use LangGraph to add simple conversation pre-processing logic.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \":::note\\n\",\n+    \"\\n\",\n+    \"If you want to avoid running the computation on the entire conversation history each time, you can follow\\n\",\n+    \"the [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) that demonstrates\\n\",\n+    \"how to discard older messages, ensuring they aren't re-processed during later turns.\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<details open>\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"id\": \"05d360e0\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"hi! I'm bob\\n\",\n+      \"Hello, Bob! How can I assist you today?\\n\",\n+      \"what was my name?\\n\",\n+      \"You mentioned that your name is Bob. How can I help you today?\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { v4 as uuidv4 } from 'uuid';\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { StateGraph, MessagesAnnotation, END, START, MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { trimMessages } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"\\n\",\n+    \"// Define a chat model\\n\",\n+    \"const model = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\",\n+    \"\\n\",\n+    \"// Define the function that calls the model\\n\",\n+    \"const callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => {\\n\",\n+    \"  // highlight-start\\n\",\n+    \"  const selectedMessages = await trimMessages(\\n\",\n+    \"    state.messages,\\n\",\n+    \"    {\\n\",\n+    \"      tokenCounter: (messages) => messages.length, // Simple message count instead of token count\\n\",\n+    \"      maxTokens: 5, // Allow up to 5 messages\\n\",\n+    \"      strategy: \\\"last\\\",\\n\",\n+    \"      startOn: \\\"human\\\",\\n\",\n+    \"      includeSystem: true,\\n\",\n+    \"      allowPartial: false,\\n\",\n+    \"    }\\n\",\n+    \"  );\\n\",\n+    \"  // highlight-end\\n\",\n+    \"\\n\",\n+    \"  const response = await model.invoke(selectedMessages);\\n\",\n+    \"\\n\",\n+    \"  // With LangGraph, we're able to return a single message, and LangGraph will concatenate\\n\",\n+    \"  // it to the existing list\\n\",\n+    \"  return { messages: [response] };\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"// Define a new graph\\n\",\n+    \"const workflow = new StateGraph(MessagesAnnotation)\\n\",\n+    \"// Define the two nodes we will cycle between\\n\",\n+    \"  .addNode(\\\"model\\\", callModel)\\n\",\n+    \"  .addEdge(START, \\\"model\\\")\\n\",\n+    \"  .addEdge(\\\"model\\\", END)\\n\",\n+    \"\\n\",\n+    \"const app = workflow.compile({\\n\",\n+    \"  // Adding memory is straightforward in LangGraph!\\n\",\n+    \"  // Just pass a checkpointer to the compile method.\\n\",\n+    \"  checkpointer: new MemorySaver()\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// The thread id is a unique key that identifies this particular conversation\\n\",\n+    \"// ---\\n\",\n+    \"// NOTE: this must be `thread_id` and not `threadId` as the LangGraph internals expect `thread_id`\\n\",\n+    \"// ---\\n\",\n+    \"const thread_id = uuidv4();\\n\",\n+    \"const config = { configurable: { thread_id }, streamMode: \\\"values\\\" as const };\\n\",\n+    \"\\n\",\n+    \"const inputMessage = {\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"hi! I'm bob\\\",\\n\",\n+    \"}\\n\",\n+    \"for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"// Here, let's confirm that the AI remembers our name!\\n\",\n+    \"const followUpMessage = {\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"what was my name?\\\",\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"// ---\\n\",\n+    \"// NOTE: You must pass the same thread id to continue the conversation\\n\",\n+    \"// we do that here by passing the same `config` object to the `.stream` call.\\n\",\n+    \"// ---\\n\",\n+    \"for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"84229e2e-a578-4b21-840a-814223406402\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \"</details>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"## Usage with a pre-built langgraph agent\\n\",\n+    \"\\n\",\n+    \"This example shows usage of an Agent Executor with a pre-built agent constructed using the [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) function.\\n\",\n+    \"\\n\",\n+    \"If you are using one of the [old LangChain pre-built agents](https://js.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able\\n\",\n+    \"to replace that code with the new [LangGraph pre-built agent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/) which leverages\\n\",\n+    \"native tool calling capabilities of chat models and will likely work better out of the box.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<details open>\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"id\": \"9e54ccdc\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"hi! I'm bob. What is my age?\\n\",\n+      \"\\n\",\n+      \"42 years old\\n\",\n+      \"Hi Bob! You are 42 years old.\\n\",\n+      \"do you remember my name?\\n\",\n+      \"Yes, your name is Bob! If there's anything else you'd like to know or discuss, feel free to ask.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { z } from \\\"zod\\\";\\n\",\n+    \"import { v4 as uuidv4 } from 'uuid';\\n\",\n+    \"import { BaseMessage, trimMessages } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { MemorySaver } from \\\"@langchain/langgraph\\\";\\n\",\n+    \"import { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\";\\n\",\n+    \"\\n\",\n+    \"const getUserAge = tool(\\n\",\n+    \"  (name: string): string => {\\n\",\n+    \"    // This is a placeholder for the actual implementation\\n\",\n+    \"    if (name.toLowerCase().includes(\\\"bob\\\")) {\\n\",\n+    \"      return \\\"42 years old\\\";\\n\",\n+    \"    }\\n\",\n+    \"    return \\\"41 years old\\\";\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    name: \\\"get_user_age\\\",\\n\",\n+    \"    description: \\\"Use this tool to find the user's age.\\\",\\n\",\n+    \"    schema: z.string().describe(\\\"the name of the user\\\"),\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"const memory = new MemorySaver();\\n\",\n+    \"const model2 = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\",\n+    \"\\n\",\n+    \"// highlight-start\\n\",\n+    \"const stateModifier = async (messages: BaseMessage[]): Promise<BaseMessage[]> => {\\n\",\n+    \"  // We're using the message processor defined above.\\n\",\n+    \"  return trimMessages(\\n\",\n+    \"    messages,\\n\",\n+    \"    {\\n\",\n+    \"      tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens\\n\",\n+    \"      maxTokens: 5, // <-- allow up to 5 messages.\\n\",\n+    \"      strategy: \\\"last\\\",\\n\",\n+    \"      // The startOn is specified\\n\",\n+    \"      // to make sure we do not generate a sequence where\\n\",\n+    \"      // a ToolMessage that contains the result of a tool invocation\\n\",\n+    \"      // appears before the AIMessage that requested a tool invocation\\n\",\n+    \"      // as this will cause some chat models to raise an error.\\n\",\n+    \"      startOn: \\\"human\\\",\\n\",\n+    \"      includeSystem: true, // <-- Keep the system message\\n\",\n+    \"      allowPartial: false,\\n\",\n+    \"    }\\n\",\n+    \"  );\\n\",\n+    \"};\\n\",\n+    \"// highlight-end\\n\",\n+    \"\\n\",\n+    \"const app2 = createReactAgent({\\n\",\n+    \"  llm: model2,\\n\",\n+    \"  tools: [getUserAge],\\n\",\n+    \"  checkpointSaver: memory,\\n\",\n+    \"  // highlight-next-line\\n\",\n+    \"  messageModifier: stateModifier,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"// The thread id is a unique key that identifies\\n\",\n+    \"// this particular conversation.\\n\",\n+    \"// We'll just generate a random uuid here.\\n\",\n+    \"const threadId2 = uuidv4();\\n\",\n+    \"const config2 = { configurable: { thread_id: threadId2 }, streamMode: \\\"values\\\" as const };\\n\",\n+    \"\\n\",\n+    \"// Tell the AI that our name is Bob, and ask it to use a tool to confirm\\n\",\n+    \"// that it's capable of working like an agent.\\n\",\n+    \"const inputMessage2 = {\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"hi! I'm bob. What is my age?\\\",\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"for await (const event of await app2.stream({ messages: [inputMessage2] }, config2)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\\n\",\n+    \"\\n\",\n+    \"// Confirm that the chat bot has access to previous conversation\\n\",\n+    \"// and can respond to the user saying that the user's name is Bob.\\n\",\n+    \"const followUpMessage2 = {\\n\",\n+    \"  role: \\\"user\\\",\\n\",\n+    \"  content: \\\"do you remember my name?\\\",\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"for await (const event of await app2.stream({ messages: [followUpMessage2] }, config2)) {\\n\",\n+    \"  const lastMessage = event.messages[event.messages.length - 1];\\n\",\n+    \"  console.log(lastMessage.content);\\n\",\n+    \"}\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"f4d16e09-1d90-4153-8576-6d3996cb5a6c\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \"</details>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"## LCEL: Add a preprocessing step\\n\",\n+    \"\\n\",\n+    \"The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.\\n\",\n+    \"\\n\",\n+    \"This approach is conceptually simple and will work in many situations; for example, if using a [RunnableWithMessageHistory](/docs/how_to/message_history/) instead of wrapping the chat model, wrap the chat model with the pre-processor.\\n\",\n+    \"\\n\",\n+    \"The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:\\n\",\n+    \"\\n\",\n+    \"1. As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).\\n\",\n+    \"2. The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \":::caution\\n\",\n+    \"\\n\",\n+    \"If you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<details open>\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 5,\n+   \"id\": \"a1c8adf2\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"AIMessage {\\n\",\n+      \"  \\\"id\\\": \\\"chatcmpl-AB6uzWscxviYlbADFeDlnwIH82Fzt\\\",\\n\",\n+      \"  \\\"content\\\": \\\"\\\",\\n\",\n+      \"  \\\"additional_kwargs\\\": {\\n\",\n+      \"    \\\"tool_calls\\\": [\\n\",\n+      \"      {\\n\",\n+      \"        \\\"id\\\": \\\"call_TghBL9dzqXFMCt0zj0VYMjfp\\\",\\n\",\n+      \"        \\\"type\\\": \\\"function\\\",\\n\",\n+      \"        \\\"function\\\": \\\"[Object]\\\"\\n\",\n+      \"      }\\n\",\n+      \"    ]\\n\",\n+      \"  },\\n\",\n+      \"  \\\"response_metadata\\\": {\\n\",\n+      \"    \\\"tokenUsage\\\": {\\n\",\n+      \"      \\\"completionTokens\\\": 16,\\n\",\n+      \"      \\\"promptTokens\\\": 95,\\n\",\n+      \"      \\\"totalTokens\\\": 111\\n\",\n+      \"    },\\n\",\n+      \"    \\\"finish_reason\\\": \\\"tool_calls\\\",\\n\",\n+      \"    \\\"system_fingerprint\\\": \\\"fp_a5d11b2ef2\\\"\\n\",\n+      \"  },\\n\",\n+      \"  \\\"tool_calls\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"name\\\": \\\"what_did_the_cow_say\\\",\\n\",\n+      \"      \\\"args\\\": {},\\n\",\n+      \"      \\\"type\\\": \\\"tool_call\\\",\\n\",\n+      \"      \\\"id\\\": \\\"call_TghBL9dzqXFMCt0zj0VYMjfp\\\"\\n\",\n+      \"    }\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+      \"  \\\"usage_metadata\\\": {\\n\",\n+      \"    \\\"input_tokens\\\": 95,\\n\",\n+      \"    \\\"output_tokens\\\": 16,\\n\",\n+      \"    \\\"total_tokens\\\": 111\\n\",\n+      \"  }\\n\",\n+      \"}\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"import { AIMessage, HumanMessage, SystemMessage, BaseMessage, trimMessages } from \\\"@langchain/core/messages\\\";\\n\",\n+    \"import { tool } from \\\"@langchain/core/tools\\\";\\n\",\n+    \"import { z } from \\\"zod\\\";\\n\",\n+    \"\\n\",\n+    \"const model3 = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\",\n+    \"\\n\",\n+    \"const whatDidTheCowSay = tool(\\n\",\n+    \"  (): string => {\\n\",\n+    \"    return \\\"foo\\\";\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    name: \\\"what_did_the_cow_say\\\",\\n\",\n+    \"    description: \\\"Check to see what the cow said.\\\",\\n\",\n+    \"    schema: z.object({}),\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"\\n\",\n+    \"// highlight-start\\n\",\n+    \"const messageProcessor = trimMessages(\\n\",\n+    \"  {\\n\",\n+    \"    tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens\\n\",\n+    \"    maxTokens: 5, // <-- allow up to 5 messages.\\n\",\n+    \"    strategy: \\\"last\\\",\\n\",\n+    \"    // The startOn is specified\\n\",\n+    \"    // to make sure we do not generate a sequence where\\n\",\n+    \"    // a ToolMessage that contains the result of a tool invocation\\n\",\n+    \"    // appears before the AIMessage that requested a tool invocation\\n\",\n+    \"    // as this will cause some chat models to raise an error.\\n\",\n+    \"    startOn: \\\"human\\\",\\n\",\n+    \"    includeSystem: true, // <-- Keep the system message\\n\",\n+    \"    allowPartial: false,\\n\",\n+    \"  }\\n\",\n+    \");\\n\",\n+    \"// highlight-end\\n\",\n+    \"\\n\",\n+    \"// Note that we bind tools to the model first!\\n\",\n+    \"const modelWithTools = model3.bindTools([whatDidTheCowSay]);\\n\",\n+    \"\\n\",\n+    \"// highlight-next-line\\n\",\n+    \"const modelWithPreprocessor = messageProcessor.pipe(modelWithTools);\\n\",\n+    \"\\n\",\n+    \"const fullHistory = [\\n\",\n+    \"  new SystemMessage(\\\"you're a good assistant, you always respond with a joke.\\\"),\\n\",\n+    \"  new HumanMessage(\\\"i wonder why it's called langchain\\\"),\\n\",\n+    \"  new AIMessage('Well, I guess they thought \\\"WordRope\\\" and \\\"SentenceString\\\" just didn\\\\'t have the same ring to it!'),\\n\",\n+    \"  new HumanMessage(\\\"and who is harrison chasing anyways\\\"),\\n\",\n+    \"  new AIMessage(\\\"Hmmm let me think.\\\\n\\\\nWhy, he's probably chasing after the last cup of coffee in the office!\\\"),\\n\",\n+    \"  new HumanMessage(\\\"why is 42 always the answer?\\\"),\\n\",\n+    \"  new AIMessage(\\\"Because it's the only number that's constantly right, even when it doesn't add up!\\\"),\\n\",\n+    \"  new HumanMessage(\\\"What did the cow say?\\\"),\\n\",\n+    \"];\\n\",\n+    \"\\n\",\n+    \"// We pass it explicitly to the modelWithPreprocessor for illustrative purposes.\\n\",\n+    \"// If you're using `RunnableWithMessageHistory` the history will be automatically\\n\",\n+    \"// read from the source that you configure.\\n\",\n+    \"const result = await modelWithPreprocessor.invoke(fullHistory);\\n\",\n+    \"console.log(result);\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"5da7225a-5e94-4f53-bb0d-86b6b528d150\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"```{=mdx}\\n\",\n+    \"</details>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"If you need to implement more efficient logic and want to use `RunnableWithMessageHistory` for now the way to achieve this\\n\",\n+    \"is to subclass from [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html) and\\n\",\n+    \"define appropriate logic for `addMessages` (that doesn't simply append the history, but instead re-writes it).\\n\",\n+    \"\\n\",\n+    \"Unless you have a good reason to implement this solution, you should instead use LangGraph.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"b2717810\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Next steps\\n\",\n+    \"\\n\",\n+    \"Explore persistence with LangGraph:\\n\",\n+    \"\\n\",\n+    \"* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)\\n\",\n+    \"* [How to add persistence (\\\"memory\\\") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)\\n\",\n+    \"* [How to manage conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/manage-conversation-history/)\\n\",\n+    \"* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)\\n\",\n+    \"\\n\",\n+    \"Add persistence with simple LCEL (favor LangGraph for more complex use cases):\\n\",\n+    \"\\n\",\n+    \"* [How to add message history](/docs/how_to/message_history/)\\n\",\n+    \"\\n\",\n+    \"Working with message history:\\n\",\n+    \"\\n\",\n+    \"* [How to trim messages](/docs/how_to/trim_messages)\\n\",\n+    \"* [How to filter messages](/docs/how_to/filter_messages/)\\n\",\n+    \"* [How to merge message runs](/docs/how_to/merge_message_runs/)\\n\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/versions/migrating_memory/conversation_summary_memory.ipynb": "@@ -0,0 +1,45 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"ce8457ed-c0b1-4a74-abbd-9d3d2211270f\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory\\n\",\n+    \"\\n\",\n+    \"Follow this guide if you're trying to migrate off one of the old memory classes listed below:\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"| Memory Type                          | Description                                                                                                                                          |\\n\",\n+    \"|---------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\\n\",\n+    \"| `ConversationSummaryMemory`           | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history. |\\n\",\n+    \"| `ConversationSummaryBufferMemory`     | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\\n\",\n+    \"\\n\",\n+    \"Please follow the following [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) in LangGraph. \\n\",\n+    \"\\n\",\n+    \"This guide shows how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3 (ipykernel)\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.11.4\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/versions/migrating_memory/index.mdx": "@@ -0,0 +1,139 @@\n+---\n+sidebar_position: 1\n+---\n+\n+# How to migrate to LangGraph memory\n+\n+As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate `memory` into their LangChain application.\n+\n+- Users that rely on `RunnableWithMessageHistory` or `BaseChatMessageHistory` do **not** need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.\n+- Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.\n+\n+## Why use LangGraph for memory?\n+\n+The main advantages of persistence in LangGraph are:\n+\n+- Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.\n+- Ability to save and resume complex conversations at any point. This helps with:\n+  - Error recovery\n+  - Allowing human intervention in AI workflows\n+  - Exploring different conversation paths (\"time travel\")\n+- Full compatibility with both traditional [language models](/docs/concepts/#llms) and modern [chat models](/docs/concepts/#chat-models). Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.\n+- Highly customizable, allowing you to fully control how memory works and use different storage backends.\n+\n+## Evolution of memory in LangChain\n+\n+The concept of memory has evolved significantly in LangChain since its initial release.\n+\n+### LangChain 0.0.x memory\n+\n+Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\n+\n+| Use Case                             | Example                                                                                                                           |\n+| ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- |\n+| Managing conversation history        | Keep only the last `n` turns of the conversation between the user and the AI.                                                     |\n+| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user.                     |\n+| Composite memory implementations     | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |\n+\n+While the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\n+\n+Most of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\n+\n+### RunnableWithMessageHistory and BaseChatMessageHistory\n+\n+:::note\n+Please see [How to use BaseChatMessageHistory with LangGraph](./chat_history), if you would like to use `BaseChatMessageHistory` (with or without `RunnableWithMessageHistory`) in LangGraph.\n+:::\n+\n+As of LangChain v0.1, we started recommending that users rely primarily on [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html). `BaseChatMessageHistory` serves\n+as a simple persistence for storing and retrieving messages in a conversation.\n+\n+At that time, the only option for orchestrating LangChain chains was via [LCEL](/docs/how_to/#langchain-expression-language-lcel). To incorporate memory with `LCEL`, users had to use the [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.\n+\n+As of LangChain v0.3, we recommend that **new** code takes advantage of LangGraph for both orchestration and persistence:\n+\n+- Orchestration: In LangGraph, users define [graphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/) that specify the flow of the application. This allows users to keep using `LCEL` within individual nodes when `LCEL` is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\n+- Persistence: Users can rely on LangGraph's persistence to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the `RunnableWithMessageHistory` interface.\n+\n+:::important\n+If you have been using `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n+:::\n+\n+## Migrations\n+\n+:::info Prerequisites\n+\n+These guides assume some familiarity with the following concepts:\n+\n+- [LangGraph](https://langchain-ai.github.io/langgraphjs/)\n+- [v0.0.x Memory](https://js.langchain.com/v0.1/docs/modules/memory/)\n+- [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)\n+  :::\n+\n+### 1. Managing conversation history\n+\n+The goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.\n+\n+Often this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\n+\n+Memory classes that fall into this category include:\n+\n+| Memory Type                       | How to Migrate                                               | Description                                                                                                                                                                                                         |\n+| --------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n+| `ConversationTokenBufferMemory`   | [Link to Migration Guide](conversation_buffer_window_memory) | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.                                                   |\n+| `ConversationSummaryMemory`       | [Link to Migration Guide](conversation_summary_memory)       | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.                                              |\n+| `ConversationSummaryBufferMemory` | [Link to Migration Guide](conversation_summary_memory)       | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n+\n+### 2. Extraction of structured information from the conversation history\n+\n+Memory classes that fall into this category include:\n+\n+| Memory Type       | Description                                                                                                                                                                                                    |\n+| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n+| `BaseEntityStore` | An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as an object of key-value pairs. |\n+\n+And specific backend implementations of abstractions:\n+\n+| Memory Type           | Description                                                                                              |\n+| --------------------- | -------------------------------------------------------------------------------------------------------- |\n+| `InMemoryEntityStore` | An implementation of `BaseEntityStore` that stores the information in the literal computer memory (RAM). |\n+\n+These abstractions have not received much development since their initial release. The reason\n+is that for these abstractions to be useful they typically require a lot of specialization for a particular application, so these\n+abstractions are not as widely used as the conversation history management abstractions.\n+\n+For this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application\n+that relies on these abstractions, please pen an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.\n+\n+The general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.\n+The extracted information can then be saved into an appropriate data structure (e.g., an object), and information from it can be retrieved and added into the prompt as needed.\n+\n+### 3. Implementations that provide composite logic on top of one or more memory implementations\n+\n+Memory classes that fall into this category include:\n+\n+| Memory Type      | Description                                                                                                                    |\n+| ---------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n+| `CombinedMemory` | This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input. |\n+\n+These implementations did not seem to be used widely or provide significant value. Users should be able\n+to re-implement these without too much difficulty in custom code.\n+\n+## Related Resources\n+\n+Explore persistence with LangGraph:\n+\n+- [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)\n+- [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)\n+- [How to manage conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/manage-conversation-history/)\n+- [How to add summary of the conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)\n+\n+Add persistence with simple LCEL (favor langgraph for more complex use cases):\n+\n+- [How to add message history](/docs/how_to/message_history/)\n+\n+Working with message history:\n+\n+- [How to trim messages](/docs/how_to/trim_messages)\n+- [How to filter messages](/docs/how_to/filter_messages/)\n+- [How to merge message runs](/docs/how_to/merge_message_runs/)",
          "docs/core_docs/docs/versions/overview.mdx": "@@ -1,44 +0,0 @@\n----\n-sidebar_position: 0\n-sidebar_label: Overview\n----\n-\n-# LangChain Over Time\n-\n-Due to the rapidly evolving field, LangChain has also evolved rapidly.\n-This document serves to outline at a high level what has changed and why.\n-\n-## 0.1\n-\n-The 0.1 release marked a few key changes for LangChain.\n-By this point, the LangChain ecosystem had become large both in the breadth of what it enabled as well as the community behind it.\n-\n-**Split of packages**\n-\n-LangChain was split up into several packages to increase modularity and decrease bloat.\n-First, `@langchain/core` is created as a lightweight core library containing the base abstractions,\n-some core implementations of those abstractions, and the generic runtime for creating chains.\n-Next, all third party integrations are split into `@langchain/community` or their own individual partner packages.\n-Higher level chains and agents remain in `langchain`.\n-\n-**`Runnables`**\n-\n-Having a specific class for each chain was proving not very scalable or flexible.\n-Although these classes were left alone (without deprecation warnings) for this release,\n-in the documentation much more space was given to generic runnables.\n-\n-## < 0.1\n-\n-There are several key characteristics of LangChain pre-0.1.\n-\n-**Singular Package**\n-\n-LangChain was largely a singular package.\n-This meant that ALL integrations lived inside `langchain`.\n-\n-**Chains as classes**\n-\n-Most high level chains were largely their own classes.\n-There was a base `Chain` class from which all chains inherited.\n-This meant that in order to chain the logic inside a chain you basically had to modify the source code.\n-There were a few chains that were meant to be more generic (`SequentialChain`, `RouterChain`)",
          "docs/core_docs/docs/versions/packages.mdx": "@@ -1,54 +0,0 @@\n----\n-sidebar_position: 3\n-sidebar_label: Packages\n----\n-\n-# 📕 Package versioning\n-\n-As of now, LangChain has an ad hoc release process: releases are cut with high frequency by\n-a maintainer and published to [NPM](https://npm.org/).\n-The different packages are versioned slightly differently.\n-\n-## `@langchain/core`\n-\n-`@langchain/core` is currently on version `0.1.x`.\n-\n-As `@langchain/core` contains the base abstractions and runtime for the whole LangChain ecosystem, we will communicate any breaking changes with advance notice and version bumps.\n-The exception for this is anything marked as `beta` (you can see this in the API reference and will see warnings when using such functionality).\n-The reason for beta features is that given the rate of change of the field, being able to move quickly is still a priority.\n-\n-Minor version increases will occur for:\n-\n-- Breaking changes for any public interfaces marked as `beta`.\n-\n-Patch version increases will occur for:\n-\n-- Bug fixes\n-- New features\n-- Any changes to private interfaces\n-- Any changes to `beta` features\n-\n-## `langchain`\n-\n-`langchain` is currently on version `0.2.x`\n-\n-Minor version increases will occur for:\n-\n-- Breaking changes for any public interfaces NOT marked as `beta`.\n-\n-Patch version increases will occur for:\n-\n-- Bug fixes\n-- New features\n-- Any changes to private interfaces\n-- Any changes to `beta` features.\n-\n-## `@langchain/community`\n-\n-`@langchain/community` is currently on version `0.2.x`\n-\n-All changes will be accompanied by the same type of version increase as changes in `langchain`.\n-\n-## Partner Packages\n-\n-Partner packages are versioned independently.",
          "docs/core_docs/docs/versions/v0_2/migrating_astream_events.mdx": "@@ -1,6 +1,6 @@\n ---\n sidebar_position: 2\n-sidebar_label: streamEvents v2\n+sidebar_label: Migrating to streamEvents v2\n ---\n \n # Migrating to streamEvents v2",
          "docs/core_docs/docs/versions/v0_3/index.mdx": "@@ -0,0 +1,141 @@\n+---\n+sidebar_position: 0\n+sidebar_label: v0.3\n+---\n+\n+# LangChain v0.3\n+\n+_Last updated: 09.14.24_\n+\n+## What's changed\n+\n+- All LangChain packages now have `@langchain/core` as a peer dependency instead of a direct dependency to help avoid type errors around [core version conflicts](/docs/how_to/installation/#installing-integration-packages).\n+  - You will now need to explicitly install `@langchain/core` rather than relying on an internally resolved version from other packages.\n+- Callbacks are now backgrounded and non-blocking by default rather than blocking.\n+  - This means that if you are using e.g. LangSmith for tracing in a serverless environment, you will need to [await the callbacks to ensure they finish before your function ends](/docs/how_to/callbacks_serverless).\n+- Removed deprecated document loader and self-query entrypoints from `langchain` in favor of entrypoints in [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) and integration packages.\n+- Removed deprecated Google PaLM entrypoints from community in favor of entrypoints in [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai) and [`@langchain/google-genai`](https://www.npmjs.com/package/@langchain/google-genai).\n+- Deprecated using objects with a `\"type\"` as a [`BaseMessageLike`](https://v03.api.js.langchain.com/types/_langchain_core.messages.BaseMessageLike.html) in favor of the more OpenAI-like [`MessageWithRole`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageFieldWithRole.html)\n+\n+## What’s new\n+\n+The following features have been added during the development of 0.2.x:\n+\n+- Simplified tool definition and usage. Read more [here](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/).\n+- Added a [generalized chat model constructor](https://js.langchain.com/docs/how_to/chat_models_universal_init/).\n+- Added the ability to [dispatch custom events](https://js.langchain.com/docs/how_to/callbacks_custom_events/).\n+- Released LangGraph.js 0.2.0 and made it the [recommended way to create agents](https://js.langchain.com/docs/how_to/migrate_agent) with LangChain.js.\n+- Revamped integration docs and API reference. Read more [here](https://blog.langchain.dev/langchain-integration-docs-revamped/).\n+\n+## How to update your code\n+\n+If you're using `langchain` / `@langchain/community` / `@langchain/core` 0.0 or 0.1, we recommend that you first [upgrade to 0.2](https://js.langchain.com/v0.2/docs/versions/v0_2/).\n+\n+If you're using `@langchain/langgraph`, upgrade to `@langchain/langgraph>=0.2.3`. This will work with either 0.2 or 0.3 versions of all the base packages.\n+\n+Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to in your `package.json`.\n+Any package that now supports `@langchain/core` 0.3 had a minor version bump.\n+\n+### Base packages\n+\n+| Package                  | Latest | Recommended `package.json` constraint |\n+| ------------------------ | ------ | ------------------------------------- |\n+| langchain                | 0.3.0  | >=0.3.0 <0.4.0                        |\n+| @langchain/community     | 0.3.0  | >=0.3.0 <0.4.0                        |\n+| @langchain/textsplitters | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/core          | 0.3.0  | >=0.3.0 <0.4.0                        |\n+\n+### Downstream packages\n+\n+| Package              | Latest | Recommended `package.json` constraint |\n+| -------------------- | ------ | ------------------------------------- |\n+| @langchain/langgraph | 0.2.3  | >=0.2.3 <0.3                          |\n+\n+### Integration packages\n+\n+| Package                           | Latest | Recommended `package.json` constraint |\n+| --------------------------------- | ------ | ------------------------------------- |\n+| @langchain/anthropic              | 0.3.0  | >=0.3.0 <0.4.0                        |\n+| @langchain/aws                    | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/azure-cosmosdb         | 0.2.0  | >=0.2.0 <0.3.0                        |\n+| @langchain/azure-dynamic-sessions | 0.2.0  | >=0.2.0 <0.3.0                        |\n+| @langchain/baidu-qianfan          | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/cloudflare             | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/cohere                 | 0.3.0  | >=0.3.0 <0.4.0                        |\n+| @langchain/exa                    | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/google-genai           | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/google-vertexai        | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/google-vertexai-web    | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/groq                   | 0.1.1  | >=0.1.1 <0.2.0                        |\n+| @langchain/mistralai              | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/mixedbread-ai          | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/mongodb                | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/nomic                  | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/ollama                 | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/openai                 | 0.3.0  | >=0.3.0 <0.4.0                        |\n+| @langchain/pinecone               | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/qdrant                 | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/redis                  | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/weaviate               | 0.1.0  | >=0.1.0 <0.2.0                        |\n+| @langchain/yandex                 | 0.1.0  | >=0.1.0 <0.2.0                        |\n+\n+Once you've updated to recent versions of the packages, you will need to explicitly install `@langchain/core` if you haven't already:\n+\n+```bash npm2yarn\n+npm install @langchain/core\n+```\n+\n+We also suggest checking your lockfile or running the [appropriate package manager command](/docs/how_to/installation/#installing-integration-packages) to make sure that your package manager only has one version of `@langchain/core` installed.\n+\n+If you are currently running your code in a serverless environment (e.g., a Cloudflare Worker, Edge function, or AWS Lambda function)\n+and you are using LangSmith tracing or other callbacks, you will need to [await callbacks to ensure they finish before your function ends](/docs/how_to/callbacks_serverless).\n+Here's a quick example:\n+\n+```ts\n+import { RunnableLambda } from \"@langchain/core/runnables\";\n+import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";\n+\n+const runnable = RunnableLambda.from(() => \"hello!\");\n+\n+const customHandler = {\n+  handleChainEnd: async () => {\n+    await new Promise((resolve) => setTimeout(resolve, 2000));\n+    console.log(\"Call finished\");\n+  },\n+};\n+\n+const startTime = new Date().getTime();\n+\n+await runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n+\n+console.log(`Elapsed time: ${new Date().getTime() - startTime}ms`);\n+\n+await awaitAllCallbacks();\n+\n+console.log(`Final elapsed time: ${new Date().getTime() - startTime}ms`);\n+```\n+\n+```\n+Elapsed time: 1ms\n+Call finished\n+Final elapsed time: 2164ms\n+```\n+\n+You can also set `LANGCHAIN_CALLBACKS_BACKGROUND` to `\"false\"` to make all callbacks blocking:\n+\n+```ts\n+process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"false\";\n+\n+const startTimeBlocking = new Date().getTime();\n+\n+await runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n+\n+console.log(\n+  `Initial elapsed time: ${new Date().getTime() - startTimeBlocking}ms`\n+);\n+```\n+\n+```\n+Call finished\n+Initial elapsed time: 2002ms\n+```",
          "docs/core_docs/docusaurus.config.js": "@@ -135,13 +135,6 @@ const config = {\n   themeConfig:\n     /** @type {import('@docusaurus/preset-classic').ThemeConfig} */\n     ({\n-      announcementBar: {\n-        content:\n-          'Share your thoughts on AI agents. <a target=\"_blank\" href=\"https://langchain.typeform.com/state-of-agents\">Take the 3-min survey</a>.',\n-        isCloseable: true,\n-        backgroundColor: \"rgba(53, 151, 147, 0.1)\",\n-        textColor: \"rgb(53, 151, 147)\",\n-      },\n       prism: {\n         theme: {\n           ...baseLightCodeBlockTheme,\n@@ -172,7 +165,7 @@ const config = {\n             label: \"Integrations\",\n           },\n           {\n-            href: \"https://v02.api.js.langchain.com\",\n+            href: \"https://v03.api.js.langchain.com\",\n             label: \"API Reference\",\n             position: \"left\",\n           },\n@@ -189,9 +182,13 @@ const config = {\n                 to: \"/docs/community\",\n                 label: \"Community\",\n               },\n+              {\n+                to: \"/docs/troubleshooting/errors\",\n+                label: \"Error reference\",\n+              },\n               {\n                 to: \"/docs/additional_resources/tutorials\",\n-                label: \"Tutorials\",\n+                label: \"External guides\",\n               },\n               {\n                 to: \"/docs/contributing\",\n@@ -308,7 +305,7 @@ const config = {\n         // this is linked to erick@langchain.dev currently\n         apiKey: \"180851bbb9ba0ef6be9214849d6efeaf\",\n \n-        indexName: \"js-langchain-0.2\",\n+        indexName: \"js-langchain-latest\",\n \n         contextualSearch: false,\n       },",
          "docs/core_docs/package.json": "@@ -50,7 +50,7 @@\n   },\n   \"devDependencies\": {\n     \"@babel/eslint-parser\": \"^7.18.2\",\n-    \"@langchain/langgraph\": \"0.0.28\",\n+    \"@langchain/langgraph\": \"0.2.3\",\n     \"@langchain/scripts\": \"workspace:*\",\n     \"@microsoft/fetch-event-source\": \"^2.0.1\",\n     \"@swc/core\": \"^1.3.62\",",
          "docs/core_docs/sidebars.js": "@@ -73,9 +73,35 @@ module.exports = {\n       collapsible: false,\n       items: [\n         {\n-          type: \"autogenerated\",\n-          dirName: \"versions\",\n+          type: \"doc\",\n+          id: \"versions/v0_3/index\",\n+          label: \"v0.3\",\n+        },\n+        {\n+          type: \"category\",\n+          label: \"v0.2\",\n+          items: [\n+            {\n+              type: \"autogenerated\",\n+              dirName: \"versions/v0_2\",\n+            },\n+          ],\n+        },\n+        {\n+          type: \"category\",\n+          label: \"Migrating from v0.0 memory\",\n+          link: { type: \"doc\", id: \"versions/migrating_memory/index\" },\n+          collapsible: false,\n+          collapsed: false,\n+          items: [\n+            {\n+              type: \"autogenerated\",\n+              dirName: \"versions/migrating_memory\",\n+              className: \"hidden\",\n+            },\n+          ],\n         },\n+        \"versions/release_policy\",\n       ],\n     },\n     \"security\",",
          "docs/core_docs/src/theme/NotFound.js": "@@ -0,0 +1,1041 @@\n+/* eslint-disable import/no-extraneous-dependencies */\n+/* eslint-disable no-nested-ternary */\n+import React from \"react\";\n+import { translate } from \"@docusaurus/Translate\";\n+import { PageMetadata } from \"@docusaurus/theme-common\";\n+import Layout from \"@theme/Layout\";\n+\n+import { useLocation } from \"react-router-dom\";\n+\n+function LegacyBadge() {\n+  return <span className=\"badge badge--secondary\">LEGACY</span>;\n+}\n+\n+const suggestedLinks = {\n+  \"/docs/additional_resources/tutorials/expression_language_cheatsheet/\": {\n+    canonical: \"/docs/how_to/lcel_cheatsheet/\",\n+    alternative: [\n+      \"/v0.1/docs/additional_resources/tutorials/expression_language_cheatsheet/\",\n+    ],\n+  },\n+  \"/docs/ecosystem/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/\"],\n+  },\n+  \"/docs/ecosystem/integrations/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/\"],\n+  },\n+  \"/docs/ecosystem/integrations/databerry/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/databerry/\"],\n+  },\n+  \"/docs/ecosystem/integrations/helicone/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/helicone/\"],\n+  },\n+  \"/docs/ecosystem/integrations/lunary/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/lunary/\"],\n+  },\n+  \"/docs/ecosystem/integrations/makersuite/\": {\n+    canonical: \"/docs/integrations/platforms/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/makersuite/\"],\n+  },\n+  \"/docs/ecosystem/integrations/unstructured/\": {\n+    canonical: \"/docs/integrations/document_loaders/file_loaders/unstructured/\",\n+    alternative: [\"/v0.1/docs/ecosystem/integrations/unstructured/\"],\n+  },\n+  \"/docs/ecosystem/langserve/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/_langchain_core.runnables_remote.RemoteRunnable.html\",\n+    alternative: [\"/v0.1/docs/ecosystem/langserve/\"],\n+  },\n+  \"/docs/expression_language/\": {\n+    canonical: \"/docs/how_to/#langchain-expression-language-lcel\",\n+    alternative: [\"/v0.1/docs/expression_language/\"],\n+  },\n+  \"/docs/expression_language/cookbook/\": {\n+    canonical: \"/docs/how_to/#langchain-expression-language-lcel\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/\"],\n+  },\n+  \"/docs/expression_language/cookbook/adding_memory/\": {\n+    canonical: \"/docs/how_to/message_history\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/adding_memory/\"],\n+  },\n+  \"/docs/expression_language/cookbook/agents/\": {\n+    canonical: \"/docs/how_to/agent_executor\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/agents/\"],\n+  },\n+  \"/docs/expression_language/cookbook/multiple_chains/\": {\n+    canonical: \"/docs/how_to/parallel\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/multiple_chains/\"],\n+  },\n+  \"/docs/expression_language/cookbook/prompt_llm_parser/\": {\n+    canonical: \"/docs/tutorials/llm_chain\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/prompt_llm_parser/\"],\n+  },\n+  \"/docs/expression_language/cookbook/retrieval/\": {\n+    canonical: \"/docs/tutorials/rag\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/retrieval/\"],\n+  },\n+  \"/docs/expression_language/cookbook/sql_db/\": {\n+    canonical: \"/docs/tutorials/sql_qa\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/sql_db/\"],\n+  },\n+  \"/docs/expression_language/cookbook/tools/\": {\n+    canonical: \"/docs/how_to/tool_calling/\",\n+    alternative: [\"/v0.1/docs/expression_language/cookbook/tools/\"],\n+  },\n+  \"/docs/expression_language/get_started/\": {\n+    canonical: \"/docs/how_to/sequence\",\n+    alternative: [\"/v0.1/docs/expression_language/get_started/\"],\n+  },\n+  \"/docs/expression_language/how_to/map/\": {\n+    canonical: \"/docs/how_to/cancel_execution/\",\n+    alternative: [\"/v0.1/docs/expression_language/how_to/map/\"],\n+  },\n+  \"/docs/expression_language/how_to/message_history/\": {\n+    canonical: \"/docs/how_to/message_history\",\n+    alternative: [\"/v0.1/docs/expression_language/how_to/message_history/\"],\n+  },\n+  \"/docs/expression_language/how_to/routing/\": {\n+    canonical: \"/docs/how_to/routing\",\n+    alternative: [\"/v0.1/docs/expression_language/how_to/routing/\"],\n+  },\n+  \"/docs/expression_language/how_to/with_history/\": {\n+    canonical: \"/docs/how_to/message_history\",\n+    alternative: [\"/v0.1/docs/expression_language/how_to/with_history/\"],\n+  },\n+  \"/docs/expression_language/interface/\": {\n+    canonical: \"/docs/how_to/lcel_cheatsheet\",\n+    alternative: [\"/v0.1/docs/expression_language/interface/\"],\n+  },\n+  \"/docs/expression_language/streaming/\": {\n+    canonical: \"/docs/how_to/streaming\",\n+    alternative: [\"/v0.1/docs/expression_language/streaming/\"],\n+  },\n+  \"/docs/expression_language/why/\": {\n+    canonical: \"/docs/concepts/#langchain-expression-language\",\n+    alternative: [\"/v0.1/docs/expression_language/why/\"],\n+  },\n+  \"/docs/get_started/\": {\n+    canonical: \"/docs/introduction/\",\n+    alternative: [\"/v0.1/docs/get_started/\"],\n+  },\n+  \"/docs/get_started/installation/\": {\n+    canonical: \"/docs/tutorials/\",\n+    alternative: [\"/v0.1/docs/get_started/installation/\"],\n+  },\n+  \"/docs/get_started/introduction/\": {\n+    canonical: \"/docs/tutorials/\",\n+    alternative: [\"/v0.1/docs/get_started/introduction/\"],\n+  },\n+  \"/docs/get_started/quickstart/\": {\n+    canonical: \"/docs/tutorials/\",\n+    alternative: [\"/v0.1/docs/get_started/quickstart/\"],\n+  },\n+  \"/docs/guides/\": {\n+    canonical: \"/docs/how_to/\",\n+    alternative: [\"/v0.1/docs/guides/\"],\n+  },\n+  \"/docs/guides/debugging/\": {\n+    canonical: \"/docs/how_to/debugging\",\n+    alternative: [\"/v0.1/docs/guides/debugging/\"],\n+  },\n+  \"/docs/guides/deployment/\": {\n+    canonical: \"https://langchain-ai.github.io/langgraph/cloud/\",\n+    alternative: [\"/v0.1/docs/guides/deployment/\"],\n+  },\n+  \"/docs/guides/deployment/nextjs/\": {\n+    canonical: \"https://github.com/langchain-ai/langchain-nextjs-template\",\n+    alternative: [\"/v0.1/docs/guides/deployment/nextjs/\"],\n+  },\n+  \"/docs/guides/deployment/sveltekit/\": {\n+    canonical: \"https://github.com/langchain-ai/langchain-nextjs-template\",\n+    alternative: [\"/v0.1/docs/guides/deployment/sveltekit/\"],\n+  },\n+  \"/docs/guides/evaluation/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/\"],\n+  },\n+  \"/docs/guides/evaluation/comparison/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/comparison/\"],\n+  },\n+  \"/docs/guides/evaluation/comparison/pairwise_embedding_distance/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\n+      \"/v0.1/docs/guides/evaluation/comparison/pairwise_embedding_distance/\",\n+    ],\n+  },\n+  \"/docs/guides/evaluation/comparison/pairwise_string/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/comparison/pairwise_string/\"],\n+  },\n+  \"/docs/guides/evaluation/examples/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/examples/\"],\n+  },\n+  \"/docs/guides/evaluation/examples/comparisons/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/examples/comparisons/\"],\n+  },\n+  \"/docs/guides/evaluation/string/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/string/\"],\n+  },\n+  \"/docs/guides/evaluation/string/criteria/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/string/criteria/\"],\n+  },\n+  \"/docs/guides/evaluation/string/embedding_distance/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/string/embedding_distance/\"],\n+  },\n+  \"/docs/guides/evaluation/trajectory/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/trajectory/\"],\n+  },\n+  \"/docs/guides/evaluation/trajectory/trajectory_eval/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/evaluation/trajectory/trajectory_eval/\"],\n+  },\n+  \"/docs/guides/extending_langchain/\": {\n+    canonical: \"/docs/how_to/#custom\",\n+    alternative: [\"/v0.1/docs/guides/extending_langchain/\"],\n+  },\n+  \"/docs/guides/fallbacks/\": {\n+    canonical: \"/docs/how_to/fallbacks\",\n+    alternative: [\"/v0.1/docs/guides/fallbacks/\"],\n+  },\n+  \"/docs/guides/langsmith_evaluation/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/tutorials/Developers/evaluation\",\n+    alternative: [\"/v0.1/docs/guides/langsmith_evaluation/\"],\n+  },\n+  \"/docs/guides/migrating/\": {\n+    canonical: \"https://js.langchain.com/v0.1/docs/guides/migrating/\",\n+    alternative: [\"/v0.1/docs/guides/migrating/\"],\n+  },\n+  \"/docs/integrations/chat_memory/\": {\n+    canonical: \"/docs/integrations/memory\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/\"],\n+  },\n+  \"/docs/integrations/chat_memory/astradb/\": {\n+    canonical: \"/docs/integrations/memory/astradb\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/astradb/\"],\n+  },\n+  \"/docs/integrations/chat_memory/cassandra/\": {\n+    canonical: \"/docs/integrations/memory/cassandra\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/cassandra/\"],\n+  },\n+  \"/docs/integrations/chat_memory/cloudflare_d1/\": {\n+    canonical: \"/docs/integrations/memory/cloudflare_d1\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/cloudflare_d1/\"],\n+  },\n+  \"/docs/integrations/chat_memory/convex/\": {\n+    canonical: \"/docs/integrations/memory/convex\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/convex/\"],\n+  },\n+  \"/docs/integrations/chat_memory/dynamodb/\": {\n+    canonical: \"/docs/integrations/memory/dynamodb\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/dynamodb/\"],\n+  },\n+  \"/docs/integrations/chat_memory/firestore/\": {\n+    canonical: \"/docs/integrations/memory/firestore\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/firestore/\"],\n+  },\n+  \"/docs/integrations/chat_memory/ipfs_datastore/\": {\n+    canonical: \"/docs/integrations/memory/ipfs_datastore\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/ipfs_datastore/\"],\n+  },\n+  \"/docs/integrations/chat_memory/momento/\": {\n+    canonical: \"/docs/integrations/memory/momento\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/momento/\"],\n+  },\n+  \"/docs/integrations/chat_memory/mongodb/\": {\n+    canonical: \"/docs/integrations/memory/mongodb\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/mongodb/\"],\n+  },\n+  \"/docs/integrations/chat_memory/motorhead_memory/\": {\n+    canonical: \"/docs/integrations/memory/motorhead_memory\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/motorhead_memory/\"],\n+  },\n+  \"/docs/integrations/chat_memory/planetscale/\": {\n+    canonical: \"/docs/integrations/memory/planetscale\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/planetscale/\"],\n+  },\n+  \"/docs/integrations/chat_memory/postgres/\": {\n+    canonical: \"/docs/integrations/memory/postgres\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/postgres/\"],\n+  },\n+  \"/docs/integrations/chat_memory/redis/\": {\n+    canonical: \"/docs/integrations/memory/redis\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/redis/\"],\n+  },\n+  \"/docs/integrations/chat_memory/upstash_redis/\": {\n+    canonical: \"/docs/integrations/memory/upstash_redis\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/upstash_redis/\"],\n+  },\n+  \"/docs/integrations/chat_memory/xata/\": {\n+    canonical: \"/docs/integrations/memory/xata\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/xata/\"],\n+  },\n+  \"/docs/integrations/chat_memory/zep_memory/\": {\n+    canonical: \"/docs/integrations/memory/zep_memory\",\n+    alternative: [\"/v0.1/docs/integrations/chat_memory/zep_memory/\"],\n+  },\n+  \"/docs/integrations/document_compressors/\": {\n+    canonical: \"/docs/integrations/document_transformers\",\n+    alternative: [\"/v0.1/docs/integrations/document_compressors/\"],\n+  },\n+  \"/docs/integrations/llms/togetherai/\": {\n+    canonical: \"/docs/integrations/llms/together\",\n+    alternative: [\"/v0.1/docs/integrations/llms/togetherai/\"],\n+  },\n+  \"/docs/integrations/retrievers/vectorstore/\": {\n+    canonical: \"/docs/how_to/vectorstore_retriever\",\n+    alternative: [\"/v0.1/docs/integrations/retrievers/vectorstore/\"],\n+  },\n+  \"/docs/integrations/vectorstores/azure_cosmosdb/\": {\n+    canonical: \"/docs/integrations/vectorstores/azure_cosmosdb_mongodb\",\n+    alternative: [\"/v0.1/docs/integrations/vectorstores/azure_cosmosdb/\"],\n+  },\n+  \"/docs/langgraph/\": {\n+    canonical: \"https://langchain-ai.github.io/langgraphjs/\",\n+    alternative: [\"/v0.1/docs/langgraph/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/chat_conversation_agent/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\n+      \"/v0.1/docs/modules/agents/agent_types/chat_conversation_agent/\",\n+    ],\n+  },\n+  \"/docs/modules/agents/agent_types/openai_assistant/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/openai_assistant/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/openai_functions_agent/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\n+      \"/v0.1/docs/modules/agents/agent_types/openai_functions_agent/\",\n+    ],\n+  },\n+  \"/docs/modules/agents/agent_types/openai_tools_agent/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/openai_tools_agent/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/plan_and_execute/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/plan_and_execute/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/react/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/react/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/structured_chat/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/structured_chat/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/tool_calling/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/tool_calling/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/xml_legacy/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/xml_legacy/\"],\n+  },\n+  \"/docs/modules/agents/agent_types/xml/\": {\n+    canonical: \"/docs/how_to/migrate_agent\",\n+    alternative: [\"/v0.1/docs/modules/agents/agent_types/xml/\"],\n+  },\n+  \"/docs/modules/agents/how_to/callbacks/\": {\n+    canonical: \"/docs/how_to/#callbacks\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/callbacks/\"],\n+  },\n+  \"/docs/modules/agents/how_to/cancelling_requests/\": {\n+    canonical: \"/docs/how_to/cancel_execution\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/cancelling_requests/\"],\n+  },\n+  \"/docs/modules/agents/how_to/custom_agent/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/custom_agent/\"],\n+  },\n+  \"/docs/modules/agents/how_to/custom_llm_agent/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/custom_llm_agent/\"],\n+  },\n+  \"/docs/modules/agents/how_to/custom_llm_chat_agent/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/custom_llm_chat_agent/\"],\n+  },\n+  \"/docs/modules/agents/how_to/custom_mrkl_agent/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/custom_mrkl_agent/\"],\n+  },\n+  \"/docs/modules/agents/how_to/handle_parsing_errors/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling-errors/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/handle_parsing_errors/\"],\n+  },\n+  \"/docs/modules/agents/how_to/intermediate_steps/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/how-tos/stream-values/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/intermediate_steps/\"],\n+  },\n+  \"/docs/modules/agents/how_to/logging_and_tracing/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/logging_and_tracing/\"],\n+  },\n+  \"/docs/modules/agents/how_to/timeouts/\": {\n+    canonical: \"/docs/how_to/cancel_execution/\",\n+    alternative: [\"/v0.1/docs/modules/agents/how_to/timeouts/\"],\n+  },\n+  \"/docs/modules/agents/tools/\": {\n+    canonical:\n+      \"https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling/\",\n+    alternative: [\"/v0.1/docs/modules/agents/tools/\"],\n+  },\n+  \"/docs/modules/agents/tools/dynamic/\": {\n+    canonical: \"/docs/how_to/custom_tools/\",\n+    alternative: [\"/v0.1/docs/modules/agents/tools/dynamic/\"],\n+  },\n+  \"/docs/modules/agents/tools/how_to/agents_with_vectorstores/\": {\n+    canonical: \"/docs/how_to/custom_tools\",\n+    alternative: [\n+      \"/v0.1/docs/modules/agents/tools/how_to/agents_with_vectorstores/\",\n+    ],\n+  },\n+  \"/docs/modules/agents/tools/toolkits/\": {\n+    canonical: \"/docs/how_to/tools_builtin\",\n+    alternative: [\"/v0.1/docs/modules/agents/tools/toolkits/\"],\n+  },\n+  \"/docs/modules/callbacks/how_to/background_callbacks/\": {\n+    canonical: \"/docs/how_to/callbacks_backgrounding\",\n+    alternative: [\"/v0.1/docs/modules/callbacks/how_to/background_callbacks/\"],\n+  },\n+  \"/docs/modules/callbacks/how_to/create_handlers/\": {\n+    canonical: \"/docs/how_to/custom_callbacks\",\n+    alternative: [\"/v0.1/docs/modules/callbacks/how_to/create_handlers/\"],\n+  },\n+  \"/docs/modules/callbacks/how_to/creating_subclasses/\": {\n+    canonical: \"/docs/how_to/custom_callbacks\",\n+    alternative: [\"/v0.1/docs/modules/callbacks/how_to/creating_subclasses/\"],\n+  },\n+  \"/docs/modules/callbacks/how_to/tags/\": {\n+    canonical: \"/docs/how_to/#callbacks\",\n+    alternative: [\"/v0.1/docs/modules/callbacks/how_to/tags/\"],\n+  },\n+  \"/docs/modules/callbacks/how_to/with_listeners/\": {\n+    canonical: \"/docs/how_to/#callbacks\",\n+    alternative: [\"/v0.1/docs/modules/callbacks/how_to/with_listeners/\"],\n+  },\n+  \"/docs/modules/chains/additional/analyze_document/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/additional/analyze_document/\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/analyze_document/\"],\n+  },\n+  \"/docs/modules/chains/additional/constitutional_chain/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/additional/constitutional_chain/\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/constitutional_chain/\"],\n+  },\n+  \"/docs/modules/chains/additional/cypher_chain/\": {\n+    canonical: \"/docs/tutorials/graph\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/cypher_chain/\"],\n+  },\n+  \"/docs/modules/chains/additional/moderation/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/additional/moderation/\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/moderation/\"],\n+  },\n+  \"/docs/modules/chains/additional/multi_prompt_router/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/additional/multi_prompt_router/\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/multi_prompt_router/\"],\n+  },\n+  \"/docs/modules/chains/additional/multi_retrieval_qa_router/\": {\n+    canonical: \"/docs/how_to/multiple_queries\",\n+    alternative: [\n+      \"/v0.1/docs/modules/chains/additional/multi_retrieval_qa_router/\",\n+    ],\n+  },\n+  \"/docs/modules/chains/additional/openai_functions/\": {\n+    canonical: \"/docs/how_to/tool_calling\",\n+    alternative: [\"/v0.1/docs/modules/chains/additional/openai_functions/\"],\n+  },\n+  \"/docs/modules/chains/additional/openai_functions/extraction/\": {\n+    canonical: \"/docs/tutorials/extraction\",\n+    alternative: [\n+      \"/v0.1/docs/modules/chains/additional/openai_functions/extraction/\",\n+    ],\n+  },\n+  \"/docs/modules/chains/additional/openai_functions/openapi/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/additional/openai_functions/openapi/\",\n+    alternative: [\n+      \"/v0.1/docs/modules/chains/additional/openai_functions/openapi/\",\n+    ],\n+  },\n+  \"/docs/modules/chains/additional/openai_functions/tagging/\": {\n+    canonical: \"/docs/tutorials/extraction\",\n+    alternative: [\n+      \"/v0.1/docs/modules/chains/additional/openai_functions/tagging/\",\n+    ],\n+  },\n+  \"/docs/modules/chains/document/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html\",\n+    alternative: [\"/v0.1/docs/modules/chains/document/\"],\n+  },\n+  \"/docs/modules/chains/document/map_reduce/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/\",\n+    alternative: [\"/v0.1/docs/modules/chains/document/map_reduce/\"],\n+  },\n+  \"/docs/modules/chains/document/refine/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/document/refine/\",\n+    alternative: [\"/v0.1/docs/modules/chains/document/refine/\"],\n+  },\n+  \"/docs/modules/chains/document/stuff/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/\",\n+    alternative: [\"/v0.1/docs/modules/chains/document/stuff/\"],\n+  },\n+  \"/docs/modules/chains/foundational/llm_chain/\": {\n+    canonical: \"/docs/tutorials/llm_chain\",\n+    alternative: [\"/v0.1/docs/modules/chains/foundational/llm_chain/\"],\n+  },\n+  \"/docs/modules/chains/how_to/debugging/\": {\n+    canonical: \"/docs/how_to/debugging\",\n+    alternative: [\"/v0.1/docs/modules/chains/how_to/debugging/\"],\n+  },\n+  \"/docs/modules/chains/how_to/memory/\": {\n+    canonical: \"/docs/how_to/qa_chat_history_how_to\",\n+    alternative: [\"/v0.1/docs/modules/chains/how_to/memory/\"],\n+  },\n+  \"/docs/modules/chains/popular/api/\": {\n+    canonical: \"https://js.langchain.com/v0.1/docs/modules/chains/popular/api/\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/api/\"],\n+  },\n+  \"/docs/modules/chains/popular/chat_vector_db_legacy/\": {\n+    canonical: \"/docs/tutorials/rag\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/chat_vector_db_legacy/\"],\n+  },\n+  \"/docs/modules/chains/popular/chat_vector_db/\": {\n+    canonical: \"/docs/tutorials/rag\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/chat_vector_db/\"],\n+  },\n+  \"/docs/modules/chains/popular/sqlite_legacy/\": {\n+    canonical: \"/docs/tutorials/sql_qa\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/sqlite_legacy/\"],\n+  },\n+  \"/docs/modules/chains/popular/sqlite/\": {\n+    canonical: \"/docs/tutorials/sql_qa\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/sqlite/\"],\n+  },\n+  \"/docs/modules/chains/popular/structured_output/\": {\n+    canonical: \"/docs/how_to/structured_output\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/structured_output/\"],\n+  },\n+  \"/docs/modules/chains/popular/summarize/\": {\n+    canonical: \"/docs/tutorials/summarization\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/summarize/\"],\n+  },\n+  \"/docs/modules/chains/popular/vector_db_qa_legacy/\": {\n+    canonical: \"/docs/tutorials/rag\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/vector_db_qa_legacy/\"],\n+  },\n+  \"/docs/modules/chains/popular/vector_db_qa/\": {\n+    canonical: \"/docs/tutorials/rag\",\n+    alternative: [\"/v0.1/docs/modules/chains/popular/vector_db_qa/\"],\n+  },\n+  \"/docs/modules/data_connection/document_loaders/creating_documents/\": {\n+    canonical: \"/docs/concepts#document\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/document_loaders/creating_documents/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/document_transformers/contextual_chunk_headers/\":\n+    {\n+      canonical:\n+        \"/docs/how_to/parent_document_retriever/#with-contextual-chunk-headers\",\n+      alternative: [\n+        \"/v0.1/docs/modules/data_connection/document_transformers/contextual_chunk_headers/\",\n+      ],\n+    },\n+  \"/docs/modules/data_connection/document_transformers/custom_text_splitter/\": {\n+    canonical: \"/docs/how_to/#text-splitters\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/document_transformers/custom_text_splitter/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/document_transformers/token_splitter/\": {\n+    canonical: \"/docs/how_to/split_by_token\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/document_transformers/token_splitter/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/experimental/graph_databases/neo4j/\": {\n+    canonical: \"/docs/tutorials/graph\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/experimental/graph_databases/neo4j/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/\":\n+    {\n+      canonical:\n+        \"https://js.langchain.com/v0.1/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/\",\n+      alternative: [\n+        \"/v0.1/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/\",\n+      ],\n+    },\n+  \"/docs/modules/data_connection/retrievers/custom/\": {\n+    canonical: \"/docs/how_to/custom_retriever\",\n+    alternative: [\"/v0.1/docs/modules/data_connection/retrievers/custom/\"],\n+  },\n+  \"/docs/modules/data_connection/retrievers/matryoshka_retriever/\": {\n+    canonical: \"/docs/how_to/reduce_retrieval_latency\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/matryoshka_retriever/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/multi-query-retriever/\": {\n+    canonical: \"/docs/how_to/multiple_queries\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/multi-query-retriever/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/multi-vector-retriever/\": {\n+    canonical: \"/docs/how_to/multi_vector\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/multi-vector-retriever/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/parent-document-retriever/\": {\n+    canonical: \"/docs/how_to/parent_document_retriever\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/parent-document-retriever/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/chroma-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/chroma\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/chroma-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/hnswlib-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/hnswlib\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/hnswlib-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/memory-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/memory\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/memory-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/pinecone-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/pinecone\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/pinecone-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/qdrant-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/qdrant\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/qdrant-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/supabase-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/supabase\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/supabase-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/vectara-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/vectara\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/vectara-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/self_query/weaviate-self-query/\": {\n+    canonical: \"/docs/integrations/retrievers/self_query/weaviate\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/retrievers/self_query/weaviate-self-query/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/retrievers/similarity-score-threshold-retriever/\":\n+    {\n+      canonical:\n+        \"https://api.js.langchain.com/classes/langchain.retrievers_score_threshold.ScoreThresholdRetriever.html\",\n+      alternative: [\n+        \"/v0.1/docs/modules/data_connection/retrievers/similarity-score-threshold-retriever/\",\n+      ],\n+    },\n+  \"/docs/modules/data_connection/text_embedding/api_errors/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/api_errors/\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/text_embedding/api_errors/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/text_embedding/caching_embeddings/\": {\n+    canonical: \"/docs/how_to/caching_embeddings\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/text_embedding/caching_embeddings/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/text_embedding/rate_limits/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/rate_limits/\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/text_embedding/rate_limits/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/text_embedding/timeouts/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/timeouts/\",\n+    alternative: [\n+      \"/v0.1/docs/modules/data_connection/text_embedding/timeouts/\",\n+    ],\n+  },\n+  \"/docs/modules/data_connection/vectorstores/custom/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/data_connection/vectorstores/custom/\",\n+    alternative: [\"/v0.1/docs/modules/data_connection/vectorstores/custom/\"],\n+  },\n+  \"/docs/modules/experimental/\": {\n+    canonical: \"https://js.langchain.com/v0.1/docs/modules/experimental/\",\n+    alternative: [\"/v0.1/docs/modules/experimental/\"],\n+  },\n+  \"/docs/modules/experimental/mask/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/modules/langchain.experimental_masking.html\",\n+    alternative: [\"/v0.1/docs/modules/experimental/mask/\"],\n+  },\n+  \"/docs/modules/experimental/prompts/custom_formats/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.experimental_prompts_handlebars.HandlebarsPromptTemplate.html\",\n+    alternative: [\"/v0.1/docs/modules/experimental/prompts/custom_formats/\"],\n+  },\n+  \"/docs/modules/memory/chat_messages/custom/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/modules/memory/chat_messages/custom/\",\n+    alternative: [\"/v0.1/docs/modules/memory/chat_messages/custom/\"],\n+  },\n+  \"/docs/modules/memory/types/buffer_memory_chat/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.BufferMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/buffer_memory_chat/\"],\n+  },\n+  \"/docs/modules/memory/types/buffer_window/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.BufferWindowMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/buffer_window/\"],\n+  },\n+  \"/docs/modules/memory/types/entity_summary_memory/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.EntityMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/entity_summary_memory/\"],\n+  },\n+  \"/docs/modules/memory/types/multiple_memory/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.CombinedMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/multiple_memory/\"],\n+  },\n+  \"/docs/modules/memory/types/summary_buffer/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.ConversationSummaryBufferMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/summary_buffer/\"],\n+  },\n+  \"/docs/modules/memory/types/summary/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.ConversationSummaryMemory.html\",\n+    alternative: [\"/v0.1/docs/modules/memory/types/summary/\"],\n+  },\n+  \"/docs/modules/memory/types/vectorstore_retriever_memory/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.memory.VectorStoreRetrieverMemory.html\",\n+    alternative: [\n+      \"/v0.1/docs/modules/memory/types/vectorstore_retriever_memory/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/chat/caching/\": {\n+    canonical: \"/docs/how_to/chat_model_caching\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/caching/\"],\n+  },\n+  \"/docs/modules/model_io/chat/cancelling_requests/\": {\n+    canonical: \"/docs/how_to/cancel_execution\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/cancelling_requests/\"],\n+  },\n+  \"/docs/modules/model_io/chat/custom_chat/\": {\n+    canonical: \"/docs/how_to/custom_chat\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/custom_chat/\"],\n+  },\n+  \"/docs/modules/model_io/chat/dealing_with_api_errors/\": {\n+    canonical: \"/docs/how_to/fallbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/dealing_with_api_errors/\"],\n+  },\n+  \"/docs/modules/model_io/chat/dealing_with_rate_limits/\": {\n+    canonical: \"/docs/how_to/fallbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/dealing_with_rate_limits/\"],\n+  },\n+  \"/docs/modules/model_io/chat/subscribing_events/\": {\n+    canonical: \"/docs/how_to/custom_callbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/subscribing_events/\"],\n+  },\n+  \"/docs/modules/model_io/chat/timeouts/\": {\n+    canonical: \"/docs/how_to/custom_callbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/chat/timeouts/\"],\n+  },\n+  \"/docs/modules/model_io/llms/cancelling_requests/\": {\n+    canonical: \"/docs/how_to/cancel_execution\",\n+    alternative: [\"/v0.1/docs/modules/model_io/llms/cancelling_requests/\"],\n+  },\n+  \"/docs/modules/model_io/llms/dealing_with_api_errors/\": {\n+    canonical: \"/docs/how_to/fallbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/llms/dealing_with_api_errors/\"],\n+  },\n+  \"/docs/modules/model_io/llms/dealing_with_rate_limits/\": {\n+    canonical: \"/docs/how_to/fallbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/llms/dealing_with_rate_limits/\"],\n+  },\n+  \"/docs/modules/model_io/llms/subscribing_events/\": {\n+    canonical: \"/docs/how_to/custom_callbacks\",\n+    alternative: [\"/v0.1/docs/modules/model_io/llms/subscribing_events/\"],\n+  },\n+  \"/docs/modules/model_io/llms/timeouts/\": {\n+    canonical: \"/docs/how_to/cancel_execution\",\n+    alternative: [\"/v0.1/docs/modules/model_io/llms/timeouts/\"],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/bytes/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/modules/_langchain_core.output_parsers.html\",\n+    alternative: [\"/v0.1/docs/modules/model_io/output_parsers/types/bytes/\"],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/combining_output_parser/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.output_parsers.CombiningOutputParser.html\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/output_parsers/types/combining_output_parser/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/csv/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/_langchain_core.output_parsers.CommaSeparatedListOutputParser.html\",\n+    alternative: [\"/v0.1/docs/modules/model_io/output_parsers/types/csv/\"],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/custom_list_parser/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/_langchain_core.output_parsers.CustomListOutputParser.html\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/output_parsers/types/custom_list_parser/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/http_response/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.output_parsers.HttpResponseOutputParser.html\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/output_parsers/types/http_response/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/json_functions/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/langchain.output_parsers.JsonOutputFunctionsParser.html\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/output_parsers/types/json_functions/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/output_parsers/types/string/\": {\n+    canonical:\n+      \"https://api.js.langchain.com/classes/_langchain_core.output_parsers.StringOutputParser.html\",\n+    alternative: [\"/v0.1/docs/modules/model_io/output_parsers/types/string/\"],\n+  },\n+  \"/docs/modules/model_io/prompts/example_selector_types/\": {\n+    canonical: \"/docs/how_to/#example-selectors\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/prompts/example_selector_types/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/prompts/example_selector_types/length_based/\": {\n+    canonical: \"/docs/how_to/example_selectors_length_based\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/prompts/example_selector_types/length_based/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/prompts/example_selector_types/similarity/\": {\n+    canonical: \"/docs/how_to/example_selectors_similarity\",\n+    alternative: [\n+      \"/v0.1/docs/modules/model_io/prompts/example_selector_types/similarity/\",\n+    ],\n+  },\n+  \"/docs/modules/model_io/prompts/few_shot/\": {\n+    canonical: \"/docs/how_to/few_shot_examples\",\n+    alternative: [\"/v0.1/docs/modules/model_io/prompts/few_shot/\"],\n+  },\n+  \"/docs/modules/model_io/prompts/pipeline/\": {\n+    canonical: \"/docs/how_to/prompts_composition\",\n+    alternative: [\"/v0.1/docs/modules/model_io/prompts/pipeline/\"],\n+  },\n+  \"/docs/production/deployment/\": {\n+    canonical: \"https://langchain-ai.github.io/langgraph/cloud/\",\n+    alternative: [\"/v0.1/docs/production/deployment/\"],\n+  },\n+  \"/docs/production/tracing/\": {\n+    canonical:\n+      \"https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain\",\n+    alternative: [\"/v0.1/docs/production/tracing/\"],\n+  },\n+  \"/docs/use_cases/agent_simulations/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/\",\n+    alternative: [\"/v0.1/docs/use_cases/agent_simulations/\"],\n+  },\n+  \"/docs/use_cases/agent_simulations/generative_agents/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/generative_agents/\",\n+    alternative: [\"/v0.1/docs/use_cases/agent_simulations/generative_agents/\"],\n+  },\n+  \"/docs/use_cases/agent_simulations/violation_of_expectations_chain/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/violation_of_expectations_chain/\",\n+    alternative: [\n+      \"/v0.1/docs/use_cases/agent_simulations/violation_of_expectations_chain/\",\n+    ],\n+  },\n+  \"/docs/use_cases/api/\": {\n+    canonical: \"https://js.langchain.com/v0.1/docs/use_cases/api/\",\n+    alternative: [\"/v0.1/docs/use_cases/api/\"],\n+  },\n+  \"/docs/use_cases/autonomous_agents/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/\",\n+    alternative: [\"/v0.1/docs/use_cases/autonomous_agents/\"],\n+  },\n+  \"/docs/use_cases/autonomous_agents/auto_gpt/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/auto_gpt/\",\n+    alternative: [\"/v0.1/docs/use_cases/autonomous_agents/auto_gpt/\"],\n+  },\n+  \"/docs/use_cases/autonomous_agents/baby_agi/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/baby_agi/\",\n+    alternative: [\"/v0.1/docs/use_cases/autonomous_agents/baby_agi/\"],\n+  },\n+  \"/docs/use_cases/autonomous_agents/sales_gpt/\": {\n+    canonical:\n+      \"https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/sales_gpt/\",\n+    alternative: [\"/v0.1/docs/use_cases/autonomous_agents/sales_gpt/\"],\n+  },\n+  \"/docs/use_cases/graph/construction/\": {\n+    canonical: \"/docs/tutorials/graph\",\n+    alternative: [\"/v0.1/docs/use_cases/graph/construction/\"],\n+  },\n+  \"/docs/use_cases/media/\": {\n+    canonical: \"/docs/how_to/multimodal_prompts\",\n+    alternative: [\"/v0.1/docs/use_cases/media/\"],\n+  },\n+  \"/docs/use_cases/query_analysis/how_to/constructing_filters/\": {\n+    canonical: \"/docs/tutorials/query_analysis\",\n+    alternative: [\n+      \"/v0.1/docs/use_cases/query_analysis/how_to/constructing_filters/\",\n+    ],\n+  },\n+  \"/docs/use_cases/tabular/\": {\n+    canonical: \"/docs/tutorials/sql_qa\",\n+    alternative: [\"/v0.1/docs/use_cases/tabular/\"],\n+  },\n+};\n+\n+export default function NotFound() {\n+  const location = useLocation();\n+  const pathname = location.pathname.endsWith(\"/\")\n+    ? location.pathname\n+    : `${location.pathname}/`; // Ensure the path matches the keys in suggestedLinks\n+  const { canonical, alternative } = suggestedLinks[pathname] || {};\n+\n+  return (\n+    <>\n+      <PageMetadata\n+        title={translate({\n+          id: \"theme.NotFound.title\",\n+          message: \"Page Not Found\",\n+        })}\n+      />\n+      <Layout>\n+        <main className=\"container margin-vert--xl\">\n+          <div className=\"row\">\n+            <div className=\"col col--6 col--offset-3\">\n+              <h1 className=\"hero__title\">\n+                {canonical\n+                  ? \"Page Moved\"\n+                  : alternative\n+                  ? \"Page Removed\"\n+                  : \"Page Not Found\"}\n+              </h1>\n+              {canonical ? (\n+                <h3>\n+                  You can find the new location <a href={canonical}>here</a>.\n+                </h3>\n+              ) : alternative ? (\n+                <p>The page you were looking for has been removed.</p>\n+              ) : (\n+                <p>We could not find what you were looking for.</p>\n+              )}\n+              {alternative && (\n+                <p>\n+                  <details>\n+                    <summary>Alternative pages</summary>\n+                    <ul>\n+                      {alternative.map((alt, index) => (\n+                        // eslint-disable-next-line react/no-array-index-key\n+                        <li key={index}>\n+                          <a href={alt}>{alt}</a>\n+                          {alt.startsWith(\"/v0.1/\") && (\n+                            <>\n+                              {\" \"}\n+                              <LegacyBadge />\n+                            </>\n+                          )}\n+                        </li>\n+                      ))}\n+                    </ul>\n+                  </details>\n+                </p>\n+              )}\n+              <p>\n+                Please contact the owner of the site that linked you to the\n+                original URL and let them know their link{\" \"}\n+                {canonical\n+                  ? \"has moved.\"\n+                  : alternative\n+                  ? \"has been removed.\"\n+                  : \"is broken.\"}\n+              </p>\n+            </div>\n+          </div>\n+        </main>\n+      </Layout>\n+    </>\n+  );\n+}",
          "docs/core_docs/vercel.json": "@@ -64,6 +64,22 @@\n     {\n       \"source\": \"/docs/tutorials/agents(/?)\",\n       \"destination\": \"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/\"\n+    },\n+    {\n+      \"source\": \"/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT(/?)\",\n+      \"destination\": \"https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/GRAPH_RECURSION_LIMIT/\"\n+    },\n+    {\n+      \"source\": \"/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE(/?)\",\n+      \"destination\": \"https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE/\"\n+    },\n+    {\n+      \"source\": \"/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE(/?)\",\n+      \"destination\": \"https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE/\"\n+    },\n+    {\n+      \"source\": \"/docs/troubleshooting/errors/MULTIPLE_SUBGRAPHS(/?)\",\n+      \"destination\": \"https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/MULTIPLE_SUBGRAPHS/\"\n     }\n   ]\n }",
          "environment_tests/test-exports-cf/package.json": "@@ -12,7 +12,6 @@\n     \"@langchain/core\": \"workspace:*\",\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n-    \"d3-dsv\": \"2\",\n     \"langchain\": \"workspace:*\",\n     \"wrangler\": \"^3.19.0\",\n     \"vitest\": \"0.34.3\",",
          "environment_tests/test-exports-cf/src/index.ts": "@@ -12,14 +12,15 @@\n import \"./entrypoints.js\";\n \n // Import a few things we'll use to test the exports\n-import { LLMChain } from \"langchain/chains\";\n import { ChatOpenAI } from \"@langchain/openai\";\n import {\n   ChatPromptTemplate,\n   HumanMessagePromptTemplate,\n } from \"@langchain/core/prompts\";\n import { OpenAI } from \"@langchain/openai\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n+import { StringOutputParser } from \"@langchain/core/output_parsers\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n \n export interface Env {\n   OPENAI_API_KEY?: string;\n@@ -51,14 +52,12 @@ export default {\n     const emb = new OpenAIEmbeddings(constructorParameters);\n \n     // Test a chain + prompt + model\n-    const chain = new LLMChain({\n-      llm: new ChatOpenAI(constructorParameters),\n-      prompt: ChatPromptTemplate.fromMessages([\n-        HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n-      ]),\n-    });\n-    const res = await chain.run(\"hello\");\n-\n+    const prompt = ChatPromptTemplate.fromMessages([\n+      HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n+    ]);\n+    const llm = new ChatOpenAI(constructorParameters);\n+    const chain = prompt.pipe(llm).pipe(new StringOutputParser());\n+    const res = await chain.invoke(\"hello\");\n     return new Response(\n       `Hello, from Cloudflare Worker at ${request.url}. Assistant says: ${res}`\n     );",
          "environment_tests/test-exports-cjs/package.json": "@@ -27,8 +27,6 @@\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n     \"@xenova/transformers\": \"^2.17.2\",\n-    \"d3-dsv\": \"2\",\n-    \"hnswlib-node\": \"^3.0.0\",\n     \"langchain\": \"workspace:*\",\n     \"typescript\": \"^5.0.0\"\n   },",
          "environment_tests/test-exports-cjs/src/import.js": "@@ -3,24 +3,17 @@ async function test() {\n   const { OpenAI } = await import(\"@langchain/openai\");\n   const { LLMChain } = await import(\"langchain/chains\");\n   const { ChatPromptTemplate } = await import(\"@langchain/core/prompts\");\n-  const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n   const { HuggingFaceTransformersEmbeddings } = await import(\"@langchain/community/embeddings/hf_transformers\");\n   const { Document } = await import(\"@langchain/core/documents\");\n+  const { MemoryVectorStore } = await import(\"langchain/vectorstores/memory\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-cjs/src/index.mjs": "@@ -1,7 +1,7 @@\n import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n@@ -10,16 +10,9 @@ import { Document } from \"@langchain/core/documents\";\n assert(typeof OpenAI === \"function\");\n assert(typeof LLMChain === \"function\");\n assert(typeof ChatPromptTemplate === \"function\");\n-assert(typeof HNSWLib === \"function\");\n+assert(typeof MemoryVectorStore === \"function\");\n \n-// Test dynamic imports of peer dependencies\n-const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }), {\n-  space: \"ip\",\n-  numDimensions: 3,\n-  index: new HierarchicalNSW(\"ip\", 3),\n-});\n+const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }));\n \n await vs.addVectors(\n   [",
          "environment_tests/test-exports-cjs/src/index.ts": "@@ -2,7 +2,7 @@ import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n \n@@ -11,10 +11,9 @@ async function test(useAzure: boolean = false) {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n   // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n   const openAIParameters = useAzure\n     ? {\n         azureOpenAIApiKey: \"sk-XXXX\",\n@@ -25,11 +24,8 @@ async function test(useAzure: boolean = false) {\n     : {\n         openAIApiKey: \"sk-XXXX\",\n       };\n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-cjs/src/require.js": "@@ -2,7 +2,7 @@ const assert = require(\"assert\");\n const { OpenAI } = require(\"@langchain/openai\");\n const { LLMChain } = require(\"langchain/chains\");\n const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n-const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n+const { MemoryVectorStore } = require(\"langchain/vectorstores/memory\");\n const { HuggingFaceTransformersEmbeddings } = require(\"@langchain/community/embeddings/hf_transformers\");\n const { Document } = require(\"@langchain/core/documents\");\n \n@@ -11,16 +11,9 @@ async function test() {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\" }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esbuild/package.json": "@@ -25,7 +25,6 @@\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n     \"esbuild\": \"^0.17.18\",\n-    \"hnswlib-node\": \"^3.0.0\",\n     \"langchain\": \"workspace:*\",\n     \"typescript\": \"^5.0.0\"\n   },",
          "environment_tests/test-exports-esbuild/src/import.cjs": "@@ -3,24 +3,17 @@ async function test() {\n   const { OpenAI } = await import(\"@langchain/openai\");\n   const { LLMChain } = await import(\"langchain/chains\");\n   const { ChatPromptTemplate } = await import(\"@langchain/core/prompts\");\n-  const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n+  const { MemoryVectorStore } = await import(\"langchain/vectorstores/memory\");\n   const { OpenAIEmbeddings } = await import(\"@langchain/openai\");\n   const { Document } = await import(\"@langchain/core/documents\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esbuild/src/index.js": "@@ -2,7 +2,7 @@ import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n@@ -11,18 +11,11 @@ import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n assert(typeof OpenAI === \"function\");\n assert(typeof LLMChain === \"function\");\n assert(typeof ChatPromptTemplate === \"function\");\n-assert(typeof HNSWLib === \"function\");\n+assert(typeof MemoryVectorStore === \"function\");\n assert(typeof OpenAIEmbeddings === \"function\");\n assert(typeof CallbackManager === \"function\");\n \n-// Test dynamic imports of peer dependencies\n-const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-const vs = new HNSWLib(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }), {\n-  space: \"ip\",\n-  numDimensions: 3,\n-  index: new HierarchicalNSW(\"ip\", 3),\n-});\n+const vs = new MemoryVectorStore(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }));\n \n await vs.addVectors(\n   [",
          "environment_tests/test-exports-esbuild/src/require.cjs": "@@ -2,7 +2,7 @@ const assert = require(\"assert\");\n const { OpenAI } = require(\"@langchain/openai\");\n const { LLMChain } = require(\"langchain/chains\");\n const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n-const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n+const { MemoryVectorStore } = require(\"langchain/vectorstores/memory\");\n const { OpenAIEmbeddings } = require(\"@langchain/openai\");\n const { Document } = require(\"@langchain/core/documents\");\n \n@@ -11,16 +11,9 @@ async function test() {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new OpenAIEmbeddings({ openAIApiKey: \"sk-XXXX\" }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esbuild/src/typescript.ts": "@@ -2,7 +2,7 @@ import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n \n@@ -11,10 +11,8 @@ async function test(useAzure: boolean = false) {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n   const openAIParameters = useAzure\n     ? {\n         azureOpenAIApiKey: \"sk-XXXX\",\n@@ -26,11 +24,7 @@ async function test(useAzure: boolean = false) {\n         openAIApiKey: \"sk-XXXX\",\n       };\n \n-  const vs = new HNSWLib(new OpenAIEmbeddings(openAIParameters), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new OpenAIEmbeddings(openAIParameters));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esm/package.json": "@@ -28,7 +28,6 @@\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n     \"@xenova/transformers\": \"^2.17.2\",\n-    \"hnswlib-node\": \"^3.0.0\",\n     \"langchain\": \"workspace:*\",\n     \"typescript\": \"^5.0.0\"\n   },",
          "environment_tests/test-exports-esm/src/import.cjs": "@@ -3,24 +3,17 @@ async function test() {\n   const { OpenAI } = await import(\"@langchain/openai\");\n   const { LLMChain } = await import(\"langchain/chains\");\n   const { ChatPromptTemplate } = await import(\"@langchain/core/prompts\");\n-  const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n+  const { MemoryVectorStore } = await import(\"langchain/vectorstores/memory\");\n   const { HuggingFaceTransformersEmbeddings } = await import(\"@langchain/community/embeddings/hf_transformers\");\n   const { Document } = await import(\"@langchain/core/documents\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esm/src/index.js": "@@ -2,7 +2,7 @@ import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n@@ -11,18 +11,11 @@ import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n assert(typeof OpenAI === \"function\");\n assert(typeof LLMChain === \"function\");\n assert(typeof ChatPromptTemplate === \"function\");\n-assert(typeof HNSWLib === \"function\");\n+assert(typeof MemoryVectorStore === \"function\");\n assert(typeof HuggingFaceTransformersEmbeddings === \"function\");\n assert(typeof CallbackManager === \"function\");\n \n-// Test dynamic imports of peer dependencies\n-const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }), {\n-  space: \"ip\",\n-  numDimensions: 3,\n-  index: new HierarchicalNSW(\"ip\", 3),\n-});\n+const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }));\n \n await vs.addVectors(\n   [",
          "environment_tests/test-exports-esm/src/index.ts": "@@ -2,7 +2,7 @@ import assert from \"assert\";\n import { OpenAI } from \"@langchain/openai\";\n import { LLMChain } from \"langchain/chains\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n-import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n+import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n \n@@ -11,10 +11,8 @@ async function test(useAzure: boolean = false) {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n   const openAIParameters = useAzure\n     ? {\n         azureOpenAIApiKey: \"sk-XXXX\",\n@@ -26,11 +24,7 @@ async function test(useAzure: boolean = false) {\n         openAIApiKey: \"sk-XXXX\",\n       };\n \n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }));\n \n   await vs.addVectors(\n     [",
          "environment_tests/test-exports-esm/src/require.cjs": "@@ -2,7 +2,7 @@ const assert = require(\"assert\");\n const { OpenAI } = require(\"@langchain/openai\");\n const { LLMChain } = require(\"langchain/chains\");\n const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n-const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n+const { MemoryVectorStore } = require(\"langchain/vectorstores/memory\");\n const { HuggingFaceTransformersEmbeddings } = require(\"@langchain/community/embeddings/hf_transformers\");\n const { Document } = require(\"@langchain/core/documents\");\n \n@@ -11,16 +11,9 @@ async function test() {\n   assert(typeof OpenAI === \"function\");\n   assert(typeof LLMChain === \"function\");\n   assert(typeof ChatPromptTemplate === \"function\");\n-  assert(typeof HNSWLib === \"function\");\n+  assert(typeof MemoryVectorStore === \"function\");\n \n-  // Test dynamic imports of peer dependencies\n-  const { HierarchicalNSW } = await HNSWLib.imports();\n-\n-  const vs = new HNSWLib(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }), {\n-    space: \"ip\",\n-    numDimensions: 3,\n-    index: new HierarchicalNSW(\"ip\", 3),\n-  });\n+  const vs = new MemoryVectorStore(new HuggingFaceTransformersEmbeddings({ model: \"Xenova/all-MiniLM-L6-v2\", }));\n \n   await vs.addVectors(\n     [",
          "examples/package.json": "@@ -49,7 +49,7 @@\n     \"@langchain/google-vertexai\": \"workspace:*\",\n     \"@langchain/google-vertexai-web\": \"workspace:*\",\n     \"@langchain/groq\": \"workspace:*\",\n-    \"@langchain/langgraph\": \"^0.0.28\",\n+    \"@langchain/langgraph\": \"^0.2.3\",\n     \"@langchain/mistralai\": \"workspace:*\",\n     \"@langchain/mongodb\": \"workspace:*\",\n     \"@langchain/nomic\": \"workspace:*\",\n@@ -68,7 +68,6 @@\n     \"@planetscale/database\": \"^1.8.0\",\n     \"@prisma/client\": \"^4.11.0\",\n     \"@qdrant/js-client-rest\": \"^1.9.0\",\n-    \"@raycast/api\": \"^1.55.2\",\n     \"@rockset/client\": \"^0.9.1\",\n     \"@supabase/supabase-js\": \"^2.45.0\",\n     \"@tensorflow/tfjs-backend-cpu\": \"^4.4.0\",\n@@ -79,8 +78,8 @@\n     \"@zilliz/milvus2-sdk-node\": \"^2.3.5\",\n     \"axios\": \"^0.26.0\",\n     \"chromadb\": \"^1.5.3\",\n+    \"cohere-ai\": \"^7.14.0\",\n     \"convex\": \"^1.3.1\",\n-    \"couchbase\": \"^4.3.0\",\n     \"date-fns\": \"^3.3.1\",\n     \"duck-duck-scrape\": \"^2.2.5\",\n     \"exa-js\": \"^1.0.12\",\n@@ -91,7 +90,7 @@\n     \"ioredis\": \"^5.3.2\",\n     \"js-yaml\": \"^4.1.0\",\n     \"langchain\": \"workspace:*\",\n-    \"langsmith\": \"^0.1.43\",\n+    \"langsmith\": \"^0.1.56\",\n     \"mongodb\": \"^6.3.0\",\n     \"pg\": \"^8.11.0\",\n     \"pickleparser\": \"^0.2.1\",\n@@ -102,7 +101,7 @@\n     \"typeorm\": \"^0.3.20\",\n     \"typesense\": \"^1.5.3\",\n     \"uuid\": \"^10.0.0\",\n-    \"vectordb\": \"^0.1.4\",\n+    \"vectordb\": \"^0.9.0\",\n     \"voy-search\": \"0.6.2\",\n     \"weaviate-ts-client\": \"^2.0.0\",\n     \"zod\": \"^3.22.4\",",
          "examples/src/document_compressors/cohere_rerank_custom_client.ts": "@@ -0,0 +1,54 @@\n+import { CohereRerank } from \"@langchain/cohere\";\n+import { CohereClient } from \"cohere-ai\";\n+import { Document } from \"@langchain/core/documents\";\n+\n+const query = \"What is the capital of the United States?\";\n+const docs = [\n+  new Document({\n+    pageContent:\n+      \"Carson City is the capital city of the American state of Nevada. At the 2010 United States Census, Carson City had a population of 55,274.\",\n+  }),\n+  new Document({\n+    pageContent:\n+      \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\",\n+  }),\n+  new Document({\n+    pageContent:\n+      \"Charlotte Amalie is the capital and largest city of the United States Virgin Islands. It has about 20,000 people. The city is on the island of Saint Thomas.\",\n+  }),\n+  new Document({\n+    pageContent:\n+      \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.\",\n+  }),\n+  new Document({\n+    pageContent:\n+      \"Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states. The federal government (including the United States military) also uses capital punishment.\",\n+  }),\n+];\n+\n+const client = new CohereClient({\n+  token: process.env.COHERE_API_KEY,\n+  environment: \"<your-cohere-deployment-url>\", // optional\n+  // other params\n+});\n+\n+const cohereRerank = new CohereRerank({\n+  client, // apiKey will be ignored even if provided\n+  model: \"rerank-english-v2.0\",\n+});\n+\n+const rerankedDocuments = await cohereRerank.rerank(docs, query, {\n+  topN: 5,\n+});\n+\n+console.log(rerankedDocuments);\n+\n+/*\n+  [\n+    { index: 3, relevanceScore: 0.9871293 },\n+    { index: 1, relevanceScore: 0.29961726 },\n+    { index: 4, relevanceScore: 0.27542195 },\n+    { index: 0, relevanceScore: 0.08977329 },\n+    { index: 2, relevanceScore: 0.041462272 }\n+  ]\n+*/",
          "examples/src/document_loaders/apify_dataset_existing.ts": "@@ -6,6 +6,9 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { createRetrievalChain } from \"langchain/chains/retrieval\";\n import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n \n+const APIFY_API_TOKEN = \"YOUR-APIFY-API-TOKEN\"; // or set as process.env.APIFY_API_TOKEN\n+const OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\"; // or set as process.env.OPENAI_API_KEY\n+\n /*\n  * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.\n  * In the below example, the Apify dataset format looks like this:\n@@ -21,16 +24,20 @@ const loader = new ApifyDatasetLoader(\"your-dataset-id\", {\n       metadata: { source: item.url },\n     }),\n   clientOptions: {\n-    token: \"your-apify-token\", // Or set as process.env.APIFY_API_TOKEN\n+    token: APIFY_API_TOKEN,\n   },\n });\n \n const docs = await loader.load();\n \n-const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n+const vectorStore = await HNSWLib.fromDocuments(\n+  docs,\n+  new OpenAIEmbeddings({ apiKey: OPENAI_API_KEY })\n+);\n \n const model = new ChatOpenAI({\n   temperature: 0,\n+  apiKey: OPENAI_API_KEY,\n });\n \n const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([",
          "examples/src/document_loaders/apify_dataset_new.ts": "@@ -6,6 +6,9 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n import { createRetrievalChain } from \"langchain/chains/retrieval\";\n \n+const APIFY_API_TOKEN = \"YOUR-APIFY-API-TOKEN\"; // or set as process.env.APIFY_API_TOKEN\n+const OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\"; // or set as process.env.OPENAI_API_KEY\n+\n /*\n  * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.\n  * In the below example, the Apify dataset format looks like this:\n@@ -17,6 +20,8 @@ import { createRetrievalChain } from \"langchain/chains/retrieval\";\n const loader = await ApifyDatasetLoader.fromActorCall(\n   \"apify/website-content-crawler\",\n   {\n+    maxCrawlPages: 10,\n+    crawlerType: \"cheerio\",\n     startUrls: [{ url: \"https://js.langchain.com/docs/\" }],\n   },\n   {\n@@ -26,17 +31,21 @@ const loader = await ApifyDatasetLoader.fromActorCall(\n         metadata: { source: item.url },\n       }),\n     clientOptions: {\n-      token: \"your-apify-token\", // Or set as process.env.APIFY_API_TOKEN\n+      token: APIFY_API_TOKEN,\n     },\n   }\n );\n \n const docs = await loader.load();\n \n-const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n+const vectorStore = await HNSWLib.fromDocuments(\n+  docs,\n+  new OpenAIEmbeddings({ apiKey: OPENAI_API_KEY })\n+);\n \n const model = new ChatOpenAI({\n   temperature: 0,\n+  apiKey: OPENAI_API_KEY,\n });\n \n const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([",
          "examples/src/indexes/vector_stores/couchbase/similaritySearch.ts": "@@ -1,67 +0,0 @@\n-import { OpenAIEmbeddings } from \"@langchain/openai\";\n-import {\n-  CouchbaseVectorStoreArgs,\n-  CouchbaseVectorStore,\n-} from \"@langchain/community/vectorstores/couchbase\";\n-import { Cluster } from \"couchbase\";\n-import { TextLoader } from \"langchain/document_loaders/fs/text\";\n-import { CharacterTextSplitter } from \"@langchain/textsplitters\";\n-\n-const connectionString =\n-  process.env.COUCHBASE_DB_CONN_STR ?? \"couchbase://localhost\";\n-const databaseUsername = process.env.COUCHBASE_DB_USERNAME ?? \"Administrator\";\n-const databasePassword = process.env.COUCHBASE_DB_PASSWORD ?? \"Password\";\n-\n-// Load documents from file\n-const loader = new TextLoader(\"./state_of_the_union.txt\");\n-const rawDocuments = await loader.load();\n-const splitter = new CharacterTextSplitter({\n-  chunkSize: 500,\n-  chunkOverlap: 0,\n-});\n-const docs = await splitter.splitDocuments(rawDocuments);\n-\n-const couchbaseClient = await Cluster.connect(connectionString, {\n-  username: databaseUsername,\n-  password: databasePassword,\n-  configProfile: \"wanDevelopment\",\n-});\n-\n-// Open AI API Key is required to use OpenAIEmbeddings, some other embeddings may also be used\n-const embeddings = new OpenAIEmbeddings({\n-  apiKey: process.env.OPENAI_API_KEY,\n-});\n-\n-const couchbaseConfig: CouchbaseVectorStoreArgs = {\n-  cluster: couchbaseClient,\n-  bucketName: \"testing\",\n-  scopeName: \"_default\",\n-  collectionName: \"_default\",\n-  indexName: \"vector-index\",\n-  textKey: \"text\",\n-  embeddingKey: \"embedding\",\n-};\n-\n-const store = await CouchbaseVectorStore.fromDocuments(\n-  docs,\n-  embeddings,\n-  couchbaseConfig\n-);\n-\n-const query = \"What did president say about Ketanji Brown Jackson\";\n-\n-const resultsSimilaritySearch = await store.similaritySearch(query);\n-console.log(\"resulting documents: \", resultsSimilaritySearch[0]);\n-\n-// Similarity Search With Score\n-const resultsSimilaritySearchWithScore = await store.similaritySearchWithScore(\n-  query,\n-  1\n-);\n-console.log(\"resulting documents: \", resultsSimilaritySearchWithScore[0][0]);\n-console.log(\"resulting scores: \", resultsSimilaritySearchWithScore[0][1]);\n-\n-const result = await store.similaritySearch(query, 1, {\n-  fields: [\"metadata.source\"],\n-});\n-console.log(result[0]);",
          "examples/src/indexes/vector_stores/lancedb/fromDocs.ts": "@@ -4,24 +4,29 @@ import { TextLoader } from \"langchain/document_loaders/fs/text\";\n import fs from \"node:fs/promises\";\n import path from \"node:path\";\n import os from \"node:os\";\n-import { connect } from \"vectordb\";\n \n // Create docs with a loader\n const loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n const docs = await loader.load();\n \n export const run = async () => {\n+  const vectorStore = await LanceDB.fromDocuments(docs, new OpenAIEmbeddings());\n+\n+  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n+  console.log(resultOne);\n+\n+  // [\n+  //   Document {\n+  //     pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n+  //     metadata: { source: 'src/document_loaders/example_data/example.txt' }\n+  //   }\n+  // ]\n+};\n+\n+export const run_with_existing_table = async () => {\n   const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n-  const db = await connect(dir);\n-  const table = await db.createTable(\"vectors\", [\n-    { vector: Array(1536), text: \"sample\", source: \"a\" },\n-  ]);\n-\n-  const vectorStore = await LanceDB.fromDocuments(\n-    docs,\n-    new OpenAIEmbeddings(),\n-    { table }\n-  );\n+\n+  const vectorStore = await LanceDB.fromDocuments(docs, new OpenAIEmbeddings());\n \n   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n   console.log(resultOne);",
          "examples/src/indexes/vector_stores/lancedb/fromTexts.ts": "@@ -1,22 +1,27 @@\n import { LanceDB } from \"@langchain/community/vectorstores/lancedb\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n-import { connect } from \"vectordb\";\n import * as fs from \"node:fs/promises\";\n import * as path from \"node:path\";\n import os from \"node:os\";\n \n export const run = async () => {\n-  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n-  const db = await connect(dir);\n-  const table = await db.createTable(\"vectors\", [\n-    { vector: Array(1536), text: \"sample\", id: 1 },\n-  ]);\n+  const vectorStore = await LanceDB.fromTexts(\n+    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n+    [{ id: 2 }, { id: 1 }, { id: 3 }],\n+    new OpenAIEmbeddings()\n+  );\n \n+  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n+  console.log(resultOne);\n+  // [ Document { pageContent: 'hello nice world', metadata: { id: 3 } } ]\n+};\n+\n+export const run_with_existing_table = async () => {\n+  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n   const vectorStore = await LanceDB.fromTexts(\n     [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n     [{ id: 2 }, { id: 1 }, { id: 3 }],\n-    new OpenAIEmbeddings(),\n-    { table }\n+    new OpenAIEmbeddings()\n   );\n \n   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);",
          "examples/src/models/llm/raycast.ts": "@@ -1,28 +0,0 @@\n-import { RaycastAI } from \"@langchain/community/llms/raycast\";\n-\n-import { showHUD } from \"@raycast/api\";\n-import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n-import { Tool } from \"@langchain/core/tools\";\n-\n-const model = new RaycastAI({\n-  rateLimitPerMinute: 10, // It is 10 by default so you can omit this line\n-  model: \"gpt-3.5-turbo\",\n-  creativity: 0, // `creativity` is a term used by Raycast which is equivalent to `temperature` in some other LLMs\n-});\n-\n-const tools: Tool[] = [\n-  // Add your tools here\n-];\n-\n-export default async function main() {\n-  // Initialize the agent executor with RaycastAI model\n-  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n-    agentType: \"chat-conversational-react-description\",\n-  });\n-\n-  const input = `Describe my today's schedule as Gabriel Garcia Marquez would describe it`;\n-\n-  const answer = await executor.invoke({ input });\n-\n-  await showHUD(answer.output);\n-}",
          "langchain-core/.gitignore": "@@ -30,6 +30,10 @@ chat_history.cjs\n chat_history.js\n chat_history.d.ts\n chat_history.d.cts\n+context.cjs\n+context.js\n+context.d.ts\n+context.d.cts\n documents.cjs\n documents.js\n documents.d.ts",
          "langchain-core/langchain.config.js": "@@ -20,6 +20,7 @@ export const config = {\n     \"callbacks/manager\": \"callbacks/manager\",\n     \"callbacks/promises\": \"callbacks/promises\",\n     chat_history: \"chat_history\",\n+    context: \"context\",\n     documents: \"documents/index\",\n     \"document_loaders/base\": \"document_loaders/base\",\n     \"document_loaders/langsmith\": \"document_loaders/langsmith\",",
          "langchain-core/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/core\",\n-  \"version\": \"0.3.1\",\n+  \"version\": \"0.3.13\",\n   \"description\": \"Core LangChain.js abstractions and schemas\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -37,7 +37,7 @@\n     \"camelcase\": \"6\",\n     \"decamelize\": \"1.2.0\",\n     \"js-tiktoken\": \"^1.0.12\",\n-    \"langsmith\": \"^0.1.56-rc.1\",\n+    \"langsmith\": \"^0.1.65\",\n     \"mustache\": \"^4.2.0\",\n     \"p-queue\": \"^6.6.2\",\n     \"p-retry\": \"4\",\n@@ -68,7 +68,7 @@\n     \"rimraf\": \"^5.0.1\",\n     \"ts-jest\": \"^29.1.0\",\n     \"typescript\": \"~5.1.6\",\n-    \"web-streams-polyfill\": \"^3.3.3\"\n+    \"web-streams-polyfill\": \"^4.0.0\"\n   },\n   \"publishConfig\": {\n     \"access\": \"public\"\n@@ -160,6 +160,15 @@\n       \"import\": \"./chat_history.js\",\n       \"require\": \"./chat_history.cjs\"\n     },\n+    \"./context\": {\n+      \"types\": {\n+        \"import\": \"./context.d.ts\",\n+        \"require\": \"./context.d.cts\",\n+        \"default\": \"./context.d.ts\"\n+      },\n+      \"import\": \"./context.js\",\n+      \"require\": \"./context.cjs\"\n+    },\n     \"./documents\": {\n       \"types\": {\n         \"import\": \"./documents.d.ts\",\n@@ -646,6 +655,10 @@\n     \"chat_history.js\",\n     \"chat_history.d.ts\",\n     \"chat_history.d.cts\",\n+    \"context.cjs\",\n+    \"context.js\",\n+    \"context.d.ts\",\n+    \"context.d.cts\",\n     \"documents.cjs\",\n     \"documents.js\",\n     \"documents.d.ts\",",
          "langchain-core/src/callbacks/dispatch/index.ts": "@@ -1,10 +1,12 @@\n+/* __LC_ALLOW_ENTRYPOINT_SIDE_EFFECTS__ */\n+\n import { AsyncLocalStorage } from \"node:async_hooks\";\n import { dispatchCustomEvent as dispatchCustomEventWeb } from \"./web.js\";\n import { type RunnableConfig, ensureConfig } from \"../../runnables/config.js\";\n import { AsyncLocalStorageProviderSingleton } from \"../../singletons/index.js\";\n \n-/* #__PURE__ */ AsyncLocalStorageProviderSingleton.initializeGlobalInstance(\n-  /* #__PURE__ */ new AsyncLocalStorage()\n+AsyncLocalStorageProviderSingleton.initializeGlobalInstance(\n+  new AsyncLocalStorage()\n );\n \n /**",
          "langchain-core/src/context.ts": "@@ -0,0 +1,131 @@\n+/* __LC_ALLOW_ENTRYPOINT_SIDE_EFFECTS__ */\n+import { AsyncLocalStorage } from \"node:async_hooks\";\n+import { RunTree } from \"langsmith\";\n+import { isRunTree } from \"langsmith/run_trees\";\n+import {\n+  _CONTEXT_VARIABLES_KEY,\n+  AsyncLocalStorageProviderSingleton,\n+} from \"./singletons/index.js\";\n+\n+AsyncLocalStorageProviderSingleton.initializeGlobalInstance(\n+  new AsyncLocalStorage()\n+);\n+\n+/**\n+ * Set a context variable. Context variables are scoped to any\n+ * child runnables called by the current runnable, or globally if set outside\n+ * of any runnable.\n+ *\n+ * @remarks\n+ * This function is only supported in environments that support AsyncLocalStorage,\n+ * including Node.js, Deno, and Cloudflare Workers.\n+ *\n+ * @example\n+ * ```ts\n+ * import { RunnableLambda } from \"@langchain/core/runnables\";\n+ * import {\n+ *   getContextVariable,\n+ *   setContextVariable\n+ * } from \"@langchain/core/context\";\n+ *\n+ * const nested = RunnableLambda.from(() => {\n+ *   // \"bar\" because it was set by a parent\n+ *   console.log(getContextVariable(\"foo\"));\n+ *\n+ *   // Override to \"baz\", but only for child runnables\n+ *   setContextVariable(\"foo\", \"baz\");\n+ *\n+ *   // Now \"baz\", but only for child runnables\n+ *   return getContextVariable(\"foo\");\n+ * });\n+ *\n+ * const runnable = RunnableLambda.from(async () => {\n+ *   // Set a context variable named \"foo\"\n+ *   setContextVariable(\"foo\", \"bar\");\n+ *\n+ *   const res = await nested.invoke({});\n+ *\n+ *   // Still \"bar\" since child changes do not affect parents\n+ *   console.log(getContextVariable(\"foo\"));\n+ *\n+ *   return res;\n+ * });\n+ *\n+ * // undefined, because context variable has not been set yet\n+ * console.log(getContextVariable(\"foo\"));\n+ *\n+ * // Final return value is \"baz\"\n+ * const result = await runnable.invoke({});\n+ * ```\n+ *\n+ * @param name The name of the context variable.\n+ * @param value The value to set.\n+ */\n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n+export function setContextVariable<T>(name: PropertyKey, value: T): void {\n+  const runTree = AsyncLocalStorageProviderSingleton.getInstance().getStore();\n+  const contextVars = { ...runTree?.[_CONTEXT_VARIABLES_KEY] };\n+  contextVars[name] = value;\n+  let newValue = {};\n+  if (isRunTree(runTree)) {\n+    newValue = new RunTree(runTree);\n+  }\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  (newValue as any)[_CONTEXT_VARIABLES_KEY] = contextVars;\n+  AsyncLocalStorageProviderSingleton.getInstance().enterWith(newValue);\n+}\n+\n+/**\n+ * Get the value of a previously set context variable. Context variables\n+ * are scoped to any child runnables called by the current runnable,\n+ * or globally if set outside of any runnable.\n+ *\n+ * @remarks\n+ * This function is only supported in environments that support AsyncLocalStorage,\n+ * including Node.js, Deno, and Cloudflare Workers.\n+ *\n+ * @example\n+ * ```ts\n+ * import { RunnableLambda } from \"@langchain/core/runnables\";\n+ * import {\n+ *   getContextVariable,\n+ *   setContextVariable\n+ * } from \"@langchain/core/context\";\n+ *\n+ * const nested = RunnableLambda.from(() => {\n+ *   // \"bar\" because it was set by a parent\n+ *   console.log(getContextVariable(\"foo\"));\n+ *\n+ *   // Override to \"baz\", but only for child runnables\n+ *   setContextVariable(\"foo\", \"baz\");\n+ *\n+ *   // Now \"baz\", but only for child runnables\n+ *   return getContextVariable(\"foo\");\n+ * });\n+ *\n+ * const runnable = RunnableLambda.from(async () => {\n+ *   // Set a context variable named \"foo\"\n+ *   setContextVariable(\"foo\", \"bar\");\n+ *\n+ *   const res = await nested.invoke({});\n+ *\n+ *   // Still \"bar\" since child changes do not affect parents\n+ *   console.log(getContextVariable(\"foo\"));\n+ *\n+ *   return res;\n+ * });\n+ *\n+ * // undefined, because context variable has not been set yet\n+ * console.log(getContextVariable(\"foo\"));\n+ *\n+ * // Final return value is \"baz\"\n+ * const result = await runnable.invoke({});\n+ * ```\n+ *\n+ * @param name The name of the context variable.\n+ */\n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n+export function getContextVariable<T = any>(name: PropertyKey): T | undefined {\n+  const runTree = AsyncLocalStorageProviderSingleton.getInstance().getStore();\n+  return runTree?.[_CONTEXT_VARIABLES_KEY]?.[name];\n+}",
          "langchain-core/src/errors/index.ts": "@@ -0,0 +1,20 @@\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n+/* eslint-disable no-param-reassign */\n+\n+export type LangChainErrorCodes =\n+  | \"INVALID_PROMPT_INPUT\"\n+  | \"INVALID_TOOL_RESULTS\"\n+  | \"MESSAGE_COERCION_FAILURE\"\n+  | \"MODEL_AUTHENTICATION\"\n+  | \"MODEL_NOT_FOUND\"\n+  | \"MODEL_RATE_LIMIT\"\n+  | \"OUTPUT_PARSING_FAILURE\";\n+\n+export function addLangChainErrorFields(\n+  error: any,\n+  lc_error_code: LangChainErrorCodes\n+) {\n+  (error as any).lc_error_code = lc_error_code;\n+  error.message = `${error.message}\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/${lc_error_code}/\\n`;\n+  return error;\n+}",
          "langchain-core/src/language_models/base.ts": "@@ -363,13 +363,14 @@ export abstract class BaseLanguageModel<\n     callbackManager,\n     ...params\n   }: BaseLanguageModelParams) {\n+    const { cache, ...rest } = params;\n     super({\n       callbacks: callbacks ?? callbackManager,\n-      ...params,\n+      ...rest,\n     });\n-    if (typeof params.cache === \"object\") {\n-      this.cache = params.cache;\n-    } else if (params.cache) {\n+    if (typeof cache === \"object\") {\n+      this.cache = cache;\n+    } else if (cache) {\n       this.cache = InMemoryCache.global();\n     } else {\n       this.cache = undefined;",
          "langchain-core/src/language_models/chat_models.ts": "@@ -8,6 +8,7 @@ import {\n   HumanMessage,\n   coerceMessageLikeToMessage,\n   AIMessageChunk,\n+  isAIMessageChunk,\n } from \"../messages/index.js\";\n import type { BasePromptValueInterface } from \"../prompt_values.js\";\n import {\n@@ -141,7 +142,7 @@ export type BindToolsInput =\n export abstract class BaseChatModel<\n   CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n   // TODO: Fix the parameter order on the next minor version.\n-  OutputMessageType extends BaseMessageChunk = BaseMessageChunk\n+  OutputMessageType extends BaseMessageChunk = AIMessageChunk\n > extends BaseLanguageModel<OutputMessageType, CallOptions> {\n   // Backwards compatibility since fields have been moved to RunnableConfig\n   declare ParsedCallOptions: Omit<\n@@ -258,6 +259,8 @@ export abstract class BaseChatModel<\n         runnableConfig.runName\n       );\n       let generationChunk: ChatGenerationChunk | undefined;\n+      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+      let llmOutput: Record<string, any> | undefined;\n       try {\n         for await (const chunk of this._streamResponseChunks(\n           messages,\n@@ -278,6 +281,18 @@ export abstract class BaseChatModel<\n           } else {\n             generationChunk = generationChunk.concat(chunk);\n           }\n+          if (\n+            isAIMessageChunk(chunk.message) &&\n+            chunk.message.usage_metadata !== undefined\n+          ) {\n+            llmOutput = {\n+              tokenUsage: {\n+                promptTokens: chunk.message.usage_metadata.input_tokens,\n+                completionTokens: chunk.message.usage_metadata.output_tokens,\n+                totalTokens: chunk.message.usage_metadata.total_tokens,\n+              },\n+            };\n+          }\n         }\n       } catch (err) {\n         await Promise.all(\n@@ -292,6 +307,7 @@ export abstract class BaseChatModel<\n           runManager?.handleLLMEnd({\n             // TODO: Remove cast after figuring out inheritance\n             generations: [[generationChunk as ChatGeneration]],\n+            llmOutput,\n           })\n         )\n       );\n@@ -365,6 +381,8 @@ export abstract class BaseChatModel<\n           runManagers?.[0]\n         );\n         let aggregated;\n+        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+        let llmOutput: Record<string, any> | undefined;\n         for await (const chunk of stream) {\n           if (chunk.message.id == null) {\n             const runId = runManagers?.at(0)?.runId;\n@@ -375,14 +393,26 @@ export abstract class BaseChatModel<\n           } else {\n             aggregated = concat(aggregated, chunk);\n           }\n+          if (\n+            isAIMessageChunk(chunk.message) &&\n+            chunk.message.usage_metadata !== undefined\n+          ) {\n+            llmOutput = {\n+              tokenUsage: {\n+                promptTokens: chunk.message.usage_metadata.input_tokens,\n+                completionTokens: chunk.message.usage_metadata.output_tokens,\n+                totalTokens: chunk.message.usage_metadata.total_tokens,\n+              },\n+            };\n+          }\n         }\n         if (aggregated === undefined) {\n           throw new Error(\"Received empty response from chat model call.\");\n         }\n         generations.push([aggregated]);\n         await runManagers?.[0].handleLLMEnd({\n           generations,\n-          llmOutput: {},\n+          llmOutput,\n         });\n       } catch (e) {\n         await runManagers?.[0].handleLLMError(e);",
          "langchain-core/src/language_models/tests/chat_models.test.ts": "@@ -323,3 +323,15 @@ test(\"Test ChatModel can stream back a custom event\", async () => {\n   }\n   expect(customEvent).toBeDefined();\n });\n+\n+test(`Test ChatModel should not serialize a passed \"cache\" parameter`, async () => {\n+  const model = new FakeListChatModel({\n+    responses: [\"hi\"],\n+    emitCustomEvent: true,\n+    cache: true,\n+  });\n+  console.log(JSON.stringify(model));\n+  expect(JSON.stringify(model)).toEqual(\n+    `{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"chat_models\",\"fake-list\",\"FakeListChatModel\"],\"kwargs\":{\"responses\":[\"hi\"],\"emit_custom_event\":true}}`\n+  );\n+});",
          "langchain-core/src/messages/ai.ts": "@@ -21,22 +21,83 @@ export type AIMessageFields = BaseMessageFields & {\n   usage_metadata?: UsageMetadata;\n };\n \n+/**\n+ * Breakdown of input token counts.\n+ *\n+ * Does not *need* to sum to full input token count. Does *not* need to have all keys.\n+ */\n+export type InputTokenDetails = {\n+  /**\n+   * Audio input tokens.\n+   */\n+  audio?: number;\n+\n+  /**\n+   * Input tokens that were cached and there was a cache hit.\n+   *\n+   * Since there was a cache hit, the tokens were read from the cache.\n+   * More precisely, the model state given these tokens was read from the cache.\n+   */\n+  cache_read?: number;\n+\n+  /**\n+   * Input tokens that were cached and there was a cache miss.\n+   *\n+   * Since there was a cache miss, the cache was created from these tokens.\n+   */\n+  cache_creation?: number;\n+};\n+\n+/**\n+ * Breakdown of output token counts.\n+ *\n+ * Does *not* need to sum to full output token count. Does *not* need to have all keys.\n+ */\n+export type OutputTokenDetails = {\n+  /**\n+   * Audio output tokens\n+   */\n+  audio?: number;\n+\n+  /**\n+   * Reasoning output tokens.\n+   *\n+   * Tokens generated by the model in a chain of thought process (i.e. by\n+   * OpenAI's o1 models) that are not returned as part of model output.\n+   */\n+  reasoning?: number;\n+};\n+\n /**\n  * Usage metadata for a message, such as token counts.\n  */\n export type UsageMetadata = {\n   /**\n-   * The count of input (or prompt) tokens.\n+   * Count of input (or prompt) tokens. Sum of all input token types.\n    */\n   input_tokens: number;\n   /**\n-   * The count of output (or completion) tokens\n+   * Count of output (or completion) tokens. Sum of all output token types.\n    */\n   output_tokens: number;\n   /**\n-   * The total token count\n+   * Total token count. Sum of input_tokens + output_tokens.\n    */\n   total_tokens: number;\n+\n+  /**\n+   * Breakdown of input token counts.\n+   *\n+   * Does *not* need to sum to full input token count. Does *not* need to have all keys.\n+   */\n+  input_token_details?: InputTokenDetails;\n+\n+  /**\n+   * Breakdown of output token counts.\n+   *\n+   * Does *not* need to sum to full output token count. Does *not* need to have all keys.\n+   */\n+  output_token_details?: OutputTokenDetails;\n };\n \n /**\n@@ -185,6 +246,10 @@ export class AIMessageChunk extends BaseMessageChunk {\n         tool_calls: fields.tool_calls ?? [],\n         invalid_tool_calls: [],\n         tool_call_chunks: [],\n+        usage_metadata:\n+          fields.usage_metadata !== undefined\n+            ? fields.usage_metadata\n+            : undefined,\n       };\n     } else {\n       const toolCalls: ToolCall[] = [];\n@@ -220,6 +285,10 @@ export class AIMessageChunk extends BaseMessageChunk {\n         ...fields,\n         tool_calls: toolCalls,\n         invalid_tool_calls: invalidToolCalls,\n+        usage_metadata:\n+          fields.usage_metadata !== undefined\n+            ? fields.usage_metadata\n+            : undefined,\n       };\n     }\n     // Sadly, TypeScript only allows super() calls at root if the class has\n@@ -291,6 +360,48 @@ export class AIMessageChunk extends BaseMessageChunk {\n       this.usage_metadata !== undefined ||\n       chunk.usage_metadata !== undefined\n     ) {\n+      const inputTokenDetails: InputTokenDetails = {\n+        ...((this.usage_metadata?.input_token_details?.audio !== undefined ||\n+          chunk.usage_metadata?.input_token_details?.audio !== undefined) && {\n+          audio:\n+            (this.usage_metadata?.input_token_details?.audio ?? 0) +\n+            (chunk.usage_metadata?.input_token_details?.audio ?? 0),\n+        }),\n+        ...((this.usage_metadata?.input_token_details?.cache_read !==\n+          undefined ||\n+          chunk.usage_metadata?.input_token_details?.cache_read !==\n+            undefined) && {\n+          cache_read:\n+            (this.usage_metadata?.input_token_details?.cache_read ?? 0) +\n+            (chunk.usage_metadata?.input_token_details?.cache_read ?? 0),\n+        }),\n+        ...((this.usage_metadata?.input_token_details?.cache_creation !==\n+          undefined ||\n+          chunk.usage_metadata?.input_token_details?.cache_creation !==\n+            undefined) && {\n+          cache_creation:\n+            (this.usage_metadata?.input_token_details?.cache_creation ?? 0) +\n+            (chunk.usage_metadata?.input_token_details?.cache_creation ?? 0),\n+        }),\n+      };\n+\n+      const outputTokenDetails: OutputTokenDetails = {\n+        ...((this.usage_metadata?.output_token_details?.audio !== undefined ||\n+          chunk.usage_metadata?.output_token_details?.audio !== undefined) && {\n+          audio:\n+            (this.usage_metadata?.output_token_details?.audio ?? 0) +\n+            (chunk.usage_metadata?.output_token_details?.audio ?? 0),\n+        }),\n+        ...((this.usage_metadata?.output_token_details?.reasoning !==\n+          undefined ||\n+          chunk.usage_metadata?.output_token_details?.reasoning !==\n+            undefined) && {\n+          reasoning:\n+            (this.usage_metadata?.output_token_details?.reasoning ?? 0) +\n+            (chunk.usage_metadata?.output_token_details?.reasoning ?? 0),\n+        }),\n+      };\n+\n       const left: UsageMetadata = this.usage_metadata ?? {\n         input_tokens: 0,\n         output_tokens: 0,\n@@ -305,6 +416,14 @@ export class AIMessageChunk extends BaseMessageChunk {\n         input_tokens: left.input_tokens + right.input_tokens,\n         output_tokens: left.output_tokens + right.output_tokens,\n         total_tokens: left.total_tokens + right.total_tokens,\n+        // Do not include `input_token_details` / `output_token_details` keys in combined fields\n+        // unless their values are defined.\n+        ...(Object.keys(inputTokenDetails).length > 0 && {\n+          input_token_details: inputTokenDetails,\n+        }),\n+        ...(Object.keys(outputTokenDetails).length > 0 && {\n+          output_token_details: outputTokenDetails,\n+        }),\n       };\n       combinedFields.usage_metadata = usage_metadata;\n     }",
          "langchain-core/src/messages/base.ts": "@@ -218,9 +218,24 @@ export abstract class BaseMessage\n    */\n   id?: string;\n \n-  /** The type of the message. */\n+  /**\n+   * @deprecated Use .getType() instead or import the proper typeguard.\n+   * For example:\n+   *\n+   * ```ts\n+   * import { isAIMessage } from \"@langchain/core/messages\";\n+   *\n+   * const message = new AIMessage(\"Hello!\");\n+   * isAIMessage(message); // true\n+   * ```\n+   */\n   abstract _getType(): MessageType;\n \n+  /** The type of the message. */\n+  getType(): MessageType {\n+    return this._getType();\n+  }\n+\n   constructor(\n     fields: string | BaseMessageFields,\n     /** @deprecated */\n@@ -480,7 +495,8 @@ export type BaseMessageLike =\n   | ({\n       type: MessageType | \"user\" | \"assistant\" | \"placeholder\";\n     } & BaseMessageFields &\n-      Record<string, unknown>);\n+      Record<string, unknown>)\n+  | SerializedConstructor;\n \n export function isBaseMessage(\n   messageLike?: unknown",
          "langchain-core/src/messages/chat.ts": "@@ -108,3 +108,11 @@ export class ChatMessageChunk extends BaseMessageChunk {\n     };\n   }\n }\n+\n+export function isChatMessage(x: BaseMessage): x is ChatMessage {\n+  return x._getType() === \"generic\";\n+}\n+\n+export function isChatMessageChunk(x: BaseMessageChunk): x is ChatMessageChunk {\n+  return x._getType() === \"generic\";\n+}",
          "langchain-core/src/messages/function.ts": "@@ -73,3 +73,13 @@ export class FunctionMessageChunk extends BaseMessageChunk {\n     });\n   }\n }\n+\n+export function isFunctionMessage(x: BaseMessage): x is FunctionMessage {\n+  return x._getType() === \"function\";\n+}\n+\n+export function isFunctionMessageChunk(\n+  x: BaseMessageChunk\n+): x is FunctionMessageChunk {\n+  return x._getType() === \"function\";\n+}",
          "langchain-core/src/messages/human.ts": "@@ -47,3 +47,13 @@ export class HumanMessageChunk extends BaseMessageChunk {\n     });\n   }\n }\n+\n+export function isHumanMessage(x: BaseMessage): x is HumanMessage {\n+  return x.getType() === \"human\";\n+}\n+\n+export function isHumanMessageChunk(\n+  x: BaseMessageChunk\n+): x is HumanMessageChunk {\n+  return x.getType() === \"human\";\n+}",
          "langchain-core/src/messages/index.ts": "@@ -14,4 +14,6 @@ export {\n   ToolMessage,\n   ToolMessageChunk,\n   type InvalidToolCall,\n+  isToolMessage,\n+  isToolMessageChunk,\n } from \"./tool.js\";",
          "langchain-core/src/messages/system.ts": "@@ -47,3 +47,13 @@ export class SystemMessageChunk extends BaseMessageChunk {\n     });\n   }\n }\n+\n+export function isSystemMessage(x: BaseMessage): x is SystemMessage {\n+  return x._getType() === \"system\";\n+}\n+\n+export function isSystemMessageChunk(\n+  x: BaseMessageChunk\n+): x is SystemMessageChunk {\n+  return x._getType() === \"system\";\n+}",
          "langchain-core/src/messages/tests/base_message.test.ts": "@@ -10,6 +10,7 @@ import {\n   SystemMessage,\n } from \"../index.js\";\n import { load } from \"../../load/index.js\";\n+import { concat } from \"../../utils/stream.js\";\n \n test(\"Test ChatPromptTemplate can format OpenAI content image messages\", async () => {\n   const message = new HumanMessage({\n@@ -396,3 +397,60 @@ describe(\"Message like coercion\", () => {\n     ]);\n   });\n });\n+\n+describe(\"usage_metadata serialized\", () => {\n+  test(\"usage_metadata is serialized when included in constructor\", async () => {\n+    const aiMsg = new AIMessage({\n+      content: \"hello\",\n+      usage_metadata: {\n+        input_tokens: 1,\n+        output_tokens: 1,\n+        total_tokens: 2,\n+      },\n+    });\n+    const jsonAIMessage = JSON.stringify(aiMsg);\n+    expect(jsonAIMessage).toContain(\"usage_metadata\");\n+    expect(jsonAIMessage).toContain(\"input_tokens\");\n+    expect(jsonAIMessage).toContain(\"output_tokens\");\n+    expect(jsonAIMessage).toContain(\"total_tokens\");\n+  });\n+\n+  test(\"usage_metadata is serialized when included in constructor\", async () => {\n+    const aiMsg = new AIMessageChunk({\n+      content: \"hello\",\n+      usage_metadata: {\n+        input_tokens: 1,\n+        output_tokens: 1,\n+        total_tokens: 2,\n+      },\n+    });\n+    const jsonAIMessage = JSON.stringify(aiMsg);\n+    expect(jsonAIMessage).toContain(\"usage_metadata\");\n+    expect(jsonAIMessage).toContain(\"input_tokens\");\n+    expect(jsonAIMessage).toContain(\"output_tokens\");\n+    expect(jsonAIMessage).toContain(\"total_tokens\");\n+  });\n+\n+  test(\"usage_metadata is serialized even when not included in constructor\", async () => {\n+    const aiMsg = new AIMessageChunk(\"hello\");\n+\n+    const concatenatedAIMessageChunk = concat(\n+      aiMsg,\n+      new AIMessageChunk({\n+        content: \"\",\n+        usage_metadata: {\n+          input_tokens: 1,\n+          output_tokens: 1,\n+          total_tokens: 2,\n+        },\n+      })\n+    );\n+    const jsonConcatenatedAIMessageChunk = JSON.stringify(\n+      concatenatedAIMessageChunk\n+    );\n+    expect(jsonConcatenatedAIMessageChunk).toContain(\"usage_metadata\");\n+    expect(jsonConcatenatedAIMessageChunk).toContain(\"input_tokens\");\n+    expect(jsonConcatenatedAIMessageChunk).toContain(\"output_tokens\");\n+    expect(jsonConcatenatedAIMessageChunk).toContain(\"total_tokens\");\n+  });\n+});",
          "langchain-core/src/messages/tool.ts": "@@ -281,3 +281,11 @@ export function defaultToolCallParser(\n   }\n   return [toolCalls, invalidToolCalls];\n }\n+\n+export function isToolMessage(x: BaseMessage): x is ToolMessage {\n+  return x._getType() === \"tool\";\n+}\n+\n+export function isToolMessageChunk(x: BaseMessageChunk): x is ToolMessageChunk {\n+  return x._getType() === \"tool\";\n+}",
          "langchain-core/src/messages/utils.ts": "@@ -1,3 +1,5 @@\n+import { addLangChainErrorFields } from \"../errors/index.js\";\n+import { SerializedConstructor } from \"../load/serializable.js\";\n import { _isToolCall } from \"../tools/utils.js\";\n import { AIMessage, AIMessageChunk, AIMessageChunkFields } from \"./ai.js\";\n import {\n@@ -55,10 +57,45 @@ function _coerceToolCall(\n   }\n }\n \n+function isSerializedConstructor(x: unknown): x is SerializedConstructor {\n+  return (\n+    typeof x === \"object\" &&\n+    x != null &&\n+    (x as SerializedConstructor).lc === 1 &&\n+    Array.isArray((x as SerializedConstructor).id) &&\n+    (x as SerializedConstructor).kwargs != null &&\n+    typeof (x as SerializedConstructor).kwargs === \"object\"\n+  );\n+}\n+\n function _constructMessageFromParams(\n-  params: BaseMessageFields & { type: string } & Record<string, unknown>\n+  params:\n+    | (BaseMessageFields & { type: string } & Record<string, unknown>)\n+    | SerializedConstructor\n ) {\n-  const { type, ...rest } = params;\n+  let type: string;\n+  let rest: BaseMessageFields & Record<string, unknown>;\n+  // Support serialized messages\n+  if (isSerializedConstructor(params)) {\n+    const className = params.id.at(-1);\n+    if (className === \"HumanMessage\" || className === \"HumanMessageChunk\") {\n+      type = \"user\";\n+    } else if (className === \"AIMessage\" || className === \"AIMessageChunk\") {\n+      type = \"assistant\";\n+    } else if (\n+      className === \"SystemMessage\" ||\n+      className === \"SystemMessageChunk\"\n+    ) {\n+      type = \"system\";\n+    } else {\n+      type = \"unknown\";\n+    }\n+    rest = params.kwargs as BaseMessageFields;\n+  } else {\n+    const { type: extractedType, ...otherParams } = params;\n+    type = extractedType;\n+    rest = otherParams;\n+  }\n   if (type === \"human\" || type === \"user\") {\n     return new HumanMessage(rest);\n   } else if (type === \"ai\" || type === \"assistant\") {\n@@ -78,9 +115,17 @@ function _constructMessageFromParams(\n       name: rest.name,\n     });\n   } else {\n-    throw new Error(\n-      `Unable to coerce message from array: only human, AI, or system message coercion is currently supported.`\n+    const error = addLangChainErrorFields(\n+      new Error(\n+        `Unable to coerce message from array: only human, AI, system, or tool message coercion is currently supported.\\n\\nReceived: ${JSON.stringify(\n+          params,\n+          null,\n+          2\n+        )}`\n+      ),\n+      \"MESSAGE_COERCION_FAILURE\"\n     );\n+    throw error;\n   }\n }\n ",
          "langchain-core/src/output_parsers/base.ts": "@@ -4,6 +4,7 @@ import type { BasePromptValueInterface } from \"../prompt_values.js\";\n import type { BaseMessage, MessageContentComplex } from \"../messages/index.js\";\n import type { Callbacks } from \"../callbacks/manager.js\";\n import type { Generation, ChatGeneration } from \"../outputs.js\";\n+import { addLangChainErrorFields } from \"../errors/index.js\";\n \n /**\n  * Options for formatting instructions.\n@@ -193,5 +194,7 @@ export class OutputParserException extends Error {\n         );\n       }\n     }\n+\n+    addLangChainErrorFields(this, \"OUTPUT_PARSING_FAILURE\");\n   }\n }",
          "langchain-core/src/prompts/chat.ts": "@@ -39,6 +39,7 @@ import {\n   parseFString,\n   parseMustache,\n } from \"./template.js\";\n+import { addLangChainErrorFields } from \"../errors/index.js\";\n \n /**\n  * Abstract class that serves as a base for creating message prompt\n@@ -168,6 +169,8 @@ export class MessagesPlaceholder<\n         ].join(\"\\n\\n\")\n       );\n       error.name = \"InputFormatError\";\n+      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+      (error as any).lc_error_code = e.lc_error_code;\n       throw error;\n     }\n \n@@ -996,9 +999,13 @@ export class ChatPromptTemplate<\n               !(inputVariable in allValues) &&\n               !(isMessagesPlaceholder(promptMessage) && promptMessage.optional)\n             ) {\n-              throw new Error(\n-                `Missing value for input variable \\`${inputVariable.toString()}\\``\n+              const error = addLangChainErrorFields(\n+                new Error(\n+                  `Missing value for input variable \\`${inputVariable.toString()}\\``\n+                ),\n+                \"INVALID_PROMPT_INPUT\"\n               );\n+              throw error;\n             }\n             acc[inputVariable] = allValues[inputVariable];\n             return acc;",
          "langchain-core/src/prompts/prompt.ts": "@@ -84,7 +84,8 @@ type ExtractTemplateParamsRecursive<\n export type ParamsFromFString<T extends string> = {\n   [Key in\n     | ExtractTemplateParamsRecursive<T>[number]\n-    | (string & Record<never, never>)]: string;\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    | (string & Record<never, never>)]: any;\n };\n \n export type ExtractedFStringParams<",
          "langchain-core/src/prompts/template.ts": "@@ -1,6 +1,7 @@\n import mustache from \"mustache\";\n import { MessageContent } from \"../messages/index.js\";\n import type { InputValues } from \"../utils/types/index.js\";\n+import { addLangChainErrorFields } from \"../errors/index.js\";\n \n function configureMustache() {\n   // Use unescaped HTML\n@@ -106,17 +107,22 @@ export const parseMustache = (template: string) => {\n   return mustacheTemplateToNodes(parsed);\n };\n \n-export const interpolateFString = (template: string, values: InputValues) =>\n-  parseFString(template).reduce((res, node) => {\n+export const interpolateFString = (template: string, values: InputValues) => {\n+  return parseFString(template).reduce((res, node) => {\n     if (node.type === \"variable\") {\n       if (node.name in values) {\n-        return res + values[node.name];\n+        const stringValue =\n+          typeof values[node.name] === \"string\"\n+            ? values[node.name]\n+            : JSON.stringify(values[node.name]);\n+        return res + stringValue;\n       }\n       throw new Error(`(f-string) Missing value for input ${node.name}`);\n     }\n \n     return res + node.text;\n   }, \"\");\n+};\n \n export const interpolateMustache = (template: string, values: InputValues) => {\n   configureMustache();\n@@ -150,7 +156,14 @@ export const renderTemplate = (\n   template: string,\n   templateFormat: TemplateFormat,\n   inputValues: InputValues\n-) => DEFAULT_FORMATTER_MAPPING[templateFormat](template, inputValues);\n+) => {\n+  try {\n+    return DEFAULT_FORMATTER_MAPPING[templateFormat](template, inputValues);\n+  } catch (e) {\n+    const error = addLangChainErrorFields(e, \"INVALID_PROMPT_INPUT\");\n+    throw error;\n+  }\n+};\n \n export const parseTemplate = (\n   template: string,",
          "langchain-core/src/prompts/tests/chat.test.ts": "@@ -1,3 +1,5 @@\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n+\n import { expect, test } from \"@jest/globals\";\n import {\n   AIMessagePromptTemplate,\n@@ -15,6 +17,7 @@ import {\n   ChatMessage,\n   FunctionMessage,\n } from \"../../messages/index.js\";\n+import { Document } from \"../../documents/document.js\";\n \n function createChatPromptTemplate() {\n   const systemPrompt = new PromptTemplate({\n@@ -72,13 +75,18 @@ test(\"Test format\", async () => {\n \n test(\"Test format with invalid input values\", async () => {\n   const chatPrompt = createChatPromptTemplate();\n-  await expect(\n+  let error: any | undefined;\n+  try {\n     // @ts-expect-error TS compiler should flag missing input variables\n-    chatPrompt.formatPromptValue({\n+    await chatPrompt.formatPromptValue({\n       context: \"This is a context\",\n       foo: \"Foo\",\n-    })\n-  ).rejects.toThrow(\"Missing value for input variable `bar`\");\n+    });\n+  } catch (e) {\n+    error = e;\n+  }\n+  expect(error?.message).toContain(\"Missing value for input variable `bar`\");\n+  expect(error?.lc_error_code).toEqual(\"INVALID_PROMPT_INPUT\");\n });\n \n test(\"Test format with invalid input variables\", async () => {\n@@ -129,6 +137,23 @@ test(\"Test fromTemplate\", async () => {\n   ]);\n });\n \n+test(\"Test fromTemplate\", async () => {\n+  const chatPrompt = ChatPromptTemplate.fromTemplate(\"Hello {foo}, I'm {bar}\");\n+  expect(chatPrompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n+  expect(\n+    (\n+      await chatPrompt.invoke({\n+        foo: [\"barbar\"],\n+        bar: [new Document({ pageContent: \"bar\" })],\n+      })\n+    ).toChatMessages()\n+  ).toEqual([\n+    new HumanMessage(\n+      `Hello [\"barbar\"], I'm [{\"pageContent\":\"bar\",\"metadata\":{}}]`\n+    ),\n+  ]);\n+});\n+\n test(\"Test fromMessages\", async () => {\n   const systemPrompt = new PromptTemplate({\n     template: \"Here's some context: {context}\",\n@@ -155,6 +180,34 @@ test(\"Test fromMessages\", async () => {\n   ]);\n });\n \n+test(\"Test fromMessages with non-string inputs\", async () => {\n+  const systemPrompt = new PromptTemplate({\n+    template: \"Here's some context: {context}\",\n+    inputVariables: [\"context\"],\n+  });\n+  const userPrompt = new PromptTemplate({\n+    template: \"Hello {foo}, I'm {bar}\",\n+    inputVariables: [\"foo\", \"bar\"],\n+  });\n+  // TODO: Fix autocomplete for the fromMessages method\n+  const chatPrompt = ChatPromptTemplate.fromMessages([\n+    new SystemMessagePromptTemplate(systemPrompt),\n+    new HumanMessagePromptTemplate(userPrompt),\n+  ]);\n+  expect(chatPrompt.inputVariables).toEqual([\"context\", \"foo\", \"bar\"]);\n+  const messages = await chatPrompt.formatPromptValue({\n+    context: [new Document({ pageContent: \"bar\" })],\n+    foo: \"Foo\",\n+    bar: \"Bar\",\n+  });\n+  expect(messages.toChatMessages()).toEqual([\n+    new SystemMessage(\n+      `Here's some context: [{\"pageContent\":\"bar\",\"metadata\":{}}]`\n+    ),\n+    new HumanMessage(\"Hello Foo, I'm Bar\"),\n+  ]);\n+});\n+\n test(\"Test fromMessages with a variety of ways to declare prompt messages\", async () => {\n   const systemPrompt = new PromptTemplate({\n     template: \"Here's some context: {context}\",\n@@ -306,6 +359,24 @@ test(\"Test MessagesPlaceholder not optional\", async () => {\n   );\n });\n \n+test(\"Test MessagesPlaceholder not optional with invalid input should throw\", async () => {\n+  const prompt = new MessagesPlaceholder({\n+    variableName: \"foo\",\n+  });\n+  const badInput = [new Document({ pageContent: \"barbar\", metadata: {} })];\n+  await expect(\n+    prompt.formatMessages({\n+      foo: [new Document({ pageContent: \"barbar\", metadata: {} })],\n+    })\n+  ).rejects.toThrow(\n+    `Field \"foo\" in prompt uses a MessagesPlaceholder, which expects an array of BaseMessages or coerceable values as input.\\n\\nReceived value: ${JSON.stringify(\n+      badInput,\n+      null,\n+      2\n+    )}\\n\\nAdditional message: Unable to coerce message from array: only human, AI, system, or tool message coercion is currently supported.`\n+  );\n+});\n+\n test(\"Test MessagesPlaceholder shorthand in a chat prompt template should throw for invalid syntax\", async () => {\n   expect(() =>\n     ChatPromptTemplate.fromMessages([[\"placeholder\", \"foo\"]])",
          "langchain-core/src/prompts/tests/prompt.test.ts": "@@ -1,5 +1,6 @@\n import { expect, test } from \"@jest/globals\";\n import { PromptTemplate } from \"../prompt.js\";\n+import { Document } from \"../../documents/document.js\";\n \n test(\"Test using partial\", async () => {\n   const prompt = new PromptTemplate({\n@@ -26,6 +27,18 @@ test(\"Test fromTemplate\", async () => {\n   ).toBe(\"foobaz\");\n });\n \n+test(\"Test fromTemplate with a non-string value\", async () => {\n+  const prompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\n+  expect(\n+    (\n+      await prompt.invoke({\n+        foo: [\"barbar\"],\n+        bar: [new Document({ pageContent: \"bar\" })],\n+      })\n+    ).value\n+  ).toBe(`[\"barbar\"][{\"pageContent\":\"bar\",\"metadata\":{}}]`);\n+});\n+\n test(\"Test fromTemplate with escaped strings\", async () => {\n   const prompt = PromptTemplate.fromTemplate(\"{{foo}}{{bar}}\");\n   expect(await prompt.format({ unused: \"eee\" })).toBe(\"{foo}{bar}\");",
          "langchain-core/src/runnables/base.ts": "@@ -58,24 +58,34 @@ import { ToolCall } from \"../messages/tool.js\";\n \n export { type RunnableInterface, RunnableBatchOptions };\n \n-export type RunnableFunc<RunInput, RunOutput> = (\n+export type RunnableFunc<\n+  RunInput,\n+  RunOutput,\n+  CallOptions extends RunnableConfig = RunnableConfig\n+> = (\n   input: RunInput,\n   options:\n-    | RunnableConfig\n+    | CallOptions\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-    | (Record<string, any> & RunnableConfig)\n+    | (Record<string, any> & CallOptions)\n ) => RunOutput | Promise<RunOutput>;\n \n export type RunnableMapLike<RunInput, RunOutput> = {\n   [K in keyof RunOutput]: RunnableLike<RunInput, RunOutput[K]>;\n };\n \n // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-export type RunnableLike<RunInput = any, RunOutput = any> =\n-  | RunnableInterface<RunInput, RunOutput>\n-  | RunnableFunc<RunInput, RunOutput>\n+export type RunnableLike<\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  RunInput = any,\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  RunOutput = any,\n+  CallOptions extends RunnableConfig = RunnableConfig\n+> =\n+  | RunnableInterface<RunInput, RunOutput, CallOptions>\n+  | RunnableFunc<RunInput, RunOutput, CallOptions>\n   | RunnableMapLike<RunInput, RunOutput>;\n \n export type RunnableRetryFailedAttemptHandler = (\n@@ -174,7 +184,7 @@ export abstract class Runnable<\n    */\n   withConfig(\n     config: RunnableConfig\n-  ): RunnableBinding<RunInput, RunOutput, CallOptions> {\n+  ): Runnable<RunInput, RunOutput, CallOptions> {\n     // eslint-disable-next-line @typescript-eslint/no-use-before-define\n     return new RunnableBinding({\n       bound: this,\n@@ -471,7 +481,7 @@ export abstract class Runnable<\n       runManager?: CallbackManagerForChainRun,\n       options?: Partial<CallOptions>\n     ) => AsyncGenerator<O>,\n-    options?: CallOptions & { runType?: string }\n+    options?: Partial<CallOptions> & { runType?: string }\n   ): AsyncGenerator<O> {\n     let finalInput: I | undefined;\n     let finalInputSupported = true;\n@@ -695,7 +705,7 @@ export abstract class Runnable<\n       config.callbacks = callbacks.concat([logStreamCallbackHandler]);\n     } else {\n       const copiedCallbacks = callbacks.copy();\n-      copiedCallbacks.inheritableHandlers.push(logStreamCallbackHandler);\n+      copiedCallbacks.addHandler(logStreamCallbackHandler, true);\n       // eslint-disable-next-line no-param-reassign\n       config.callbacks = copiedCallbacks;\n     }\n@@ -896,7 +906,7 @@ export abstract class Runnable<\n       config.callbacks = callbacks.concat(eventStreamer);\n     } else {\n       const copiedCallbacks = callbacks.copy();\n-      copiedCallbacks.inheritableHandlers.push(eventStreamer);\n+      copiedCallbacks.addHandler(eventStreamer, true);\n       // eslint-disable-next-line no-param-reassign\n       config.callbacks = copiedCallbacks;\n     }\n@@ -1236,7 +1246,7 @@ export class RunnableBinding<\n \n   withConfig(\n     config: RunnableConfig\n-  ): RunnableBinding<RunInput, RunOutput, CallOptions> {\n+  ): Runnable<RunInput, RunOutput, CallOptions> {\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     return new (this.constructor as any)({\n       bound: this.bound,\n@@ -2234,15 +2244,28 @@ export class RunnableTraceable<RunInput, RunOutput> extends Runnable<\n   }\n }\n \n-function assertNonTraceableFunction<RunInput, RunOutput>(\n+function assertNonTraceableFunction<\n+  RunInput,\n+  RunOutput,\n+  CallOptions extends RunnableConfig = RunnableConfig\n+>(\n   func:\n-    | RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+    | RunnableFunc<\n+        RunInput,\n+        RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+        CallOptions\n+      >\n     | TraceableFunction<\n-        RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+        RunnableFunc<\n+          RunInput,\n+          RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+          CallOptions\n+        >\n       >\n ): asserts func is RunnableFunc<\n   RunInput,\n-  RunOutput | Runnable<RunInput, RunOutput>\n+  RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+  CallOptions\n > {\n   if (isTraceableFunction(func)) {\n     throw new Error(\n@@ -2254,10 +2277,11 @@ function assertNonTraceableFunction<RunInput, RunOutput>(\n /**\n  * A runnable that runs a callable.\n  */\n-export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n+export class RunnableLambda<\n   RunInput,\n-  RunOutput\n-> {\n+  RunOutput,\n+  CallOptions extends RunnableConfig = RunnableConfig\n+> extends Runnable<RunInput, RunOutput, CallOptions> {\n   static lc_name() {\n     return \"RunnableLambda\";\n   }\n@@ -2266,21 +2290,31 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n \n   protected func: RunnableFunc<\n     RunInput,\n-    RunOutput | Runnable<RunInput, RunOutput>\n+    RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+    CallOptions\n   >;\n \n   constructor(fields: {\n     func:\n-      | RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+      | RunnableFunc<\n+          RunInput,\n+          RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+          CallOptions\n+        >\n       | TraceableFunction<\n-          RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+          RunnableFunc<\n+            RunInput,\n+            RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+            CallOptions\n+          >\n         >;\n   }) {\n     if (isTraceableFunction(fields.func)) {\n       // eslint-disable-next-line no-constructor-return\n       return RunnableTraceable.from(fields.func) as unknown as RunnableLambda<\n         RunInput,\n-        RunOutput\n+        RunOutput,\n+        CallOptions\n       >;\n     }\n \n@@ -2290,31 +2324,59 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n     this.func = fields.func;\n   }\n \n-  static from<RunInput, RunOutput>(\n-    func: RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n-  ): RunnableLambda<RunInput, RunOutput>;\n+  static from<\n+    RunInput,\n+    RunOutput,\n+    CallOptions extends RunnableConfig = RunnableConfig\n+  >(\n+    func: RunnableFunc<\n+      RunInput,\n+      RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+      CallOptions\n+    >\n+  ): RunnableLambda<RunInput, RunOutput, CallOptions>;\n \n-  static from<RunInput, RunOutput>(\n+  static from<\n+    RunInput,\n+    RunOutput,\n+    CallOptions extends RunnableConfig = RunnableConfig\n+  >(\n     func: TraceableFunction<\n-      RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+      RunnableFunc<\n+        RunInput,\n+        RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+        CallOptions\n+      >\n     >\n-  ): RunnableLambda<RunInput, RunOutput>;\n+  ): RunnableLambda<RunInput, RunOutput, CallOptions>;\n \n-  static from<RunInput, RunOutput>(\n+  static from<\n+    RunInput,\n+    RunOutput,\n+    CallOptions extends RunnableConfig = RunnableConfig\n+  >(\n     func:\n-      | RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+      | RunnableFunc<\n+          RunInput,\n+          RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+          CallOptions\n+        >\n       | TraceableFunction<\n-          RunnableFunc<RunInput, RunOutput | Runnable<RunInput, RunOutput>>\n+          RunnableFunc<\n+            RunInput,\n+            RunOutput | Runnable<RunInput, RunOutput, CallOptions>,\n+            CallOptions\n+          >\n         >\n-  ): RunnableLambda<RunInput, RunOutput> {\n+  ): RunnableLambda<RunInput, RunOutput, CallOptions> {\n     return new RunnableLambda({\n       func,\n     });\n   }\n \n   async _invoke(\n     input: RunInput,\n-    config?: Partial<RunnableConfig>,\n+    config?: Partial<CallOptions>,\n     runManager?: CallbackManagerForChainRun\n   ) {\n     return new Promise<RunOutput>((resolve, reject) => {\n@@ -2328,7 +2390,6 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n           try {\n             let output = await this.func(input, {\n               ...childConfig,\n-              config: childConfig,\n             });\n             if (output && Runnable.isRunnable(output)) {\n               if (config?.recursionLimit === 0) {\n@@ -2391,15 +2452,15 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n \n   async invoke(\n     input: RunInput,\n-    options?: Partial<RunnableConfig>\n+    options?: Partial<CallOptions>\n   ): Promise<RunOutput> {\n     return this._callWithConfig(this._invoke.bind(this), input, options);\n   }\n \n   async *_transform(\n     generator: AsyncGenerator<RunInput>,\n     runManager?: CallbackManagerForChainRun,\n-    config?: Partial<RunnableConfig>\n+    config?: Partial<CallOptions>\n   ): AsyncGenerator<RunOutput> {\n     let finalChunk: RunInput | undefined;\n     for await (const chunk of generator) {\n@@ -2465,7 +2526,7 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n \n   transform(\n     generator: AsyncGenerator<RunInput>,\n-    options?: Partial<RunnableConfig>\n+    options?: Partial<CallOptions>\n   ): AsyncGenerator<RunOutput> {\n     return this._transformStreamWithConfig(\n       generator,\n@@ -2476,7 +2537,7 @@ export class RunnableLambda<RunInput, RunOutput> extends Runnable<\n \n   async stream(\n     input: RunInput,\n-    options?: Partial<RunnableConfig>\n+    options?: Partial<CallOptions>\n   ): Promise<IterableReadableStream<RunOutput>> {\n     async function* generator() {\n       yield input;\n@@ -2721,24 +2782,33 @@ export class RunnableWithFallbacks<RunInput, RunOutput> extends Runnable<\n }\n \n // TODO: Figure out why the compiler needs help eliminating Error as a RunOutput type\n-export function _coerceToRunnable<RunInput, RunOutput>(\n-  coerceable: RunnableLike<RunInput, RunOutput>\n-): Runnable<RunInput, Exclude<RunOutput, Error>> {\n+export function _coerceToRunnable<\n+  RunInput,\n+  RunOutput,\n+  CallOptions extends RunnableConfig = RunnableConfig\n+>(\n+  coerceable: RunnableLike<RunInput, RunOutput, CallOptions>\n+): Runnable<RunInput, Exclude<RunOutput, Error>, CallOptions> {\n   if (typeof coerceable === \"function\") {\n     return new RunnableLambda({ func: coerceable }) as Runnable<\n       RunInput,\n-      Exclude<RunOutput, Error>\n+      Exclude<RunOutput, Error>,\n+      CallOptions\n     >;\n   } else if (Runnable.isRunnable(coerceable)) {\n-    return coerceable as Runnable<RunInput, Exclude<RunOutput, Error>>;\n+    return coerceable as Runnable<\n+      RunInput,\n+      Exclude<RunOutput, Error>,\n+      CallOptions\n+    >;\n   } else if (!Array.isArray(coerceable) && typeof coerceable === \"object\") {\n     const runnables: Record<string, Runnable<RunInput>> = {};\n     for (const [key, value] of Object.entries(coerceable)) {\n       runnables[key] = _coerceToRunnable(value as RunnableLike);\n     }\n     return new RunnableMap({\n       steps: runnables,\n-    }) as unknown as Runnable<RunInput, Exclude<RunOutput, Error>>;\n+    }) as unknown as Runnable<RunInput, Exclude<RunOutput, Error>, CallOptions>;\n   } else {\n     throw new Error(\n       `Expected a Runnable, function or object.\\nInstead got an unsupported type.`",
          "langchain-core/src/runnables/graph.ts": "@@ -9,31 +9,31 @@ import type {\n import { isRunnableInterface } from \"./utils.js\";\n import { drawMermaid, drawMermaidPng } from \"./graph_mermaid.js\";\n \n-const MAX_DATA_DISPLAY_NAME_LENGTH = 42;\n-\n export { Node, Edge };\n \n-function nodeDataStr(node: Node): string {\n-  if (!isUuid(node.id)) {\n-    return node.id;\n-  } else if (isRunnableInterface(node.data)) {\n+function nodeDataStr(\n+  id: string | undefined,\n+  data: RunnableInterface | RunnableIOSchema\n+): string {\n+  if (id !== undefined && !isUuid(id)) {\n+    return id;\n+  } else if (isRunnableInterface(data)) {\n     try {\n-      let data = node.data.getName();\n-      data = data.startsWith(\"Runnable\") ? data.slice(\"Runnable\".length) : data;\n-      if (data.length > MAX_DATA_DISPLAY_NAME_LENGTH) {\n-        data = `${data.substring(0, MAX_DATA_DISPLAY_NAME_LENGTH)}...`;\n-      }\n-      return data;\n+      let dataStr = data.getName();\n+      dataStr = dataStr.startsWith(\"Runnable\")\n+        ? dataStr.slice(\"Runnable\".length)\n+        : dataStr;\n+      return dataStr;\n     } catch (error) {\n-      return node.data.getName();\n+      return data.getName();\n     }\n   } else {\n-    return node.data.name ?? \"UnknownSchema\";\n+    return data.name ?? \"UnknownSchema\";\n   }\n }\n \n function nodeDataJson(node: Node) {\n-  // if node.data is implements Runnable\n+  // if node.data implements Runnable\n   if (isRunnableInterface(node.data)) {\n     return {\n       type: \"runnable\",\n@@ -55,6 +55,11 @@ export class Graph {\n \n   edges: Edge[] = [];\n \n+  constructor(params?: { nodes: Record<string, Node>; edges: Edge[] }) {\n+    this.nodes = params?.nodes ?? this.nodes;\n+    this.edges = params?.edges ?? this.edges;\n+  }\n+\n   // Convert the graph to a JSON-serializable format.\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n   toJSON(): Record<string, any> {\n@@ -86,12 +91,22 @@ export class Graph {\n     };\n   }\n \n-  addNode(data: RunnableInterface | RunnableIOSchema, id?: string): Node {\n+  addNode(\n+    data: RunnableInterface | RunnableIOSchema,\n+    id?: string,\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    metadata?: Record<string, any>\n+  ): Node {\n     if (id !== undefined && this.nodes[id] !== undefined) {\n       throw new Error(`Node with id ${id} already exists`);\n     }\n-    const nodeId = id || uuidv4();\n-    const node: Node = { id: nodeId, data };\n+    const nodeId = id ?? uuidv4();\n+    const node: Node = {\n+      id: nodeId,\n+      data,\n+      name: nodeDataStr(id, data),\n+      metadata,\n+    };\n     this.nodes[nodeId] = node;\n     return node;\n   }\n@@ -129,25 +144,11 @@ export class Graph {\n   }\n \n   firstNode(): Node | undefined {\n-    const targets = new Set(this.edges.map((edge) => edge.target));\n-    const found: Node[] = [];\n-    Object.values(this.nodes).forEach((node) => {\n-      if (!targets.has(node.id)) {\n-        found.push(node);\n-      }\n-    });\n-    return found[0];\n+    return _firstNode(this);\n   }\n \n   lastNode(): Node | undefined {\n-    const sources = new Set(this.edges.map((edge) => edge.source));\n-    const found: Node[] = [];\n-    Object.values(this.nodes).forEach((node) => {\n-      if (!sources.has(node.id)) {\n-        found.push(node);\n-      }\n-    });\n-    return found[0];\n+    return _lastNode(this);\n   }\n \n   /**\n@@ -188,28 +189,55 @@ export class Graph {\n \n   trimFirstNode(): void {\n     const firstNode = this.firstNode();\n-    if (firstNode) {\n-      const outgoingEdges = this.edges.filter(\n-        (edge) => edge.source === firstNode.id\n-      );\n-      if (Object.keys(this.nodes).length === 1 || outgoingEdges.length === 1) {\n-        this.removeNode(firstNode);\n-      }\n+    if (firstNode && _firstNode(this, [firstNode.id])) {\n+      this.removeNode(firstNode);\n     }\n   }\n \n   trimLastNode(): void {\n     const lastNode = this.lastNode();\n-    if (lastNode) {\n-      const incomingEdges = this.edges.filter(\n-        (edge) => edge.target === lastNode.id\n-      );\n-      if (Object.keys(this.nodes).length === 1 || incomingEdges.length === 1) {\n-        this.removeNode(lastNode);\n-      }\n+    if (lastNode && _lastNode(this, [lastNode.id])) {\n+      this.removeNode(lastNode);\n     }\n   }\n \n+  /**\n+   * Return a new graph with all nodes re-identified,\n+   * using their unique, readable names where possible.\n+   */\n+  reid(): Graph {\n+    const nodeLabels: Record<string, string> = Object.fromEntries(\n+      Object.values(this.nodes).map((node) => [node.id, node.name])\n+    );\n+    const nodeLabelCounts = new Map<string, number>();\n+    Object.values(nodeLabels).forEach((label) => {\n+      nodeLabelCounts.set(label, (nodeLabelCounts.get(label) || 0) + 1);\n+    });\n+\n+    const getNodeId = (nodeId: string): string => {\n+      const label = nodeLabels[nodeId];\n+      if (isUuid(nodeId) && nodeLabelCounts.get(label) === 1) {\n+        return label;\n+      } else {\n+        return nodeId;\n+      }\n+    };\n+\n+    return new Graph({\n+      nodes: Object.fromEntries(\n+        Object.entries(this.nodes).map(([id, node]) => [\n+          getNodeId(id),\n+          { ...node, id: getNodeId(id) },\n+        ])\n+      ),\n+      edges: this.edges.map((edge) => ({\n+        ...edge,\n+        source: getNodeId(edge.source),\n+        target: getNodeId(edge.target),\n+      })),\n+    });\n+  }\n+\n   drawMermaid(params?: {\n     withStyles?: boolean;\n     curveStyle?: string;\n@@ -219,23 +247,21 @@ export class Graph {\n     const {\n       withStyles,\n       curveStyle,\n-      nodeColors = { start: \"#ffdfba\", end: \"#baffc9\", other: \"#fad7de\" },\n+      nodeColors = {\n+        default: \"fill:#f2f0ff,line-height:1.2\",\n+        first: \"fill-opacity:0\",\n+        last: \"fill:#bfb6fc\",\n+      },\n       wrapLabelNWords,\n     } = params ?? {};\n-    const nodes: Record<string, string> = {};\n-    for (const node of Object.values(this.nodes)) {\n-      nodes[node.id] = nodeDataStr(node);\n-    }\n+    const graph = this.reid();\n+    const firstNode = graph.firstNode();\n \n-    const firstNode = this.firstNode();\n-    const firstNodeLabel = firstNode ? nodeDataStr(firstNode) : undefined;\n-\n-    const lastNode = this.lastNode();\n-    const lastNodeLabel = lastNode ? nodeDataStr(lastNode) : undefined;\n+    const lastNode = graph.lastNode();\n \n-    return drawMermaid(nodes, this.edges, {\n-      firstNodeLabel,\n-      lastNodeLabel,\n+    return drawMermaid(graph.nodes, graph.edges, {\n+      firstNode: firstNode?.id,\n+      lastNode: lastNode?.id,\n       withStyles,\n       curveStyle,\n       nodeColors,\n@@ -256,3 +282,46 @@ export class Graph {\n     });\n   }\n }\n+/**\n+ * Find the single node that is not a target of any edge.\n+ * Exclude nodes/sources with ids in the exclude list.\n+ * If there is no such node, or there are multiple, return undefined.\n+ * When drawing the graph, this node would be the origin.\n+ */\n+function _firstNode(graph: Graph, exclude: string[] = []): Node | undefined {\n+  const targets = new Set(\n+    graph.edges\n+      .filter((edge) => !exclude.includes(edge.source))\n+      .map((edge) => edge.target)\n+  );\n+\n+  const found: Node[] = [];\n+  for (const node of Object.values(graph.nodes)) {\n+    if (!exclude.includes(node.id) && !targets.has(node.id)) {\n+      found.push(node);\n+    }\n+  }\n+  return found.length === 1 ? found[0] : undefined;\n+}\n+\n+/**\n+ * Find the single node that is not a source of any edge.\n+ * Exclude nodes/targets with ids in the exclude list.\n+ * If there is no such node, or there are multiple, return undefined.\n+ * When drawing the graph, this node would be the destination.\n+ */\n+function _lastNode(graph: Graph, exclude: string[] = []): Node | undefined {\n+  const sources = new Set(\n+    graph.edges\n+      .filter((edge) => !exclude.includes(edge.target))\n+      .map((edge) => edge.source)\n+  );\n+\n+  const found: Node[] = [];\n+  for (const node of Object.values(graph.nodes)) {\n+    if (!exclude.includes(node.id) && !sources.has(node.id)) {\n+      found.push(node);\n+    }\n+  }\n+  return found.length === 1 ? found[0] : undefined;\n+}",
          "langchain-core/src/runnables/graph_mermaid.ts": "@@ -1,23 +1,18 @@\n-import { Edge } from \"./types.js\";\n+import { Edge, Node } from \"./types.js\";\n \n function _escapeNodeLabel(nodeLabel: string): string {\n   // Escapes the node label for Mermaid syntax.\n   return nodeLabel.replace(/[^a-zA-Z-_0-9]/g, \"_\");\n }\n \n-// Adjusts Mermaid edge to map conditional nodes to pure nodes.\n-function _adjustMermaidEdge(edge: Edge, nodes: Record<string, string>) {\n-  const sourceNodeLabel = nodes[edge.source] ?? edge.source;\n-  const targetNodeLabel = nodes[edge.target] ?? edge.target;\n-  return [sourceNodeLabel, targetNodeLabel];\n-}\n+const MARKDOWN_SPECIAL_CHARS = [\"*\", \"_\", \"`\"];\n \n function _generateMermaidGraphStyles(\n   nodeColors: Record<string, string>\n ): string {\n   let styles = \"\";\n   for (const [className, color] of Object.entries(nodeColors)) {\n-    styles += `\\tclassDef ${className}class fill:${color};\\n`;\n+    styles += `\\tclassDef ${className} ${color};\\n`;\n   }\n   return styles;\n }\n@@ -26,20 +21,20 @@ function _generateMermaidGraphStyles(\n  * Draws a Mermaid graph using the provided graph data\n  */\n export function drawMermaid(\n-  nodes: Record<string, string>,\n+  nodes: Record<string, Node>,\n   edges: Edge[],\n   config?: {\n-    firstNodeLabel?: string;\n-    lastNodeLabel?: string;\n+    firstNode?: string;\n+    lastNode?: string;\n     curveStyle?: string;\n     withStyles?: boolean;\n     nodeColors?: Record<string, string>;\n     wrapLabelNWords?: number;\n   }\n ): string {\n   const {\n-    firstNodeLabel,\n-    lastNodeLabel,\n+    firstNode,\n+    lastNode,\n     nodeColors,\n     withStyles = true,\n     curveStyle = \"linear\",\n@@ -53,92 +48,126 @@ export function drawMermaid(\n     // Node formatting templates\n     const defaultClassLabel = \"default\";\n     const formatDict: Record<string, string> = {\n-      [defaultClassLabel]: \"{0}([{1}]):::otherclass\",\n+      [defaultClassLabel]: \"{0}({1})\",\n     };\n-    if (firstNodeLabel !== undefined) {\n-      formatDict[firstNodeLabel] = \"{0}[{0}]:::startclass\";\n+    if (firstNode !== undefined) {\n+      formatDict[firstNode] = \"{0}([{1}]):::first\";\n     }\n-    if (lastNodeLabel !== undefined) {\n-      formatDict[lastNodeLabel] = \"{0}[{0}]:::endclass\";\n+    if (lastNode !== undefined) {\n+      formatDict[lastNode] = \"{0}([{1}]):::last\";\n     }\n \n     // Add nodes to the graph\n-    for (const node of Object.values(nodes)) {\n-      const nodeLabel = formatDict[node] ?? formatDict[defaultClassLabel];\n-      const escapedNodeLabel = _escapeNodeLabel(node);\n-      const nodeParts = node.split(\":\");\n-      const nodeSplit = nodeParts[nodeParts.length - 1];\n-      mermaidGraph += `\\t${nodeLabel\n-        .replace(/\\{0\\}/g, escapedNodeLabel)\n-        .replace(/\\{1\\}/g, nodeSplit)};\\n`;\n+    for (const [key, node] of Object.entries(nodes)) {\n+      const nodeName = node.name.split(\":\").pop() ?? \"\";\n+      const label = MARKDOWN_SPECIAL_CHARS.some(\n+        (char) => nodeName.startsWith(char) && nodeName.endsWith(char)\n+      )\n+        ? `<p>${nodeName}</p>`\n+        : nodeName;\n+\n+      let finalLabel = label;\n+      if (Object.keys(node.metadata ?? {}).length) {\n+        finalLabel += `<hr/><small><em>${Object.entries(node.metadata ?? {})\n+          .map(([k, v]) => `${k} = ${v}`)\n+          .join(\"\\n\")}</em></small>`;\n+      }\n+\n+      const nodeLabel = (formatDict[key] ?? formatDict[defaultClassLabel])\n+        .replace(\"{0}\", _escapeNodeLabel(key))\n+        .replace(\"{1}\", finalLabel);\n+\n+      mermaidGraph += `\\t${nodeLabel}\\n`;\n     }\n   }\n-  let subgraph = \"\";\n-  // Add edges to the graph\n+\n+  // Group edges by their common prefixes\n+  const edgeGroups: Record<string, Edge[]> = {};\n   for (const edge of edges) {\n-    const sourcePrefix = edge.source.includes(\":\")\n-      ? edge.source.split(\":\")[0]\n-      : undefined;\n-    const targetPrefix = edge.target.includes(\":\")\n-      ? edge.target.split(\":\")[0]\n-      : undefined;\n-    // Exit subgraph if source or target is not in the same subgraph\n-    if (\n-      subgraph !== \"\" &&\n-      (subgraph !== sourcePrefix || subgraph !== targetPrefix)\n-    ) {\n-      mermaidGraph += \"\\tend\\n\";\n-      subgraph = \"\";\n+    const srcParts = edge.source.split(\":\");\n+    const tgtParts = edge.target.split(\":\");\n+    const commonPrefix = srcParts\n+      .filter((src, i) => src === tgtParts[i])\n+      .join(\":\");\n+    if (!edgeGroups[commonPrefix]) {\n+      edgeGroups[commonPrefix] = [];\n     }\n-    // Enter subgraph if source and target are in the same subgraph\n-    if (\n-      subgraph === \"\" &&\n-      sourcePrefix !== undefined &&\n-      sourcePrefix === targetPrefix\n-    ) {\n-      mermaidGraph = `\\tsubgraph ${sourcePrefix}\\n`;\n-      subgraph = sourcePrefix;\n-    }\n-    const [source, target] = _adjustMermaidEdge(edge, nodes);\n-    let edgeLabel = \"\";\n-    // Add BR every wrapLabelNWords words\n-    if (edge.data !== undefined) {\n-      let edgeData = edge.data;\n-      const words = edgeData.split(\" \");\n-      // Group words into chunks of wrapLabelNWords size\n-      if (words.length > wrapLabelNWords) {\n-        edgeData = words\n-          .reduce((acc: string[], word: string, i: number) => {\n-            if (i % wrapLabelNWords === 0) acc.push(\"\");\n-            acc[acc.length - 1] += ` ${word}`;\n-            return acc;\n-          }, [])\n-          .join(\"<br>\");\n+    edgeGroups[commonPrefix].push(edge);\n+  }\n+\n+  const seenSubgraphs = new Set<string>();\n+\n+  function addSubgraph(edges: Edge[], prefix: string): void {\n+    const selfLoop = edges.length === 1 && edges[0].source === edges[0].target;\n+    if (prefix && !selfLoop) {\n+      const subgraph = prefix.split(\":\").pop()!;\n+      if (seenSubgraphs.has(subgraph)) {\n+        throw new Error(\n+          `Found duplicate subgraph '${subgraph}' -- this likely means that ` +\n+            \"you're reusing a subgraph node with the same name. \" +\n+            \"Please adjust your graph to have subgraph nodes with unique names.\"\n+        );\n       }\n-      if (edge.conditional) {\n-        edgeLabel = ` -. ${edgeData} .-> `;\n+\n+      seenSubgraphs.add(subgraph);\n+      mermaidGraph += `\\tsubgraph ${subgraph}\\n`;\n+    }\n+\n+    for (const edge of edges) {\n+      const { source, target, data, conditional } = edge;\n+\n+      let edgeLabel = \"\";\n+      if (data !== undefined) {\n+        let edgeData = data;\n+        const words = edgeData.split(\" \");\n+        if (words.length > wrapLabelNWords) {\n+          edgeData = Array.from(\n+            { length: Math.ceil(words.length / wrapLabelNWords) },\n+            (_, i) =>\n+              words\n+                .slice(i * wrapLabelNWords, (i + 1) * wrapLabelNWords)\n+                .join(\" \")\n+          ).join(\"&nbsp;<br>&nbsp;\");\n+        }\n+        edgeLabel = conditional\n+          ? ` -. &nbsp;${edgeData}&nbsp; .-> `\n+          : ` -- &nbsp;${edgeData}&nbsp; --> `;\n       } else {\n-        edgeLabel = ` -- ${edgeData} --> `;\n+        edgeLabel = conditional ? \" -.-> \" : \" --> \";\n       }\n-    } else {\n-      if (edge.conditional) {\n-        edgeLabel = ` -.-> `;\n-      } else {\n-        edgeLabel = ` --> `;\n+\n+      mermaidGraph += `\\t${_escapeNodeLabel(\n+        source\n+      )}${edgeLabel}${_escapeNodeLabel(target)};\\n`;\n+    }\n+\n+    // Recursively add nested subgraphs\n+    for (const nestedPrefix in edgeGroups) {\n+      if (nestedPrefix.startsWith(`${prefix}:`) && nestedPrefix !== prefix) {\n+        addSubgraph(edgeGroups[nestedPrefix], nestedPrefix);\n       }\n     }\n-    mermaidGraph += `\\t${_escapeNodeLabel(\n-      source\n-    )}${edgeLabel}${_escapeNodeLabel(target)};\\n`;\n+\n+    if (prefix && !selfLoop) {\n+      mermaidGraph += \"\\tend\\n\";\n+    }\n   }\n-  if (subgraph !== \"\") {\n-    mermaidGraph += \"end\\n\";\n+\n+  // Start with the top-level edges (no common prefix)\n+  addSubgraph(edgeGroups[\"\"] ?? [], \"\");\n+\n+  // Add remaining subgraphs\n+  for (const prefix in edgeGroups) {\n+    if (!prefix.includes(\":\") && prefix !== \"\") {\n+      addSubgraph(edgeGroups[prefix], prefix);\n+    }\n   }\n \n   // Add custom styles for nodes\n-  if (withStyles && nodeColors !== undefined) {\n-    mermaidGraph += _generateMermaidGraphStyles(nodeColors);\n+  if (withStyles) {\n+    mermaidGraph += _generateMermaidGraphStyles(nodeColors ?? {});\n   }\n+\n   return mermaidGraph;\n }\n ",
          "langchain-core/src/runnables/history.ts": "@@ -115,9 +115,9 @@ export class RunnableWithMessageHistory<\n   getMessageHistory: GetSessionHistoryCallable;\n \n   constructor(fields: RunnableWithMessageHistoryInputs<RunInput, RunOutput>) {\n-    let historyChain: Runnable = new RunnableLambda({\n-      func: (input, options) => this._enterHistory(input, options ?? {}),\n-    }).withConfig({ runName: \"loadHistory\" });\n+    let historyChain: Runnable = RunnableLambda.from((input, options) =>\n+      this._enterHistory(input, options ?? {})\n+    ).withConfig({ runName: \"loadHistory\" });\n \n     const messagesKey = fields.historyMessagesKey ?? fields.inputMessagesKey;\n     if (messagesKey) {",
          "langchain-core/src/runnables/tests/runnable.test.ts": "@@ -427,10 +427,14 @@ test(\"Create a runnable sequence with a static method with invalid output and ca\n     }\n   };\n   const runnable = RunnableSequence.from([promptTemplate, llm, parser]);\n-  await expect(async () => {\n-    const result = await runnable.invoke({ input: \"Hello sequence!\" });\n-    console.log(result);\n-  }).rejects.toThrow(OutputParserException);\n+  let error: any | undefined;\n+  try {\n+    await runnable.invoke({ input: \"Hello sequence!\" });\n+  } catch (e: any) {\n+    error = e;\n+  }\n+  expect(error).toBeInstanceOf(OutputParserException);\n+  expect(error?.lc_error_code).toEqual(\"OUTPUT_PARSING_FAILURE\");\n });\n \n test(\"RunnableSequence can pass config to every step in batched request\", async () => {",
          "langchain-core/src/runnables/tests/runnable_graph.test.ts": "@@ -91,17 +91,17 @@ test(\"Test graph sequence\", async () => {\n   expect(graph.drawMermaid())\n     .toEqual(`%%{init: {'flowchart': {'curve': 'linear'}}}%%\n graph TD;\n-\\tPromptTemplateInput[PromptTemplateInput]:::startclass;\n-\\tPromptTemplate([PromptTemplate]):::otherclass;\n-\\tFakeLLM([FakeLLM]):::otherclass;\n-\\tCommaSeparatedListOutputParser([CommaSeparatedListOutputParser]):::otherclass;\n-\\tCommaSeparatedListOutputParserOutput[CommaSeparatedListOutputParserOutput]:::endclass;\n+\\tPromptTemplateInput([PromptTemplateInput]):::first\n+\\tPromptTemplate(PromptTemplate)\n+\\tFakeLLM(FakeLLM)\n+\\tCommaSeparatedListOutputParser(CommaSeparatedListOutputParser)\n+\\tCommaSeparatedListOutputParserOutput([CommaSeparatedListOutputParserOutput]):::last\n \\tPromptTemplateInput --> PromptTemplate;\n \\tPromptTemplate --> FakeLLM;\n \\tCommaSeparatedListOutputParser --> CommaSeparatedListOutputParserOutput;\n \\tFakeLLM --> CommaSeparatedListOutputParser;\n-\\tclassDef startclass fill:#ffdfba;\n-\\tclassDef endclass fill:#baffc9;\n-\\tclassDef otherclass fill:#fad7de;\n+\\tclassDef default fill:#f2f0ff,line-height:1.2;\n+\\tclassDef first fill-opacity:0;\n+\\tclassDef last fill:#bfb6fc;\n `);\n });",
          "langchain-core/src/runnables/tests/runnable_stream_events_v2.test.ts": "@@ -4,6 +4,7 @@\n \n import { test, expect, afterEach } from \"@jest/globals\";\n import { z } from \"zod\";\n+import { AsyncLocalStorage } from \"node:async_hooks\";\n import {\n   RunnableLambda,\n   RunnableMap,\n@@ -28,8 +29,9 @@ import { DynamicStructuredTool, DynamicTool, tool } from \"../../tools/index.js\";\n import { Document } from \"../../documents/document.js\";\n import { PromptTemplate } from \"../../prompts/prompt.js\";\n import { GenerationChunk } from \"../../outputs.js\";\n-// Import from web to avoid side-effects from AsyncLocalStorage\n+// Import from web to avoid top-level side-effects from AsyncLocalStorage\n import { dispatchCustomEvent } from \"../../callbacks/dispatch/web.js\";\n+import { AsyncLocalStorageProviderSingleton } from \"../../singletons/index.js\";\n \n function reverse(s: string) {\n   // Reverse a string.\n@@ -138,6 +140,73 @@ test(\"Runnable streamEvents method on a chat model\", async () => {\n   ]);\n });\n \n+test(\"Runnable streamEvents call nested in another runnable + passed callbacks should still work\", async () => {\n+  AsyncLocalStorageProviderSingleton.initializeGlobalInstance(\n+    new AsyncLocalStorage()\n+  );\n+\n+  const model = new FakeListChatModel({\n+    responses: [\"abc\"],\n+  });\n+\n+  const events: any[] = [];\n+  const container = RunnableLambda.from(async (_) => {\n+    const eventStream = model.streamEvents(\"hello\", { version: \"v2\" });\n+    for await (const event of eventStream) {\n+      events.push(event);\n+    }\n+    return events;\n+  });\n+\n+  await container.invoke({}, { callbacks: [{ handleLLMStart: () => {} }] });\n+\n+  // used here to avoid casting every ID\n+  const anyString = expect.any(String) as unknown as string;\n+\n+  expect(events).toMatchObject([\n+    {\n+      data: { input: \"hello\" },\n+      event: \"on_chat_model_start\",\n+      name: \"FakeListChatModel\",\n+      metadata: expect.any(Object),\n+      run_id: expect.any(String),\n+      tags: [],\n+    },\n+    {\n+      data: { chunk: new AIMessageChunk({ id: anyString, content: \"a\" }) },\n+      event: \"on_chat_model_stream\",\n+      name: \"FakeListChatModel\",\n+      metadata: expect.any(Object),\n+      run_id: expect.any(String),\n+      tags: [],\n+    },\n+    {\n+      data: { chunk: new AIMessageChunk({ id: anyString, content: \"b\" }) },\n+      event: \"on_chat_model_stream\",\n+      name: \"FakeListChatModel\",\n+      metadata: expect.any(Object),\n+      run_id: expect.any(String),\n+      tags: [],\n+    },\n+    {\n+      data: { chunk: new AIMessageChunk({ id: anyString, content: \"c\" }) },\n+      event: \"on_chat_model_stream\",\n+      name: \"FakeListChatModel\",\n+      metadata: expect.any(Object),\n+      run_id: expect.any(String),\n+      tags: [],\n+    },\n+    {\n+      data: { output: new AIMessageChunk({ id: anyString, content: \"abc\" }) },\n+      event: \"on_chat_model_end\",\n+      name: \"FakeListChatModel\",\n+      metadata: expect.any(Object),\n+      run_id: expect.any(String),\n+      tags: [],\n+    },\n+  ]);\n+});\n+\n test(\"Runnable streamEvents method with three runnables\", async () => {\n   const r = RunnableLambda.from(reverse);\n ",
          "langchain-core/src/runnables/types.ts": "@@ -71,16 +71,22 @@ export interface Edge {\n \n export interface Node {\n   id: string;\n+  name: string;\n   data: RunnableIOSchema | RunnableInterface;\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  metadata?: Record<string, any>;\n }\n \n-export interface RunnableConfig extends BaseCallbackConfig {\n+export interface RunnableConfig<\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  ConfigurableFieldType extends Record<string, any> = Record<string, any>\n+> extends BaseCallbackConfig {\n   /**\n    * Runtime values for attributes previously made configurable on this Runnable,\n    * or sub-Runnables.\n    */\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-  configurable?: Record<string, any>;\n+  configurable?: ConfigurableFieldType;\n \n   /**\n    * Maximum number of times a call can recurse. If not provided, defaults to 25.",
          "langchain-core/src/singletons/index.ts": "@@ -7,6 +7,8 @@ export interface AsyncLocalStorageInterface {\n   getStore: () => any | undefined;\n \n   run: <T>(store: any, callback: () => T) => T;\n+\n+  enterWith: (store: any) => void;\n }\n \n export class MockAsyncLocalStorage implements AsyncLocalStorageInterface {\n@@ -17,13 +19,19 @@ export class MockAsyncLocalStorage implements AsyncLocalStorageInterface {\n   run<T>(_store: any, callback: () => T): T {\n     return callback();\n   }\n+\n+  enterWith(_store: any) {\n+    return undefined;\n+  }\n }\n \n const mockAsyncLocalStorage = new MockAsyncLocalStorage();\n \n const TRACING_ALS_KEY = Symbol.for(\"ls:tracing_async_local_storage\");\n const LC_CHILD_KEY = Symbol.for(\"lc:child_config\");\n \n+export const _CONTEXT_VARIABLES_KEY = Symbol.for(\"lc:context_variables\");\n+\n class AsyncLocalStorageProvider {\n   getInstance(): AsyncLocalStorageInterface {\n     return (globalThis as any)[TRACING_ALS_KEY] ?? mockAsyncLocalStorage;\n@@ -50,6 +58,7 @@ class AsyncLocalStorageProvider {\n       config?.metadata\n     );\n     const storage = this.getInstance();\n+    const previousValue = storage.getStore();\n     const parentRunId = callbackManager?.getParentRunId();\n \n     const langChainTracer = callbackManager?.handlers?.find(\n@@ -70,6 +79,14 @@ class AsyncLocalStorageProvider {\n       runTree.extra = { ...runTree.extra, [LC_CHILD_KEY]: config };\n     }\n \n+    if (\n+      previousValue !== undefined &&\n+      previousValue[_CONTEXT_VARIABLES_KEY] !== undefined\n+    ) {\n+      (runTree as any)[_CONTEXT_VARIABLES_KEY] =\n+        previousValue[_CONTEXT_VARIABLES_KEY];\n+    }\n+\n     return storage.run(runTree, callback);\n   }\n ",
          "langchain-core/src/tests/context.test.ts": "@@ -0,0 +1,26 @@\n+import { test, expect } from \"@jest/globals\";\n+import { RunnableLambda } from \"../runnables/base.js\";\n+import { getContextVariable, setContextVariable } from \"../context.js\";\n+\n+test(\"Getting and setting context variables within nested runnables\", async () => {\n+  const nested = RunnableLambda.from(() => {\n+    expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+    expect(getContextVariable(\"toplevel\")).toEqual(9);\n+    setContextVariable(\"foo\", \"baz\");\n+    return getContextVariable(\"foo\");\n+  });\n+  const runnable = RunnableLambda.from(async () => {\n+    setContextVariable(\"foo\", \"bar\");\n+    expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+    expect(getContextVariable(\"toplevel\")).toEqual(9);\n+    const res = await nested.invoke({});\n+    expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+    return res;\n+  });\n+  expect(getContextVariable(\"foo\")).toEqual(undefined);\n+  setContextVariable(\"toplevel\", 9);\n+  expect(getContextVariable(\"toplevel\")).toEqual(9);\n+  const result = await runnable.invoke({});\n+  expect(getContextVariable(\"toplevel\")).toEqual(9);\n+  expect(result).toEqual(\"baz\");\n+});",
          "langchain-core/src/tools/index.ts": "@@ -586,9 +586,25 @@ export function tool<\n         fields.description ??\n         fields.schema?.description ??\n         `${fields.name} tool`,\n-      // TS doesn't restrict the type here based on the guard above\n-      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-      func: func as any,\n+      func: async (input, runManager, config) => {\n+        return new Promise((resolve, reject) => {\n+          const childConfig = patchConfig(config, {\n+            callbacks: runManager?.getChild(),\n+          });\n+          void AsyncLocalStorageProviderSingleton.runWithConfig(\n+            childConfig,\n+            async () => {\n+              try {\n+                // TS doesn't restrict the type here based on the guard above\n+                // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+                resolve(func(input as any, childConfig));\n+              } catch (e) {\n+                reject(e);\n+              }\n+            }\n+          );\n+        });\n+      },\n     });\n   }\n \n@@ -606,7 +622,7 @@ export function tool<\n         const childConfig = patchConfig(config, {\n           callbacks: runManager?.getChild(),\n         });\n-        void AsyncLocalStorageProviderSingleton.getInstance().run(\n+        void AsyncLocalStorageProviderSingleton.runWithConfig(\n           childConfig,\n           async () => {\n             try {",
          "langchain-core/src/tools/tests/tools.test.ts": "@@ -1,5 +1,6 @@\n import { test, expect } from \"@jest/globals\";\n import { z } from \"zod\";\n+\n import { DynamicStructuredTool, tool } from \"../index.js\";\n import { ToolMessage } from \"../../messages/tool.js\";\n \n@@ -102,7 +103,8 @@ test(\"Returns tool message if responseFormat is content_and_artifact and returns\n \n test(\"Tool can accept single string input\", async () => {\n   const stringTool = tool<z.ZodString>(\n-    (input: string): string => {\n+    (input: string, config): string => {\n+      expect(config).toMatchObject({ configurable: { foo: \"bar\" } });\n       return `${input}a`;\n     },\n     {\n@@ -112,7 +114,7 @@ test(\"Tool can accept single string input\", async () => {\n     }\n   );\n \n-  const result = await stringTool.invoke(\"b\");\n+  const result = await stringTool.invoke(\"b\", { configurable: { foo: \"bar\" } });\n   expect(result).toBe(\"ba\");\n });\n ",
          "langchain-core/src/tracers/event_stream.ts": "@@ -364,7 +364,7 @@ export class EventStreamCallbackHandler extends BaseTracer {\n       throw new Error(`onLLMNewToken: Run ID ${run.id} not found in run map.`);\n     }\n     // Top-level streaming events are covered by tapOutputIterable\n-    if (run.parent_run_id === undefined) {\n+    if (this.runInfoMap.size === 1) {\n       return;\n     }\n     if (runInfo.runType === \"chat_model\") {",
          "langchain-core/src/tracers/tests/langsmith_interop.test.ts": "@@ -178,6 +178,155 @@ test.each([\"true\", \"false\"])(\n   }\n );\n \n+test.each([\"true\", \"false\"])(\n+  \"traceables nested within runnables with a context var set and with background callbacks %s\",\n+  async (value) => {\n+    const { setContextVariable, getContextVariable } = await import(\n+      \"../../context.js\"\n+    );\n+    process.env.LANGCHAIN_CALLBACKS_BACKGROUND = value;\n+\n+    setContextVariable(\"foo\", \"bar\");\n+    const aiGreet = traceable(\n+      async (msg: BaseMessage, name = \"world\") => {\n+        await new Promise((resolve) => setTimeout(resolve, 300));\n+        expect(getContextVariable(\"foo\")).toEqual(\"baz\");\n+        return msg.content + name;\n+      },\n+      { name: \"aiGreet\", tracingEnabled: true }\n+    );\n+\n+    const root = RunnableLambda.from(async (messages: BaseMessage[]) => {\n+      const lastMsg = messages.at(-1) as HumanMessage;\n+      expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+      setContextVariable(\"foo\", \"baz\");\n+      const greetOne = await aiGreet(lastMsg, \"David\");\n+\n+      return [greetOne];\n+    });\n+\n+    await root.invoke([new HumanMessage({ content: \"Hello!\" })]);\n+\n+    const relevantCalls = fetchMock.mock.calls.filter((call: any) => {\n+      return call[0].startsWith(\"https://api.smith.langchain.com/runs\");\n+    });\n+\n+    expect(relevantCalls.length).toEqual(4);\n+    const firstCallParams = JSON.parse((relevantCalls[0][1] as any).body);\n+    const secondCallParams = JSON.parse((relevantCalls[1][1] as any).body);\n+    const thirdCallParams = JSON.parse((relevantCalls[2][1] as any).body);\n+    const fourthCallParams = JSON.parse((relevantCalls[3][1] as any).body);\n+    expect(firstCallParams).toMatchObject({\n+      id: firstCallParams.id,\n+      name: \"RunnableLambda\",\n+      start_time: expect.any(Number),\n+      serialized: {\n+        lc: 1,\n+        type: \"not_implemented\",\n+        id: [\"langchain_core\", \"runnables\", \"RunnableLambda\"],\n+      },\n+      events: [{ name: \"start\", time: expect.any(String) }],\n+      inputs: {\n+        input: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+        ],\n+      },\n+      execution_order: 1,\n+      child_execution_order: 1,\n+      run_type: \"chain\",\n+      extra: expect.any(Object),\n+      tags: [],\n+      trace_id: firstCallParams.id,\n+      dotted_order: expect.any(String),\n+    });\n+    expect(secondCallParams).toMatchObject({\n+      id: expect.any(String),\n+      name: \"aiGreet\",\n+      start_time: expect.any(Number),\n+      run_type: \"chain\",\n+      extra: expect.any(Object),\n+      serialized: {},\n+      inputs: {\n+        args: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+          \"David\",\n+        ],\n+      },\n+      child_runs: [],\n+      parent_run_id: firstCallParams.id,\n+      trace_id: firstCallParams.id,\n+      dotted_order: expect.stringContaining(`${firstCallParams.dotted_order}.`),\n+      tags: [],\n+    });\n+    expect(thirdCallParams).toMatchObject({\n+      end_time: expect.any(Number),\n+      inputs: {\n+        args: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+          \"David\",\n+        ],\n+      },\n+      outputs: { outputs: \"Hello!David\" },\n+      parent_run_id: firstCallParams.id,\n+      extra: expect.any(Object),\n+      dotted_order: secondCallParams.dotted_order,\n+      trace_id: firstCallParams.id,\n+      tags: [],\n+    });\n+    expect(fourthCallParams).toMatchObject({\n+      end_time: expect.any(Number),\n+      outputs: { output: [\"Hello!David\"] },\n+      events: [\n+        { name: \"start\", time: expect.any(String) },\n+        { name: \"end\", time: expect.any(String) },\n+      ],\n+      inputs: {\n+        input: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+        ],\n+      },\n+      trace_id: firstCallParams.id,\n+      dotted_order: firstCallParams.dotted_order,\n+    });\n+  }\n+);\n+\n test.each([\"true\", \"false\"])(\n   \"streaming traceables nested within runnables with background callbacks %s\",\n   async (value) => {\n@@ -457,6 +606,153 @@ test.each([\"true\", \"false\"])(\n   }\n );\n \n+test.each([\"true\", \"false\"])(\n+  \"runnables nested within traceables and a context var set with background callbacks %s\",\n+  async (value) => {\n+    const { setContextVariable, getContextVariable } = await import(\n+      \"../../context.js\"\n+    );\n+    process.env.LANGCHAIN_CALLBACKS_BACKGROUND = value;\n+    setContextVariable(\"foo\", \"bar\");\n+\n+    const nested = RunnableLambda.from(async (messages: BaseMessage[]) => {\n+      const lastMsg = messages.at(-1) as HumanMessage;\n+      await new Promise((resolve) => setTimeout(resolve, 300));\n+      expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+      return [lastMsg.content];\n+    });\n+\n+    const aiGreet = traceable(\n+      async (msg: BaseMessage, name = \"world\") => {\n+        const contents = await nested.invoke([msg]);\n+        expect(getContextVariable(\"foo\")).toEqual(\"bar\");\n+        return contents[0] + name;\n+      },\n+      { name: \"aiGreet\", tracingEnabled: true }\n+    );\n+\n+    await aiGreet(new HumanMessage({ content: \"Hello!\" }), \"mitochondria\");\n+\n+    const relevantCalls = fetchMock.mock.calls.filter((call: any) => {\n+      return call[0].startsWith(\"https://api.smith.langchain.com/runs\");\n+    });\n+\n+    expect(relevantCalls.length).toEqual(4);\n+    const firstCallParams = JSON.parse((relevantCalls[0][1] as any).body);\n+    const secondCallParams = JSON.parse((relevantCalls[1][1] as any).body);\n+    const thirdCallParams = JSON.parse((relevantCalls[2][1] as any).body);\n+    const fourthCallParams = JSON.parse((relevantCalls[3][1] as any).body);\n+    expect(firstCallParams).toMatchObject({\n+      id: firstCallParams.id,\n+      name: \"aiGreet\",\n+      start_time: expect.any(Number),\n+      run_type: \"chain\",\n+      extra: expect.any(Object),\n+      serialized: {},\n+      inputs: {\n+        args: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+          \"mitochondria\",\n+        ],\n+      },\n+      child_runs: [],\n+      trace_id: firstCallParams.id,\n+      dotted_order: firstCallParams.dotted_order,\n+      tags: [],\n+    });\n+    expect(secondCallParams).toMatchObject({\n+      id: secondCallParams.id,\n+      name: \"RunnableLambda\",\n+      parent_run_id: firstCallParams.id,\n+      start_time: expect.any(Number),\n+      serialized: {\n+        lc: 1,\n+        type: \"not_implemented\",\n+        id: [\"langchain_core\", \"runnables\", \"RunnableLambda\"],\n+      },\n+      events: [{ name: \"start\", time: expect.any(String) }],\n+      inputs: {\n+        input: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+        ],\n+      },\n+      execution_order: 2,\n+      child_execution_order: 2,\n+      run_type: \"chain\",\n+      extra: expect.any(Object),\n+      tags: [],\n+      trace_id: firstCallParams.id,\n+      dotted_order: expect.stringContaining(`${firstCallParams.dotted_order}.`),\n+    });\n+    expect(thirdCallParams).toMatchObject({\n+      end_time: expect.any(Number),\n+      outputs: { output: [\"Hello!\"] },\n+      events: [\n+        { name: \"start\", time: expect.any(String) },\n+        { name: \"end\", time: expect.any(String) },\n+      ],\n+      inputs: {\n+        input: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+        ],\n+      },\n+      trace_id: firstCallParams.id,\n+      dotted_order: expect.stringContaining(`${firstCallParams.dotted_order}.`),\n+      parent_run_id: firstCallParams.id,\n+    });\n+    expect(fourthCallParams).toMatchObject({\n+      end_time: expect.any(Number),\n+      inputs: {\n+        args: [\n+          {\n+            lc: 1,\n+            type: \"constructor\",\n+            id: [\"langchain_core\", \"messages\", \"HumanMessage\"],\n+            kwargs: {\n+              content: \"Hello!\",\n+              additional_kwargs: {},\n+              response_metadata: {},\n+            },\n+          },\n+          \"mitochondria\",\n+        ],\n+      },\n+      outputs: { outputs: \"Hello!mitochondria\" },\n+      extra: expect.any(Object),\n+      dotted_order: firstCallParams.dotted_order,\n+      trace_id: firstCallParams.id,\n+      tags: [],\n+    });\n+  }\n+);\n+\n test.each([\"true\", \"false\"])(\n   \"streaming runnables nested within traceables with background callbacks %s\",\n   async (value) => {",
          "langchain-core/src/utils/testing/index.ts": "@@ -354,6 +354,8 @@ export class FakeListChatModel extends BaseChatModel<FakeListChatModelCallOption\n     return \"FakeListChatModel\";\n   }\n \n+  lc_serializable = true;\n+\n   responses: string[];\n \n   i = 0;\n@@ -362,8 +364,9 @@ export class FakeListChatModel extends BaseChatModel<FakeListChatModelCallOption\n \n   emitCustomEvent = false;\n \n-  constructor({ responses, sleep, emitCustomEvent }: FakeChatInput) {\n-    super({});\n+  constructor(params: FakeChatInput) {\n+    super(params);\n+    const { responses, sleep, emitCustomEvent } = params;\n     this.responses = responses;\n     this.sleep = sleep;\n     this.emitCustomEvent = emitCustomEvent ?? this.emitCustomEvent;",
          "langchain-core/src/utils/tests/polyfill_stream.test.ts": "@@ -1,4 +1,4 @@\n-import \"web-streams-polyfill\";\n+import \"web-streams-polyfill/polyfill\";\n import { test, expect } from \"@jest/globals\";\n import { FakeStreamingLLM } from \"../testing/index.js\";\n import { StringOutputParser } from \"../../output_parsers/string.js\";",
          "langchain/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"langchain\",\n-  \"version\": \"0.3.2\",\n+  \"version\": \"0.3.3\",\n   \"description\": \"Typescript bindings for langchain\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -520,7 +520,7 @@\n     \"js-tiktoken\": \"^1.0.12\",\n     \"js-yaml\": \"^4.1.0\",\n     \"jsonpointer\": \"^5.0.1\",\n-    \"langsmith\": \"^0.1.56-rc.1\",\n+    \"langsmith\": \"^0.1.56\",\n     \"openapi-types\": \"^12.1.3\",\n     \"p-retry\": \"4\",\n     \"uuid\": \"^10.0.0\",",
          "langchain/src/chains/graph_qa/cypher.ts": "@@ -39,6 +39,8 @@ export interface FromLLMInput {\n }\n \n /**\n+ * Chain for question-answering against a graph by generating Cypher statements.\n+ *\n  * @example\n  * ```typescript\n  * const chain = new GraphCypherQAChain({\n@@ -47,6 +49,18 @@ export interface FromLLMInput {\n  * });\n  * const res = await chain.invoke(\"Who played in Pulp Fiction?\");\n  * ```\n+ *\n+ * @security\n+ * This chain will execute Cypher statements against the provided database.\n+ * Make sure that the database connection uses credentials\n+ * that are narrowly-scoped to only include necessary permissions.\n+ * Failure to do so may result in data corruption or loss, since the calling code\n+ * may attempt commands that would result in deletion, mutation of data\n+ * if appropriately prompted or reading sensitive data if such data is present in the database.\n+ * The best way to guard against such negative outcomes is to (as appropriate) limit the\n+ * permissions granted to the credentials used with this tool.\n+ *\n+ * See https://js.langchain.com/docs/security for more information.\n  */\n export class GraphCypherQAChain extends BaseChain {\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any",
          "langchain/src/chat_models/universal.ts": "@@ -1,3 +1,4 @@\n+/* eslint-disable import/no-extraneous-dependencies */\n import {\n   BaseLanguageModelInput,\n   ToolDefinition,",
          "langchain/src/output_parsers/openai_tools.ts": "@@ -106,6 +106,7 @@ export type JsonOutputKeyToolsParserParams = {\n /**\n  * @deprecated Import from \"@langchain/core/output_parsers/openai_tools\"\n  */\n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n export class JsonOutputKeyToolsParser extends BaseLLMOutputParser<any> {\n   static lc_name() {\n     return \"JsonOutputKeyToolsParser\";",
          "libs/langchain-anthropic/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/anthropic\",\n-  \"version\": \"0.3.0\",\n+  \"version\": \"0.3.5\",\n   \"description\": \"Anthropic integrations for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -35,7 +35,7 @@\n   \"author\": \"LangChain\",\n   \"license\": \"MIT\",\n   \"dependencies\": {\n-    \"@anthropic-ai/sdk\": \"^0.25.2\",\n+    \"@anthropic-ai/sdk\": \"^0.27.3\",\n     \"fast-xml-parser\": \"^4.4.1\",\n     \"zod\": \"^3.22.4\",\n     \"zod-to-json-schema\": \"^3.22.4\"",
          "libs/langchain-anthropic/src/chat_models.ts": "@@ -33,7 +33,7 @@ import type {\n import { isLangChainTool } from \"@langchain/core/utils/function_calling\";\n import { AnthropicToolsOutputParser } from \"./output_parsers.js\";\n import { extractToolCallChunk, handleToolChoice } from \"./utils/tools.js\";\n-import { _formatMessagesForAnthropic } from \"./utils/message_inputs.js\";\n+import { _convertMessagesToAnthropicPayload } from \"./utils/message_inputs.js\";\n import {\n   _makeMessageChunkFromAnthropicEvent,\n   anthropicResponseToChatMessages,\n@@ -46,6 +46,7 @@ import {\n   AnthropicToolChoice,\n   ChatAnthropicToolType,\n } from \"./types.js\";\n+import { wrapAnthropicClientError } from \"./utils/errors.js\";\n \n export interface ChatAnthropicCallOptions\n   extends BaseChatModelCallOptions,\n@@ -782,7 +783,7 @@ export class ChatAnthropicMessages<\n     runManager?: CallbackManagerForLLMRun\n   ): AsyncGenerator<ChatGenerationChunk> {\n     const params = this.invocationParams(options);\n-    const formattedMessages = _formatMessagesForAnthropic(messages);\n+    const formattedMessages = _convertMessagesToAnthropicPayload(messages);\n     const coerceContentToString = !_toolsInParams({\n       ...params,\n       ...formattedMessages,\n@@ -818,7 +819,7 @@ export class ChatAnthropicMessages<\n \n       // Extract the text content token for text field and runManager.\n       const token = extractToken(chunk);\n-      yield new ChatGenerationChunk({\n+      const generationChunk = new ChatGenerationChunk({\n         message: new AIMessageChunk({\n           // Just yield chunk as it is and tool_use will be concat by BaseChatModel._generateUncached().\n           content: chunk.content,\n@@ -830,10 +831,16 @@ export class ChatAnthropicMessages<\n         }),\n         text: token ?? \"\",\n       });\n-\n-      if (token) {\n-        await runManager?.handleLLMNewToken(token);\n-      }\n+      yield generationChunk;\n+\n+      await runManager?.handleLLMNewToken(\n+        token ?? \"\",\n+        undefined,\n+        undefined,\n+        undefined,\n+        undefined,\n+        { chunk: generationChunk }\n+      );\n     }\n   }\n \n@@ -852,7 +859,7 @@ export class ChatAnthropicMessages<\n       {\n         ...params,\n         stream: false,\n-        ..._formatMessagesForAnthropic(messages),\n+        ..._convertMessagesToAnthropicPayload(messages),\n       },\n       requestOptions\n     );\n@@ -923,22 +930,29 @@ export class ChatAnthropicMessages<\n     if (!this.streamingClient) {\n       const options_ = this.apiUrl ? { baseURL: this.apiUrl } : undefined;\n       this.streamingClient = this.createClient({\n+        dangerouslyAllowBrowser: true,\n         ...this.clientOptions,\n         ...options_,\n         apiKey: this.apiKey,\n         // Prefer LangChain built-in retries\n         maxRetries: 0,\n       });\n     }\n-    const makeCompletionRequest = async () =>\n-      this.streamingClient.messages.create(\n-        {\n-          ...request,\n-          ...this.invocationKwargs,\n-          stream: true,\n-        } as AnthropicStreamingMessageCreateParams,\n-        options\n-      );\n+    const makeCompletionRequest = async () => {\n+      try {\n+        return await this.streamingClient.messages.create(\n+          {\n+            ...request,\n+            ...this.invocationKwargs,\n+            stream: true,\n+          } as AnthropicStreamingMessageCreateParams,\n+          options\n+        );\n+      } catch (e) {\n+        const error = wrapAnthropicClientError(e);\n+        throw error;\n+      }\n+    };\n     return this.caller.call(makeCompletionRequest);\n   }\n \n@@ -950,20 +964,27 @@ export class ChatAnthropicMessages<\n     if (!this.batchClient) {\n       const options = this.apiUrl ? { baseURL: this.apiUrl } : undefined;\n       this.batchClient = this.createClient({\n+        dangerouslyAllowBrowser: true,\n         ...this.clientOptions,\n         ...options,\n         apiKey: this.apiKey,\n         maxRetries: 0,\n       });\n     }\n-    const makeCompletionRequest = async () =>\n-      this.batchClient.messages.create(\n-        {\n-          ...request,\n-          ...this.invocationKwargs,\n-        } as AnthropicMessageCreateParams,\n-        options\n-      );\n+    const makeCompletionRequest = async () => {\n+      try {\n+        return await this.batchClient.messages.create(\n+          {\n+            ...request,\n+            ...this.invocationKwargs,\n+          } as AnthropicMessageCreateParams,\n+          options\n+        );\n+      } catch (e) {\n+        const error = wrapAnthropicClientError(e);\n+        throw error;\n+      }\n+    };\n     return this.caller.callWithOptions(\n       { signal: options.signal ?? undefined },\n       makeCompletionRequest",
          "libs/langchain-anthropic/src/index.ts": "@@ -1 +1,2 @@\n export * from \"./chat_models.js\";\n+export { convertPromptToAnthropic } from \"./utils/prompts.js\";",
          "libs/langchain-anthropic/src/tests/chat_models-tools.int.test.ts": "@@ -1,4 +1,5 @@\n /* eslint-disable no-process-env */\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n \n import { expect, test } from \"@jest/globals\";\n import {\n@@ -11,6 +12,7 @@ import { StructuredTool, tool } from \"@langchain/core/tools\";\n import { concat } from \"@langchain/core/utils/stream\";\n import { z } from \"zod\";\n import { zodToJsonSchema } from \"zod-to-json-schema\";\n+import { RunnableLambda } from \"@langchain/core/runnables\";\n import { ChatAnthropic } from \"../chat_models.js\";\n import { AnthropicToolResponse } from \"../types.js\";\n \n@@ -86,6 +88,36 @@ test(\"Few shotting with tool calls\", async () => {\n   expect(res.content).toContain(\"24\");\n });\n \n+test(\"Invalid tool calls should throw an appropriate error\", async () => {\n+  const chat = model.bindTools([new WeatherTool()]);\n+  let error;\n+  try {\n+    await chat.invoke([\n+      new HumanMessage(\"What is the weather in SF?\"),\n+      new AIMessage({\n+        content: \"Let me look up the current weather.\",\n+        tool_calls: [\n+          {\n+            id: \"toolu_feiwjf9u98r389u498\",\n+            name: \"get_weather\",\n+            args: {\n+              location: \"SF\",\n+            },\n+          },\n+        ],\n+      }),\n+      new ToolMessage({\n+        tool_call_id: \"badbadbad\",\n+        content: \"It is currently 24 degrees with hail in San Francisco.\",\n+      }),\n+    ]);\n+  } catch (e) {\n+    error = e;\n+  }\n+  expect(error).toBeDefined();\n+  expect((error as any).lc_error_code).toEqual(\"INVALID_TOOL_RESULTS\");\n+});\n+\n test(\"Can bind & invoke StructuredTools\", async () => {\n   const tools = [new WeatherTool()];\n \n@@ -202,6 +234,73 @@ test(\"Can bind & stream AnthropicTools\", async () => {\n   expect(args.location).toBeTruthy();\n });\n \n+test(\"stream events with no tool calls has string message content\", async () => {\n+  const wrapper = RunnableLambda.from(async (_, config) => {\n+    const res = await model.invoke(\n+      \"What is the weather in London today?\",\n+      config\n+    );\n+    return res;\n+  });\n+  const eventStream = await wrapper.streamEvents(\n+    \"What is the weather in London today?\",\n+    {\n+      version: \"v2\",\n+    }\n+  );\n+\n+  const chatModelStreamEvents = [];\n+  for await (const event of eventStream) {\n+    if (event.event === \"on_chat_model_stream\") {\n+      chatModelStreamEvents.push(event);\n+    }\n+  }\n+  expect(chatModelStreamEvents.length).toBeGreaterThan(0);\n+  expect(\n+    chatModelStreamEvents.every(\n+      (event) => typeof event.data.chunk.content === \"string\"\n+    )\n+  ).toBe(true);\n+});\n+\n+test(\"stream events with tool calls has raw message content\", async () => {\n+  const modelWithTools = model.bind({\n+    tools: [anthropicTool],\n+    tool_choice: {\n+      type: \"tool\",\n+      name: \"get_weather\",\n+    },\n+  });\n+\n+  const wrapper = RunnableLambda.from(async (_, config) => {\n+    const res = await modelWithTools.invoke(\n+      \"What is the weather in London today?\",\n+      config\n+    );\n+    return res;\n+  });\n+  const eventStream = await wrapper.streamEvents(\n+    \"What is the weather in London today?\",\n+    {\n+      version: \"v2\",\n+    }\n+  );\n+\n+  const chatModelStreamEvents = [];\n+  for await (const event of eventStream) {\n+    if (event.event === \"on_chat_model_stream\") {\n+      console.log(event);\n+      chatModelStreamEvents.push(event);\n+    }\n+  }\n+  expect(chatModelStreamEvents.length).toBeGreaterThan(0);\n+  expect(\n+    chatModelStreamEvents.every((event) =>\n+      Array.isArray(event.data.chunk.content)\n+    )\n+  ).toBe(true);\n+});\n+\n test(\"withStructuredOutput with zod schema\", async () => {\n   const modelWithTools = model.withStructuredOutput<{ location: string }>(\n     zodSchema,",
          "libs/langchain-anthropic/src/tests/chat_models.int.test.ts": "@@ -1,4 +1,5 @@\n /* eslint-disable no-process-env */\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n \n import { expect, test } from \"@jest/globals\";\n import {\n@@ -30,6 +31,39 @@ test(\"Test ChatAnthropic\", async () => {\n   expect(res.response_metadata.usage).toBeDefined();\n });\n \n+test(\"Test ChatAnthropic with a bad API key throws appropriate error\", async () => {\n+  const chat = new ChatAnthropic({\n+    modelName: \"claude-3-sonnet-20240229\",\n+    maxRetries: 0,\n+    apiKey: \"bad\",\n+  });\n+  let error;\n+  try {\n+    const message = new HumanMessage(\"Hello!\");\n+    await chat.invoke([message]);\n+  } catch (e) {\n+    error = e;\n+  }\n+  expect(error).toBeDefined();\n+  expect((error as any).lc_error_code).toEqual(\"MODEL_AUTHENTICATION\");\n+});\n+\n+test(\"Test ChatAnthropic with unknown model throws appropriate error\", async () => {\n+  const chat = new ChatAnthropic({\n+    modelName: \"badbad\",\n+    maxRetries: 0,\n+  });\n+  let error;\n+  try {\n+    const message = new HumanMessage(\"Hello!\");\n+    await chat.invoke([message]);\n+  } catch (e) {\n+    error = e;\n+  }\n+  expect(error).toBeDefined();\n+  expect((error as any).lc_error_code).toEqual(\"MODEL_NOT_FOUND\");\n+});\n+\n test(\"Test ChatAnthropic Generate\", async () => {\n   const chat = new ChatAnthropic({\n     modelName: \"claude-3-sonnet-20240229\",",
          "libs/langchain-anthropic/src/tests/chat_models.test.ts": "@@ -3,7 +3,7 @@ import { AIMessage, HumanMessage, ToolMessage } from \"@langchain/core/messages\";\n import { z } from \"zod\";\n import { OutputParserException } from \"@langchain/core/output_parsers\";\n import { ChatAnthropic } from \"../chat_models.js\";\n-import { _formatMessagesForAnthropic } from \"../utils/message_inputs.js\";\n+import { _convertMessagesToAnthropicPayload } from \"../utils/message_inputs.js\";\n \n test(\"withStructuredOutput with output validation\", async () => {\n   const model = new ChatAnthropic({\n@@ -143,7 +143,7 @@ test(\"Can properly format anthropic messages when given two tool results\", async\n     }),\n   ];\n \n-  const formattedMessages = _formatMessagesForAnthropic(messageHistory);\n+  const formattedMessages = _convertMessagesToAnthropicPayload(messageHistory);\n \n   expect(formattedMessages).toEqual({\n     messages: [",
          "libs/langchain-anthropic/src/tests/prompts.int.test.ts": "@@ -0,0 +1,28 @@\n+import Anthropic from \"@anthropic-ai/sdk\";\n+import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n+\n+import { convertPromptToAnthropic } from \"../utils/prompts.js\";\n+\n+test(\"Convert hub prompt to Anthropic payload and invoke\", async () => {\n+  const prompt = ChatPromptTemplate.fromMessages([\n+    [\"system\", \"You are a world class comedian\"],\n+    [\"human\", \"Tell me a joke about {topic}\"],\n+  ]);\n+  const formattedPrompt = await prompt.invoke({\n+    topic: \"cats\",\n+  });\n+\n+  const { system, messages } = convertPromptToAnthropic(formattedPrompt);\n+\n+  const anthropicClient = new Anthropic();\n+\n+  const anthropicResponse = await anthropicClient.messages.create({\n+    model: \"claude-3-haiku-20240307\",\n+    system,\n+    messages,\n+    max_tokens: 1024,\n+    stream: false,\n+  });\n+\n+  expect(anthropicResponse.content).toBeDefined();\n+});",
          "libs/langchain-anthropic/src/utils/errors.ts": "@@ -0,0 +1,39 @@\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n+/* eslint-disable no-param-reassign */\n+\n+// Duplicate of core\n+// TODO: Remove once we stop supporting 0.2.x core versions\n+export type LangChainErrorCodes =\n+  | \"INVALID_PROMPT_INPUT\"\n+  | \"INVALID_TOOL_RESULTS\"\n+  | \"MESSAGE_COERCION_FAILURE\"\n+  | \"MODEL_AUTHENTICATION\"\n+  | \"MODEL_NOT_FOUND\"\n+  | \"MODEL_RATE_LIMIT\"\n+  | \"OUTPUT_PARSING_FAILURE\";\n+\n+export function addLangChainErrorFields(\n+  error: any,\n+  lc_error_code: LangChainErrorCodes\n+) {\n+  (error as any).lc_error_code = lc_error_code;\n+  error.message = `${error.message}\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/${lc_error_code}/\\n`;\n+  return error;\n+}\n+\n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n+export function wrapAnthropicClientError(e: any) {\n+  let error;\n+  if (e.status === 400 && e.message.includes(\"tool\")) {\n+    error = addLangChainErrorFields(e, \"INVALID_TOOL_RESULTS\");\n+  } else if (e.status === 401) {\n+    error = addLangChainErrorFields(e, \"MODEL_AUTHENTICATION\");\n+  } else if (e.status === 404) {\n+    error = addLangChainErrorFields(e, \"MODEL_NOT_FOUND\");\n+  } else if (e.status === 429) {\n+    error = addLangChainErrorFields(e, \"MODEL_RATE_LIMIT\");\n+  } else {\n+    error = e;\n+  }\n+  return error;\n+}",
          "libs/langchain-anthropic/src/utils/message_inputs.ts": "@@ -185,10 +185,11 @@ function _formatContent(content: MessageContent) {\n \n /**\n  * Formats messages as a prompt for the model.\n+ * Used in LangSmith, export is important here.\n  * @param messages The base messages to format as a prompt.\n  * @returns The formatted prompt.\n  */\n-export function _formatMessagesForAnthropic(\n+export function _convertMessagesToAnthropicPayload(\n   messages: BaseMessage[]\n ): AnthropicMessageCreateParams {\n   const mergedMessages = _mergeMessages(messages);",
          "libs/langchain-anthropic/src/utils/prompts.ts": "@@ -0,0 +1,51 @@\n+import type { BasePromptValue } from \"@langchain/core/prompt_values\";\n+import Anthropic from \"@anthropic-ai/sdk\";\n+\n+import { _convertMessagesToAnthropicPayload } from \"./message_inputs.js\";\n+\n+/**\n+ * Convert a formatted LangChain prompt (e.g. pulled from the hub) into\n+ * a format expected by Anthropic's JS SDK.\n+ *\n+ * Requires the \"@langchain/anthropic\" package to be installed in addition\n+ * to the Anthropic SDK.\n+ *\n+ * @example\n+ * ```ts\n+ * import { convertPromptToAnthropic } from \"langsmith/utils/hub/anthropic\";\n+ * import { pull } from \"langchain/hub\";\n+ *\n+ * import Anthropic from '@anthropic-ai/sdk';\n+ *\n+ * const prompt = await pull(\"jacob/joke-generator\");\n+ * const formattedPrompt = await prompt.invoke({\n+ *   topic: \"cats\",\n+ * });\n+ *\n+ * const { system, messages } = convertPromptToAnthropic(formattedPrompt);\n+ *\n+ * const anthropicClient = new Anthropic({\n+ *   apiKey: 'your_api_key',\n+ * });\n+ *\n+ * const anthropicResponse = await anthropicClient.messages.create({\n+ *   model: \"claude-3-5-sonnet-20240620\",\n+ *   max_tokens: 1024,\n+ *   stream: false,\n+ *   system,\n+ *   messages,\n+ * });\n+ * ```\n+ * @param formattedPrompt\n+ * @returns A partial Anthropic payload.\n+ */\n+export function convertPromptToAnthropic(\n+  formattedPrompt: BasePromptValue\n+): Anthropic.Messages.MessageCreateParams {\n+  const messages = formattedPrompt.toChatMessages();\n+  const anthropicBody = _convertMessagesToAnthropicPayload(messages);\n+  if (anthropicBody.messages === undefined) {\n+    anthropicBody.messages = [];\n+  }\n+  return anthropicBody;\n+}",
          "libs/langchain-aws/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/aws\",\n-  \"version\": \"0.1.0\",\n+  \"version\": \"0.1.1\",\n   \"description\": \"LangChain AWS integration\",\n   \"type\": \"module\",\n   \"engines\": {",
          "libs/langchain-aws/src/common.ts": "@@ -144,6 +144,17 @@ export function convertToConverseMessages(messages: BaseMessage[]): {\n               return {\n                 text: block.text,\n               };\n+            } else if (\n+              block.type === \"document\" &&\n+              block.document !== undefined\n+            ) {\n+              return {\n+                document: block.document,\n+              };\n+            } else if (block.type === \"image\" && block.image !== undefined) {\n+              return {\n+                image: block.image,\n+              };\n             } else {\n               throw new Error(`Unsupported content block type: ${block.type}`);\n             }",
          "libs/langchain-baidu-qianfan/package.json": "@@ -42,7 +42,7 @@\n   \"devDependencies\": {\n     \"@jest/globals\": \"^29.5.0\",\n     \"@langchain/core\": \"workspace:*\",\n-    \"@langchain/openai\": \"~0.1.0\",\n+    \"@langchain/openai\": \"~0.3.0\",\n     \"@langchain/scripts\": \">=0.1.0 <0.2.0\",\n     \"@swc/core\": \"^1.3.90\",\n     \"@swc/jest\": \"^0.2.29\",",
          "libs/langchain-cohere/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/cohere\",\n-  \"version\": \"0.3.0\",\n+  \"version\": \"0.3.1\",\n   \"description\": \"Cohere integration for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -35,7 +35,7 @@\n   \"author\": \"LangChain\",\n   \"license\": \"MIT\",\n   \"dependencies\": {\n-    \"cohere-ai\": \"^7.10.5\",\n+    \"cohere-ai\": \"^7.14.0\",\n     \"uuid\": \"^10.0.0\",\n     \"zod\": \"^3.23.8\",\n     \"zod-to-json-schema\": \"^3.23.1\"",
          "libs/langchain-cohere/src/chat_models.ts": "@@ -1,14 +1,14 @@\n /* eslint-disable @typescript-eslint/no-explicit-any */\n-import { CohereClient, Cohere } from \"cohere-ai\";\n+import { Cohere, CohereClient } from \"cohere-ai\";\n import { ToolResult } from \"cohere-ai/api/index.js\";\n \n import { zodToJsonSchema } from \"zod-to-json-schema\";\n import {\n-  MessageType,\n-  type BaseMessage,\n-  MessageContent,\n   AIMessage,\n+  type BaseMessage,\n   isAIMessage,\n+  MessageContent,\n+  MessageType,\n } from \"@langchain/core/messages\";\n import {\n   BaseLanguageModelInput,\n@@ -17,34 +17,34 @@ import {\n import { isLangChainTool } from \"@langchain/core/utils/function_calling\";\n import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n import {\n-  type BaseChatModelParams,\n   BaseChatModel,\n-  LangSmithParams,\n   BaseChatModelCallOptions,\n+  type BaseChatModelParams,\n   BindToolsInput,\n+  LangSmithParams,\n } from \"@langchain/core/language_models/chat_models\";\n import {\n   ChatGeneration,\n   ChatGenerationChunk,\n   ChatResult,\n } from \"@langchain/core/outputs\";\n import { AIMessageChunk } from \"@langchain/core/messages\";\n-import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n import { NewTokenIndices } from \"@langchain/core/callbacks/base\";\n import {\n-  ToolMessage,\n   ToolCall,\n   ToolCallChunk,\n+  ToolMessage,\n } from \"@langchain/core/messages/tool\";\n import * as uuid from \"uuid\";\n import { Runnable } from \"@langchain/core/runnables\";\n+import { CohereClientOptions, getCohereClient } from \"./client.js\";\n \n type ChatCohereToolType = BindToolsInput | Cohere.Tool;\n \n /**\n  * Input interface for ChatCohere\n  */\n-export interface ChatCohereInput extends BaseChatModelParams {\n+export interface BaseChatCohereInput extends BaseChatModelParams {\n   /**\n    * The API key to use.\n    * @default {process.env.COHERE_API_KEY}\n@@ -78,6 +78,8 @@ export interface ChatCohereInput extends BaseChatModelParams {\n   streamUsage?: boolean;\n }\n \n+export type ChatCohereInput = BaseChatCohereInput & CohereClientOptions;\n+\n interface TokenUsage {\n   completionTokens?: number;\n   promptTokens?: number;\n@@ -732,14 +734,8 @@ export class ChatCohere<\n   constructor(fields?: ChatCohereInput) {\n     super(fields ?? {});\n \n-    const token = fields?.apiKey ?? getEnvironmentVariable(\"COHERE_API_KEY\");\n-    if (!token) {\n-      throw new Error(\"No API key provided for ChatCohere.\");\n-    }\n+    this.client = getCohereClient(fields);\n \n-    this.client = new CohereClient({\n-      token,\n-    });\n     this.model = fields?.model ?? this.model;\n     this.temperature = fields?.temperature ?? this.temperature;\n     this.streaming = fields?.streaming ?? this.streaming;",
          "libs/langchain-cohere/src/client.ts": "@@ -0,0 +1,28 @@\n+import { CohereClient } from \"cohere-ai\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+\n+export type CohereClientOptions = {\n+  /**\n+   * The API key to use. Ignored if `client` is provided\n+   * @default {process.env.COHERE_API_KEY}\n+   */\n+  apiKey?: string;\n+\n+  /**\n+   * The CohereClient instance to use. Superseeds `apiKey`\n+   */\n+  client?: CohereClient;\n+};\n+\n+export function getCohereClient(fields?: CohereClientOptions): CohereClient {\n+  if (fields?.client) {\n+    return fields.client;\n+  }\n+\n+  const apiKey = fields?.apiKey ?? getEnvironmentVariable(\"COHERE_API_KEY\");\n+\n+  if (!apiKey) {\n+    throw new Error(\"COHERE_API_KEY must be set\");\n+  }\n+  return new CohereClient({ token: apiKey });\n+}",
          "libs/langchain-cohere/src/embeddings.ts": "@@ -1,8 +1,8 @@\n import { CohereClient } from \"cohere-ai\";\n \n-import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n import { Embeddings, EmbeddingsParams } from \"@langchain/core/embeddings\";\n import { chunkArray } from \"@langchain/core/utils/chunk_array\";\n+import { CohereClientOptions, getCohereClient } from \"./client.js\";\n \n /**\n  * Interface that extends EmbeddingsParams and defines additional\n@@ -57,23 +57,13 @@ export class CohereEmbeddings\n   constructor(\n     fields?: Partial<CohereEmbeddingsParams> & {\n       verbose?: boolean;\n-      apiKey?: string;\n-    }\n+    } & CohereClientOptions\n   ) {\n     const fieldsWithDefaults = { maxConcurrency: 2, ...fields };\n \n     super(fieldsWithDefaults);\n \n-    const apiKey =\n-      fieldsWithDefaults?.apiKey || getEnvironmentVariable(\"COHERE_API_KEY\");\n-\n-    if (!apiKey) {\n-      throw new Error(\"Cohere API key not found\");\n-    }\n-\n-    this.client = new CohereClient({\n-      token: apiKey,\n-    });\n+    this.client = getCohereClient(fieldsWithDefaults);\n     this.model = fieldsWithDefaults?.model ?? this.model;\n \n     if (!this.model) {"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:L",
      "score": 4.9
    },
    "cwes": [
      {
        "cwe_id": "CWE-89",
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:L",
        "score": 4.9
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:L/AC:L/AT:P/PR:N/UI:N/VC:L/VI:L/VA:L/SC:N/SI:N/SA:N",
        "score": 2.1
      }
    },
    "epss": {
      "percentage": 0.00087,
      "percentile": 0.38266
    },
    "cve_description": "A vulnerability in the GraphCypherQAChain class of langchain-ai/langchainjs versions 0.2.5 and all versions with this class allows for prompt injection, leading to SQL injection. This vulnerability permits unauthorized data manipulation, data exfiltration, denial of service (DoS) by deleting all data, breaches in multi-tenant security environments, and data integrity issues. Attackers can create, update, or delete nodes and relationships without proper authorization, extract sensitive data, disrupt services, access data across different tenants, and compromise the integrity of the database."
  },
  {
    "ghsa_id": "GHSA-fc9h-whq2-v747",
    "cve_id": "CVE-2024-48948",
    "url": "https://api.github.com/advisories/GHSA-fc9h-whq2-v747",
    "html_url": "https://github.com/advisories/GHSA-fc9h-whq2-v747",
    "summary": "Valid ECDSA signatures erroneously rejected in Elliptic",
    "description": "The Elliptic prior to 6.6.0 for Node.js, in its for ECDSA implementation, does not correctly verify valid signatures if the hash contains at least four leading 0 bytes and when the order of the elliptic curve's base point is smaller than the hash, because of an _truncateToN anomaly. This leads to valid signatures being rejected. Legitimate transactions or communications may be incorrectly flagged as invalid.",
    "type": "reviewed",
    "severity": "low",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/indutny/elliptic",
    "identifiers": [
      {
        "value": "GHSA-fc9h-whq2-v747",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-48948",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-48948",
      "https://github.com/indutny/elliptic/issues/321",
      "https://github.com/indutny/elliptic/pull/322",
      "https://github.com/indutny/elliptic/commit/34c853478cec1be4e37260ed2cb12cdbdc6402cf",
      "https://github.com/advisories/GHSA-fc9h-whq2-v747"
    ],
    "published_at": "2024-10-15T15:30:56Z",
    "updated_at": "2024-11-05T21:34:53Z",
    "github_reviewed_at": "2024-10-17T22:05:18Z",
    "nvd_published_at": "2024-10-15T14:15:05Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "elliptic"
        },
        "vulnerable_version_range": "< 6.6.0",
        "first_patched_version": "6.6.0",
        "vulnerable_functions": [],
        "vulnerable_version": "6.5.7",
        "patches": {
          "dist/elliptic.js": "@@ -2252,8 +2252,27 @@ EC.prototype.genKeyPair = function genKeyPair(options) {\n   }\n };\n \n-EC.prototype._truncateToN = function _truncateToN(msg, truncOnly) {\n-  var delta = msg.byteLength() * 8 - this.n.bitLength();\n+EC.prototype._truncateToN = function _truncateToN(msg, truncOnly, bitLength) {\n+  var byteLength;\n+  if (BN.isBN(msg) || typeof msg === 'number') {\n+    msg = new BN(msg, 16);\n+    byteLength = msg.byteLength();\n+  } else if (typeof msg === 'object') {\n+    // BN assumes an array-like input and asserts length\n+    byteLength = msg.length;\n+    msg = new BN(msg, 16);\n+  } else {\n+    // BN converts the value to string\n+    var str = msg.toString();\n+    // HEX encoding\n+    byteLength = (str.length + 1) >>> 1;\n+    msg = new BN(str, 16);\n+  }\n+  // Allow overriding\n+  if (typeof bitLength !== 'number') {\n+    bitLength = byteLength * 8;\n+  }\n+  var delta = bitLength - this.n.bitLength();\n   if (delta > 0)\n     msg = msg.ushrn(delta);\n   if (!truncOnly && msg.cmp(this.n) >= 0)\n@@ -2271,7 +2290,7 @@ EC.prototype.sign = function sign(msg, key, enc, options) {\n     options = {};\n \n   key = this.keyFromPrivate(key, enc);\n-  msg = this._truncateToN(new BN(msg, 16));\n+  msg = this._truncateToN(msg, false, options.msgBitLength);\n \n   // Zero-extend key to provide enough entropy\n   var bytes = this.n.byteLength();\n@@ -2327,8 +2346,11 @@ EC.prototype.sign = function sign(msg, key, enc, options) {\n   }\n };\n \n-EC.prototype.verify = function verify(msg, signature, key, enc) {\n-  msg = this._truncateToN(new BN(msg, 16));\n+EC.prototype.verify = function verify(msg, signature, key, enc, options) {\n+  if (!options)\n+    options = {};\n+\n+  msg = this._truncateToN(msg, false, options.msgBitLength);\n   key = this.keyFromPublic(key, enc);\n   signature = new Signature(signature, 'hex');\n \n@@ -2530,8 +2552,8 @@ KeyPair.prototype.sign = function sign(msg, enc, options) {\n   return this.ec.sign(msg, this, enc, options);\n };\n \n-KeyPair.prototype.verify = function verify(msg, signature) {\n-  return this.ec.verify(msg, signature, this);\n+KeyPair.prototype.verify = function verify(msg, signature, options) {\n+  return this.ec.verify(msg, signature, this, undefined, options);\n };\n \n KeyPair.prototype.inspect = function inspect() {\n@@ -8867,7 +8889,7 @@ utils.encode = function encode(arr, enc) {\n },{}],35:[function(require,module,exports){\n module.exports={\n   \"name\": \"elliptic\",\n-  \"version\": \"6.5.7\",\n+  \"version\": \"6.6.0\",\n   \"description\": \"EC cryptography\",\n   \"main\": \"lib/elliptic.js\",\n   \"files\": [",
          "lib/elliptic/ec/index.js": "@@ -78,8 +78,27 @@ EC.prototype.genKeyPair = function genKeyPair(options) {\n   }\n };\n \n-EC.prototype._truncateToN = function _truncateToN(msg, truncOnly) {\n-  var delta = msg.byteLength() * 8 - this.n.bitLength();\n+EC.prototype._truncateToN = function _truncateToN(msg, truncOnly, bitLength) {\n+  var byteLength;\n+  if (BN.isBN(msg) || typeof msg === 'number') {\n+    msg = new BN(msg, 16);\n+    byteLength = msg.byteLength();\n+  } else if (typeof msg === 'object') {\n+    // BN assumes an array-like input and asserts length\n+    byteLength = msg.length;\n+    msg = new BN(msg, 16);\n+  } else {\n+    // BN converts the value to string\n+    var str = msg.toString();\n+    // HEX encoding\n+    byteLength = (str.length + 1) >>> 1;\n+    msg = new BN(str, 16);\n+  }\n+  // Allow overriding\n+  if (typeof bitLength !== 'number') {\n+    bitLength = byteLength * 8;\n+  }\n+  var delta = bitLength - this.n.bitLength();\n   if (delta > 0)\n     msg = msg.ushrn(delta);\n   if (!truncOnly && msg.cmp(this.n) >= 0)\n@@ -97,7 +116,7 @@ EC.prototype.sign = function sign(msg, key, enc, options) {\n     options = {};\n \n   key = this.keyFromPrivate(key, enc);\n-  msg = this._truncateToN(new BN(msg, 16));\n+  msg = this._truncateToN(msg, false, options.msgBitLength);\n \n   // Zero-extend key to provide enough entropy\n   var bytes = this.n.byteLength();\n@@ -153,8 +172,11 @@ EC.prototype.sign = function sign(msg, key, enc, options) {\n   }\n };\n \n-EC.prototype.verify = function verify(msg, signature, key, enc) {\n-  msg = this._truncateToN(new BN(msg, 16));\n+EC.prototype.verify = function verify(msg, signature, key, enc, options) {\n+  if (!options)\n+    options = {};\n+\n+  msg = this._truncateToN(msg, false, options.msgBitLength);\n   key = this.keyFromPublic(key, enc);\n   signature = new Signature(signature, 'hex');\n ",
          "lib/elliptic/ec/key.js": "@@ -111,8 +111,8 @@ KeyPair.prototype.sign = function sign(msg, enc, options) {\n   return this.ec.sign(msg, this, enc, options);\n };\n \n-KeyPair.prototype.verify = function verify(msg, signature) {\n-  return this.ec.verify(msg, signature, this);\n+KeyPair.prototype.verify = function verify(msg, signature, options) {\n+  return this.ec.verify(msg, signature, this, undefined, options);\n };\n \n KeyPair.prototype.inspect = function inspect() {",
          "package-lock.json": "@@ -1,11 +1,11 @@\n {\n   \"name\": \"elliptic\",\n-  \"version\": \"6.5.7\",\n+  \"version\": \"6.6.0\",\n   \"lockfileVersion\": 2,\n   \"requires\": true,\n   \"packages\": {\n     \"\": {\n-      \"version\": \"6.5.7\",\n+      \"version\": \"6.6.0\",\n       \"license\": \"MIT\",\n       \"dependencies\": {\n         \"bn.js\": \"^4.11.9\",",
          "package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"elliptic\",\n-  \"version\": \"6.5.7\",\n+  \"version\": \"6.6.0\",\n   \"description\": \"EC cryptography\",\n   \"main\": \"lib/elliptic.js\",\n   \"files\": [",
          "test/ecdsa-test.js": "@@ -489,6 +489,50 @@ describe('ECDSA', function() {\n       });\n     });\n \n+  it('Wycheproof special hash case with hex', function() {\n+    var curve = new elliptic.ec('p192');\n+    var msg =\n+      '00000000690ed426ccf17803ebe2bd0884bcd58a1bb5e7477ead3645f356e7a9';\n+    var sig = '303502186f20676c0d04fc40ea55d5702f798355787363a9' +\n+              '1e97a7e50219009d1c8c171b2b02e7d791c204c17cea4cf5' +\n+              '56a2034288885b';\n+    var pub = '04cd35a0b18eeb8fcd87ff019780012828745f046e785deb' +\n+              'a28150de1be6cb4376523006beff30ff09b4049125ced29723';\n+    var pubKey = curve.keyFromPublic(pub, 'hex');\n+    assert(pubKey.verify(msg, sig) === true);\n+  });\n+\n+  it('Wycheproof special hash case with Array', function() {\n+    var curve = new elliptic.ec('p192');\n+    var msg = [\n+      0x00, 0x00, 0x00, 0x00, 0x69, 0x0e, 0xd4, 0x26, 0xcc, 0xf1, 0x78,\n+      0x03, 0xeb, 0xe2, 0xbd, 0x08, 0x84, 0xbc, 0xd5, 0x8a, 0x1b, 0xb5,\n+      0xe7, 0x47, 0x7e, 0xad, 0x36, 0x45, 0xf3, 0x56, 0xe7, 0xa9,\n+    ];\n+    var sig = '303502186f20676c0d04fc40ea55d5702f798355787363a9' +\n+              '1e97a7e50219009d1c8c171b2b02e7d791c204c17cea4cf5' +\n+              '56a2034288885b';\n+    var pub = '04cd35a0b18eeb8fcd87ff019780012828745f046e785deb' +\n+              'a28150de1be6cb4376523006beff30ff09b4049125ced29723';\n+    var pubKey = curve.keyFromPublic(pub, 'hex');\n+    assert(pubKey.verify(msg, sig) === true);\n+  });\n+\n+  it('Wycheproof special hash case with BN', function() {\n+    var curve = new elliptic.ec('p192');\n+    var msg = new BN(\n+      '00000000690ed426ccf17803ebe2bd0884bcd58a1bb5e7477ead3645f356e7a9',\n+      16,\n+    );\n+    var sig = '303502186f20676c0d04fc40ea55d5702f798355787363a9' +\n+              '1e97a7e50219009d1c8c171b2b02e7d791c204c17cea4cf5' +\n+              '56a2034288885b';\n+    var pub = '04cd35a0b18eeb8fcd87ff019780012828745f046e785deb' +\n+              'a28150de1be6cb4376523006beff30ff09b4049125ced29723';\n+    var pubKey = curve.keyFromPublic(pub, 'hex');\n+    assert(pubKey.verify(msg, sig, { msgBitLength: 32 * 8 }) === true);\n+  });\n+\n   describe('Signature', function () {\n     it('recoveryParam is 0', function () {\n       var sig = new Signature({ r: '00', s: '00', recoveryParam: 0 });"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:L/A:L",
      "score": 4.8
    },
    "cwes": [
      {
        "cwe_id": "CWE-347",
        "name": "Improper Verification of Cryptographic Signature"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "martincostello",
          "id": 1439341,
          "node_id": "MDQ6VXNlcjE0MzkzNDE=",
          "avatar_url": "https://avatars.githubusercontent.com/u/1439341?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/martincostello",
          "html_url": "https://github.com/martincostello",
          "followers_url": "https://api.github.com/users/martincostello/followers",
          "following_url": "https://api.github.com/users/martincostello/following{/other_user}",
          "gists_url": "https://api.github.com/users/martincostello/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/martincostello/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/martincostello/subscriptions",
          "organizations_url": "https://api.github.com/users/martincostello/orgs",
          "repos_url": "https://api.github.com/users/martincostello/repos",
          "events_url": "https://api.github.com/users/martincostello/events{/privacy}",
          "received_events_url": "https://api.github.com/users/martincostello/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      },
      {
        "user": {
          "login": "IchordeDionysos",
          "id": 10195482,
          "node_id": "MDQ6VXNlcjEwMTk1NDgy",
          "avatar_url": "https://avatars.githubusercontent.com/u/10195482?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/IchordeDionysos",
          "html_url": "https://github.com/IchordeDionysos",
          "followers_url": "https://api.github.com/users/IchordeDionysos/followers",
          "following_url": "https://api.github.com/users/IchordeDionysos/following{/other_user}",
          "gists_url": "https://api.github.com/users/IchordeDionysos/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/IchordeDionysos/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/IchordeDionysos/subscriptions",
          "organizations_url": "https://api.github.com/users/IchordeDionysos/orgs",
          "repos_url": "https://api.github.com/users/IchordeDionysos/repos",
          "events_url": "https://api.github.com/users/IchordeDionysos/events{/privacy}",
          "received_events_url": "https://api.github.com/users/IchordeDionysos/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:L/A:L",
        "score": 4.8
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:H/AT:P/PR:N/UI:P/VC:N/VI:L/VA:L/SC:N/SI:N/SA:N",
        "score": 2.3
      }
    },
    "epss": {
      "percentage": 0.00043,
      "percentile": 0.097
    },
    "cve_description": "The Elliptic package 6.5.7 for Node.js, in its for ECDSA implementation, does not correctly verify valid signatures if the hash contains at least four leading 0 bytes and when the order of the elliptic curve's base point is smaller than the hash, because of an _truncateToN anomaly. This leads to valid signatures being rejected. Legitimate transactions or communications may be incorrectly flagged as invalid."
  },
  {
    "ghsa_id": "GHSA-hc5w-c9f8-9cc4",
    "cve_id": "CVE-2024-7774",
    "url": "https://api.github.com/advisories/GHSA-hc5w-c9f8-9cc4",
    "html_url": "https://github.com/advisories/GHSA-hc5w-c9f8-9cc4",
    "summary": "Langchain Path Traversal vulnerability",
    "description": "A path traversal vulnerability exists in the `getFullPath` method of langchain-ai/langchainjs version 0.2.5. This vulnerability allows attackers to save files anywhere in the filesystem, overwrite existing text files, read `.txt` files, and delete files. The vulnerability is exploited through the `setFileContent`, `getParsedFile`, and `mdelete` methods, which do not properly sanitize user input.",
    "type": "reviewed",
    "severity": "medium",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/langchain-ai/langchainjs",
    "identifiers": [
      {
        "value": "GHSA-hc5w-c9f8-9cc4",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-7774",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-7774",
      "https://github.com/langchain-ai/langchainjs/commit/a0fad77d6b569e5872bd4a9d33be0c0785e538a9",
      "https://huntr.com/bounties/8fe40685-b714-4191-af7a-3de5e5628cee",
      "https://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2024-111.yaml",
      "https://github.com/advisories/GHSA-hc5w-c9f8-9cc4"
    ],
    "published_at": "2024-10-29T15:32:05Z",
    "updated_at": "2024-11-01T15:24:38Z",
    "github_reviewed_at": "2024-10-29T19:40:07Z",
    "nvd_published_at": "2024-10-29T13:15:09Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "langchain"
        },
        "vulnerable_version_range": "< 0.2.19",
        "first_patched_version": "0.2.19",
        "vulnerable_functions": [],
        "vulnerable_version": "0.2.18",
        "patches": {
          "docs/core_docs/.gitignore": "@@ -240,18 +240,18 @@ docs/integrations/vectorstores/elasticsearch.md\n docs/integrations/vectorstores/elasticsearch.mdx\n docs/integrations/vectorstores/chroma.md\n docs/integrations/vectorstores/chroma.mdx\n-docs/integrations/toolkits/vectorstore.md\n-docs/integrations/toolkits/vectorstore.mdx\n-docs/integrations/toolkits/sql.md\n-docs/integrations/toolkits/sql.mdx\n-docs/integrations/toolkits/openapi.md\n-docs/integrations/toolkits/openapi.mdx\n docs/integrations/tools/tavily_search.md\n docs/integrations/tools/tavily_search.mdx\n docs/integrations/tools/exa_search.md\n docs/integrations/tools/exa_search.mdx\n docs/integrations/tools/duckduckgo_search.md\n docs/integrations/tools/duckduckgo_search.mdx\n+docs/integrations/toolkits/vectorstore.md\n+docs/integrations/toolkits/vectorstore.mdx\n+docs/integrations/toolkits/sql.md\n+docs/integrations/toolkits/sql.mdx\n+docs/integrations/toolkits/openapi.md\n+docs/integrations/toolkits/openapi.mdx\n docs/integrations/text_embedding/togetherai.md\n docs/integrations/text_embedding/togetherai.mdx\n docs/integrations/text_embedding/openai.md\n@@ -304,6 +304,8 @@ docs/integrations/llms/cloudflare_workersai.md\n docs/integrations/llms/cloudflare_workersai.mdx\n docs/integrations/llms/bedrock.md\n docs/integrations/llms/bedrock.mdx\n+docs/integrations/llms/arcjet.md\n+docs/integrations/llms/arcjet.mdx\n docs/integrations/llms/azure.md\n docs/integrations/llms/azure.mdx\n docs/integrations/chat/togetherai.md\n@@ -330,6 +332,8 @@ docs/integrations/chat/bedrock_converse.md\n docs/integrations/chat/bedrock_converse.mdx\n docs/integrations/chat/bedrock.md\n docs/integrations/chat/bedrock.mdx\n+docs/integrations/chat/arcjet.md\n+docs/integrations/chat/arcjet.mdx\n docs/integrations/chat/azure.md\n docs/integrations/chat/azure.mdx\n docs/integrations/chat/anthropic.md\n@@ -371,4 +375,4 @@ docs/integrations/document_loaders/web_loaders/pdf.mdx\n docs/integrations/document_loaders/web_loaders/langsmith.md\n docs/integrations/document_loaders/web_loaders/langsmith.mdx\n docs/integrations/document_loaders/web_loaders/firecrawl.md\n-docs/integrations/document_loaders/web_loaders/firecrawl.mdx\n\\ No newline at end of file\n+docs/integrations/document_loaders/web_loaders/firecrawl.mdx",
          "docs/core_docs/docs/how_to/tool_choice.ipynb": "@@ -103,8 +103,8 @@\n     \"const llmForcedToMultiply = llm.bindTools(tools, {\\n\",\n     \"  tool_choice: \\\"Multiply\\\",\\n\",\n     \"})\\n\",\n-    \"const result = await llmForcedToMultiply.invoke(\\\"what is 2 + 4\\\");\\n\",\n-    \"console.log(JSON.stringify(result.tool_calls, null, 2));\"\n+    \"const multiplyResult = await llmForcedToMultiply.invoke(\\\"what is 2 + 4\\\");\\n\",\n+    \"console.log(JSON.stringify(multiplyResult.tool_calls, null, 2));\"\n    ]\n   },\n   {\n@@ -118,7 +118,7 @@\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We can also just force our tool to select at least one of our tools by passing in the \\\"any\\\" (or \\\"required\\\" which is OpenAI specific) keyword to the `tool_choice` parameter.\"\n+    \"We can also just force our tool to select at least one of our tools by passing `\\\"any\\\"` (or for OpenAI models, the equivalent, `\\\"required\\\"`) to the `tool_choice` parameter.\"\n    ]\n   },\n   {\n@@ -148,29 +148,8 @@\n     \"const llmForcedToUseTool = llm.bindTools(tools, {\\n\",\n     \"  tool_choice: \\\"any\\\",\\n\",\n     \"})\\n\",\n-    \"const result = await llmForcedToUseTool.invoke(\\\"What day is today?\\\");\\n\",\n-    \"console.log(JSON.stringify(result.tool_calls, null, 2));\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"markdown\",\n-   \"metadata\": {},\n-   \"source\": [\n-    \"```{=mdx}\\n\",\n-    \"\\n\",\n-    \":::note\\n\",\n-    \"\\n\",\n-    \"Currently, the following packages have a minimum version for this style of forced tool calls:\\n\",\n-    \"\\n\",\n-    \"| Package Name        | Min Package Version | Min Core Version |\\n\",\n-    \"|---------------------|---------------------|------------------|\\n\",\n-    \"| `@langchain/aws`    | `0.0.3`             | `0.2.17`         |\\n\",\n-    \"| `@langchain/openai` | `0.2.4`             | `0.2.17`         |\\n\",\n-    \"| `@langchain/groq`   | `0.0.14`            | `0.2.17`         |\\n\",\n-    \"\\n\",\n-    \":::\\n\",\n-    \"\\n\",\n-    \"```\"\n+    \"const anyToolResult = await llmForcedToUseTool.invoke(\\\"What day is today?\\\");\\n\",\n+    \"console.log(JSON.stringify(anyToolResult.tool_calls, null, 2));\"\n    ]\n   }\n  ],",
          "docs/core_docs/docs/integrations/chat/anthropic.ipynb": "@@ -953,7 +953,7 @@\n     \"const customClient = new AnthropicVertex();\\n\",\n     \"\\n\",\n     \"const modelWithCustomClient = new ChatAnthropic({\\n\",\n-    \"  modelName: \\\"claude-3-sonnet-20240229\\\",\\n\",\n+    \"  modelName: \\\"claude-3-sonnet@20240229\\\",\\n\",\n     \"  maxRetries: 0,\\n\",\n     \"  createClient: () => customClient,\\n\",\n     \"});\\n\",",
          "docs/core_docs/docs/integrations/chat/arcjet.ipynb": "@@ -0,0 +1,164 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"raw\",\n+   \"id\": \"67db2992\",\n+   \"metadata\": {\n+    \"vscode\": {\n+     \"languageId\": \"raw\"\n+    }\n+   },\n+   \"source\": [\n+    \"---\\n\",\n+    \"sidebar_label: Arcjet Redact\\n\",\n+    \"---\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"9597802c\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Arcjet Redact\\n\",\n+    \"\\n\",\n+    \"The [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to a Chat Model.\\n\",\n+    \"\\n\",\n+    \"Arcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.\\n\",\n+    \"\\n\",\n+    \"The Arcjet Redact object is not a chat model itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped chat model before returning it. \\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Overview\\n\",\n+    \"### Integration details\\n\",\n+    \"\\n\",\n+    \"| Class | Package | Local | Serializable | PY Support | Package downloads | Package latest |\\n\",\n+    \"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\n\",\n+    \"| Arcjet | @langchain/community | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |\\n\",\n+    \"\\n\",\n+    \"### Installation\\n\",\n+    \"\\n\",\n+    \"Install the Arcjet Redaction Library:\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import IntegrationInstallTooltip from \\\"@mdx_components/integration_install_tooltip.mdx\\\";\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\";\\n\",\n+    \"\\n\",\n+    \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @arcjet/redact\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"And install LangChain Community:\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/community\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"\\n\",\n+    \"And now you're ready to start protecting your chat model calls with Arcjet Redaction!\\n\",\n+    \"\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0a760037\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Usage\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"id\": \"a0562a13\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import {\\n\",\n+    \"  ArcjetRedact,\\n\",\n+    \"  ArcjetSensitiveInfoType,\\n\",\n+    \"} from \\\"@langchain/community/chat_models/arcjet\\\";\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"// Create an instance of another chat model for Arcjet to wrap\\n\",\n+    \"const openai = new ChatOpenAI({\\n\",\n+    \"  temperature: 0.8,\\n\",\n+    \"  model: \\\"gpt-3.5-turbo-0125\\\",\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const arcjetRedactOptions = {\\n\",\n+    \"  // Specify a LLM that Arcjet Redact will call once it has redacted the input.\\n\",\n+    \"  chatModel: openai,\\n\",\n+    \"\\n\",\n+    \"  // Specify the list of entities that should be redacted.\\n\",\n+    \"  // If this isn't specified then all entities will be redacted.\\n\",\n+    \"  entities: [\\\"email\\\", \\\"phone-number\\\", \\\"ip-address\\\", \\\"custom-entity\\\"] as ArcjetSensitiveInfoType[],\\n\",\n+    \"\\n\",\n+    \"  // You can provide a custom detect function to detect entities that we don't support yet.\\n\",\n+    \"  // It takes a list of tokens and you return a list of identified types or undefined.\\n\",\n+    \"  // The undefined types that you return should be added to the entities list if used.\\n\",\n+    \"  detect: (tokens: string[]) => {\\n\",\n+    \"    return tokens.map((t) => t === \\\"some-sensitive-info\\\" ? \\\"custom-entity\\\" : undefined)\\n\",\n+    \"  },\\n\",\n+    \"\\n\",\n+    \"  // The number of tokens to provide to the custom detect function. This defaults to 1.\\n\",\n+    \"  // It can be used to provide additional context when detecting custom entity types.\\n\",\n+    \"  contextWindowSize: 1,\\n\",\n+    \"\\n\",\n+    \"  // This allows you to provide custom replacements when redacting. Please ensure\\n\",\n+    \"  // that the replacements are unique so that unredaction works as expected.\\n\",\n+    \"  replace: (identifiedType: string) => {\\n\",\n+    \"    return identifiedType === \\\"email\\\" ? \\\"redacted@example.com\\\" : undefined;\\n\",\n+    \"  },\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"const arcjetRedact = new ArcjetRedact(arcjetRedactOptions);\\n\",\n+    \"\\n\",\n+    \"const response = await arcjetRedact.invoke(\\n\",\n+    \"  \\\"My email address is test@example.com, here is some-sensitive-info\\\"\\n\",\n+    \");\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"165c0f4f\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  },\n+  \"vscode\": {\n+   \"interpreter\": {\n+    \"hash\": \"e971737741ff4ec9aff7dc6155a1060a59a8a6d52c757dbbe66bf8ee389494b1\"\n+   }\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/integrations/llms/arcjet.ipynb": "@@ -0,0 +1,156 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"raw\",\n+   \"id\": \"67db2992\",\n+   \"metadata\": {\n+    \"vscode\": {\n+     \"languageId\": \"raw\"\n+    }\n+   },\n+   \"source\": [\n+    \"---\\n\",\n+    \"sidebar_label: Arcjet Redact\\n\",\n+    \"---\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"9597802c\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Arcjet Redact\\n\",\n+    \"\\n\",\n+    \"The [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to an LLM.\\n\",\n+    \"\\n\",\n+    \"Arcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.\\n\",\n+    \"\\n\",\n+    \"The Arcjet Redact object is not an LLM itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped LLM before returning it. \\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Overview\\n\",\n+    \"### Integration details\\n\",\n+    \"\\n\",\n+    \"| Class | Package | Local | Serializable | PY Support | Package downloads | Package latest |\\n\",\n+    \"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\n\",\n+    \"| Arcjet | @langchain/community | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |\\n\",\n+    \"\\n\",\n+    \"### Installation\\n\",\n+    \"\\n\",\n+    \"Install the Arcjet Redaction Library:\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import IntegrationInstallTooltip from \\\"@mdx_components/integration_install_tooltip.mdx\\\";\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\";\\n\",\n+    \"\\n\",\n+    \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @arcjet/redact\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"And install LangChain Community:\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/community\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"\\n\",\n+    \"And now you're ready to start protecting your LLM calls with Arcjet Redaction!\\n\",\n+    \"\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0a760037\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Usage\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"a0562a13\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import {\\n\",\n+    \"  ArcjetRedact,\\n\",\n+    \"  ArcjetSensitiveInfoType,\\n\",\n+    \"} from \\\"@langchain/community/llms/arcjet\\\";\\n\",\n+    \"import { OpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"// Create an instance of another LLM for Arcjet to wrap\\n\",\n+    \"const openai = new OpenAI({\\n\",\n+    \"  modelName: \\\"gpt-3.5-turbo-instruct\\\",\\n\",\n+    \"  openAIApiKey: process.env.OPENAI_API_KEY,\\n\",\n+    \"});\\n\",\n+    \"\\n\",\n+    \"const arcjetRedactOptions = {\\n\",\n+    \"  // Specify a LLM that Arcjet Redact will call once it has redacted the input.\\n\",\n+    \"  llm: openai,\\n\",\n+    \"\\n\",\n+    \"  // Specify the list of entities that should be redacted.\\n\",\n+    \"  // If this isn't specified then all entities will be redacted.\\n\",\n+    \"  entities: [\\\"email\\\", \\\"phone-number\\\", \\\"ip-address\\\", \\\"credit-card\\\"] as ArcjetSensitiveInfoType[],\\n\",\n+    \"\\n\",\n+    \"  // You can provide a custom detect function to detect entities that we don't support yet.\\n\",\n+    \"  // It takes a list of tokens and you return a list of identified types or undefined.\\n\",\n+    \"  // The undefined types that you return should be added to the entities list if used.\\n\",\n+    \"  detect: (tokens: string[]) => {\\n\",\n+    \"    return tokens.map((t) => t === \\\"some-sensitive-info\\\" ? \\\"custom-entity\\\" : undefined)\\n\",\n+    \"  },\\n\",\n+    \"\\n\",\n+    \"  // The number of tokens to provide to the custom detect function. This defaults to 1.\\n\",\n+    \"  // It can be used to provide additional context when detecting custom entity types.\\n\",\n+    \"  contextWindowSize: 1,\\n\",\n+    \"\\n\",\n+    \"  // This allows you to provide custom replacements when redacting. Please ensure\\n\",\n+    \"  // that the replacements are unique so that unredaction works as expected.\\n\",\n+    \"  replace: (identifiedType: string) => {\\n\",\n+    \"    return identifiedType === \\\"email\\\" ? \\\"redacted@example.com\\\" : undefined;\\n\",\n+    \"  },\\n\",\n+    \"\\n\",\n+    \"};\\n\",\n+    \"\\n\",\n+    \"const arcjetRedact = new ArcjetRedact(arcjetRedactOptions);\\n\",\n+    \"const response = await arcjetRedact.invoke(\\n\",\n+    \"  \\\"My email address is test@example.com, here is some-sensitive-info\\\"\\n\",\n+    \");\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  },\n+  \"vscode\": {\n+   \"interpreter\": {\n+    \"hash\": \"e971737741ff4ec9aff7dc6155a1060a59a8a6d52c757dbbe66bf8ee389494b1\"\n+   }\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}",
          "docs/core_docs/docs/integrations/retrievers/bm25.ipynb": "@@ -0,0 +1,101 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# BM25\\n\",\n+    \"\\n\",\n+    \"BM25, also known as [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25), is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\\n\",\n+    \"\\n\",\n+    \"You can use it as part of your retrieval pipeline as a to rerank documents as a postprocessing step after retrieving an initial set of documents from another source.\\n\",\n+    \"\\n\",\n+    \"## Setup\\n\",\n+    \"\\n\",\n+    \"The `BM25Retriever` is exported from `@langchain/community`. You'll need to install it like this:\\n\",\n+    \"\\n\",\n+    \"```{=mdx}\\n\",\n+    \"import IntegrationInstallTooltip from \\\"@mdx_components/integration_install_tooltip.mdx\\\";\\n\",\n+    \"import Npm2Yarn from \\\"@theme/Npm2Yarn\\\";\\n\",\n+    \"\\n\",\n+    \"<IntegrationInstallTooltip></IntegrationInstallTooltip>\\n\",\n+    \"\\n\",\n+    \"<Npm2Yarn>\\n\",\n+    \"  @langchain/community @langchain/core\\n\",\n+    \"</Npm2Yarn>\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"This retriever uses code from [`this implementation`](https://github.com/FurkanToprak/OkapiBM25) of Okapi BM25.\\n\",\n+    \"\\n\",\n+    \"## Usage\\n\",\n+    \"\\n\",\n+    \"You can now create a new retriever with previously retrieved documents:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"[\\n\",\n+      \"  { pageContent: 'mitochondria is made of lipids', metadata: {} },\\n\",\n+      \"  {\\n\",\n+      \"    pageContent: 'mitochondria is the powerhouse of the cell',\\n\",\n+      \"    metadata: {}\\n\",\n+      \"  },\\n\",\n+      \"  { pageContent: 'Buildings are made out of brick', metadata: {} },\\n\",\n+      \"  { pageContent: 'Buildings are made out of wood', metadata: {} }\\n\",\n+      \"]\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import { BM25Retriever } from \\\"@langchain/community/retrievers/bm25\\\";\\n\",\n+    \"\\n\",\n+    \"const retriever = BM25Retriever.fromDocuments([\\n\",\n+    \"  { pageContent: \\\"Buildings are made out of brick\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"Buildings are made out of wood\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"Buildings are made out of stone\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"Cars are made out of metal\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"Cars are made out of plastic\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"mitochondria is the powerhouse of the cell\\\", metadata: {} },\\n\",\n+    \"  { pageContent: \\\"mitochondria is made of lipids\\\", metadata: {} },\\n\",\n+    \"], { k: 4 });\\n\",\n+    \"\\n\",\n+    \"// Will return the 4 documents reranked by the BM25 algorithm\\n\",\n+    \"await retriever.invoke(\\\"mitochondria\\\");\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"TypeScript\",\n+   \"language\": \"typescript\",\n+   \"name\": \"tslab\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"mode\": \"typescript\",\n+    \"name\": \"javascript\",\n+    \"typescript\": true\n+   },\n+   \"file_extension\": \".ts\",\n+   \"mimetype\": \"text/typescript\",\n+   \"name\": \"typescript\",\n+   \"version\": \"3.7.2\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}",
          "docs/core_docs/docs/integrations/stores/file_system.ipynb": "@@ -46,6 +46,14 @@\n     \"\\n\",\n     \":::\\n\",\n     \"\\n\",\n+    \":::warning\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"This file store can alter any text file in the provided directory and any subfolders.\\n\",\n+    \"Make sure that the path you specify when initializing the store is free of other files.\\n\",\n+    \"\\n\",\n+    \":::\\n\",\n+    \"\\n\",\n     \"```\\n\",\n     \"\\n\",\n     \"### Integration details\\n\",",
          "docs/core_docs/docs/tutorials/chatbot.ipynb": "@@ -98,52 +98,49 @@\n    \"cell_type\": \"code\",\n    \"execution_count\": 1,\n    \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"// @lc-docs-hide-cell\\n\",\n+    \"\\n\",\n+    \"import { ChatOpenAI } from \\\"@langchain/openai\\\";\\n\",\n+    \"\\n\",\n+    \"const model = new ChatOpenAI({\\n\",\n+    \"  model: \\\"gpt-4o-mini\\\",\\n\",\n+    \"  temperature: 0,\\n\",\n+    \"});\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n        \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"Hello Bob, it's nice to meet you! I'm an AI assistant created by Anthropic. How are you doing today?\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: {\\n\",\n-       \"      id: \\u001b[32m\\\"msg_015Qvu91azZviks5VzGvYT7z\\\"\\u001b[39m,\\n\",\n-       \"      type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"      role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"      model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"      stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"      usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m30\\u001b[39m },\\n\",\n-       \"      stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n+       \"  \\\"id\\\": \\\"chatcmpl-A64of8iD4GIFNSYlOaFHxPdCeyl9E\\\",\\n\",\n+       \"  \\\"content\\\": \\\"Hi Bob! How can I assist you today?\\\",\\n\",\n+       \"  \\\"additional_kwargs\\\": {},\\n\",\n+       \"  \\\"response_metadata\\\": {\\n\",\n+       \"    \\\"tokenUsage\\\": {\\n\",\n+       \"      \\\"completionTokens\\\": 10,\\n\",\n+       \"      \\\"promptTokens\\\": 11,\\n\",\n+       \"      \\\"totalTokens\\\": 21\\n\",\n        \"    },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"Hello Bob, it's nice to meet you! I'm an AI assistant created by Anthropic. How are you doing today?\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_015Qvu91azZviks5VzGvYT7z\\\"\\u001b[39m,\\n\",\n-       \"    type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"    role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m30\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n+       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n        \"  },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_015Qvu91azZviks5VzGvYT7z\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m30\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n+       \"  \\\"tool_calls\\\": [],\\n\",\n+       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"  \\\"usage_metadata\\\": {\\n\",\n+       \"    \\\"input_tokens\\\": 11,\\n\",\n+       \"    \\\"output_tokens\\\": 10,\\n\",\n+       \"    \\\"total_tokens\\\": 21\\n\",\n+       \"  }\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 1,\n+     \"execution_count\": 2,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -163,54 +160,35 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 3,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n        \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"I'm afraid I don't actually know your name. I'm Claude, an AI assistant created by Anthropic.\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: {\\n\",\n-       \"      id: \\u001b[32m\\\"msg_01TNDCwsU7ruVoqJwjKqNrzJ\\\"\\u001b[39m,\\n\",\n-       \"      type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"      role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"      model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"      stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"      usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m27\\u001b[39m },\\n\",\n-       \"      stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n+       \"  \\\"id\\\": \\\"chatcmpl-A64ogC7owxmPla3ggZERNCFZpVHSp\\\",\\n\",\n+       \"  \\\"content\\\": \\\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. If you'd like to tell me your name, feel free!\\\",\\n\",\n+       \"  \\\"additional_kwargs\\\": {},\\n\",\n+       \"  \\\"response_metadata\\\": {\\n\",\n+       \"    \\\"tokenUsage\\\": {\\n\",\n+       \"      \\\"completionTokens\\\": 39,\\n\",\n+       \"      \\\"promptTokens\\\": 11,\\n\",\n+       \"      \\\"totalTokens\\\": 50\\n\",\n        \"    },\\n\",\n-       \"    response_metadata: {}\\n\",\n-       \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"I'm afraid I don't actually know your name. I'm Claude, an AI assistant created by Anthropic.\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01TNDCwsU7ruVoqJwjKqNrzJ\\\"\\u001b[39m,\\n\",\n-       \"    type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"    role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m27\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n+       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n        \"  },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01TNDCwsU7ruVoqJwjKqNrzJ\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m12\\u001b[39m, output_tokens: \\u001b[33m27\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n+       \"  \\\"tool_calls\\\": [],\\n\",\n+       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"  \\\"usage_metadata\\\": {\\n\",\n+       \"    \\\"input_tokens\\\": 11,\\n\",\n+       \"    \\\"output_tokens\\\": 39,\\n\",\n+       \"    \\\"total_tokens\\\": 50\\n\",\n+       \"  }\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 2,\n+     \"execution_count\": 3,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -233,54 +211,35 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 4,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n        \"AIMessage {\\n\",\n-       \"  lc_serializable: \\u001b[33mtrue\\u001b[39m,\\n\",\n-       \"  lc_kwargs: {\\n\",\n-       \"    content: \\u001b[32m\\\"You said your name is Bob.\\\"\\u001b[39m,\\n\",\n-       \"    tool_calls: [],\\n\",\n-       \"    invalid_tool_calls: [],\\n\",\n-       \"    additional_kwargs: {\\n\",\n-       \"      id: \\u001b[32m\\\"msg_01AEQMme3Z1MFKHW8PeDBJ7g\\\"\\u001b[39m,\\n\",\n-       \"      type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"      role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"      model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"      stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"      usage: { input_tokens: \\u001b[33m33\\u001b[39m, output_tokens: \\u001b[33m10\\u001b[39m },\\n\",\n-       \"      stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n+       \"  \\\"id\\\": \\\"chatcmpl-A64ohhg3P4BuIiw8mUCLI3zYHNOvS\\\",\\n\",\n+       \"  \\\"content\\\": \\\"Your name is Bob! How can I help you today, Bob?\\\",\\n\",\n+       \"  \\\"additional_kwargs\\\": {},\\n\",\n+       \"  \\\"response_metadata\\\": {\\n\",\n+       \"    \\\"tokenUsage\\\": {\\n\",\n+       \"      \\\"completionTokens\\\": 14,\\n\",\n+       \"      \\\"promptTokens\\\": 33,\\n\",\n+       \"      \\\"totalTokens\\\": 47\\n\",\n        \"    },\\n\",\n-       \"    response_metadata: {}\\n\",\n+       \"    \\\"finish_reason\\\": \\\"stop\\\"\\n\",\n        \"  },\\n\",\n-       \"  lc_namespace: [ \\u001b[32m\\\"langchain_core\\\"\\u001b[39m, \\u001b[32m\\\"messages\\\"\\u001b[39m ],\\n\",\n-       \"  content: \\u001b[32m\\\"You said your name is Bob.\\\"\\u001b[39m,\\n\",\n-       \"  name: \\u001b[90mundefined\\u001b[39m,\\n\",\n-       \"  additional_kwargs: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01AEQMme3Z1MFKHW8PeDBJ7g\\\"\\u001b[39m,\\n\",\n-       \"    type: \\u001b[32m\\\"message\\\"\\u001b[39m,\\n\",\n-       \"    role: \\u001b[32m\\\"assistant\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m33\\u001b[39m, output_tokens: \\u001b[33m10\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  response_metadata: {\\n\",\n-       \"    id: \\u001b[32m\\\"msg_01AEQMme3Z1MFKHW8PeDBJ7g\\\"\\u001b[39m,\\n\",\n-       \"    model: \\u001b[32m\\\"claude-3-sonnet-20240229\\\"\\u001b[39m,\\n\",\n-       \"    stop_sequence: \\u001b[1mnull\\u001b[22m,\\n\",\n-       \"    usage: { input_tokens: \\u001b[33m33\\u001b[39m, output_tokens: \\u001b[33m10\\u001b[39m },\\n\",\n-       \"    stop_reason: \\u001b[32m\\\"end_turn\\\"\\u001b[39m\\n\",\n-       \"  },\\n\",\n-       \"  tool_calls: [],\\n\",\n-       \"  invalid_tool_calls: []\\n\",\n+       \"  \\\"tool_calls\\\": [],\\n\",\n+       \"  \\\"invalid_tool_calls\\\": [],\\n\",\n+       \"  \\\"usage_metadata\\\": {\\n\",\n+       \"    \\\"input_tokens\\\": 33,\\n\",\n+       \"    \\\"output_tokens\\\": 14,\\n\",\n+       \"    \\\"total_tokens\\\": 47\\n\",\n+       \"  }\\n\",\n        \"}\"\n       ]\n      },\n-     \"execution_count\": 3,\n+     \"execution_count\": 4,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -330,7 +289,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 5,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -371,16 +330,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 6,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"Hi Bob, nice to meet you! I'm an AI assistant. I'll remember that your name is Bob as we continue ou\\\"\\u001b[39m... 110 more characters\"\n+       \"\\u001b[32m\\\"Hi Bob! How can I assist you today?\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 5,\n+     \"execution_count\": 6,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -401,16 +360,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 7,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"Your name is Bob. You introduced yourself as Bob at the start of our conversation.\\\"\\u001b[39m\"\n+       \"\\u001b[32m\\\"Your name is Bob. How can I help you today?\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 7,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -432,16 +391,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 8,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"I'm afraid I don't actually know your name. As an AI assistant without any prior context about you, \\\"\\u001b[39m... 61 more characters\"\n+       \"\\u001b[32m\\\"I'm sorry, but I don't have your name. If you tell me, I'll remember it for our future conversations\\\"\\u001b[39m... 1 more character\"\n       ]\n      },\n-     \"execution_count\": 7,\n+     \"execution_count\": 8,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -469,16 +428,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 9,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m`Your name is Bob. I clearly remember you telling me \\\"Hi! I'm Bob\\\" when we started talking.`\\u001b[39m\"\n+       \"\\u001b[32m\\\"Your name is Bob. What would you like to talk about?\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 8,\n+     \"execution_count\": 9,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -519,7 +478,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 10,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -551,7 +510,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 11,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -573,16 +532,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 12,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"I'm afraid I don't actually know your name. You haven't provided that detail to me yet.\\\"\\u001b[39m\"\n+       \"\\u001b[32m\\\"You haven't shared your name with me yet. What is it?\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 11,\n+     \"execution_count\": 12,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -606,16 +565,16 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 13,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"You said earlier that you like vanilla ice cream.\\\"\\u001b[39m\"\n+       \"\\u001b[32m\\\"Your favorite ice cream is vanilla!\\\"\\u001b[39m\"\n       ]\n      },\n-     \"execution_count\": 12,\n+     \"execution_count\": 13,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -645,7 +604,7 @@\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"I'm afraid I don't actually know your name since you haven't provided it to me yet.  I don't have pe\\\"\\u001b[39m... 66 more characters\"\n+       \"\\u001b[32m\\\"You haven't shared your name with me yet. What is it?\\\"\\u001b[39m\"\n       ]\n      },\n      \"execution_count\": 14,\n@@ -657,7 +616,7 @@\n     \"const messageHistories2: Record<string, InMemoryChatMessageHistory> = {};\\n\",\n     \"\\n\",\n     \"const withMessageHistory2 = new RunnableWithMessageHistory({\\n\",\n-    \"  runnable: chain,\\n\",\n+    \"  runnable: chain2,\\n\",\n     \"  getMessageHistory: async (sessionId) => {\\n\",\n     \"    if (messageHistories2[sessionId] === undefined) {\\n\",\n     \"      const messageHistory = new InMemoryChatMessageHistory();\\n\",\n@@ -679,6 +638,7 @@\n     \"const response7 = await withMessageHistory2.invoke(\\n\",\n     \"  {\\n\",\n     \"    input: \\\"whats my name?\\\",\\n\",\n+    \"    chat_history: [],\\n\",\n     \"  },\\n\",\n     \"  config4,\\n\",\n     \")\\n\",\n@@ -701,7 +661,7 @@\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"\\u001b[32m\\\"I'm sorry, I don't have any information about your favorite ice cream flavor since you haven't share\\\"\\u001b[39m... 167 more characters\"\n+       \"\\u001b[32m\\\"You haven't mentioned your favorite ice cream yet. What is it?\\\"\\u001b[39m\"\n       ]\n      },\n      \"execution_count\": 15,\n@@ -711,7 +671,8 @@\n    ],\n    \"source\": [\n     \"const response8 = await withMessageHistory2.invoke({\\n\",\n-    \"  input: \\\"whats my favorite ice cream?\\\"\\n\",\n+    \"  input: \\\"whats my favorite ice cream?\\\",\\n\",\n+    \"  chat_history: [],\\n\",\n     \"}, config4);\\n\",\n     \"\\n\",\n     \"response8.content\"\n@@ -739,7 +700,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 18,\n+   \"execution_count\": 16,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n@@ -748,36 +709,38 @@\n      \"text\": [\n       \"| \\n\",\n       \"| Hi\\n\",\n-      \"|  Tod\\n\",\n-      \"| d!\\n\",\n+      \"|  Todd\\n\",\n+      \"| !\\n\",\n       \"|  Here\\n\",\n-      \"| 's\\n\",\n+      \"| ’s\\n\",\n       \"|  a\\n\",\n-      \"|  silly\\n\",\n       \"|  joke\\n\",\n       \"|  for\\n\",\n       \"|  you\\n\",\n       \"| :\\n\",\n-      \"| \\n\",\n+      \"|  \\n\",\n+      \"\\n\",\n       \"\\n\",\n-      \"Why\\n\",\n-      \"|  di\\n\",\n-      \"| d the\\n\",\n-      \"|  tom\\n\",\n-      \"| ato\\n\",\n-      \"|  turn\\n\",\n-      \"|  re\\n\",\n-      \"| d?\\n\",\n-      \"|  Because\\n\",\n-      \"|  it\\n\",\n-      \"|  saw\\n\",\n+      \"| Why\\n\",\n+      \"|  did\\n\",\n       \"|  the\\n\",\n-      \"|  sal\\n\",\n-      \"| a\\n\",\n-      \"| d \\n\",\n-      \"| dressing\\n\",\n+      \"|  scare\\n\",\n+      \"| crow\\n\",\n+      \"|  win\\n\",\n+      \"|  an\\n\",\n+      \"|  award\\n\",\n+      \"| ?\\n\",\n+      \"|  \\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"| Because\\n\",\n+      \"|  he\\n\",\n+      \"|  was\\n\",\n+      \"|  outstanding\\n\",\n+      \"|  in\\n\",\n+      \"|  his\\n\",\n+      \"|  field\\n\",\n       \"| !\\n\",\n-      \"| \\n\",\n       \"| \\n\"\n      ]\n     }\n@@ -791,6 +754,7 @@\n     \"\\n\",\n     \"const stream = await withMessageHistory2.stream({\\n\",\n     \"  input: \\\"hi! I'm todd. tell me a joke\\\",\\n\",\n+    \"  chat_history: [],\\n\",\n     \"}, config5);\\n\",\n     \"\\n\",\n     \"for await (const chunk of stream) {\\n\",",
          "docs/core_docs/docusaurus.config.js": "@@ -137,8 +137,10 @@ const config = {\n     ({\n       announcementBar: {\n         content:\n-          'LangChain 0.2 is out! Leave feedback on the v0.2 docs <a href=\"https://github.com/langchain-ai/langchainjs/discussions/5386\">here</a>. You can view the v0.1 docs <a href=\"/v0.1/docs/get_started/introduction/\">here</a>.',\n+          'Share your thoughts on AI agents. <a target=\"_blank\" href=\"https://langchain.typeform.com/state-of-agents\">Take the 3-min survey</a>.',\n         isCloseable: true,\n+        backgroundColor: \"rgba(53, 151, 147, 0.1)\",\n+        textColor: \"rgb(53, 151, 147)\",\n       },\n       prism: {\n         theme: {",
          "docs/core_docs/src/theme/FeatureTables.js": "@@ -239,6 +239,16 @@ const FEATURE_TABLES = {\n         apiLink:\n           \"https://api.python.langchain.com/en/latest/chat_models/langchain_upstage.chat_models.ChatUpstage.html#langchain_upstage.chat_models.ChatUpstage\",\n       },\n+      {\n+        name: \"Arcjet Redact\",\n+        package: \"langchain-community\",\n+        link: \"arcjet\",\n+        structured_output: false,\n+        tool_calling: false,\n+        json_mode: false,\n+        multimodal: false,\n+        local: true,\n+      },\n     ],\n   },\n   llms: {\n@@ -261,6 +271,11 @@ const FEATURE_TABLES = {\n         apiLink:\n           \"https://api.python.langchain.com/en/latest/llms/langchain_ai21.llms.AI21LLM.html#langchain_ai21.llms.AI21LLM\",\n       },\n+      {\n+        name: \"Arcjet Redact\",\n+        link: \"arcjet\",\n+        package: \"langchain-community\",\n+      },\n       {\n         name: \"AnthropicLLM\",\n         link: \"anthropic\",",
          "environment_tests/test-exports-bun/src/import.cjs": "@@ -6,7 +6,6 @@ async function test() {\n   const { MemoryVectorStore } = await import(\"langchain/vectorstores/memory\");\n   const { OpenAIEmbeddings } = await import(\"@langchain/openai\");\n   const { Document } = await import(\"@langchain/core/documents\");\n-  const { CSVLoader } = await import(\"@langchain/community/document_loaders/fs/csv\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n@@ -32,13 +31,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-bun/src/index.js": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n \n // Test exports\n@@ -34,10 +33,3 @@ await vs.addVectors(\n );\n \n assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-// Test CSVLoader\n-const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-const docs = await loader.load();\n-\n-assert(docs.length === 2);",
          "environment_tests/test-exports-bun/src/index.ts": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n \n async function test(useAzure: boolean = false) {\n   // Test exports\n@@ -44,13 +43,6 @@ async function test(useAzure: boolean = false) {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test(false)",
          "environment_tests/test-exports-bun/src/require.cjs": "@@ -5,7 +5,6 @@ const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n const { MemoryVectorStore } = require(\"langchain/vectorstores/memory\");\n const { OpenAIEmbeddings } = require(\"@langchain/openai\");\n const { Document } = require(\"@langchain/core/documents\");\n-const { CSVLoader } = require(\"@langchain/community/document_loaders/fs/csv\");\n \n async function test() {\n   // Test exports\n@@ -32,13 +31,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-cf/package.json": "@@ -9,9 +9,10 @@\n   },\n   \"dependencies\": {\n     \"@langchain/anthropic\": \"workspace:*\",\n-    \"@langchain/community\": \"workspace:*\",\n     \"@langchain/core\": \"workspace:*\",\n     \"@langchain/openai\": \"workspace:*\",\n+    \"@tsconfig/recommended\": \"^1.0.2\",\n+    \"d3-dsv\": \"2\",\n     \"langchain\": \"workspace:*\",\n     \"wrangler\": \"^3.19.0\",\n     \"vitest\": \"0.34.3\",",
          "environment_tests/test-exports-cf/src/index.ts": "@@ -20,7 +20,6 @@ import {\n } from \"@langchain/core/prompts\";\n import { OpenAI } from \"@langchain/openai\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n-import { HNLoader } from \"@langchain/community/document_loaders/web/hn\";\n \n export interface Env {\n   OPENAI_API_KEY?: string;\n@@ -51,9 +50,6 @@ export default {\n     new OpenAI(constructorParameters);\n     const emb = new OpenAIEmbeddings(constructorParameters);\n \n-    // Test a document loader\n-    new HNLoader(\"https://news.ycombinator.com/item?id=28275939\");\n-\n     // Test a chain + prompt + model\n     const chain = new LLMChain({\n       llm: new ChatOpenAI(constructorParameters),",
          "environment_tests/test-exports-cjs/src/import.js": "@@ -6,7 +6,6 @@ async function test() {\n   const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n   const { HuggingFaceTransformersEmbeddings } = await import(\"@langchain/community/embeddings/hf_transformers\");\n   const { Document } = await import(\"@langchain/core/documents\");\n-  const { CSVLoader } = await import(\"@langchain/community/document_loaders/fs/csv\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-cjs/src/index.mjs": "@@ -5,7 +5,6 @@ import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n \n // Test exports\n assert(typeof OpenAI === \"function\");\n@@ -38,10 +37,3 @@ await vs.addVectors(\n );\n \n assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-// Test CSVLoader\n-const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-const docs = await loader.load();\n-\n-assert(docs.length === 2);",
          "environment_tests/test-exports-cjs/src/index.ts": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n \n async function test(useAzure: boolean = false) {\n   // Test exports\n@@ -48,13 +47,6 @@ async function test(useAzure: boolean = false) {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test(false)",
          "environment_tests/test-exports-cjs/src/require.js": "@@ -5,7 +5,6 @@ const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n const { HuggingFaceTransformersEmbeddings } = require(\"@langchain/community/embeddings/hf_transformers\");\n const { Document } = require(\"@langchain/core/documents\");\n-const { CSVLoader } = require(\"@langchain/community/document_loaders/fs/csv\");\n \n async function test() {\n   // Test exports\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-esbuild/package.json": "@@ -24,7 +24,6 @@\n     \"@langchain/core\": \"workspace:*\",\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n-    \"d3-dsv\": \"2\",\n     \"esbuild\": \"^0.17.18\",\n     \"hnswlib-node\": \"^3.0.0\",\n     \"langchain\": \"workspace:*\",",
          "environment_tests/test-exports-esbuild/src/import.cjs": "@@ -6,7 +6,6 @@ async function test() {\n   const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n   const { OpenAIEmbeddings } = await import(\"@langchain/openai\");\n   const { Document } = await import(\"@langchain/core/documents\");\n-  const { CSVLoader } = await import(\"@langchain/community/document_loaders/fs/csv\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-esbuild/src/index.js": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n \n // Test exports\n@@ -41,10 +40,3 @@ await vs.addVectors(\n );\n \n assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-// Test CSVLoader\n-const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-const docs = await loader.load();\n-\n-assert(docs.length === 2);",
          "environment_tests/test-exports-esbuild/src/require.cjs": "@@ -5,7 +5,6 @@ const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n const { OpenAIEmbeddings } = require(\"@langchain/openai\");\n const { Document } = require(\"@langchain/core/documents\");\n-const { CSVLoader } = require(\"@langchain/community/document_loaders/fs/csv\");\n \n async function test() {\n   // Test exports\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-esbuild/src/typescript.ts": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { OpenAIEmbeddings } from \"@langchain/openai\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n \n async function test(useAzure: boolean = false) {\n   // Test exports\n@@ -49,13 +48,6 @@ async function test(useAzure: boolean = false) {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test(false)",
          "environment_tests/test-exports-esm/package.json": "@@ -28,7 +28,6 @@\n     \"@langchain/openai\": \"workspace:*\",\n     \"@tsconfig/recommended\": \"^1.0.2\",\n     \"@xenova/transformers\": \"^2.17.2\",\n-    \"d3-dsv\": \"2\",\n     \"hnswlib-node\": \"^3.0.0\",\n     \"langchain\": \"workspace:*\",\n     \"typescript\": \"^5.0.0\"",
          "environment_tests/test-exports-esm/src/import.cjs": "@@ -6,7 +6,6 @@ async function test() {\n   const { HNSWLib } = await import(\"@langchain/community/vectorstores/hnswlib\");\n   const { HuggingFaceTransformersEmbeddings } = await import(\"@langchain/community/embeddings/hf_transformers\");\n   const { Document } = await import(\"@langchain/core/documents\");\n-  const { CSVLoader } = await import(\"@langchain/community/document_loaders/fs/csv\");\n \n   // Test exports\n   assert(typeof OpenAI === \"function\");\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-esm/src/index.js": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n import { CallbackManager } from \"@langchain/core/callbacks/manager\";\n \n // Test exports\n@@ -41,10 +40,3 @@ await vs.addVectors(\n );\n \n assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-// Test CSVLoader\n-const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-const docs = await loader.load();\n-\n-assert(docs.length === 2);",
          "environment_tests/test-exports-esm/src/index.ts": "@@ -5,7 +5,6 @@ import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n import { HuggingFaceTransformersEmbeddings } from \"@langchain/community/embeddings/hf_transformers\";\n import { Document } from \"@langchain/core/documents\";\n-import { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n \n async function test(useAzure: boolean = false) {\n   // Test exports\n@@ -49,13 +48,6 @@ async function test(useAzure: boolean = false) {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test(false)",
          "environment_tests/test-exports-esm/src/require.cjs": "@@ -5,7 +5,6 @@ const { ChatPromptTemplate } = require(\"@langchain/core/prompts\");\n const { HNSWLib } = require(\"@langchain/community/vectorstores/hnswlib\");\n const { HuggingFaceTransformersEmbeddings } = require(\"@langchain/community/embeddings/hf_transformers\");\n const { Document } = require(\"@langchain/core/documents\");\n-const { CSVLoader } = require(\"@langchain/community/document_loaders/fs/csv\");\n \n async function test() {\n   // Test exports\n@@ -39,13 +38,6 @@ async function test() {\n   );\n \n   assert((await vs.similaritySearchVectorWithScore([0, 0, 1], 1)).length === 1);\n-\n-  // Test CSVLoader\n-  const loader = new CSVLoader(new Blob([\"a,b,c\\n1,2,3\\n4,5,6\"]));\n-\n-  const docs = await loader.load();\n-\n-  assert(docs.length === 2);\n }\n \n test()",
          "environment_tests/test-exports-tsc/main.ts": "@@ -1,5 +1,5 @@\n import { ChatOpenAI } from \"@langchain/openai\";\n-import { createOpenAIFunctionsAgent, AgentExecutor } from \"langchain/agents\";\n+import { createOpenAIToolsAgent, AgentExecutor } from \"langchain/agents\";\n import { pull } from \"langchain/hub\";\n import type { ChatPromptTemplate } from \"@langchain/core/prompts\";\n \n@@ -11,7 +11,7 @@ const prompt = await pull<ChatPromptTemplate>(\n   \"hwchase17/openai-functions-agent\"\n );\n \n-const agent = await createOpenAIFunctionsAgent({\n+const agent = await createOpenAIToolsAgent({\n   llm: model,\n   prompt,\n   tools: []",
          "environment_tests/test-exports-tsc/package.json": "@@ -20,13 +20,10 @@\n     \"@langchain/core\": \"workspace:*\",\n     \"@langchain/openai\": \"workspace:*\",\n     \"langchain\": \"workspace:*\",\n-    \"typescript\": \"latest\"\n+    \"typescript\": \"5.5.4\"\n   },\n   \"devDependencies\": {\n     \"@types/node\": \"^18.15.11\",\n     \"prettier\": \"^2.8.3\"\n-  },\n-  \"resolutions\": {\n-    \"@langchain/core\": \"~0.2.0\"\n   }\n }",
          "examples/src/indexes/vector_stores/chroma/fromDocs.ts": "@@ -12,7 +12,7 @@ const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n   url: \"http://localhost:8000\", // Optional, will default to this value\n   collectionMetadata: {\n     \"hnsw:space\": \"cosine\",\n-  }, // Optional, can be used to specify the distance method of the embedding space https://docs.trychroma.com/usage-guide#changing-the-distance-function\n+  }, // Optional, can be used to specify the distance method of the embedding space https://docs.trychroma.com/guides#changing-the-distance-function\n });\n \n // Search for the most similar document",
          "langchain-core/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/core\",\n-  \"version\": \"0.2.31\",\n+  \"version\": \"0.2.32\",\n   \"description\": \"Core LangChain.js abstractions and schemas\",\n   \"type\": \"module\",\n   \"engines\": {",
          "langchain-core/src/messages/ai.ts": "@@ -143,6 +143,10 @@ export function isAIMessage(x: BaseMessage): x is AIMessage {\n   return x._getType() === \"ai\";\n }\n \n+export function isAIMessageChunk(x: BaseMessageChunk): x is AIMessageChunk {\n+  return x._getType() === \"ai\";\n+}\n+\n export type AIMessageChunkFields = AIMessageFields & {\n   tool_call_chunks?: ToolCallChunk[];\n };",
          "langchain-core/src/output_parsers/openai_tools/json_output_tools_parsers.ts": "@@ -1,8 +1,13 @@\n import { z } from \"zod\";\n-import { ChatGeneration } from \"../../outputs.js\";\n-import { BaseLLMOutputParser, OutputParserException } from \"../base.js\";\n+import { ChatGeneration, ChatGenerationChunk } from \"../../outputs.js\";\n+import { OutputParserException } from \"../base.js\";\n import { parsePartialJson } from \"../json.js\";\n import { InvalidToolCall, ToolCall } from \"../../messages/tool.js\";\n+import {\n+  BaseCumulativeTransformOutputParser,\n+  BaseCumulativeTransformOutputParserInput,\n+} from \"../transform.js\";\n+import { isAIMessage } from \"../../messages/ai.js\";\n \n export type ParsedToolCall = {\n   id?: string;\n@@ -23,7 +28,7 @@ export type ParsedToolCall = {\n export type JsonOutputToolsParserParams = {\n   /** Whether to return the tool call id. */\n   returnId?: boolean;\n-};\n+} & BaseCumulativeTransformOutputParserInput;\n \n export function parseToolCall(\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n@@ -35,6 +40,11 @@ export function parseToolCall(\n   rawToolCall: Record<string, any>,\n   options?: { returnId?: boolean; partial?: false }\n ): ToolCall;\n+export function parseToolCall(\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  rawToolCall: Record<string, any>,\n+  options?: { returnId?: boolean; partial?: boolean }\n+): ToolCall | undefined;\n export function parseToolCall(\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n   rawToolCall: Record<string, any>,\n@@ -112,9 +122,9 @@ export function makeInvalidToolCall(\n /**\n  * Class for parsing the output of a tool-calling LLM into a JSON object.\n  */\n-export class JsonOutputToolsParser extends BaseLLMOutputParser<\n-  ParsedToolCall[]\n-> {\n+export class JsonOutputToolsParser<\n+  T\n+> extends BaseCumulativeTransformOutputParser<T> {\n   static lc_name() {\n     return \"JsonOutputToolsParser\";\n   }\n@@ -130,31 +140,64 @@ export class JsonOutputToolsParser extends BaseLLMOutputParser<\n     this.returnId = fields?.returnId ?? this.returnId;\n   }\n \n+  protected _diff() {\n+    throw new Error(\"Not supported.\");\n+  }\n+\n+  async parse(): Promise<T> {\n+    throw new Error(\"Not implemented.\");\n+  }\n+\n+  async parseResult(generations: ChatGeneration[]): Promise<T> {\n+    const result = await this.parsePartialResult(generations, false);\n+    return result;\n+  }\n+\n   /**\n    * Parses the output and returns a JSON object. If `argsOnly` is true,\n    * only the arguments of the function call are returned.\n    * @param generations The output of the LLM to parse.\n    * @returns A JSON object representation of the function call or its arguments.\n    */\n-  async parseResult(generations: ChatGeneration[]): Promise<ParsedToolCall[]> {\n-    const toolCalls = generations[0].message.additional_kwargs.tool_calls;\n-    if (!toolCalls) {\n-      throw new Error(\n-        `No tools_call in message ${JSON.stringify(generations)}`\n+  async parsePartialResult(\n+    generations: ChatGenerationChunk[] | ChatGeneration[],\n+    partial = true\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  ): Promise<any> {\n+    const message = generations[0].message;\n+    let toolCalls;\n+    if (isAIMessage(message) && message.tool_calls?.length) {\n+      toolCalls = message.tool_calls.map((toolCall) => {\n+        const { id, ...rest } = toolCall;\n+        if (!this.returnId) {\n+          return rest;\n+        }\n+        return {\n+          id,\n+          ...rest,\n+        };\n+      });\n+    } else if (message.additional_kwargs.tool_calls !== undefined) {\n+      const rawToolCalls = JSON.parse(\n+        JSON.stringify(message.additional_kwargs.tool_calls)\n       );\n+      toolCalls = rawToolCalls.map((rawToolCall: Record<string, unknown>) => {\n+        return parseToolCall(rawToolCall, { returnId: this.returnId, partial });\n+      });\n+    }\n+    if (!toolCalls) {\n+      return [];\n     }\n-    const clonedToolCalls = JSON.parse(JSON.stringify(toolCalls));\n     const parsedToolCalls = [];\n-    for (const toolCall of clonedToolCalls) {\n-      const parsedToolCall = parseToolCall(toolCall, { partial: true });\n-      if (parsedToolCall !== undefined) {\n+    for (const toolCall of toolCalls) {\n+      if (toolCall !== undefined) {\n         // backward-compatibility with previous\n         // versions of Langchain JS, which uses `name` and `arguments`\n         // @ts-expect-error name and arguemnts are defined by Object.defineProperty\n         const backwardsCompatibleToolCall: ParsedToolCall = {\n-          type: parsedToolCall.name,\n-          args: parsedToolCall.args,\n-          id: parsedToolCall.id,\n+          type: toolCall.name,\n+          args: toolCall.args,\n+          id: toolCall.id,\n         };\n         Object.defineProperty(backwardsCompatibleToolCall, \"name\", {\n           get() {\n@@ -180,10 +223,8 @@ export type JsonOutputKeyToolsParserParams<\n > = {\n   keyName: string;\n   returnSingle?: boolean;\n-  /** Whether to return the tool call id. */\n-  returnId?: boolean;\n   zodSchema?: z.ZodType<T>;\n-};\n+} & JsonOutputToolsParserParams;\n \n /**\n  * Class for parsing the output of a tool-calling LLM into a JSON object if you are\n@@ -192,7 +233,7 @@ export type JsonOutputKeyToolsParserParams<\n export class JsonOutputKeyToolsParser<\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n   T extends Record<string, any> = Record<string, any>\n-> extends BaseLLMOutputParser<T> {\n+> extends JsonOutputToolsParser<T> {\n   static lc_name() {\n     return \"JsonOutputKeyToolsParser\";\n   }\n@@ -209,15 +250,12 @@ export class JsonOutputKeyToolsParser<\n   /** Whether to return only the first tool call. */\n   returnSingle = false;\n \n-  initialParser: JsonOutputToolsParser;\n-\n   zodSchema?: z.ZodType<T>;\n \n   constructor(params: JsonOutputKeyToolsParserParams<T>) {\n     super(params);\n     this.keyName = params.keyName;\n     this.returnSingle = params.returnSingle ?? this.returnSingle;\n-    this.initialParser = new JsonOutputToolsParser(params);\n     this.zodSchema = params.zodSchema;\n   }\n \n@@ -240,17 +278,45 @@ export class JsonOutputKeyToolsParser<\n     }\n   }\n \n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  async parsePartialResult(generations: ChatGeneration[]): Promise<any> {\n+    const results = await super.parsePartialResult(generations);\n+    const matchingResults = results.filter(\n+      (result: ParsedToolCall) => result.type === this.keyName\n+    );\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    let returnedValues: ParsedToolCall[] | Record<string, any>[] =\n+      matchingResults;\n+    if (!matchingResults.length) {\n+      return undefined;\n+    }\n+    if (!this.returnId) {\n+      returnedValues = matchingResults.map(\n+        (result: ParsedToolCall) => result.args\n+      );\n+    }\n+    if (this.returnSingle) {\n+      return returnedValues[0];\n+    }\n+    return returnedValues;\n+  }\n+\n   // eslint-disable-next-line @typescript-eslint/no-explicit-any\n   async parseResult(generations: ChatGeneration[]): Promise<any> {\n-    const results = await this.initialParser.parseResult(generations);\n+    const results = await super.parsePartialResult(generations, false);\n     const matchingResults = results.filter(\n-      (result) => result.type === this.keyName\n+      (result: ParsedToolCall) => result.type === this.keyName\n     );\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     let returnedValues: ParsedToolCall[] | Record<string, any>[] =\n       matchingResults;\n+    if (!matchingResults.length) {\n+      return undefined;\n+    }\n     if (!this.returnId) {\n-      returnedValues = matchingResults.map((result) => result.args);\n+      returnedValues = matchingResults.map(\n+        (result: ParsedToolCall) => result.args\n+      );\n     }\n     if (this.returnSingle) {\n       return this._validateResult(returnedValues[0]);",
          "langchain-core/src/output_parsers/openai_tools/tests/json_output_tools_parser.test.ts": "@@ -1,8 +1,10 @@\n+/* eslint-disable @typescript-eslint/no-explicit-any */\n import { test, expect } from \"@jest/globals\";\n import { z } from \"zod\";\n import { JsonOutputKeyToolsParser } from \"../json_output_tools_parsers.js\";\n-import { AIMessage } from \"../../../messages/index.js\";\n import { OutputParserException } from \"../../base.js\";\n+import { AIMessage, AIMessageChunk } from \"../../../messages/ai.js\";\n+import { RunnableLambda } from \"../../../runnables/base.js\";\n \n test(\"JSONOutputKeyToolsParser invoke\", async () => {\n   const outputParser = new JsonOutputKeyToolsParser({\n@@ -87,3 +89,144 @@ test(\"JSONOutputKeyToolsParser can validate a proper input\", async () => {\n   );\n   expect(result).toEqual({ testKey: \"testval\" });\n });\n+\n+test(\"JSONOutputKeyToolsParser invoke with a top-level tool call\", async () => {\n+  const outputParser = new JsonOutputKeyToolsParser({\n+    keyName: \"testing\",\n+    returnSingle: true,\n+  });\n+  const result = await outputParser.invoke(\n+    new AIMessage({\n+      content: \"\",\n+      tool_calls: [\n+        {\n+          id: \"test\",\n+          name: \"testing\",\n+          args: { testKey: 9 },\n+        },\n+      ],\n+    })\n+  );\n+  expect(result).toEqual({ testKey: 9 });\n+});\n+\n+test(\"JSONOutputKeyToolsParser with a top-level tool call and passed schema throws\", async () => {\n+  const outputParser = new JsonOutputKeyToolsParser({\n+    keyName: \"testing\",\n+    returnSingle: true,\n+    zodSchema: z.object({\n+      testKey: z.string(),\n+    }),\n+  });\n+  try {\n+    await outputParser.invoke(\n+      new AIMessage({\n+        content: \"\",\n+        tool_calls: [\n+          {\n+            id: \"test\",\n+            name: \"testing\",\n+            args: { testKey: 9 },\n+          },\n+        ],\n+      })\n+    );\n+  } catch (e) {\n+    expect(e).toBeInstanceOf(OutputParserException);\n+  }\n+});\n+\n+test(\"JSONOutputKeyToolsParser with a top-level tool call can validate a proper input\", async () => {\n+  const outputParser = new JsonOutputKeyToolsParser({\n+    keyName: \"testing\",\n+    returnSingle: true,\n+    zodSchema: z.object({\n+      testKey: z.string(),\n+    }),\n+  });\n+  const result = await outputParser.invoke(\n+    new AIMessage({\n+      content: \"\",\n+      tool_calls: [\n+        {\n+          id: \"test\",\n+          name: \"testing\",\n+          args: { testKey: \"testval\" },\n+        },\n+      ],\n+    })\n+  );\n+  expect(result).toEqual({ testKey: \"testval\" });\n+});\n+\n+test(\"JSONOutputKeyToolsParser can handle streaming input\", async () => {\n+  const outputParser = new JsonOutputKeyToolsParser({\n+    keyName: \"testing\",\n+    returnSingle: true,\n+    zodSchema: z.object({\n+      testKey: z.string(),\n+    }),\n+  });\n+  const fakeModel = RunnableLambda.from(async function* () {\n+    yield new AIMessageChunk({\n+      content: \"\",\n+      tool_call_chunks: [\n+        {\n+          index: 0,\n+          id: \"test\",\n+          name: \"testing\",\n+          args: `{ \"testKey\":`,\n+          type: \"tool_call_chunk\",\n+        },\n+      ],\n+    });\n+    yield new AIMessageChunk({\n+      content: \"\",\n+      tool_call_chunks: [],\n+    });\n+    yield new AIMessageChunk({\n+      content: \"\",\n+      tool_call_chunks: [\n+        {\n+          index: 0,\n+          id: \"test\",\n+          args: ` \"testv`,\n+          type: \"tool_call_chunk\",\n+        },\n+      ],\n+    });\n+    yield new AIMessageChunk({\n+      content: \"\",\n+      tool_call_chunks: [\n+        {\n+          index: 0,\n+          id: \"test\",\n+          args: `al\" }`,\n+          type: \"tool_call_chunk\",\n+        },\n+      ],\n+    });\n+  });\n+  const stream = await (fakeModel as any).pipe(outputParser).stream();\n+  const chunks = [];\n+  for await (const chunk of stream) {\n+    chunks.push(chunk);\n+  }\n+  expect(chunks.length).toBeGreaterThan(1);\n+  expect(chunks.at(-1)).toEqual({ testKey: \"testval\" });\n+  // TODO: Fix typing issue\n+  const result = await (fakeModel as any).pipe(outputParser).invoke(\n+    new AIMessage({\n+      content: \"\",\n+      tool_calls: [\n+        {\n+          id: \"test\",\n+          name: \"testing\",\n+          args: { testKey: \"testval\" },\n+          type: \"tool_call\",\n+        },\n+      ],\n+    })\n+  );\n+  expect(result).toEqual({ testKey: \"testval\" });\n+});",
          "langchain-core/src/output_parsers/transform.ts": "@@ -134,4 +134,8 @@ export abstract class BaseCumulativeTransformOutputParser<\n       }\n     }\n   }\n+\n+  getFormatInstructions(): string {\n+    return \"\";\n+  }\n }",
          "langchain-core/src/utils/stream.ts": "@@ -71,6 +71,12 @@ export class IterableReadableStream<T>\n     return this;\n   }\n \n+  // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+  // @ts-ignore Not present in Node 18 types, required in latest Node 22\n+  async [Symbol.asyncDispose]() {\n+    await this.return();\n+  }\n+\n   static fromReadableStream<T>(stream: ReadableStream<T>) {\n     // From https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Using_readable_streams#reading_the_stream\n     const reader = stream.getReader();\n@@ -245,7 +251,7 @@ export class AsyncGeneratorWithSetup<\n   }\n \n   async return(\n-    value: TReturn | PromiseLike<TReturn>\n+    value?: TReturn | PromiseLike<TReturn>\n   ): Promise<IteratorResult<T>> {\n     return this.generator.return(value);\n   }\n@@ -257,6 +263,12 @@ export class AsyncGeneratorWithSetup<\n   [Symbol.asyncIterator]() {\n     return this;\n   }\n+\n+  // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n+  // @ts-ignore Not present in Node 18 types, required in latest Node 22\n+  async [Symbol.asyncDispose]() {\n+    await this.return();\n+  }\n }\n \n export async function pipeGeneratorWithSetup<",
          "langchain/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"langchain\",\n-  \"version\": \"0.2.18\",\n+  \"version\": \"0.2.19\",\n   \"description\": \"Typescript bindings for langchain\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -600,7 +600,7 @@\n     \"@aws-sdk/types\": \"^3.357.0\",\n     \"@azure/storage-blob\": \"^12.15.0\",\n     \"@browserbasehq/sdk\": \"^1.1.5\",\n-    \"@cloudflare/workers-types\": \"^4.20230922.0\",\n+    \"@cloudflare/workers-types\": \"^4.20240909.0\",\n     \"@faker-js/faker\": \"^7.6.0\",\n     \"@gomomento/sdk\": \"^1.51.1\",\n     \"@gomomento/sdk-core\": \"^1.51.1\",",
          "langchain/src/chains/openai_functions/openapi.ts": "@@ -175,7 +175,11 @@ export function convertOpenAPISchemaToJSONSchema(\n           openAPIProperty,\n           spec\n         );\n-        if (openAPIProperty.required && jsonSchema.required !== undefined) {\n+        if (\n+          (openAPIProperty.required ||\n+            schema.required?.includes(propertyName)) &&\n+          jsonSchema.required !== undefined\n+        ) {\n           jsonSchema.required.push(propertyName);\n         }\n         return jsonSchema;",
          "langchain/src/chains/openai_functions/tests/openapi.test.ts": "@@ -46,6 +46,19 @@ test(\"Test convert OpenAPI params to JSON Schema\", async () => {\n                 },\n               },\n             },\n+            {\n+              name: \"objectParamWithRequiredFields\",\n+              in: \"query\",\n+              schema: {\n+                type: \"object\",\n+                required: [\"fooRequired\"],\n+                properties: {\n+                  fooRequired: {\n+                    type: \"string\",\n+                  },\n+                },\n+              },\n+            },\n             {\n               name: \"stringArrayParam\",\n               in: \"query\",\n@@ -195,6 +208,12 @@ test(\"Test convert OpenAPI params to JSON Schema\", async () => {\n   expectType(\"string\", typedObjectParamSchema.properties.foo);\n   expectType(\"number\", typedObjectParamSchema.properties.bar);\n \n+  const objectParamWithRequiredFieldSchema = convertOpenAPISchemaToJSONSchema(\n+    getParamSchema(createWidget, \"objectParamWithRequiredFields\"),\n+    spec\n+  ) as JsonSchema7ObjectType;\n+  expect(objectParamWithRequiredFieldSchema.required).toContain(\"fooRequired\");\n+\n   const stringArrayParamSchema = convertOpenAPISchemaToJSONSchema(\n     getParamSchema(createWidget, \"stringArrayParam\"),\n     spec",
          "langchain/src/storage/file_system.ts": "@@ -26,6 +26,11 @@ import { BaseStore } from \"@langchain/core/stores\";\n  *   await store.mdelete([key]);\n  * }\n  * ```\n+ *\n+ * @security **Security Notice** This file store\n+ * can alter any text file in the provided directory and any subfolders.\n+ * Make sure that the path you specify when initializing the store is free\n+ * of other files.\n  */\n export class LocalFileStore extends BaseStore<string, Uint8Array> {\n   lc_namespace = [\"langchain\", \"storage\"];\n@@ -43,6 +48,12 @@ export class LocalFileStore extends BaseStore<string, Uint8Array> {\n    * @returns Promise that resolves to the parsed file content.\n    */\n   private async getParsedFile(key: string): Promise<Uint8Array | undefined> {\n+    // Validate the key to prevent path traversal\n+    if (!/^[a-zA-Z0-9_\\-:.]+$/.test(key)) {\n+      throw new Error(\n+        \"Invalid key. Only alphanumeric characters, underscores, hyphens, colons, and periods are allowed.\"\n+      );\n+    }\n     try {\n       const fileContent = await fs.readFile(this.getFullPath(key));\n       if (!fileContent) {\n@@ -87,11 +98,26 @@ export class LocalFileStore extends BaseStore<string, Uint8Array> {\n   private getFullPath(key: string): string {\n     try {\n       const keyAsTxtFile = `${key}.txt`;\n-      const fullPath = path.join(this.rootPath, keyAsTxtFile);\n+\n+      // Validate the key to prevent path traversal\n+      if (!/^[a-zA-Z0-9_.\\-/]+$/.test(key)) {\n+        throw new Error(`Invalid characters in key: ${key}`);\n+      }\n+\n+      const fullPath = path.resolve(this.rootPath, keyAsTxtFile);\n+      const commonPath = path.resolve(this.rootPath);\n+\n+      if (!fullPath.startsWith(commonPath)) {\n+        throw new Error(\n+          `Invalid key: ${key}. Key should be relative to the root path. ` +\n+            `Root path: ${this.rootPath}, Full path: ${fullPath}`\n+        );\n+      }\n+\n       return fullPath;\n     } catch (e) {\n       throw new Error(\n-        `Error getting full path for key: ${key}.\\nError: ${JSON.stringify(e)}`\n+        `Error getting full path for key: ${key}.\\nError: ${String(e)}`\n       );\n     }\n   }",
          "langchain/src/storage/tests/file_system.test.ts": "@@ -88,4 +88,23 @@ describe(\"LocalFileStore\", () => {\n     ).toEqual([value1, value2]);\n     await fs.promises.rm(secondaryRootPath, { recursive: true, force: true });\n   });\n+\n+  test(\"Should disallow attempts to traverse paths outside of a subfolder\", async () => {\n+    const encoder = new TextEncoder();\n+    const store = await LocalFileStore.fromPath(secondaryRootPath);\n+    const value1 = new Date().toISOString();\n+    await expect(\n+      store.mset([[\"../foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(\n+      store.mset([[\"/foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(\n+      store.mset([[\"\\\\foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(store.mget([\"../foo\"])).rejects.toThrowError();\n+    await expect(store.mget([\"/foo\"])).rejects.toThrowError();\n+    await expect(store.mget([\"\\\\foo\"])).rejects.toThrowError();\n+    await fs.promises.rm(secondaryRootPath, { recursive: true, force: true });\n+  });\n });",
          "libs/langchain-anthropic/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/anthropic\",\n-  \"version\": \"0.2.16\",\n+  \"version\": \"0.2.18\",\n   \"description\": \"Anthropic integrations for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {",
          "libs/langchain-anthropic/src/chat_models.ts": "@@ -14,7 +14,6 @@ import {\n import {\n   type StructuredOutputMethodOptions,\n   type BaseLanguageModelInput,\n-  type ToolDefinition,\n   isOpenAITool,\n } from \"@langchain/core/language_models/base\";\n import { zodToJsonSchema } from \"zod-to-json-schema\";\n@@ -634,7 +633,7 @@ export class ChatAnthropicMessages<\n       fields?.anthropicApiKey ??\n       getEnvironmentVariable(\"ANTHROPIC_API_KEY\");\n \n-    if (!this.anthropicApiKey) {\n+    if (!this.anthropicApiKey && !fields?.createClient) {\n       throw new Error(\"Anthropic API key not found\");\n     }\n     this.clientOptions = fields?.clientOptions ?? {};\n@@ -682,45 +681,41 @@ export class ChatAnthropicMessages<\n    *\n    * @param {ChatAnthropicCallOptions[\"tools\"]} tools The tools to format\n    * @returns {AnthropicTool[] | undefined} The formatted tools, or undefined if none are passed.\n-   * @throws {Error} If a mix of AnthropicTools and StructuredTools are passed.\n    */\n   formatStructuredToolToAnthropic(\n     tools: ChatAnthropicCallOptions[\"tools\"]\n   ): AnthropicTool[] | undefined {\n     if (!tools || !tools.length) {\n       return undefined;\n     }\n-\n-    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-    if ((tools as any[]).every((tool) => isAnthropicTool(tool))) {\n-      // If the tool is already an anthropic tool, return it\n-      return tools as AnthropicTool[];\n-    }\n-\n-    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-    if ((tools as any[]).every((tool) => isOpenAITool(tool))) {\n-      // Formatted as OpenAI tool, convert to Anthropic tool\n-      return (tools as ToolDefinition[]).map((tc) => ({\n-        name: tc.function.name,\n-        description: tc.function.description,\n-        input_schema: tc.function.parameters as AnthropicTool.InputSchema,\n-      }));\n-    }\n-\n-    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-    if ((tools as any[]).some((tool) => isAnthropicTool(tool))) {\n-      throw new Error(`Can not pass in a mix of tool schemas to ChatAnthropic`);\n-    }\n-\n-    if (tools.every(isLangChainTool)) {\n-      return tools.map((t) => ({\n-        name: t.name,\n-        description: t.description,\n-        input_schema: zodToJsonSchema(t.schema) as AnthropicTool.InputSchema,\n-      }));\n-    }\n-\n-    throw new Error(\"Unsupported tool type passed to ChatAnthropic\");\n+    return tools.map((tool) => {\n+      if (isAnthropicTool(tool)) {\n+        return tool;\n+      }\n+      if (isOpenAITool(tool)) {\n+        return {\n+          name: tool.function.name,\n+          description: tool.function.description,\n+          input_schema: tool.function.parameters as AnthropicTool.InputSchema,\n+        };\n+      }\n+      if (isLangChainTool(tool)) {\n+        return {\n+          name: tool.name,\n+          description: tool.description,\n+          input_schema: zodToJsonSchema(\n+            tool.schema\n+          ) as AnthropicTool.InputSchema,\n+        };\n+      }\n+      throw new Error(\n+        `Unknown tool type passed to ChatAnthropic: ${JSON.stringify(\n+          tool,\n+          null,\n+          2\n+        )}`\n+      );\n+    });\n   }\n \n   override bindTools(\n@@ -954,9 +949,6 @@ export class ChatAnthropicMessages<\n   ): Promise<Anthropic.Message> {\n     if (!this.batchClient) {\n       const options = this.apiUrl ? { baseURL: this.apiUrl } : undefined;\n-      if (!this.apiKey) {\n-        throw new Error(\"Missing Anthropic API key.\");\n-      }\n       this.batchClient = this.createClient({\n         ...this.clientOptions,\n         ...options,",
          "libs/langchain-anthropic/src/tests/chat_models-tools.int.test.ts": "@@ -461,3 +461,30 @@ test(\"streaming with structured output\", async () => {\n   }\n   expect(typeof finalChunk2).toEqual(\"object\");\n });\n+\n+test(\"Can bound and invoke different tool types\", async () => {\n+  const langchainTool = {\n+    name: \"get_weather_lc\",\n+    description: \"Get the weather of a specific location.\",\n+    schema: zodSchema,\n+  };\n+  const openaiTool = {\n+    type: \"function\",\n+    function: {\n+      name: \"get_weather_oai\",\n+      description: \"Get the weather of a specific location.\",\n+      parameters: zodToJsonSchema(zodSchema),\n+    },\n+  };\n+  const anthropicTool = {\n+    name: \"get_weather_ant\",\n+    description: \"Get the weather of a specific location.\",\n+    input_schema: zodToJsonSchema(zodSchema),\n+  };\n+  const tools = [langchainTool, openaiTool, anthropicTool];\n+  const modelWithTools = model.bindTools(tools);\n+  const result = await modelWithTools.invoke(\n+    \"Whats the current weather in san francisco?\"\n+  );\n+  expect(result.tool_calls?.length).toBeGreaterThanOrEqual(1);\n+});",
          "libs/langchain-anthropic/src/tests/chat_models.int.test.ts": "@@ -393,8 +393,7 @@ test(\"id is supplied when streaming\", async () => {\n   expect(finalChunk.id).not.toEqual(\"\");\n });\n \n-test(\"system prompt caching\", async () => {\n-  const CACHED_TEXT = `## Components\n+const CACHED_TEXT = `## Components\n \n LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\n Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\n@@ -655,6 +654,7 @@ LangChain has many different types of output parsers. This is a list of output p\n \n The current date is ${new Date().toISOString()}`;\n \n+test(\"system prompt caching\", async () => {\n   const model = new ChatAnthropic({\n     model: \"claude-3-haiku-20240307\",\n     clientOptions: {\n@@ -749,3 +749,45 @@ test.skip(\"Test ChatAnthropic with custom client\", async () => {\n   // console.log({ res });\n   expect(res.response_metadata.usage).toBeDefined();\n });\n+\n+test(\"human message caching\", async () => {\n+  const model = new ChatAnthropic({\n+    model: \"claude-3-haiku-20240307\",\n+    clientOptions: {\n+      defaultHeaders: {\n+        \"anthropic-beta\": \"prompt-caching-2024-07-31\",\n+      },\n+    },\n+  });\n+\n+  const messages = [\n+    new SystemMessage({\n+      content: [\n+        {\n+          type: \"text\",\n+          text: `You are a pirate. Always respond in pirate dialect.\\nUse the following as context when answering questions: ${CACHED_TEXT}`,\n+        },\n+      ],\n+    }),\n+    new HumanMessage({\n+      content: [\n+        {\n+          type: \"text\",\n+          text: \"What types of messages are supported in LangChain?\",\n+          cache_control: { type: \"ephemeral\" },\n+        },\n+      ],\n+    }),\n+  ];\n+\n+  const res = await model.invoke(messages);\n+  expect(\n+    res.response_metadata.usage.cache_creation_input_tokens\n+  ).toBeGreaterThan(0);\n+  expect(res.response_metadata.usage.cache_read_input_tokens).toBe(0);\n+  const res2 = await model.invoke(messages);\n+  expect(res2.response_metadata.usage.cache_creation_input_tokens).toBe(0);\n+  expect(res2.response_metadata.usage.cache_read_input_tokens).toBeGreaterThan(\n+    0\n+  );\n+});",
          "libs/langchain-anthropic/src/utils/message_inputs.ts": "@@ -122,6 +122,9 @@ function _formatContent(content: MessageContent) {\n     return content;\n   } else {\n     const contentBlocks = content.map((contentPart) => {\n+      const cacheControl =\n+        \"cache_control\" in contentPart ? contentPart.cache_control : undefined;\n+\n       if (contentPart.type === \"image_url\") {\n         let source;\n         if (typeof contentPart.image_url === \"string\") {\n@@ -132,6 +135,7 @@ function _formatContent(content: MessageContent) {\n         return {\n           type: \"image\" as const, // Explicitly setting the type as \"image\"\n           source,\n+          ...(cacheControl ? { cache_control: cacheControl } : {}),\n         };\n       } else if (\n         textTypes.find((t) => t === contentPart.type) &&\n@@ -141,6 +145,7 @@ function _formatContent(content: MessageContent) {\n         return {\n           type: \"text\" as const, // Explicitly setting the type as \"text\"\n           text: contentPart.text,\n+          ...(cacheControl ? { cache_control: cacheControl } : {}),\n         };\n       } else if (toolTypes.find((t) => t === contentPart.type)) {\n         const contentPartCopy = { ...contentPart };\n@@ -167,6 +172,7 @@ function _formatContent(content: MessageContent) {\n         // TODO: Fix when SDK types are fixed\n         return {\n           ...contentPartCopy,\n+          ...(cacheControl ? { cache_control: cacheControl } : {}),\n           // eslint-disable-next-line @typescript-eslint/no-explicit-any\n         } as any;\n       } else {",
          "libs/langchain-baidu-qianfan/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/baidu-qianfan\",\n-  \"version\": \"0.0.3\",\n+  \"version\": \"0.0.4\",\n   \"description\": \"Baidu Qianfan integration for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {",
          "libs/langchain-baidu-qianfan/src/chat_models.ts": "@@ -15,7 +15,6 @@ import {\n } from \"@langchain/core/outputs\";\n import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n-import { convertEventStreamToIterableReadableDataStream } from \"@langchain/core/utils/event_source_parse\";\n import { ChatCompletion } from \"@baiducloud/qianfan\";\n \n /**\n@@ -60,8 +59,12 @@ interface ChatCompletionResponse {\n   id: string;\n   object: string;\n   created: number;\n+  sentence_id: number;\n+  is_end: boolean;\n+  is_truncated: boolean;\n   result: string;\n   need_clear_history: boolean;\n+  finish_reason: string;\n   usage: TokenUsage;\n }\n \n@@ -305,167 +308,96 @@ export class ChatBaiduQianfan\n   private _ensureMessages(messages: BaseMessage[]): Qianfan[] {\n     return messages.map((message) => ({\n       role: messageToQianfanRole(message),\n-      content: message.text,\n+      content: message.content.toString(),\n     }));\n   }\n \n   /** @ignore */\n   async _generate(\n     messages: BaseMessage[],\n-    _options?: this[\"ParsedCallOptions\"],\n+    options: this[\"ParsedCallOptions\"],\n     runManager?: CallbackManagerForLLMRun\n   ): Promise<ChatResult> {\n-    const tokenUsage: TokenUsage = {};\n-\n-    const params = this.invocationParams();\n+    if (this.streaming) {\n+      let finalChunk: ChatGenerationChunk | undefined;\n+      const stream = this._streamResponseChunks(messages, options, runManager);\n+      for await (const chunk of stream) {\n+        if (finalChunk === undefined) {\n+          finalChunk = chunk;\n+        } else {\n+          finalChunk = finalChunk.concat(chunk);\n+        }\n+      }\n \n-    // Qianfan requires the system message to be put in the params, not messages array\n-    const systemMessage = messages.find(\n-      (message) => message._getType() === \"system\"\n-    );\n-    if (systemMessage) {\n-      // eslint-disable-next-line no-param-reassign\n-      messages = messages.filter((message) => message !== systemMessage);\n-      params.system = systemMessage.text;\n-    }\n-    const messagesMapped = this._ensureMessages(messages);\n+      if (finalChunk === undefined) {\n+        throw new Error(\"No chunks returned from BaiduQianFan API.\");\n+      }\n \n-    const data = params.stream\n-      ? await new Promise<ChatCompletionResponse>((resolve, reject) => {\n-          let rejected = false;\n-          this.completionWithRetry(\n-            {\n-              ...params,\n-              messages: messagesMapped,\n-            },\n-            true,\n-            (event) => {\n-              resolve(event.data);\n-              // eslint-disable-next-line no-void\n-              void runManager?.handleLLMNewToken(event.data ?? \"\");\n-            }\n-          ).catch((error) => {\n-            if (!rejected) {\n-              rejected = true;\n-              reject(error);\n-            }\n-          });\n-        })\n-      : await this.completionWithRetry(\n+      return {\n+        generations: [\n           {\n-            ...params,\n-            messages: messagesMapped,\n+            text: finalChunk.text,\n+            message: finalChunk.message,\n           },\n-          false\n-        ).then((data) => {\n-          if (data?.error_code) {\n-            throw new Error(data?.error_msg);\n-          }\n-          return data;\n-        });\n-\n-    const {\n-      completion_tokens: completionTokens,\n-      prompt_tokens: promptTokens,\n-      total_tokens: totalTokens,\n-    } = data.usage ?? {};\n-\n-    if (completionTokens) {\n-      tokenUsage.completionTokens =\n-        (tokenUsage.completionTokens ?? 0) + completionTokens;\n-    }\n-\n-    if (promptTokens) {\n-      tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n-    }\n-\n-    if (totalTokens) {\n-      tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n+        ],\n+        llmOutput: finalChunk.generationInfo?.usage ?? {},\n+      };\n+    } else {\n+      const params = this.invocationParams();\n+\n+      const systemMessage = messages.find(\n+        (message) => message._getType() === \"system\"\n+      );\n+      if (systemMessage) {\n+        // eslint-disable-next-line no-param-reassign\n+        messages = messages.filter((message) => message !== systemMessage);\n+        params.system = systemMessage.content.toString();\n+      }\n+      const messagesMapped = this._ensureMessages(messages);\n+\n+      const data = (await this.completionWithRetry(\n+        {\n+          ...params,\n+          messages: messagesMapped,\n+        },\n+        false\n+      )) as ChatCompletionResponse;\n+\n+      const tokenUsage = data.usage || {};\n+\n+      const generations: ChatGeneration[] = [\n+        {\n+          text: data.result || \"\",\n+          message: new AIMessage(data.result || \"\"),\n+        },\n+      ];\n+\n+      return {\n+        generations,\n+        llmOutput: { tokenUsage },\n+      };\n     }\n-\n-    const generations: ChatGeneration[] = [];\n-    const text = data.result ?? \"\";\n-    generations.push({\n-      text,\n-      message: new AIMessage(text),\n-    });\n-    return {\n-      generations,\n-      llmOutput: { tokenUsage },\n-    };\n   }\n \n   /** @ignore */\n   async completionWithRetry(\n     request: ChatCompletionRequest,\n-    stream: boolean,\n-    onmessage?: (event: MessageEvent) => void\n-  ) {\n+    stream: boolean\n+  ): Promise<\n+    ChatCompletionResponse | AsyncIterableIterator<ChatCompletionResponse>\n+  > {\n     const makeCompletionRequest = async () => {\n-      console.log(request);\n       const response = await this.client.chat(request, this.model);\n-\n       if (!stream) {\n         return response;\n       } else {\n-        let streamResponse = { result: \"\" } as {\n-          id: string;\n-          object: string;\n-          created: number;\n-          sentence_id?: number;\n-          result: string;\n-          need_clear_history: boolean;\n-          usage: TokenUsage;\n-        };\n-        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-        for await (const message of response as AsyncIterableIterator<any>) {\n-          // 返回结果\n-          if (!streamResponse) {\n-            streamResponse = {\n-              id: message.id,\n-              object: message.object,\n-              created: message.created,\n-              result: message.result,\n-              need_clear_history: message.need_clear_history,\n-              usage: message.usage,\n-            };\n-          } else {\n-            streamResponse.result += message.result;\n-            streamResponse.created = message.created;\n-            streamResponse.need_clear_history = message.need_clear_history;\n-            streamResponse.usage = message.usage;\n-          }\n-        }\n-        const event = new MessageEvent(\"message\", {\n-          data: streamResponse,\n-        });\n-        onmessage?.(event);\n+        return response as AsyncIterableIterator<ChatCompletionResponse>;\n       }\n     };\n \n     return this.caller.call(makeCompletionRequest);\n   }\n \n-  private async createStream(request: ChatCompletionRequest) {\n-    const response = await this.client.chat(\n-      {\n-        ...request,\n-        stream: true,\n-      },\n-      this.model\n-    );\n-\n-    return convertEventStreamToIterableReadableDataStream(response);\n-  }\n-\n-  private _deserialize(json: string) {\n-    try {\n-      return JSON.parse(json);\n-    } catch (e) {\n-      console.warn(`Received a non-JSON parseable chunk: ${json}`);\n-    }\n-  }\n-\n   async *_streamResponseChunks(\n     messages: BaseMessage[],\n     _options?: this[\"ParsedCallOptions\"],\n@@ -476,27 +408,28 @@ export class ChatBaiduQianfan\n       stream: true,\n     };\n \n-    // Qianfan requires the system message to be put in the params, not messages array\n     const systemMessage = messages.find(\n       (message) => message._getType() === \"system\"\n     );\n     if (systemMessage) {\n       // eslint-disable-next-line no-param-reassign\n       messages = messages.filter((message) => message !== systemMessage);\n-      parameters.system = systemMessage.text;\n+      parameters.system = systemMessage.content.toString();\n     }\n     const messagesMapped = this._ensureMessages(messages);\n \n-    const stream = await this.caller.call(async () =>\n-      this.createStream({\n-        ...parameters,\n-        messages: messagesMapped,\n-      })\n-    );\n+    const stream = (await this.caller.call(async () =>\n+      this.completionWithRetry(\n+        {\n+          ...parameters,\n+          messages: messagesMapped,\n+        },\n+        true\n+      )\n+    )) as AsyncIterableIterator<ChatCompletionResponse>;\n \n     for await (const chunk of stream) {\n-      const deserializedChunk = this._deserialize(chunk);\n-      const { result, is_end, id } = deserializedChunk;\n+      const { result, is_end, id } = chunk;\n       yield new ChatGenerationChunk({\n         text: result,\n         message: new AIMessageChunk({ content: result }),",
          "libs/langchain-cloudflare/package.json": "@@ -35,12 +35,11 @@\n   \"author\": \"LangChain\",\n   \"license\": \"MIT\",\n   \"dependencies\": {\n-    \"@cloudflare/ai\": \"1.0.47\",\n     \"@langchain/core\": \">0.1.0 <0.3.0\",\n     \"uuid\": \"^10.0.0\"\n   },\n   \"devDependencies\": {\n-    \"@cloudflare/workers-types\": \"^4.20231218.0\",\n+    \"@cloudflare/workers-types\": \"^4.20240909.0\",\n     \"@jest/globals\": \"^29.5.0\",\n     \"@langchain/langgraph\": \"~0.0.31\",\n     \"@langchain/scripts\": \">=0.1.0 <0.2.0\",",
          "libs/langchain-cloudflare/src/embeddings.ts": "@@ -1,5 +1,4 @@\n-import { Ai } from \"@cloudflare/ai\";\n-import { Fetcher } from \"@cloudflare/workers-types\";\n+import { Ai } from \"@cloudflare/workers-types\";\n import { Embeddings, EmbeddingsParams } from \"@langchain/core/embeddings\";\n import { chunkArray } from \"@langchain/core/utils/chunk_array\";\n \n@@ -14,7 +13,7 @@ type AiTextEmbeddingsOutput = {\n \n export interface CloudflareWorkersAIEmbeddingsParams extends EmbeddingsParams {\n   /** Binding */\n-  binding: Fetcher;\n+  binding: Ai;\n \n   /**\n    * Model name to use\n@@ -57,7 +56,7 @@ export class CloudflareWorkersAIEmbeddings extends Embeddings {\n         \"Must supply a Workers AI binding, eg { binding: env.AI }\"\n       );\n     }\n-    this.ai = new Ai(fields.binding);\n+    this.ai = fields.binding;\n     this.modelName = fields?.model ?? fields.modelName ?? this.model;\n     this.model = this.modelName;\n     this.stripNewLines = fields.stripNewLines ?? this.stripNewLines;",
          "libs/langchain-community/.gitignore": "@@ -222,6 +222,10 @@ llms/aleph_alpha.cjs\n llms/aleph_alpha.js\n llms/aleph_alpha.d.ts\n llms/aleph_alpha.d.cts\n+llms/arcjet.cjs\n+llms/arcjet.js\n+llms/arcjet.d.ts\n+llms/arcjet.d.cts\n llms/bedrock.cjs\n llms/bedrock.js\n llms/bedrock.d.ts\n@@ -502,6 +506,10 @@ chat_models/alibaba_tongyi.cjs\n chat_models/alibaba_tongyi.js\n chat_models/alibaba_tongyi.d.ts\n chat_models/alibaba_tongyi.d.cts\n+chat_models/arcjet.cjs\n+chat_models/arcjet.js\n+chat_models/arcjet.d.ts\n+chat_models/arcjet.d.cts\n chat_models/baiduwenxin.cjs\n chat_models/baiduwenxin.js\n chat_models/baiduwenxin.d.ts\n@@ -618,6 +626,10 @@ retrievers/amazon_knowledge_base.cjs\n retrievers/amazon_knowledge_base.js\n retrievers/amazon_knowledge_base.d.ts\n retrievers/amazon_knowledge_base.d.cts\n+retrievers/bm25.cjs\n+retrievers/bm25.js\n+retrievers/bm25.d.ts\n+retrievers/bm25.d.cts\n retrievers/chaindesk.cjs\n retrievers/chaindesk.js\n retrievers/chaindesk.d.ts",
          "libs/langchain-community/langchain.config.js": "@@ -91,6 +91,7 @@ export const config = {\n     // llms\n     \"llms/ai21\": \"llms/ai21\",\n     \"llms/aleph_alpha\": \"llms/aleph_alpha\",\n+    \"llms/arcjet\": \"llms/arcjet\",\n     \"llms/bedrock\": \"llms/bedrock/index\",\n     \"llms/bedrock/web\": \"llms/bedrock/web\",\n     \"llms/cloudflare_workersai\": \"llms/cloudflare_workersai\",\n@@ -163,6 +164,7 @@ export const config = {\n     \"vectorstores/zep_cloud\": \"vectorstores/zep_cloud\",\n     // chat_models\n     \"chat_models/alibaba_tongyi\": \"chat_models/alibaba_tongyi\",\n+    \"chat_models/arcjet\": \"chat_models/arcjet\",\n     \"chat_models/baiduwenxin\": \"chat_models/baiduwenxin\",\n     \"chat_models/bedrock\": \"chat_models/bedrock/index\",\n     \"chat_models/bedrock/web\": \"chat_models/bedrock/web\",\n@@ -194,6 +196,7 @@ export const config = {\n     // retrievers\n     \"retrievers/amazon_kendra\": \"retrievers/amazon_kendra\",\n     \"retrievers/amazon_knowledge_base\": \"retrievers/amazon_knowledge_base\",\n+    \"retrievers/bm25\": \"retrievers/bm25\",\n     \"retrievers/chaindesk\": \"retrievers/chaindesk\",\n     \"retrievers/databerry\": \"retrievers/databerry\",\n     \"retrievers/dria\": \"retrievers/dria\",\n@@ -351,6 +354,7 @@ export const config = {\n     \"embeddings/tencent_hunyuan/web\",\n     \"embeddings/zhipuai\",\n     \"llms/load\",\n+    \"llms/arcjet\",\n     \"llms/cohere\",\n     \"llms/googlevertexai\",\n     \"llms/googlevertexai/web\",\n@@ -410,6 +414,7 @@ export const config = {\n     \"vectorstores/xata\",\n     \"vectorstores/zep\",\n     \"vectorstores/zep_cloud\",\n+    \"chat_models/arcjet\",\n     \"chat_models/bedrock\",\n     \"chat_models/bedrock/web\",\n     \"chat_models/googlevertexai\",",
          "libs/langchain-community/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/community\",\n-  \"version\": \"0.2.32\",\n+  \"version\": \"0.2.33\",\n   \"description\": \"Third-party integrations for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -48,6 +48,7 @@\n     \"zod-to-json-schema\": \"^3.22.5\"\n   },\n   \"devDependencies\": {\n+    \"@arcjet/redact\": \"^v1.0.0-alpha.23\",\n     \"@aws-crypto/sha256-js\": \"^5.0.0\",\n     \"@aws-sdk/client-bedrock-agent-runtime\": \"^3.583.0\",\n     \"@aws-sdk/client-bedrock-runtime\": \"^3.422.0\",\n@@ -83,7 +84,7 @@\n     \"@langchain/standard-tests\": \"0.0.0\",\n     \"@layerup/layerup-security\": \"^1.5.12\",\n     \"@mendable/firecrawl-js\": \"^0.0.36\",\n-    \"@mlc-ai/web-llm\": \"0.2.46\",\n+    \"@mlc-ai/web-llm\": \">=0.2.62 <0.3.0\",\n     \"@mozilla/readability\": \"^0.4.4\",\n     \"@neondatabase/serverless\": \"^0.9.1\",\n     \"@notionhq/client\": \"^2.2.10\",\n@@ -213,6 +214,7 @@\n     \"youtubei.js\": \"^9.1.0\"\n   },\n   \"peerDependencies\": {\n+    \"@arcjet/redact\": \"^v1.0.0-alpha.23\",\n     \"@aws-crypto/sha256-js\": \"^5.0.0\",\n     \"@aws-sdk/client-bedrock-agent-runtime\": \"^3.583.0\",\n     \"@aws-sdk/client-bedrock-runtime\": \"^3.422.0\",\n@@ -242,7 +244,7 @@\n     \"@langchain/langgraph\": \"*\",\n     \"@layerup/layerup-security\": \"^1.5.12\",\n     \"@mendable/firecrawl-js\": \"^0.0.13\",\n-    \"@mlc-ai/web-llm\": \"0.2.46\",\n+    \"@mlc-ai/web-llm\": \"*\",\n     \"@mozilla/readability\": \"*\",\n     \"@neondatabase/serverless\": \"*\",\n     \"@notionhq/client\": \"^2.2.10\",\n@@ -335,6 +337,9 @@\n     \"youtubei.js\": \"^9.1.0\"\n   },\n   \"peerDependenciesMeta\": {\n+    \"@arcjet/redact\": {\n+      \"optional\": true\n+    },\n     \"@aws-crypto/sha256-js\": {\n       \"optional\": true\n     },\n@@ -1204,6 +1209,15 @@\n       \"import\": \"./llms/aleph_alpha.js\",\n       \"require\": \"./llms/aleph_alpha.cjs\"\n     },\n+    \"./llms/arcjet\": {\n+      \"types\": {\n+        \"import\": \"./llms/arcjet.d.ts\",\n+        \"require\": \"./llms/arcjet.d.cts\",\n+        \"default\": \"./llms/arcjet.d.ts\"\n+      },\n+      \"import\": \"./llms/arcjet.js\",\n+      \"require\": \"./llms/arcjet.cjs\"\n+    },\n     \"./llms/bedrock\": {\n       \"types\": {\n         \"import\": \"./llms/bedrock.d.ts\",\n@@ -1834,6 +1848,15 @@\n       \"import\": \"./chat_models/alibaba_tongyi.js\",\n       \"require\": \"./chat_models/alibaba_tongyi.cjs\"\n     },\n+    \"./chat_models/arcjet\": {\n+      \"types\": {\n+        \"import\": \"./chat_models/arcjet.d.ts\",\n+        \"require\": \"./chat_models/arcjet.d.cts\",\n+        \"default\": \"./chat_models/arcjet.d.ts\"\n+      },\n+      \"import\": \"./chat_models/arcjet.js\",\n+      \"require\": \"./chat_models/arcjet.cjs\"\n+    },\n     \"./chat_models/baiduwenxin\": {\n       \"types\": {\n         \"import\": \"./chat_models/baiduwenxin.d.ts\",\n@@ -2095,6 +2118,15 @@\n       \"import\": \"./retrievers/amazon_knowledge_base.js\",\n       \"require\": \"./retrievers/amazon_knowledge_base.cjs\"\n     },\n+    \"./retrievers/bm25\": {\n+      \"types\": {\n+        \"import\": \"./retrievers/bm25.d.ts\",\n+        \"require\": \"./retrievers/bm25.d.cts\",\n+        \"default\": \"./retrievers/bm25.d.ts\"\n+      },\n+      \"import\": \"./retrievers/bm25.js\",\n+      \"require\": \"./retrievers/bm25.cjs\"\n+    },\n     \"./retrievers/chaindesk\": {\n       \"types\": {\n         \"import\": \"./retrievers/chaindesk.d.ts\",\n@@ -3286,6 +3318,10 @@\n     \"llms/aleph_alpha.js\",\n     \"llms/aleph_alpha.d.ts\",\n     \"llms/aleph_alpha.d.cts\",\n+    \"llms/arcjet.cjs\",\n+    \"llms/arcjet.js\",\n+    \"llms/arcjet.d.ts\",\n+    \"llms/arcjet.d.cts\",\n     \"llms/bedrock.cjs\",\n     \"llms/bedrock.js\",\n     \"llms/bedrock.d.ts\",\n@@ -3566,6 +3602,10 @@\n     \"chat_models/alibaba_tongyi.js\",\n     \"chat_models/alibaba_tongyi.d.ts\",\n     \"chat_models/alibaba_tongyi.d.cts\",\n+    \"chat_models/arcjet.cjs\",\n+    \"chat_models/arcjet.js\",\n+    \"chat_models/arcjet.d.ts\",\n+    \"chat_models/arcjet.d.cts\",\n     \"chat_models/baiduwenxin.cjs\",\n     \"chat_models/baiduwenxin.js\",\n     \"chat_models/baiduwenxin.d.ts\",\n@@ -3682,6 +3722,10 @@\n     \"retrievers/amazon_knowledge_base.js\",\n     \"retrievers/amazon_knowledge_base.d.ts\",\n     \"retrievers/amazon_knowledge_base.d.cts\",\n+    \"retrievers/bm25.cjs\",\n+    \"retrievers/bm25.js\",\n+    \"retrievers/bm25.d.ts\",\n+    \"retrievers/bm25.d.cts\",\n     \"retrievers/chaindesk.cjs\",\n     \"retrievers/chaindesk.js\",\n     \"retrievers/chaindesk.d.ts\",",
          "libs/langchain-community/src/chat_models/arcjet.ts": "@@ -0,0 +1,194 @@\n+/* eslint-disable no-param-reassign */\n+/* eslint-disable no-plusplus */\n+import {\n+  BaseChatModel,\n+  type BaseChatModelParams,\n+} from \"@langchain/core/language_models/chat_models\";\n+import type { ArcjetSensitiveInfoType, RedactOptions } from \"@arcjet/redact\";\n+import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n+import { BaseMessage } from \"@langchain/core/messages\";\n+import { ChatResult } from \"@langchain/core/outputs\";\n+\n+type DetectSensitiveInfoEntities<T> = (\n+  tokens: string[]\n+) => Array<ArcjetSensitiveInfoType | T | undefined>;\n+type ValidEntities<Detect> = Array<\n+  undefined extends Detect\n+    ? ArcjetSensitiveInfoType\n+    : Detect extends DetectSensitiveInfoEntities<infer CustomEntities>\n+    ? ArcjetSensitiveInfoType | CustomEntities\n+    : never\n+>;\n+\n+export interface ArcjetRedactOptions<Detect> extends BaseChatModelParams {\n+  chatModel: BaseChatModel;\n+  entities?: ValidEntities<Detect>;\n+  contextWindowSize?: number;\n+  detect?: Detect;\n+  replace?: (entity: ValidEntities<Detect>[number]) => string | undefined;\n+}\n+\n+export type { ArcjetSensitiveInfoType, RedactOptions };\n+\n+async function transformTextMessageAsync(\n+  message: BaseMessage,\n+  transformer: (text: string) => Promise<string>\n+): Promise<BaseMessage> {\n+  if (typeof message.content === \"string\") {\n+    message.content = await transformer(message.content);\n+    return message;\n+  }\n+\n+  const redactedContent = await Promise.all(\n+    message.content.map(async (m) => {\n+      if (m.type === \"text\") {\n+        return {\n+          ...m,\n+          text: await transformer(m.text),\n+        };\n+      } else {\n+        return Promise.resolve(m);\n+      }\n+    })\n+  );\n+  message.content = redactedContent;\n+  return message;\n+}\n+\n+function transformTextMessage(\n+  message: BaseMessage,\n+  transformer: (text: string) => string\n+): BaseMessage {\n+  if (typeof message.content === \"string\") {\n+    message.content = transformer(message.content);\n+    return message;\n+  }\n+\n+  const redactedContent = message.content.map((m) => {\n+    if (m.type === \"text\") {\n+      return {\n+        ...m,\n+        text: transformer(m.text),\n+      };\n+    } else {\n+      return m;\n+    }\n+  });\n+  message.content = redactedContent;\n+  return message;\n+}\n+\n+export class ArcjetRedact<\n+  Detect extends DetectSensitiveInfoEntities<CustomEntities> | undefined,\n+  CustomEntities extends string\n+> extends BaseChatModel {\n+  static lc_name() {\n+    return \"ArcjetRedact\";\n+  }\n+\n+  chatModel: BaseChatModel;\n+\n+  entities?: ValidEntities<Detect>;\n+\n+  contextWindowSize?: number;\n+\n+  detect?: Detect;\n+\n+  replace?: (entity: ValidEntities<Detect>[number]) => string | undefined;\n+\n+  index: number;\n+\n+  constructor(options: ArcjetRedactOptions<Detect>) {\n+    super(options);\n+\n+    if (options.entities && options.entities.length === 0) {\n+      throw new Error(\"no entities configured for redaction\");\n+    }\n+\n+    this.chatModel = options.chatModel;\n+    this.entities = options.entities;\n+    this.contextWindowSize = options.contextWindowSize;\n+    this.detect = options.detect;\n+    this.replace = options.replace;\n+    this.index = 0;\n+  }\n+\n+  _createUniqueReplacement(entity: ValidEntities<Detect>[number]): string {\n+    const userReplacement =\n+      typeof this.replace !== \"undefined\" ? this.replace(entity) : undefined;\n+    if (typeof userReplacement !== \"undefined\") {\n+      return userReplacement;\n+    }\n+\n+    this.index++;\n+\n+    if (entity === \"email\") {\n+      return `<Redacted email #${this.index}>`;\n+    }\n+\n+    if (entity === \"phone-number\") {\n+      return `<Redacted phone number #${this.index}>`;\n+    }\n+\n+    if (entity === \"ip-address\") {\n+      return `<Redacted IP address #${this.index}>`;\n+    }\n+\n+    if (entity === \"credit-card-number\") {\n+      return `<Redacted credit card number #${this.index}>`;\n+    }\n+\n+    return `<Redacted ${entity} #${this.index}>`;\n+  }\n+\n+  _llmType() {\n+    return \"arcjet_redact\";\n+  }\n+\n+  async _generate(\n+    messages: BaseMessage[],\n+    options: this[\"ParsedCallOptions\"],\n+    runManager?: CallbackManagerForLLMRun | undefined\n+  ): Promise<ChatResult> {\n+    const ajOptions: RedactOptions<Detect> = {\n+      entities: this.entities,\n+      contextWindowSize: this.contextWindowSize,\n+      detect: this.detect,\n+      replace: this._createUniqueReplacement.bind(this),\n+    };\n+\n+    const unredactors: Array<(message: string) => string> = [];\n+    // Support CommonJS\n+    const { redact } = await import(\"@arcjet/redact\");\n+    const redacted = await Promise.all(\n+      messages.map(async (message) => {\n+        return await transformTextMessageAsync(message, async (message) => {\n+          const [redacted, unredact] = await redact(message, ajOptions);\n+          unredactors.push(unredact);\n+          return redacted;\n+        });\n+      })\n+    );\n+\n+    const response = await this.chatModel._generate(\n+      redacted,\n+      options,\n+      runManager\n+    );\n+\n+    return {\n+      ...response,\n+      generations: response.generations.map((resp) => {\n+        return {\n+          ...resp,\n+          message: transformTextMessage(resp.message, (message: string) => {\n+            for (const unredact of unredactors) {\n+              message = unredact(message);\n+            }\n+            return message;\n+          }),\n+        };\n+      }),\n+    };\n+  }\n+}",
          "libs/langchain-community/src/chat_models/tests/chatarcjet.test.ts": "@@ -0,0 +1,263 @@\n+/* eslint-disable no-process-env */\n+/* eslint-disable @typescript-eslint/no-non-null-assertion */\n+\n+import { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\n+import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n+import { BaseMessage, MessageType } from \"@langchain/core/messages\";\n+import { ChatGeneration, ChatResult } from \"@langchain/core/outputs\";\n+import { ArcjetRedact } from \"../arcjet.js\";\n+\n+class MockChatModel extends BaseChatModel {\n+  callback?: (input: BaseMessage[]) => ChatGeneration[];\n+\n+  constructor(callback?: (input: BaseMessage[]) => ChatGeneration[]) {\n+    super({});\n+    this.callback = callback;\n+  }\n+\n+  _llmType(): string {\n+    return \"mock_chat_model\";\n+  }\n+\n+  async _generate(\n+    messages: BaseMessage[],\n+    _options: this[\"ParsedCallOptions\"],\n+    _runManager?: CallbackManagerForLLMRun | undefined\n+  ): Promise<ChatResult> {\n+    return {\n+      generations: this.callback ? this.callback(messages) : [],\n+    };\n+  }\n+}\n+\n+class GenericMessage extends BaseMessage {\n+  constructor(text: string) {\n+    super(text);\n+  }\n+\n+  _getType(): MessageType {\n+    return \"human\";\n+  }\n+}\n+\n+test(\"It passes messages through correctly\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\"this is the output\"),\n+    text: \"this is the output\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"this is the input\");\n+    expect(input[1].content).toEqual(\"this is a second input\");\n+    return [generationA];\n+  };\n+  const mockLLM = new MockChatModel(callback);\n+  const options = {\n+    chatModel: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke([\n+    \"this is the input\",\n+    \"this is a second input\",\n+  ]);\n+\n+  expect(output.content).toEqual(\"this is the output\");\n+});\n+\n+test(\"It passes messages through correctly in the streaming interface\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\"this is the output\"),\n+    text: \"this is the output\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"this is the input\");\n+    expect(input[1].content).toEqual(\"this is a second input\");\n+    return [generationA];\n+  };\n+  const mockLLM = new MockChatModel(callback);\n+  const options = {\n+    chatModel: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const stream = await arcjetRedact.stream([\n+    \"this is the input\",\n+    \"this is a second input\",\n+  ]);\n+\n+  const output = await stream.next();\n+\n+  expect(output.value.content).toEqual(\"this is the output\");\n+});\n+\n+test(\"It redacts built in entities across multiple messages and unredacts them in the response\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\n+      \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\"\n+    ),\n+    text: \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"my email address is <Redacted email #1>\");\n+    expect(input[1].content).toEqual(\n+      \"my card number is <Redacted credit card number #2>\"\n+    );\n+    return [generationA];\n+  };\n+\n+  const mockLLM = new MockChatModel(callback);\n+  const options = {\n+    chatModel: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.stream([\n+    \"my email address is test@example.com\",\n+    \"my card number is 4242424242424242\",\n+  ]);\n+\n+  const first = await output.next();\n+  expect(first.value.content).toEqual(\n+    \"Your email is test@example.com and your card number is 4242424242424242\"\n+  );\n+});\n+\n+test(\"it redacts and unredacts correctly\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\n+      \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\"\n+    ),\n+    text: \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"my email address is <Redacted email #1>\");\n+    expect(input[1].content).toEqual(\n+      \"my card number is <Redacted credit card number #2>\"\n+    );\n+    return [generationA];\n+  };\n+\n+  const mockLLM = new MockChatModel(callback);\n+  const options = {\n+    chatModel: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.stream([\n+    \"my email address is test@example.com\",\n+    \"my card number is 4242424242424242\",\n+  ]);\n+\n+  const first = await output.next();\n+  expect(first.value.content).toEqual(\n+    \"Your email is test@example.com and your card number is 4242424242424242\"\n+  );\n+});\n+\n+test(\"it redacts and unredacts correctly\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\n+      \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\"\n+    ),\n+    text: \"Your email is <Redacted email #1> and your card number is <Redacted credit card number #2>\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"my email address is <Redacted email #1>\");\n+    expect(input[1].content).toEqual(\n+      \"my card number is <Redacted credit card number #2>\"\n+    );\n+    return [generationA];\n+  };\n+\n+  const mockLLM = new MockChatModel(callback);\n+  const options = {\n+    chatModel: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke([\n+    \"my email address is test@example.com\",\n+    \"my card number is 4242424242424242\",\n+  ]);\n+\n+  expect(output.content).toEqual(\n+    \"Your email is test@example.com and your card number is 4242424242424242\"\n+  );\n+});\n+\n+test(\"it handles custom detect functions correctly\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\"custom <Redacted custom-entity #1>\"),\n+    text: \"custom <Redacted custom-entity #0>\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\"custom <Redacted custom-entity #1>\");\n+    return [generationA];\n+  };\n+\n+  const mockLLM = new MockChatModel(callback);\n+  const customDetector = (tokens: string[]) => {\n+    return tokens.map((t) =>\n+      t === \"my-custom-string-to-be-detected\" ? \"custom-entity\" : undefined\n+    );\n+  };\n+  const options = {\n+    chatModel: mockLLM,\n+    entities: [\"custom-entity\" as const],\n+    detect: customDetector,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke([\n+    \"custom my-custom-string-to-be-detected\",\n+  ]);\n+\n+  expect(output.content).toEqual(\"custom my-custom-string-to-be-detected\");\n+});\n+\n+test(\"it handles custom replace functions correctly\", async () => {\n+  const generationA = {\n+    message: new GenericMessage(\n+      \"custom is <Redacted custom-entity #1> email is redacted@example.com\"\n+    ),\n+    text: \"custom is <Redacted custom-entity #1> email is redacted@example.com\",\n+  };\n+\n+  const callback = (input: BaseMessage[]) => {\n+    expect(input[0].content).toEqual(\n+      \"custom <Redacted custom-entity #1> email redacted@example.com\"\n+    );\n+    return [generationA];\n+  };\n+\n+  const mockLLM = new MockChatModel(callback);\n+  const customDetector = (tokens: string[]) => {\n+    return tokens.map((t) =>\n+      t === \"my-custom-string-to-be-detected\" ? \"custom-entity\" : undefined\n+    );\n+  };\n+  const customReplacer = (detected: string) => {\n+    return detected === \"email\" ? \"redacted@example.com\" : undefined;\n+  };\n+  const options = {\n+    chatModel: mockLLM,\n+    entities: [\"custom-entity\" as const, \"email\" as const],\n+    detect: customDetector,\n+    replace: customReplacer,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke([\n+    \"custom my-custom-string-to-be-detected email test@example.com\",\n+  ]);\n+\n+  expect(output.content).toEqual(\n+    \"custom is my-custom-string-to-be-detected email is test@example.com\"\n+  );\n+});",
          "libs/langchain-community/src/llms/arcjet.ts": "@@ -0,0 +1,80 @@\n+import {\n+  LLM,\n+  BaseLLM,\n+  type BaseLLMCallOptions,\n+} from \"@langchain/core/language_models/llms\";\n+import type { ArcjetSensitiveInfoType, RedactOptions } from \"@arcjet/redact\";\n+\n+type DetectSensitiveInfoEntities<T> = (\n+  tokens: string[]\n+) => Array<ArcjetSensitiveInfoType | T | undefined>;\n+type ValidEntities<Detect> = Array<\n+  undefined extends Detect\n+    ? ArcjetSensitiveInfoType\n+    : Detect extends DetectSensitiveInfoEntities<infer CustomEntities>\n+    ? ArcjetSensitiveInfoType | CustomEntities\n+    : never\n+>;\n+\n+export type { ArcjetSensitiveInfoType, RedactOptions };\n+\n+export interface ArcjetRedactOptions<Detect> extends BaseLLMCallOptions {\n+  llm: BaseLLM;\n+  entities?: ValidEntities<Detect>;\n+  contextWindowSize?: number;\n+  detect?: Detect;\n+  replace?: (entity: ValidEntities<Detect>[number]) => string | undefined;\n+}\n+\n+export class ArcjetRedact<\n+  Detect extends DetectSensitiveInfoEntities<CustomEntities> | undefined,\n+  CustomEntities extends string\n+> extends LLM {\n+  static lc_name() {\n+    return \"ArcjetRedact\";\n+  }\n+\n+  llm: BaseLLM;\n+\n+  entities?: ValidEntities<Detect>;\n+\n+  contextWindowSize?: number;\n+\n+  detect?: Detect;\n+\n+  replace?: (entity: ValidEntities<Detect>[number]) => string | undefined;\n+\n+  constructor(options: ArcjetRedactOptions<Detect>) {\n+    super(options);\n+\n+    if (options.entities && options.entities.length === 0) {\n+      throw new Error(\"no entities configured for redaction\");\n+    }\n+    this.llm = options.llm;\n+    this.entities = options.entities;\n+    this.contextWindowSize = options.contextWindowSize;\n+    this.detect = options.detect;\n+    this.replace = options.replace;\n+  }\n+\n+  _llmType() {\n+    return \"arcjet_redact\";\n+  }\n+\n+  async _call(input: string, options?: BaseLLMCallOptions): Promise<string> {\n+    const ajOptions: RedactOptions<Detect> = {\n+      entities: this.entities,\n+      contextWindowSize: this.contextWindowSize,\n+      detect: this.detect,\n+      replace: this.replace,\n+    };\n+\n+    const { redact } = await import(\"@arcjet/redact\");\n+    const [redacted, unredact] = await redact(input, ajOptions);\n+\n+    // Invoke the underlying LLM with the prompt and options\n+    const result = await this.llm.invoke(redacted, options);\n+\n+    return unredact(result);\n+  }\n+}",
          "libs/langchain-community/src/llms/tests/arcjet.test.ts": "@@ -0,0 +1,181 @@\n+import { test } from \"@jest/globals\";\n+import {\n+  LLM,\n+  type BaseLLMCallOptions,\n+} from \"@langchain/core/language_models/llms\";\n+import { ArcjetRedact } from \"../arcjet.js\";\n+\n+// Mock LLM for testing purposes\n+export class MockLLM extends LLM {\n+  static lc_name() {\n+    return \"MockLLM\";\n+  }\n+\n+  lc_serializable = true;\n+\n+  callback?: (input: string) => string;\n+\n+  constructor(callback?: (input: string) => string) {\n+    super({});\n+    this.callback = callback;\n+  }\n+\n+  _llmType() {\n+    return \"mock_llm\";\n+  }\n+\n+  async _call(input: string, _options?: BaseLLMCallOptions): Promise<string> {\n+    if (typeof this.callback !== \"undefined\") {\n+      return this.callback(input);\n+    } else {\n+      throw new Error(\"no callback\");\n+    }\n+  }\n+}\n+\n+test(\"It calls the base LLM correctly\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\"this is the input\");\n+    return \"this is the output\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const options = {\n+    llm: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\"this is the input\");\n+\n+  expect(output).toEqual(\"this is the output\");\n+});\n+\n+test(\"It performs redactions and unredactions\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\"email <Redacted email #0>\");\n+    return \"your email is <Redacted email #0>\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const options = {\n+    llm: mockLLM,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\"email test@example.com\");\n+\n+  expect(output).toEqual(\"your email is test@example.com\");\n+});\n+\n+test(\"It only redacts configured entities\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\n+      \"email test@example.com phone <Redacted phone number #0>\"\n+    );\n+    return \"your phone number is <Redacted phone number #0>\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const options = {\n+    llm: mockLLM,\n+    entities: [\"phone-number\" as const],\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\n+    \"email test@example.com phone +35312345678\"\n+  );\n+\n+  expect(output).toEqual(\"your phone number is +35312345678\");\n+});\n+\n+test(\"It redacts custom entities\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\"custom <Redacted custom-entity #0>\");\n+    return \"custom is <Redacted custom-entity #0>\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const customDetector = (tokens: string[]) => {\n+    return tokens.map((t) =>\n+      t === \"my-custom-string-to-be-detected\" ? \"custom-entity\" : undefined\n+    );\n+  };\n+  const options = {\n+    llm: mockLLM,\n+    entities: [\"custom-entity\" as const],\n+    detect: customDetector,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\n+    \"custom my-custom-string-to-be-detected\"\n+  );\n+\n+  expect(output).toEqual(\"custom is my-custom-string-to-be-detected\");\n+});\n+\n+test(\"It provides the correct number of tokens to the context window\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\"this is a sentence for testing\");\n+    return \"this is a sentence for testing\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const customDetector = (tokens: string[]) => {\n+    expect(tokens).toHaveLength(4);\n+    return tokens.map(() => undefined);\n+  };\n+  const options = {\n+    llm: mockLLM,\n+    entities: [\"email\" as const],\n+    detect: customDetector,\n+    contextWindowSize: 4,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\"this is a sentence for testing\");\n+\n+  expect(output).toEqual(\"this is a sentence for testing\");\n+});\n+\n+test(\"It uses custom replacers\", async () => {\n+  const callback = (input: string) => {\n+    expect(input).toEqual(\n+      \"custom <Redacted custom-entity #0> email redacted@example.com\"\n+    );\n+    return \"custom is <Redacted custom-entity #0> email is redacted@example.com\";\n+  };\n+  const mockLLM = new MockLLM(callback);\n+  const customDetector = (tokens: string[]) => {\n+    return tokens.map((t) =>\n+      t === \"my-custom-string-to-be-detected\" ? \"custom-entity\" : undefined\n+    );\n+  };\n+  const customReplacer = (detected: string) => {\n+    return detected === \"email\" ? \"redacted@example.com\" : undefined;\n+  };\n+  const options = {\n+    llm: mockLLM,\n+    entities: [\"custom-entity\" as const, \"email\" as const],\n+    detect: customDetector,\n+    replace: customReplacer,\n+  };\n+\n+  const arcjetRedact = new ArcjetRedact(options);\n+  const output = await arcjetRedact.invoke(\n+    \"custom my-custom-string-to-be-detected email test@example.com\"\n+  );\n+\n+  expect(output).toEqual(\n+    \"custom is my-custom-string-to-be-detected email is test@example.com\"\n+  );\n+});\n+\n+test(\"It throws when no entities are configured\", async () => {\n+  const mockLLM = new MockLLM();\n+  const options = {\n+    llm: mockLLM,\n+    entities: [],\n+  };\n+\n+  expect(() => {\n+    // eslint-disable-next-line no-new\n+    new ArcjetRedact(options);\n+  }).toThrow(\"no entities configured for redaction\");\n+});",
          "libs/langchain-community/src/load/import_constants.ts": "@@ -23,6 +23,7 @@ export const optionalImportEntrypoints: string[] = [\n   \"langchain_community/embeddings/tencent_hunyuan\",\n   \"langchain_community/embeddings/tencent_hunyuan/web\",\n   \"langchain_community/embeddings/zhipuai\",\n+  \"langchain_community/llms/arcjet\",\n   \"langchain_community/llms/bedrock\",\n   \"langchain_community/llms/bedrock/web\",\n   \"langchain_community/llms/cohere\",\n@@ -82,6 +83,7 @@ export const optionalImportEntrypoints: string[] = [\n   \"langchain_community/vectorstores/xata\",\n   \"langchain_community/vectorstores/zep\",\n   \"langchain_community/vectorstores/zep_cloud\",\n+  \"langchain_community/chat_models/arcjet\",\n   \"langchain_community/chat_models/bedrock\",\n   \"langchain_community/chat_models/bedrock/web\",\n   \"langchain_community/chat_models/googlevertexai\",",
          "libs/langchain-community/src/load/import_map.ts": "@@ -54,6 +54,7 @@ export * as chat_models__moonshot from \"../chat_models/moonshot.js\";\n export * as chat_models__ollama from \"../chat_models/ollama.js\";\n export * as chat_models__togetherai from \"../chat_models/togetherai.js\";\n export * as chat_models__yandex from \"../chat_models/yandex.js\";\n+export * as retrievers__bm25 from \"../retrievers/bm25.js\";\n export * as retrievers__chaindesk from \"../retrievers/chaindesk.js\";\n export * as retrievers__databerry from \"../retrievers/databerry.js\";\n export * as retrievers__remote from \"../retrievers/remote/index.js\";",
          "libs/langchain-community/src/retrievers/bm25.ts": "@@ -0,0 +1,58 @@\n+import { BaseRetriever, BaseRetrieverInput } from \"@langchain/core/retrievers\";\n+import { Document } from \"@langchain/core/documents\";\n+\n+import { BM25 } from \"../utils/@furkantoprak/bm25/BM25.js\";\n+\n+export type BM25RetrieverOptions = {\n+  docs: Document[];\n+  k: number;\n+} & BaseRetrieverInput;\n+\n+/**\n+ * A retriever that uses the BM25 algorithm to rank documents based on their\n+ * similarity to a query. It uses the \"okapibm25\" package for BM25 scoring.\n+ * The k parameter determines the number of documents to return for each query.\n+ */\n+export class BM25Retriever extends BaseRetriever {\n+  static lc_name() {\n+    return \"BM25Retriever\";\n+  }\n+\n+  lc_namespace = [\"langchain\", \"retrievers\", \"bm25_retriever\"];\n+\n+  static fromDocuments(\n+    documents: Document[],\n+    options: Omit<BM25RetrieverOptions, \"docs\">\n+  ) {\n+    return new this({ ...options, docs: documents });\n+  }\n+\n+  docs: Document[];\n+\n+  k: number;\n+\n+  constructor(options: BM25RetrieverOptions) {\n+    super(options);\n+    this.docs = options.docs;\n+    this.k = options.k;\n+  }\n+\n+  private preprocessFunc(text: string): string[] {\n+    return text.toLowerCase().split(/\\s+/);\n+  }\n+\n+  async _getRelevantDocuments(query: string) {\n+    const processedQuery = this.preprocessFunc(query);\n+    const documents = this.docs.map((doc) => doc.pageContent);\n+    const scores = BM25(documents, processedQuery) as number[];\n+\n+    const scoredDocs = this.docs.map((doc, index) => ({\n+      document: doc,\n+      score: scores[index],\n+    }));\n+\n+    scoredDocs.sort((a, b) => b.score - a.score);\n+\n+    return scoredDocs.slice(0, this.k).map((item) => item.document);\n+  }\n+}",
          "libs/langchain-community/src/retrievers/tests/bm25.test.ts": "@@ -0,0 +1,27 @@\n+import { expect, test } from \"@jest/globals\";\n+import { Document } from \"@langchain/core/documents\";\n+import { BM25Retriever } from \"../bm25.js\";\n+\n+test(\"BM25Retriever\", async () => {\n+  const docs = [\n+    new Document({\n+      pageContent: \"The quick brown fox jumps over the lazy dog\",\n+    }),\n+    new Document({\n+      pageContent: \"A lazy dog sleeps all day\",\n+    }),\n+    new Document({\n+      pageContent: \"The brown fox is quick and clever\",\n+    }),\n+  ];\n+\n+  const retriever = BM25Retriever.fromDocuments(docs, {\n+    k: 2,\n+  });\n+  const results = await retriever.invoke(\"the fox and the dog\");\n+\n+  expect(results).toHaveLength(2);\n+  expect(results[0].pageContent).toBe(\n+    \"The quick brown fox jumps over the lazy dog\"\n+  );\n+});",
          "libs/langchain-community/src/utils/@furkantoprak/bm25/BM25.ts": "@@ -0,0 +1,100 @@\n+/**\n+ * Adapted from\n+ * https://github.com/FurkanToprak/OkapiBM25\n+ *\n+ * Inlined due to CJS import issues.\n+ */\n+\n+/** Gets word count. */\n+export const getWordCount = (corpus: string) => {\n+  return ((corpus || \"\").match(/\\w+/g) || []).length;\n+};\n+\n+/** Number of occurences of a word in a string. */\n+export const getTermFrequency = (term: string, corpus: string) => {\n+  return ((corpus || \"\").match(new RegExp(term, \"g\")) || []).length;\n+};\n+\n+/** Inverse document frequency. */\n+export const getIDF = (term: string, documents: string[]) => {\n+  // Number of relevant documents.\n+  const relevantDocuments = documents.filter((document: string) =>\n+    document.includes(term)\n+  ).length;\n+  return Math.log(\n+    (documents.length - relevantDocuments + 0.5) / (relevantDocuments + 0.5) + 1\n+  );\n+};\n+\n+/** Represents a document; useful when sorting results.\n+ */\n+export interface BMDocument {\n+  /** The document is originally scoreed. */\n+  document: string;\n+  /** The score that the document recieves. */\n+  score: number;\n+}\n+\n+/** Constants that are free parameters used in BM25, specifically when generating inverse document frequency. */\n+export interface BMConstants {\n+  /** Free parameter. Is 0.75 by default.  */\n+  b?: number;\n+  /** Free parameter. Is 1.2 by default. Generally in range [1.2, 2.0] */\n+  k1?: number;\n+}\n+\n+/** If returns positive, the sorting results in secondEl coming before firstEl, else, firstEl comes before secondEL  */\n+export type BMSorter = (firstEl: BMDocument, secondEl: BMDocument) => number;\n+\n+/** Implementation of Okapi BM25 algorithm.\n+ *  @param documents: Collection of documents.\n+ *  @param keywords: query terms.\n+ *  @param constants: Contains free parameters k1 and b. b=0.75 and k1=1.2 by default.\n+ *  @param sort: A function that allows you to sort queries by a given rule. If not provided, returns results corresponding to the original order.\n+ * If this option is provided, the return type will not be an array of scores but an array of documents with their scores.\n+ */\n+export function BM25(\n+  documents: string[],\n+  keywords: string[],\n+  constants?: BMConstants,\n+  sorter?: BMSorter\n+): number[] | BMDocument[] {\n+  const b = constants && constants.b ? constants.b : 0.75;\n+  const k1 = constants && constants.k1 ? constants.k1 : 1.2;\n+  const documentLengths = documents.map((document: string) =>\n+    getWordCount(document)\n+  );\n+  const averageDocumentLength =\n+    documentLengths.reduce((a, b) => a + b, 0) / documents.length;\n+  const idfByKeyword = keywords.reduce((obj, keyword) => {\n+    obj.set(keyword, getIDF(keyword, documents));\n+    return obj;\n+  }, new Map<string, number>());\n+\n+  const scores = documents.map((document: string, index: number) => {\n+    const score = keywords\n+      .map((keyword: string) => {\n+        const inverseDocumentFrequency = idfByKeyword.get(keyword);\n+        if (inverseDocumentFrequency === undefined) {\n+          throw new Error(\"Missing keyword.\");\n+        }\n+        const termFrequency = getTermFrequency(keyword, document);\n+        const documentLength = documentLengths[index];\n+        return (\n+          (inverseDocumentFrequency * (termFrequency * (k1 + 1))) /\n+          (termFrequency +\n+            k1 * (1 - b + (b * documentLength) / averageDocumentLength))\n+        );\n+      })\n+      .reduce((a: number, b: number) => a + b, 0);\n+    if (sorter) {\n+      return { score, document } as BMDocument;\n+    }\n+    return score;\n+  });\n+  // sort the results\n+  if (sorter) {\n+    return (scores as BMDocument[]).sort(sorter);\n+  }\n+  return scores as number[];\n+}",
          "libs/langchain-community/src/utils/@furkantoprak/bm25/LICENSE.md": "@@ -0,0 +1,21 @@\n+# MIT License\n+\n+## Copyright (c) 2020 Furkan Toprak\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy\n+of this software and associated documentation files (the \"Software\"), to deal\n+in the Software without restriction, including without limitation the rights\n+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+copies of the Software, and to permit persons to whom the Software is\n+furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.",
          "libs/langchain-google-common/.gitignore": "@@ -10,6 +10,14 @@ types.cjs\n types.js\n types.d.ts\n types.d.cts\n+experimental/media.cjs\n+experimental/media.js\n+experimental/media.d.ts\n+experimental/media.d.cts\n+experimental/utils/media_core.cjs\n+experimental/utils/media_core.js\n+experimental/utils/media_core.d.ts\n+experimental/utils/media_core.d.cts\n node_modules\n dist\n .yarn",
          "libs/langchain-google-common/langchain.config.js": "@@ -16,6 +16,8 @@ export const config = {\n     index: \"index\",\n     utils: \"utils/index\",\n     types: \"types\",\n+    \"experimental/media\": \"experimental/media\",\n+    \"experimental/utils/media_core\": \"experimental/utils/media_core\",\n   },\n   tsConfigPath: resolve(\"./tsconfig.json\"),\n   cjsSource: \"./dist-cjs\",",
          "libs/langchain-google-common/package.json": "@@ -92,6 +92,24 @@\n       \"import\": \"./types.js\",\n       \"require\": \"./types.cjs\"\n     },\n+    \"./experimental/media\": {\n+      \"types\": {\n+        \"import\": \"./experimental/media.d.ts\",\n+        \"require\": \"./experimental/media.d.cts\",\n+        \"default\": \"./experimental/media.d.ts\"\n+      },\n+      \"import\": \"./experimental/media.js\",\n+      \"require\": \"./experimental/media.cjs\"\n+    },\n+    \"./experimental/utils/media_core\": {\n+      \"types\": {\n+        \"import\": \"./experimental/utils/media_core.d.ts\",\n+        \"require\": \"./experimental/utils/media_core.d.cts\",\n+        \"default\": \"./experimental/utils/media_core.d.ts\"\n+      },\n+      \"import\": \"./experimental/utils/media_core.js\",\n+      \"require\": \"./experimental/utils/media_core.cjs\"\n+    },\n     \"./package.json\": \"./package.json\"\n   },\n   \"files\": [\n@@ -107,6 +125,14 @@\n     \"types.cjs\",\n     \"types.js\",\n     \"types.d.ts\",\n-    \"types.d.cts\"\n+    \"types.d.cts\",\n+    \"experimental/media.cjs\",\n+    \"experimental/media.js\",\n+    \"experimental/media.d.ts\",\n+    \"experimental/media.d.cts\",\n+    \"experimental/utils/media_core.cjs\",\n+    \"experimental/utils/media_core.js\",\n+    \"experimental/utils/media_core.d.ts\",\n+    \"experimental/utils/media_core.d.cts\"\n   ]\n }",
          "libs/langchain-google-common/src/auth.ts": "@@ -1,9 +1,9 @@\n import { ReadableJsonStream } from \"./utils/stream.js\";\n import { GooglePlatformType } from \"./types.js\";\n \n-export type GoogleAbstractedClientOpsMethod = \"GET\" | \"POST\";\n+export type GoogleAbstractedClientOpsMethod = \"GET\" | \"POST\" | \"DELETE\";\n \n-export type GoogleAbstractedClientOpsResponseType = \"json\" | \"stream\";\n+export type GoogleAbstractedClientOpsResponseType = \"json\" | \"stream\" | \"blob\";\n \n export type GoogleAbstractedClientOps = {\n   url?: string;\n@@ -28,6 +28,17 @@ export abstract class GoogleAbstractedFetchClient\n \n   abstract request(opts: GoogleAbstractedClientOps): unknown;\n \n+  async _buildData(res: Response, opts: GoogleAbstractedClientOps) {\n+    switch (opts.responseType) {\n+      case \"json\":\n+        return res.json();\n+      case \"stream\":\n+        return new ReadableJsonStream(res.body);\n+      default:\n+        return res.blob();\n+    }\n+  }\n+\n   async _request(\n     url: string | undefined,\n     opts: GoogleAbstractedClientOps,\n@@ -47,7 +58,11 @@ export abstract class GoogleAbstractedFetchClient\n       },\n     };\n     if (opts.data !== undefined) {\n-      fetchOptions.body = JSON.stringify(opts.data);\n+      if (typeof opts.data === \"string\") {\n+        fetchOptions.body = opts.data;\n+      } else {\n+        fetchOptions.body = JSON.stringify(opts.data);\n+      }\n     }\n \n     const res = await fetch(url, fetchOptions);\n@@ -57,16 +72,21 @@ export abstract class GoogleAbstractedFetchClient\n       const error = new Error(\n         `Google request failed with status code ${res.status}: ${resText}`\n       );\n-      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+      /* eslint-disable @typescript-eslint/no-explicit-any */\n       (error as any).response = res;\n+      (error as any).details = {\n+        url,\n+        opts,\n+        fetchOptions,\n+        result: res,\n+      };\n+      /* eslint-enable @typescript-eslint/no-explicit-any */\n       throw error;\n     }\n \n+    const data = await this._buildData(res, opts);\n     return {\n-      data:\n-        opts.responseType === \"json\"\n-          ? await res.json()\n-          : new ReadableJsonStream(res.body),\n+      data,\n       config: {},\n       status: res.status,\n       statusText: res.statusText,",
          "libs/langchain-google-common/src/chat_models.ts": "@@ -39,12 +39,7 @@ import {\n   copyAndValidateModelParamsInto,\n } from \"./utils/common.js\";\n import { AbstractGoogleLLMConnection } from \"./connection.js\";\n-import {\n-  baseMessageToContent,\n-  safeResponseToChatGeneration,\n-  safeResponseToChatResult,\n-  DefaultGeminiSafetyHandler,\n-} from \"./utils/gemini.js\";\n+import { DefaultGeminiSafetyHandler } from \"./utils/gemini.js\";\n import { ApiKeyGoogleAuth, GoogleAbstractedClient } from \"./auth.js\";\n import { JsonStream } from \"./utils/stream.js\";\n import { ensureParams } from \"./utils/failed_handler.js\";\n@@ -55,6 +50,7 @@ import type {\n   GeminiFunctionDeclaration,\n   GeminiFunctionSchema,\n   GoogleAIToolType,\n+  GeminiAPIConfig,\n } from \"./types.js\";\n import { zodToGeminiParameters } from \"./utils/zod_to_gemini_parameters.js\";\n \n@@ -100,61 +96,69 @@ class ChatConnection<AuthOptions> extends AbstractGoogleLLMConnection<\n     return true;\n   }\n \n-  formatContents(\n+  async formatContents(\n     input: BaseMessage[],\n     _parameters: GoogleAIModelParams\n-  ): GeminiContent[] {\n-    return input\n-      .map((msg, i) =>\n-        baseMessageToContent(msg, input[i - 1], this.useSystemInstruction)\n+  ): Promise<GeminiContent[]> {\n+    const inputPromises: Promise<GeminiContent[]>[] = input.map((msg, i) =>\n+      this.api.baseMessageToContent(\n+        msg,\n+        input[i - 1],\n+        this.useSystemInstruction\n       )\n-      .reduce((acc, cur) => {\n-        // Filter out the system content\n-        if (cur.every((content) => content.role === \"system\")) {\n-          return acc;\n-        }\n-\n-        // Combine adjacent function messages\n-        if (\n-          cur[0]?.role === \"function\" &&\n-          acc.length > 0 &&\n-          acc[acc.length - 1].role === \"function\"\n-        ) {\n-          acc[acc.length - 1].parts = [\n-            ...acc[acc.length - 1].parts,\n-            ...cur[0].parts,\n-          ];\n-        } else {\n-          acc.push(...cur);\n-        }\n+    );\n+    const inputs = await Promise.all(inputPromises);\n \n+    return inputs.reduce((acc, cur) => {\n+      // Filter out the system content\n+      if (cur.every((content) => content.role === \"system\")) {\n         return acc;\n-      }, [] as GeminiContent[]);\n+      }\n+\n+      // Combine adjacent function messages\n+      if (\n+        cur[0]?.role === \"function\" &&\n+        acc.length > 0 &&\n+        acc[acc.length - 1].role === \"function\"\n+      ) {\n+        acc[acc.length - 1].parts = [\n+          ...acc[acc.length - 1].parts,\n+          ...cur[0].parts,\n+        ];\n+      } else {\n+        acc.push(...cur);\n+      }\n+\n+      return acc;\n+    }, [] as GeminiContent[]);\n   }\n \n-  formatSystemInstruction(\n+  async formatSystemInstruction(\n     input: BaseMessage[],\n     _parameters: GoogleAIModelParams\n-  ): GeminiContent {\n+  ): Promise<GeminiContent> {\n     if (!this.useSystemInstruction) {\n       return {} as GeminiContent;\n     }\n \n     let ret = {} as GeminiContent;\n-    input.forEach((message, index) => {\n+    for (let index = 0; index < input.length; index += 1) {\n+      const message = input[index];\n       if (message._getType() === \"system\") {\n         // For system types, we only want it if it is the first message,\n         // if it appears anywhere else, it should be an error.\n         if (index === 0) {\n           // eslint-disable-next-line prefer-destructuring\n-          ret = baseMessageToContent(message, undefined, true)[0];\n+          ret = (\n+            await this.api.baseMessageToContent(message, undefined, true)\n+          )[0];\n         } else {\n           throw new Error(\n             \"System messages are only permitted as the first passed message.\"\n           );\n         }\n       }\n-    });\n+    }\n \n     return ret;\n   }\n@@ -168,6 +172,7 @@ export interface ChatGoogleBaseInput<AuthOptions>\n     GoogleConnectionParams<AuthOptions>,\n     GoogleAIModelParams,\n     GoogleAISafetyParams,\n+    GeminiAPIConfig,\n     Pick<GoogleAIBaseLanguageModelCallOptions, \"streamUsage\"> {}\n \n /**\n@@ -338,7 +343,10 @@ export abstract class ChatGoogleBase<AuthOptions>\n       parameters,\n       options\n     );\n-    const ret = safeResponseToChatResult(response, this.safetyHandler);\n+    const ret = this.connection.api.safeResponseToChatResult(\n+      response,\n+      this.safetyHandler\n+    );\n     await runManager?.handleLLMNewToken(ret.generations[0].text);\n     return ret;\n   }\n@@ -378,7 +386,10 @@ export abstract class ChatGoogleBase<AuthOptions>\n       }\n       const chunk =\n         output !== null\n-          ? safeResponseToChatGeneration({ data: output }, this.safetyHandler)\n+          ? this.connection.api.safeResponseToChatGeneration(\n+              { data: output },\n+              this.safetyHandler\n+            )\n           : new ChatGenerationChunk({\n               text: \"\",\n               generationInfo: { finishReason: \"stop\" },",
          "libs/langchain-google-common/src/connection.ts": "@@ -20,6 +20,7 @@ import type {\n   GeminiTool,\n   GeminiFunctionDeclaration,\n   GoogleAIModelRequestParams,\n+  GoogleRawResponse,\n   GoogleAIToolType,\n } from \"./types.js\";\n import {\n@@ -28,6 +29,7 @@ import {\n   GoogleAbstractedClientOpsMethod,\n } from \"./auth.js\";\n import { zodToGeminiParameters } from \"./utils/zod_to_gemini_parameters.js\";\n+import { getGeminiAPI } from \"./utils/index.js\";\n \n export abstract class GoogleConnection<\n   CallOptions extends AsyncCallerCallOptions,\n@@ -84,15 +86,23 @@ export abstract class GoogleConnection<\n     return this.constructor.name;\n   }\n \n-  async _request(\n+  async additionalHeaders(): Promise<Record<string, string>> {\n+    return {};\n+  }\n+\n+  async _buildOpts(\n     data: unknown | undefined,\n-    options: CallOptions\n-  ): Promise<ResponseType> {\n+    _options: CallOptions,\n+    requestHeaders: Record<string, string> = {}\n+  ): Promise<GoogleAbstractedClientOps> {\n     const url = await this.buildUrl();\n     const method = this.buildMethod();\n     const infoHeaders = (await this._clientInfoHeaders()) ?? {};\n+    const additionalHeaders = (await this.additionalHeaders()) ?? {};\n     const headers = {\n       ...infoHeaders,\n+      ...additionalHeaders,\n+      ...requestHeaders,\n     };\n \n     const opts: GoogleAbstractedClientOps = {\n@@ -108,7 +118,15 @@ export abstract class GoogleConnection<\n     } else {\n       opts.responseType = \"json\";\n     }\n+    return opts;\n+  }\n \n+  async _request(\n+    data: unknown | undefined,\n+    options: CallOptions,\n+    requestHeaders: Record<string, string> = {}\n+  ): Promise<ResponseType> {\n+    const opts = await this._buildOpts(data, options, requestHeaders);\n     const callResponse = await this.caller.callWithOptions(\n       { signal: options?.signal },\n       async () => this.client.request(opts)\n@@ -165,6 +183,21 @@ export abstract class GoogleHostConnection<\n   }\n }\n \n+export abstract class GoogleRawConnection<\n+  CallOptions extends AsyncCallerCallOptions,\n+  AuthOptions\n+> extends GoogleHostConnection<CallOptions, GoogleRawResponse, AuthOptions> {\n+  async _buildOpts(\n+    data: unknown | undefined,\n+    _options: CallOptions,\n+    requestHeaders: Record<string, string> = {}\n+  ): Promise<GoogleAbstractedClientOps> {\n+    const opts = await super._buildOpts(data, _options, requestHeaders);\n+    opts.responseType = \"blob\";\n+    return opts;\n+  }\n+}\n+\n export abstract class GoogleAIConnection<\n     CallOptions extends AsyncCallerCallOptions,\n     InputType,\n@@ -180,6 +213,9 @@ export abstract class GoogleAIConnection<\n \n   client: GoogleAbstractedClient;\n \n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  api: any; // FIXME: Make this a real type\n+\n   constructor(\n     fields: GoogleAIBaseLLMInput<AuthOptions> | undefined,\n     caller: AsyncCaller,\n@@ -190,6 +226,7 @@ export abstract class GoogleAIConnection<\n     this.client = client;\n     this.modelName = fields?.model ?? fields?.modelName ?? this.model;\n     this.model = this.modelName;\n+    this.api = getGeminiAPI(fields);\n   }\n \n   get modelFamily(): GoogleLLMModelFamily {\n@@ -235,14 +272,14 @@ export abstract class GoogleAIConnection<\n   abstract formatData(\n     input: InputType,\n     parameters: GoogleAIModelRequestParams\n-  ): unknown;\n+  ): Promise<unknown>;\n \n   async request(\n     input: InputType,\n     parameters: GoogleAIModelRequestParams,\n     options: CallOptions\n-  ): Promise<ResponseType> {\n-    const data = this.formatData(input, parameters);\n+  ): Promise<GoogleResponse> {\n+    const data = await this.formatData(input, parameters);\n     const response = await this._request(data, options);\n     return response;\n   }\n@@ -273,7 +310,7 @@ export abstract class AbstractGoogleLLMConnection<\n   abstract formatContents(\n     input: MessageType,\n     parameters: GoogleAIModelRequestParams\n-  ): GeminiContent[];\n+  ): Promise<GeminiContent[]>;\n \n   formatGenerationConfig(\n     _input: MessageType,\n@@ -296,10 +333,10 @@ export abstract class AbstractGoogleLLMConnection<\n     return parameters.safetySettings ?? [];\n   }\n \n-  formatSystemInstruction(\n+  async formatSystemInstruction(\n     _input: MessageType,\n     _parameters: GoogleAIModelRequestParams\n-  ): GeminiContent {\n+  ): Promise<GeminiContent> {\n     return {} as GeminiContent;\n   }\n \n@@ -362,16 +399,19 @@ export abstract class AbstractGoogleLLMConnection<\n     };\n   }\n \n-  formatData(\n+  async formatData(\n     input: MessageType,\n     parameters: GoogleAIModelRequestParams\n-  ): GeminiRequest {\n-    const contents = this.formatContents(input, parameters);\n+  ): Promise<GeminiRequest> {\n+    const contents = await this.formatContents(input, parameters);\n     const generationConfig = this.formatGenerationConfig(input, parameters);\n     const tools = this.formatTools(input, parameters);\n     const toolConfig = this.formatToolConfig(parameters);\n     const safetySettings = this.formatSafetySettings(input, parameters);\n-    const systemInstruction = this.formatSystemInstruction(input, parameters);\n+    const systemInstruction = await this.formatSystemInstruction(\n+      input,\n+      parameters\n+    );\n \n     const ret: GeminiRequest = {\n       contents,",
          "libs/langchain-google-common/src/embeddings.ts": "@@ -38,10 +38,10 @@ class EmbeddingsConnection<\n     return \"predict\";\n   }\n \n-  formatData(\n+  async formatData(\n     input: GoogleEmbeddingsInstance[],\n     parameters: GoogleAIModelRequestParams\n-  ): unknown {\n+  ): Promise<unknown> {\n     return {\n       instances: input,\n       parameters,\n@@ -172,7 +172,8 @@ export abstract class BaseGoogleEmbeddings<AuthOptions>\n         ?.map(\n           (response) =>\n             response?.data?.predictions?.map(\n-              (result) => result.embeddings.values\n+              // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+              (result: any) => result.embeddings?.values\n             ) ?? []\n         )\n         .flat() ?? [];",
          "libs/langchain-google-common/src/experimental/media.ts": "@@ -0,0 +1,803 @@\n+import {\n+  AsyncCaller,\n+  AsyncCallerCallOptions,\n+  AsyncCallerParams,\n+} from \"@langchain/core/utils/async_caller\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import {\n+  MediaBlob,\n+  BlobStore,\n+  BlobStoreOptions,\n+  MediaBlobData,\n+} from \"./utils/media_core.js\";\n+import {\n+  GoogleConnectionParams,\n+  GoogleRawResponse,\n+  GoogleResponse,\n+} from \"../types.js\";\n+import { GoogleHostConnection, GoogleRawConnection } from \"../connection.js\";\n+import {\n+  ApiKeyGoogleAuth,\n+  GoogleAbstractedClient,\n+  GoogleAbstractedClientOpsMethod,\n+} from \"../auth.js\";\n+\n+export interface GoogleUploadConnectionParams<AuthOptions>\n+  extends GoogleConnectionParams<AuthOptions> {}\n+\n+export abstract class GoogleMultipartUploadConnection<\n+  CallOptions extends AsyncCallerCallOptions,\n+  ResponseType extends GoogleResponse,\n+  AuthOptions\n+> extends GoogleHostConnection<CallOptions, ResponseType, AuthOptions> {\n+  constructor(\n+    fields: GoogleConnectionParams<AuthOptions> | undefined,\n+    caller: AsyncCaller,\n+    client: GoogleAbstractedClient\n+  ) {\n+    super(fields, caller, client);\n+  }\n+\n+  async _body(\n+    separator: string,\n+    data: MediaBlob,\n+    metadata: Record<string, unknown>\n+  ): Promise<string> {\n+    const contentType = data.mimetype;\n+    const { encoded, encoding } = await data.encode();\n+    const body = [\n+      `--${separator}`,\n+      \"Content-Type: application/json; charset=UTF-8\",\n+      \"\",\n+      JSON.stringify(metadata),\n+      \"\",\n+      `--${separator}`,\n+      `Content-Type: ${contentType}`,\n+      `Content-Transfer-Encoding: ${encoding}`,\n+      \"\",\n+      encoded,\n+      `--${separator}--`,\n+    ];\n+    return body.join(\"\\n\");\n+  }\n+\n+  async request(\n+    data: MediaBlob,\n+    metadata: Record<string, unknown>,\n+    options: CallOptions\n+  ): Promise<ResponseType> {\n+    const separator = `separator-${Date.now()}`;\n+    const body = await this._body(separator, data, metadata);\n+    const requestHeaders = {\n+      \"Content-Type\": `multipart/related; boundary=${separator}`,\n+      \"X-Goog-Upload-Protocol\": \"multipart\",\n+    };\n+    const response = this._request(body, options, requestHeaders);\n+    return response;\n+  }\n+}\n+\n+export abstract class GoogleDownloadConnection<\n+  CallOptions extends AsyncCallerCallOptions,\n+  ResponseType extends GoogleResponse,\n+  AuthOptions\n+> extends GoogleHostConnection<CallOptions, ResponseType, AuthOptions> {\n+  async request(options: CallOptions): Promise<ResponseType> {\n+    return this._request(undefined, options);\n+  }\n+}\n+\n+export abstract class GoogleDownloadRawConnection<\n+  CallOptions extends AsyncCallerCallOptions,\n+  AuthOptions\n+> extends GoogleRawConnection<CallOptions, AuthOptions> {\n+  buildMethod(): GoogleAbstractedClientOpsMethod {\n+    return \"GET\";\n+  }\n+\n+  async request(options: CallOptions): Promise<GoogleRawResponse> {\n+    return this._request(undefined, options);\n+  }\n+}\n+\n+export interface BlobStoreGoogleParams<AuthOptions>\n+  extends GoogleConnectionParams<AuthOptions>,\n+    AsyncCallerParams,\n+    BlobStoreOptions {}\n+\n+export abstract class BlobStoreGoogle<\n+  ResponseType extends GoogleResponse,\n+  AuthOptions\n+> extends BlobStore {\n+  caller: AsyncCaller;\n+\n+  client: GoogleAbstractedClient;\n+\n+  constructor(fields?: BlobStoreGoogleParams<AuthOptions>) {\n+    super(fields);\n+    this.caller = new AsyncCaller(fields ?? {});\n+    this.client = this.buildClient(fields);\n+  }\n+\n+  abstract buildClient(\n+    fields?: BlobStoreGoogleParams<AuthOptions>\n+  ): GoogleAbstractedClient;\n+\n+  abstract buildSetMetadata([key, blob]: [string, MediaBlob]): Record<\n+    string,\n+    unknown\n+  >;\n+\n+  abstract buildSetConnection([key, blob]: [\n+    string,\n+    MediaBlob\n+  ]): GoogleMultipartUploadConnection<\n+    AsyncCallerCallOptions,\n+    ResponseType,\n+    AuthOptions\n+  >;\n+\n+  async _set(keyValuePair: [string, MediaBlob]): Promise<ResponseType> {\n+    const [, blob] = keyValuePair;\n+    const setMetadata = this.buildSetMetadata(keyValuePair);\n+    const metadata = setMetadata;\n+    const options = {};\n+    const connection = this.buildSetConnection(keyValuePair);\n+    const response = await connection.request(blob, metadata, options);\n+    return response;\n+  }\n+\n+  async mset(keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    const ret = keyValuePairs.map((keyValue) => this._set(keyValue));\n+    await Promise.all(ret);\n+  }\n+\n+  abstract buildGetMetadataConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    ResponseType,\n+    AuthOptions\n+  >;\n+\n+  async _getMetadata(key: string): Promise<Record<string, unknown>> {\n+    const connection = this.buildGetMetadataConnection(key);\n+    const options = {};\n+    const response = await connection.request(options);\n+    return response.data;\n+  }\n+\n+  abstract buildGetDataConnection(\n+    key: string\n+  ): GoogleDownloadRawConnection<AsyncCallerCallOptions, AuthOptions>;\n+\n+  async _getData(key: string): Promise<Blob> {\n+    const connection = this.buildGetDataConnection(key);\n+    const options = {};\n+    const response = await connection.request(options);\n+    return response.data;\n+  }\n+\n+  _getMimetypeFromMetadata(metadata: Record<string, unknown>): string {\n+    return metadata.contentType as string;\n+  }\n+\n+  async _get(key: string): Promise<MediaBlob | undefined> {\n+    const metadata = await this._getMetadata(key);\n+    const data = await this._getData(key);\n+    if (data && metadata) {\n+      const ret = await MediaBlob.fromBlob(data, { metadata, path: key });\n+      return ret;\n+    } else {\n+      return undefined;\n+    }\n+  }\n+\n+  async mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    const ret = keys.map((key) => this._get(key));\n+    return await Promise.all(ret);\n+  }\n+\n+  abstract buildDeleteConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    GoogleResponse,\n+    AuthOptions\n+  >;\n+\n+  async _del(key: string): Promise<void> {\n+    const connection = this.buildDeleteConnection(key);\n+    const options = {};\n+    await connection.request(options);\n+  }\n+\n+  async mdelete(keys: string[]): Promise<void> {\n+    const ret = keys.map((key) => this._del(key));\n+    await Promise.all(ret);\n+  }\n+\n+  // eslint-disable-next-line require-yield\n+  async *yieldKeys(_prefix: string | undefined): AsyncGenerator<string> {\n+    // TODO: Implement. Most have an implementation that uses nextToken.\n+    throw new Error(\"yieldKeys is not implemented\");\n+  }\n+}\n+\n+/**\n+ * Based on https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n+ */\n+export interface GoogleCloudStorageObject extends Record<string, unknown> {\n+  id?: string;\n+  name?: string;\n+  contentType?: string;\n+  metadata?: Record<string, unknown>;\n+  // This is incomplete.\n+}\n+\n+export interface GoogleCloudStorageResponse extends GoogleResponse {\n+  data: GoogleCloudStorageObject;\n+}\n+\n+export type BucketAndPath = {\n+  bucket: string;\n+  path: string;\n+};\n+\n+export class GoogleCloudStorageUri {\n+  static uriRegexp = /gs:\\/\\/([a-z0-9][a-z0-9._-]+[a-z0-9])\\/(.*)/;\n+\n+  bucket: string;\n+\n+  path: string;\n+\n+  constructor(uri: string) {\n+    const bucketAndPath = GoogleCloudStorageUri.uriToBucketAndPath(uri);\n+    this.bucket = bucketAndPath.bucket;\n+    this.path = bucketAndPath.path;\n+  }\n+\n+  get uri() {\n+    return `gs://${this.bucket}/${this.path}`;\n+  }\n+\n+  get isValid() {\n+    return (\n+      typeof this.bucket !== \"undefined\" && typeof this.path !== \"undefined\"\n+    );\n+  }\n+\n+  static uriToBucketAndPath(uri: string): BucketAndPath {\n+    const match = this.uriRegexp.exec(uri);\n+    if (!match) {\n+      throw new Error(`Invalid gs:// URI: ${uri}`);\n+    }\n+    return {\n+      bucket: match[1],\n+      path: match[2],\n+    };\n+  }\n+\n+  static isValidUri(uri: string): boolean {\n+    return this.uriRegexp.test(uri);\n+  }\n+}\n+\n+export interface GoogleCloudStorageConnectionParams {\n+  uri: string;\n+}\n+\n+export interface GoogleCloudStorageUploadConnectionParams<AuthOptions>\n+  extends GoogleUploadConnectionParams<AuthOptions>,\n+    GoogleCloudStorageConnectionParams {}\n+\n+export class GoogleCloudStorageUploadConnection<\n+  AuthOptions\n+> extends GoogleMultipartUploadConnection<\n+  AsyncCallerCallOptions,\n+  GoogleCloudStorageResponse,\n+  AuthOptions\n+> {\n+  uri: GoogleCloudStorageUri;\n+\n+  constructor(\n+    fields: GoogleCloudStorageUploadConnectionParams<AuthOptions>,\n+    caller: AsyncCaller,\n+    client: GoogleAbstractedClient\n+  ) {\n+    super(fields, caller, client);\n+    this.uri = new GoogleCloudStorageUri(fields.uri);\n+  }\n+\n+  async buildUrl(): Promise<string> {\n+    return `https://storage.googleapis.com/upload/storage/${this.apiVersion}/b/${this.uri.bucket}/o?uploadType=multipart`;\n+  }\n+}\n+\n+export interface GoogleCloudStorageDownloadConnectionParams<AuthOptions>\n+  extends GoogleCloudStorageConnectionParams,\n+    GoogleConnectionParams<AuthOptions> {\n+  method: GoogleAbstractedClientOpsMethod;\n+  alt: \"media\" | undefined;\n+}\n+\n+export class GoogleCloudStorageDownloadConnection<\n+  ResponseType extends GoogleResponse,\n+  AuthOptions\n+> extends GoogleDownloadConnection<\n+  AsyncCallerCallOptions,\n+  ResponseType,\n+  AuthOptions\n+> {\n+  uri: GoogleCloudStorageUri;\n+\n+  method: GoogleAbstractedClientOpsMethod;\n+\n+  alt: \"media\" | undefined;\n+\n+  constructor(\n+    fields: GoogleCloudStorageDownloadConnectionParams<AuthOptions>,\n+    caller: AsyncCaller,\n+    client: GoogleAbstractedClient\n+  ) {\n+    super(fields, caller, client);\n+    this.uri = new GoogleCloudStorageUri(fields.uri);\n+    this.method = fields.method;\n+    this.alt = fields.alt;\n+  }\n+\n+  buildMethod(): GoogleAbstractedClientOpsMethod {\n+    return this.method;\n+  }\n+\n+  async buildUrl(): Promise<string> {\n+    const path = encodeURIComponent(this.uri.path);\n+    const ret = `https://storage.googleapis.com/storage/${this.apiVersion}/b/${this.uri.bucket}/o/${path}`;\n+    return this.alt ? `${ret}?alt=${this.alt}` : ret;\n+  }\n+}\n+\n+export interface GoogleCloudStorageRawConnectionParams<AuthOptions>\n+  extends GoogleCloudStorageConnectionParams,\n+    GoogleConnectionParams<AuthOptions> {}\n+\n+export class GoogleCloudStorageRawConnection<\n+  AuthOptions\n+> extends GoogleDownloadRawConnection<AsyncCallerCallOptions, AuthOptions> {\n+  uri: GoogleCloudStorageUri;\n+\n+  constructor(\n+    fields: GoogleCloudStorageRawConnectionParams<AuthOptions>,\n+    caller: AsyncCaller,\n+    client: GoogleAbstractedClient\n+  ) {\n+    super(fields, caller, client);\n+    this.uri = new GoogleCloudStorageUri(fields.uri);\n+  }\n+\n+  async buildUrl(): Promise<string> {\n+    const path = encodeURIComponent(this.uri.path);\n+    const ret = `https://storage.googleapis.com/storage/${this.apiVersion}/b/${this.uri.bucket}/o/${path}?alt=media`;\n+    return ret;\n+  }\n+}\n+\n+export interface BlobStoreGoogleCloudStorageBaseParams<AuthOptions>\n+  extends BlobStoreGoogleParams<AuthOptions> {\n+  uriPrefix: GoogleCloudStorageUri;\n+}\n+\n+export abstract class BlobStoreGoogleCloudStorageBase<\n+  AuthOptions\n+> extends BlobStoreGoogle<GoogleCloudStorageResponse, AuthOptions> {\n+  params: BlobStoreGoogleCloudStorageBaseParams<AuthOptions>;\n+\n+  constructor(fields: BlobStoreGoogleCloudStorageBaseParams<AuthOptions>) {\n+    super(fields);\n+    this.params = fields;\n+    this.defaultStoreOptions = {\n+      ...this.defaultStoreOptions,\n+      pathPrefix: fields.uriPrefix.uri,\n+    };\n+  }\n+\n+  buildSetConnection([key, _blob]: [\n+    string,\n+    MediaBlob\n+  ]): GoogleMultipartUploadConnection<\n+    AsyncCallerCallOptions,\n+    GoogleCloudStorageResponse,\n+    AuthOptions\n+  > {\n+    const params: GoogleCloudStorageUploadConnectionParams<AuthOptions> = {\n+      ...this.params,\n+      uri: key,\n+    };\n+    return new GoogleCloudStorageUploadConnection<AuthOptions>(\n+      params,\n+      this.caller,\n+      this.client\n+    );\n+  }\n+\n+  buildSetMetadata([key, blob]: [string, MediaBlob]): Record<string, unknown> {\n+    const uri = new GoogleCloudStorageUri(key);\n+    const ret: GoogleCloudStorageObject = {\n+      name: uri.path,\n+      metadata: blob.metadata,\n+      contentType: blob.mimetype,\n+    };\n+    return ret;\n+  }\n+\n+  buildGetMetadataConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    GoogleCloudStorageResponse,\n+    AuthOptions\n+  > {\n+    const params: GoogleCloudStorageDownloadConnectionParams<AuthOptions> = {\n+      uri: key,\n+      method: \"GET\",\n+      alt: undefined,\n+    };\n+    return new GoogleCloudStorageDownloadConnection<\n+      GoogleCloudStorageResponse,\n+      AuthOptions\n+    >(params, this.caller, this.client);\n+  }\n+\n+  buildGetDataConnection(\n+    key: string\n+  ): GoogleDownloadRawConnection<AsyncCallerCallOptions, AuthOptions> {\n+    const params: GoogleCloudStorageRawConnectionParams<AuthOptions> = {\n+      uri: key,\n+    };\n+    return new GoogleCloudStorageRawConnection<AuthOptions>(\n+      params,\n+      this.caller,\n+      this.client\n+    );\n+  }\n+\n+  buildDeleteConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    GoogleResponse,\n+    AuthOptions\n+  > {\n+    const params: GoogleCloudStorageDownloadConnectionParams<AuthOptions> = {\n+      uri: key,\n+      method: \"DELETE\",\n+      alt: undefined,\n+    };\n+    return new GoogleCloudStorageDownloadConnection<\n+      GoogleResponse,\n+      AuthOptions\n+    >(params, this.caller, this.client);\n+  }\n+}\n+\n+export type AIStudioFileState =\n+  | \"PROCESSING\"\n+  | \"ACTIVE\"\n+  | \"FAILED\"\n+  | \"STATE_UNSPECIFIED\";\n+\n+export type AIStudioFileVideoMetadata = {\n+  videoMetadata: {\n+    videoDuration: string; // Duration in seconds, possibly with fractional, ending in \"s\"\n+  };\n+};\n+\n+export type AIStudioFileMetadata = AIStudioFileVideoMetadata;\n+\n+export interface AIStudioFileObject {\n+  name?: string;\n+  displayName?: string;\n+  mimeType?: string;\n+  sizeBytes?: string; // int64 format\n+  createTime?: string; // timestamp format\n+  updateTime?: string; // timestamp format\n+  expirationTime?: string; // timestamp format\n+  sha256Hash?: string; // base64 encoded\n+  uri?: string;\n+  state?: AIStudioFileState;\n+  error?: {\n+    code: number;\n+    message: string;\n+    details: Record<string, unknown>[];\n+  };\n+  metadata?: AIStudioFileMetadata;\n+}\n+\n+export class AIStudioMediaBlob extends MediaBlob {\n+  _valueAsDate(value: string): Date {\n+    if (!value) {\n+      return new Date(0);\n+    }\n+    return new Date(value);\n+  }\n+\n+  _metadataFieldAsDate(field: string): Date {\n+    return this._valueAsDate(this.metadata?.[field]);\n+  }\n+\n+  get createDate(): Date {\n+    return this._metadataFieldAsDate(\"createTime\");\n+  }\n+\n+  get updateDate(): Date {\n+    return this._metadataFieldAsDate(\"updateTime\");\n+  }\n+\n+  get expirationDate(): Date {\n+    return this._metadataFieldAsDate(\"expirationTime\");\n+  }\n+\n+  get isExpired(): boolean {\n+    const now = new Date().toISOString();\n+    const exp = this.metadata?.expirationTime ?? now;\n+    return exp <= now;\n+  }\n+}\n+\n+export interface AIStudioFileGetResponse extends GoogleResponse {\n+  data: AIStudioFileObject;\n+}\n+\n+export interface AIStudioFileSaveResponse extends GoogleResponse {\n+  data: {\n+    file: AIStudioFileObject;\n+  };\n+}\n+\n+export interface AIStudioFileListResponse extends GoogleResponse {\n+  data: {\n+    files: AIStudioFileObject[];\n+    nextPageToken: string;\n+  };\n+}\n+\n+export type AIStudioFileResponse =\n+  | AIStudioFileGetResponse\n+  | AIStudioFileSaveResponse\n+  | AIStudioFileListResponse;\n+\n+export interface AIStudioFileConnectionParams {}\n+\n+export interface AIStudioFileUploadConnectionParams<AuthOptions>\n+  extends GoogleUploadConnectionParams<AuthOptions>,\n+    AIStudioFileConnectionParams {}\n+\n+export class AIStudioFileUploadConnection<\n+  AuthOptions\n+> extends GoogleMultipartUploadConnection<\n+  AsyncCallerCallOptions,\n+  AIStudioFileSaveResponse,\n+  AuthOptions\n+> {\n+  apiVersion = \"v1beta\";\n+\n+  async buildUrl(): Promise<string> {\n+    return `https://generativelanguage.googleapis.com/upload/${this.apiVersion}/files`;\n+  }\n+}\n+\n+export interface AIStudioFileDownloadConnectionParams<AuthOptions>\n+  extends AIStudioFileConnectionParams,\n+    GoogleConnectionParams<AuthOptions> {\n+  method: GoogleAbstractedClientOpsMethod;\n+  name: string;\n+}\n+\n+export class AIStudioFileDownloadConnection<\n+  ResponseType extends GoogleResponse,\n+  AuthOptions\n+> extends GoogleDownloadConnection<\n+  AsyncCallerCallOptions,\n+  ResponseType,\n+  AuthOptions\n+> {\n+  method: GoogleAbstractedClientOpsMethod;\n+\n+  name: string;\n+\n+  apiVersion = \"v1beta\";\n+\n+  constructor(\n+    fields: AIStudioFileDownloadConnectionParams<AuthOptions>,\n+    caller: AsyncCaller,\n+    client: GoogleAbstractedClient\n+  ) {\n+    super(fields, caller, client);\n+    this.method = fields.method;\n+    this.name = fields.name;\n+  }\n+\n+  buildMethod(): GoogleAbstractedClientOpsMethod {\n+    return this.method;\n+  }\n+\n+  async buildUrl(): Promise<string> {\n+    return `https://generativelanguage.googleapis.com/${this.apiVersion}/files/${this.name}`;\n+  }\n+}\n+\n+export interface BlobStoreAIStudioFileBaseParams<AuthOptions>\n+  extends BlobStoreGoogleParams<AuthOptions> {\n+  retryTime?: number;\n+}\n+\n+export abstract class BlobStoreAIStudioFileBase<\n+  AuthOptions\n+> extends BlobStoreGoogle<AIStudioFileResponse, AuthOptions> {\n+  params?: BlobStoreAIStudioFileBaseParams<AuthOptions>;\n+\n+  retryTime: number = 1000;\n+\n+  constructor(fields?: BlobStoreAIStudioFileBaseParams<AuthOptions>) {\n+    const params: BlobStoreAIStudioFileBaseParams<AuthOptions> = {\n+      defaultStoreOptions: {\n+        pathPrefix: \"https://generativelanguage.googleapis.com/v1beta/files/\",\n+        actionIfInvalid: \"removePath\",\n+      },\n+      ...fields,\n+    };\n+    super(params);\n+    this.params = params;\n+    this.retryTime = params?.retryTime ?? this.retryTime ?? 1000;\n+  }\n+\n+  _pathToName(path: string): string {\n+    return path.split(\"/\").pop() ?? path;\n+  }\n+\n+  abstract buildAbstractedClient(\n+    fields?: BlobStoreGoogleParams<AuthOptions>\n+  ): GoogleAbstractedClient;\n+\n+  buildApiKeyClient(apiKey: string): GoogleAbstractedClient {\n+    return new ApiKeyGoogleAuth(apiKey);\n+  }\n+\n+  buildApiKey(fields?: BlobStoreGoogleParams<AuthOptions>): string | undefined {\n+    return fields?.apiKey ?? getEnvironmentVariable(\"GOOGLE_API_KEY\");\n+  }\n+\n+  buildClient(\n+    fields?: BlobStoreGoogleParams<AuthOptions>\n+  ): GoogleAbstractedClient {\n+    const apiKey = this.buildApiKey(fields);\n+    if (apiKey) {\n+      return this.buildApiKeyClient(apiKey);\n+    } else {\n+      // TODO: Test that you can use OAuth to access\n+      return this.buildAbstractedClient(fields);\n+    }\n+  }\n+\n+  async _regetMetadata(key: string): Promise<AIStudioFileObject> {\n+    // Sleep for some time period\n+    // eslint-disable-next-line no-promise-executor-return\n+    await new Promise((resolve) => setTimeout(resolve, this.retryTime));\n+\n+    // Fetch the latest metadata\n+    return this._getMetadata(key);\n+  }\n+\n+  async _set([key, blob]: [\n+    string,\n+    MediaBlob\n+  ]): Promise<AIStudioFileSaveResponse> {\n+    const response = (await super._set([\n+      key,\n+      blob,\n+    ])) as AIStudioFileSaveResponse;\n+\n+    let file = response.data?.file ?? { state: \"FAILED\" };\n+    while (file.state === \"PROCESSING\" && file.uri && this.retryTime > 0) {\n+      file = await this._regetMetadata(file.uri);\n+    }\n+\n+    // The response should contain the name (and valid URI), so we need to\n+    // update the blob with this. We can't return a new blob, since mset()\n+    // doesn't return anything.\n+    /* eslint-disable no-param-reassign */\n+    blob.path = file.uri;\n+    blob.metadata = {\n+      ...blob.metadata,\n+      ...file,\n+    };\n+    /* eslint-enable no-param-reassign */\n+\n+    return response;\n+  }\n+\n+  buildSetConnection([_key, _blob]: [\n+    string,\n+    MediaBlob\n+  ]): GoogleMultipartUploadConnection<\n+    AsyncCallerCallOptions,\n+    AIStudioFileResponse,\n+    AuthOptions\n+  > {\n+    return new AIStudioFileUploadConnection(\n+      this.params,\n+      this.caller,\n+      this.client\n+    );\n+  }\n+\n+  buildSetMetadata([_key, _blob]: [string, MediaBlob]): Record<\n+    string,\n+    unknown\n+  > {\n+    return {};\n+  }\n+\n+  buildGetMetadataConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    AIStudioFileResponse,\n+    AuthOptions\n+  > {\n+    const params: AIStudioFileDownloadConnectionParams<AuthOptions> = {\n+      ...this.params,\n+      method: \"GET\",\n+      name: this._pathToName(key),\n+    };\n+    return new AIStudioFileDownloadConnection<\n+      AIStudioFileResponse,\n+      AuthOptions\n+    >(params, this.caller, this.client);\n+  }\n+\n+  buildGetDataConnection(\n+    _key: string\n+  ): GoogleDownloadRawConnection<AsyncCallerCallOptions, AuthOptions> {\n+    throw new Error(\"AI Studio File API does not provide data\");\n+  }\n+\n+  async _get(key: string): Promise<MediaBlob | undefined> {\n+    const metadata = await this._getMetadata(key);\n+    if (metadata) {\n+      const contentType =\n+        (metadata?.mimeType as string) ?? \"application/octet-stream\";\n+      // TODO - Get the actual data (and other metadata) from an optional backing store\n+      const data: MediaBlobData = {\n+        value: \"\",\n+        type: contentType,\n+      };\n+\n+      return new MediaBlob({\n+        path: key,\n+        data,\n+        metadata,\n+      });\n+    } else {\n+      return undefined;\n+    }\n+  }\n+\n+  buildDeleteConnection(\n+    key: string\n+  ): GoogleDownloadConnection<\n+    AsyncCallerCallOptions,\n+    AIStudioFileResponse,\n+    AuthOptions\n+  > {\n+    const params: AIStudioFileDownloadConnectionParams<AuthOptions> = {\n+      ...this.params,\n+      method: \"DELETE\",\n+      name: this._pathToName(key),\n+    };\n+    return new AIStudioFileDownloadConnection<\n+      AIStudioFileResponse,\n+      AuthOptions\n+    >(params, this.caller, this.client);\n+  }\n+}",
          "libs/langchain-google-common/src/experimental/utils/media_core.ts": "@@ -0,0 +1,669 @@\n+import { v1, v4 } from \"uuid\"; // FIXME - it is importing the wrong uuid, so v6 and v7 aren't implemented\n+import { BaseStore } from \"@langchain/core/stores\";\n+import { Serializable } from \"@langchain/core/load/serializable\";\n+\n+export type MediaBlobData = {\n+  value: string; // In Base64 encoding\n+  type: string; // The mime type and possibly encoding\n+};\n+\n+export interface MediaBlobParameters {\n+  data?: MediaBlobData;\n+\n+  metadata?: Record<string, unknown>;\n+\n+  path?: string;\n+}\n+\n+function bytesToString(dataArray: Uint8Array): string {\n+  // Need to handle the array in smaller chunks to deal with stack size limits\n+  let ret = \"\";\n+  const chunkSize = 102400;\n+  for (let i = 0; i < dataArray.length; i += chunkSize) {\n+    const chunk = dataArray.subarray(i, i + chunkSize);\n+    ret += String.fromCharCode(...chunk);\n+  }\n+\n+  return ret;\n+}\n+\n+/**\n+ * Represents a chunk of data that can be identified by the path where the\n+ * data is (or will be) located, along with optional metadata about the data.\n+ */\n+export class MediaBlob extends Serializable implements MediaBlobParameters {\n+  lc_serializable = true;\n+\n+  lc_namespace = [\n+    \"langchain\",\n+    \"google_common\",\n+    \"experimental\",\n+    \"utils\",\n+    \"media_core\",\n+  ];\n+\n+  data: MediaBlobData = {\n+    value: \"\",\n+    type: \"text/plain\",\n+  };\n+\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  metadata?: Record<string, any>;\n+\n+  path?: string;\n+\n+  constructor(params: MediaBlobParameters) {\n+    super(params);\n+\n+    this.data = params.data ?? this.data;\n+    this.metadata = params.metadata;\n+    this.path = params.path;\n+  }\n+\n+  get size(): number {\n+    return this.asBytes.length;\n+  }\n+\n+  get dataType(): string {\n+    return this.data?.type ?? \"\";\n+  }\n+\n+  get encoding(): string {\n+    const charsetEquals = this.dataType.indexOf(\"charset=\");\n+    return charsetEquals === -1\n+      ? \"utf-8\"\n+      : this.dataType.substring(charsetEquals + 8);\n+  }\n+\n+  get mimetype(): string {\n+    const semicolon = this.dataType.indexOf(\";\");\n+    return semicolon === -1\n+      ? this.dataType\n+      : this.dataType.substring(0, semicolon);\n+  }\n+\n+  get asBytes(): Uint8Array {\n+    if (!this.data) {\n+      return Uint8Array.from([]);\n+    }\n+    const binString = atob(this.data?.value);\n+    const ret = new Uint8Array(binString.length);\n+    for (let co = 0; co < binString.length; co += 1) {\n+      ret[co] = binString.charCodeAt(co);\n+    }\n+    return ret;\n+  }\n+\n+  async asString(): Promise<string> {\n+    return bytesToString(this.asBytes);\n+  }\n+\n+  async asBase64(): Promise<string> {\n+    return this.data?.value ?? \"\";\n+  }\n+\n+  async asDataUrl(): Promise<string> {\n+    return `data:${this.mimetype};base64,${await this.asBase64()}`;\n+  }\n+\n+  async asUri(): Promise<string> {\n+    return this.path ?? (await this.asDataUrl());\n+  }\n+\n+  async encode(): Promise<{ encoded: string; encoding: string }> {\n+    const dataUrl = await this.asDataUrl();\n+    const comma = dataUrl.indexOf(\",\");\n+    const encoded = dataUrl.substring(comma + 1);\n+    const encoding: string = dataUrl.indexOf(\"base64\") > -1 ? \"base64\" : \"8bit\";\n+    return {\n+      encoded,\n+      encoding,\n+    };\n+  }\n+\n+  static fromDataUrl(url: string): MediaBlob {\n+    if (!url.startsWith(\"data:\")) {\n+      throw new Error(\"Not a data: URL\");\n+    }\n+    const colon = url.indexOf(\":\");\n+    const semicolon = url.indexOf(\";\");\n+    const mimeType = url.substring(colon + 1, semicolon);\n+\n+    const comma = url.indexOf(\",\");\n+    const base64Data = url.substring(comma + 1);\n+\n+    const data: MediaBlobData = {\n+      type: mimeType,\n+      value: base64Data,\n+    };\n+\n+    return new MediaBlob({\n+      data,\n+      path: url,\n+    });\n+  }\n+\n+  static async fromBlob(\n+    blob: Blob,\n+    other?: Omit<MediaBlobParameters, \"data\">\n+  ): Promise<MediaBlob> {\n+    const valueBuffer = await blob.arrayBuffer();\n+    const valueArray = new Uint8Array(valueBuffer);\n+    const valueStr = bytesToString(valueArray);\n+    const value = btoa(valueStr);\n+\n+    return new MediaBlob({\n+      ...other,\n+      data: {\n+        value,\n+        type: blob.type,\n+      },\n+    });\n+  }\n+}\n+\n+export type ActionIfInvalidAction =\n+  | \"ignore\"\n+  | \"prefixPath\"\n+  | \"prefixUuid1\"\n+  | \"prefixUuid4\"\n+  | \"prefixUuid6\"\n+  | \"prefixUuid7\"\n+  | \"removePath\";\n+\n+export interface BlobStoreStoreOptions {\n+  /**\n+   * If the path is missing or invalid in the blob, how should we create\n+   * a new path?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an emtpy string: Reject the blob\n+   * - \"ignore\": Attempt to store it anyway (but this may fail)\n+   * - \"prefixPath\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from the URL. The original path is stored in the metadata\n+   * - \"prefixUuid\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from a generated UUID. The original path is stored\n+   *   in the metadata\n+   */\n+  actionIfInvalid?: ActionIfInvalidAction;\n+\n+  /**\n+   * The expected prefix for URIs that are stored.\n+   * This may be used to test if a MediaBlob is valid and used to create a new\n+   * path if \"prefixPath\" or \"prefixUuid\" is set for actionIfInvalid.\n+   */\n+  pathPrefix?: string;\n+}\n+\n+export type ActionIfBlobMissingAction = \"emptyBlob\";\n+\n+export interface BlobStoreFetchOptions {\n+  /**\n+   * If the blob is not found when fetching, what should we do?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an empty string: return undefined\n+   * - \"emptyBlob\": return a new MediaBlob that has the path set, but nothing else.\n+   */\n+  actionIfBlobMissing?: ActionIfBlobMissingAction;\n+}\n+\n+export interface BlobStoreOptions {\n+  defaultStoreOptions?: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions?: BlobStoreFetchOptions;\n+}\n+\n+/**\n+ * A specialized Store that is designed to handle MediaBlobs and use the\n+ * key that is included in the blob to determine exactly how it is stored.\n+ *\n+ * The full details of a MediaBlob may be changed when it is stored.\n+ * For example, it may get additional or different Metadata. This should be\n+ * what is returned when the store() method is called.\n+ *\n+ * Although BlobStore extends BaseStore, not all of the methods from\n+ * BaseStore may be implemented (or even possible). Those that are not\n+ * implemented should be documented and throw an Error if called.\n+ */\n+export abstract class BlobStore extends BaseStore<string, MediaBlob> {\n+  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n+\n+  defaultStoreOptions: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions: BlobStoreFetchOptions;\n+\n+  constructor(opts?: BlobStoreOptions) {\n+    super(opts);\n+    this.defaultStoreOptions = opts?.defaultStoreOptions ?? {};\n+    this.defaultFetchOptions = opts?.defaultFetchOptions ?? {};\n+  }\n+\n+  protected async _realKey(key: string | MediaBlob): Promise<string> {\n+    return typeof key === \"string\" ? key : await key.asUri();\n+  }\n+\n+  /**\n+   * Is the path supported by this BlobStore?\n+   *\n+   * Although this is async, this is expected to be a relatively fast operation\n+   * (ie - you shouldn't make network calls).\n+   *\n+   * @param path The path to check\n+   * @param opts Any options (if needed) that may be used to determine if it is valid\n+   * @return If the path is supported\n+   */\n+  hasValidPath(\n+    path: string | undefined,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<boolean> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const isPrefixed = typeof path !== \"undefined\" && path.startsWith(prefix);\n+    return Promise.resolve(isPrefixed);\n+  }\n+\n+  protected _blobPathSuffix(blob: MediaBlob): string {\n+    // Get the path currently set and make sure we treat it as a string\n+    const blobPath = `${blob.path}`;\n+\n+    // Advance past the first set of /\n+    let pathStart = blobPath.indexOf(\"/\") + 1;\n+    while (blobPath.charAt(pathStart) === \"/\") {\n+      pathStart += 1;\n+    }\n+\n+    // We will use the rest as the path for a replacement\n+    return blobPath.substring(pathStart);\n+  }\n+\n+  protected async _newBlob(\n+    oldBlob: MediaBlob,\n+    newPath?: string\n+  ): Promise<MediaBlob> {\n+    const oldPath = oldBlob.path;\n+    const metadata = oldBlob?.metadata ?? {};\n+    metadata.langchainOldPath = oldPath;\n+    const newBlob = new MediaBlob({\n+      ...oldBlob,\n+      metadata,\n+    });\n+    if (newPath) {\n+      newBlob.path = newPath;\n+    } else if (newBlob.path) {\n+      delete newBlob.path;\n+    }\n+    return newBlob;\n+  }\n+\n+  protected async _validBlobPrefixPath(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = this._blobPathSuffix(blob);\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected _validBlobPrefixUuidFunction(\n+    name: ActionIfInvalidAction | string\n+  ): string {\n+    switch (name) {\n+      case \"prefixUuid1\":\n+        return v1();\n+      case \"prefixUuid4\":\n+        return v4();\n+      // case \"prefixUuid6\": return v6();\n+      // case \"prefixUuid7\": return v7();\n+      default:\n+        throw new Error(`Unknown uuid function: ${name}`);\n+    }\n+  }\n+\n+  protected async _validBlobPrefixUuid(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = this._validBlobPrefixUuidFunction(\n+      opts?.actionIfInvalid ?? \"prefixUuid4\"\n+    );\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected async _validBlobRemovePath(\n+    blob: MediaBlob,\n+    _opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    return this._newBlob(blob, undefined);\n+  }\n+\n+  /**\n+   * Based on the blob and options, return a blob that has a valid path\n+   * that can be saved.\n+   * @param blob\n+   * @param opts\n+   */\n+  protected async _validStoreBlob(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob | undefined> {\n+    if (await this.hasValidPath(blob.path, opts)) {\n+      return blob;\n+    }\n+    switch (opts?.actionIfInvalid) {\n+      case \"ignore\":\n+        return blob;\n+      case \"prefixPath\":\n+        return this._validBlobPrefixPath(blob, opts);\n+      case \"prefixUuid1\":\n+      case \"prefixUuid4\":\n+      case \"prefixUuid6\":\n+      case \"prefixUuid7\":\n+        return this._validBlobPrefixUuid(blob, opts);\n+      case \"removePath\":\n+        return this._validBlobRemovePath(blob, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async store(\n+    blob: MediaBlob,\n+    opts: BlobStoreStoreOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreStoreOptions = {\n+      ...this.defaultStoreOptions,\n+      ...opts,\n+    };\n+    const validBlob = await this._validStoreBlob(blob, allOpts);\n+    if (typeof validBlob !== \"undefined\") {\n+      const validKey = await validBlob.asUri();\n+      await this.mset([[validKey, validBlob]]);\n+      const savedKey = await validBlob.asUri();\n+      return await this.fetch(savedKey);\n+    }\n+    return undefined;\n+  }\n+\n+  protected async _missingFetchBlobEmpty(\n+    path: string,\n+    _opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob> {\n+    return new MediaBlob({ path });\n+  }\n+\n+  protected async _missingFetchBlob(\n+    path: string,\n+    opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob | undefined> {\n+    switch (opts?.actionIfBlobMissing) {\n+      case \"emptyBlob\":\n+        return this._missingFetchBlobEmpty(path, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async fetch(\n+    key: string | MediaBlob,\n+    opts: BlobStoreFetchOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreFetchOptions = {\n+      ...this.defaultFetchOptions,\n+      ...opts,\n+    };\n+    const realKey = await this._realKey(key);\n+    const ret = await this.mget([realKey]);\n+    return ret?.[0] ?? (await this._missingFetchBlob(realKey, allOpts));\n+  }\n+}\n+\n+export interface BackedBlobStoreOptions extends BlobStoreOptions {\n+  backingStore: BaseStore<string, MediaBlob>;\n+}\n+\n+export class BackedBlobStore extends BlobStore {\n+  backingStore: BaseStore<string, MediaBlob>;\n+\n+  constructor(opts: BackedBlobStoreOptions) {\n+    super(opts);\n+    this.backingStore = opts.backingStore;\n+  }\n+\n+  mdelete(keys: string[]): Promise<void> {\n+    return this.backingStore.mdelete(keys);\n+  }\n+\n+  mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    return this.backingStore.mget(keys);\n+  }\n+\n+  mset(keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    return this.backingStore.mset(keyValuePairs);\n+  }\n+\n+  yieldKeys(prefix: string | undefined): AsyncGenerator<string> {\n+    return this.backingStore.yieldKeys(prefix);\n+  }\n+}\n+\n+export interface ReadThroughBlobStoreOptions extends BlobStoreOptions {\n+  baseStore: BlobStore;\n+  backingStore: BlobStore;\n+}\n+\n+export class ReadThroughBlobStore extends BlobStore {\n+  baseStore: BlobStore;\n+\n+  backingStore: BlobStore;\n+\n+  constructor(opts: ReadThroughBlobStoreOptions) {\n+    super(opts);\n+    this.baseStore = opts.baseStore;\n+    this.backingStore = opts.backingStore;\n+  }\n+\n+  async store(\n+    blob: MediaBlob,\n+    opts: BlobStoreStoreOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const originalUri = await blob.asUri();\n+    const newBlob = await this.backingStore.store(blob, opts);\n+    if (newBlob) {\n+      await this.baseStore.mset([[originalUri, newBlob]]);\n+    }\n+    return newBlob;\n+  }\n+\n+  mdelete(keys: string[]): Promise<void> {\n+    return this.baseStore.mdelete(keys);\n+  }\n+\n+  mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    return this.baseStore.mget(keys);\n+  }\n+\n+  mset(_keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    throw new Error(\"Do not call ReadThroughBlobStore.mset directly\");\n+  }\n+\n+  yieldKeys(prefix: string | undefined): AsyncGenerator<string> {\n+    return this.baseStore.yieldKeys(prefix);\n+  }\n+}\n+\n+export class SimpleWebBlobStore extends BlobStore {\n+  _notImplementedException() {\n+    throw new Error(\"Not implemented for SimpleWebBlobStore\");\n+  }\n+\n+  async hasValidPath(\n+    path: string | undefined,\n+    _opts?: BlobStoreStoreOptions\n+  ): Promise<boolean> {\n+    return (\n+      (await super.hasValidPath(path, { pathPrefix: \"https://\" })) ||\n+      (await super.hasValidPath(path, { pathPrefix: \"http://\" }))\n+    );\n+  }\n+\n+  async _fetch(url: string): Promise<MediaBlob | undefined> {\n+    const ret = new MediaBlob({\n+      path: url,\n+    });\n+    const metadata: Record<string, unknown> = {};\n+    const fetchOptions = {\n+      method: \"GET\",\n+    };\n+    const res = await fetch(url, fetchOptions);\n+    metadata.status = res.status;\n+\n+    const headers: Record<string, string> = {};\n+    for (const [key, value] of res.headers.entries()) {\n+      headers[key] = value;\n+    }\n+    metadata.headers = headers;\n+\n+    metadata.ok = res.ok;\n+    if (res.ok) {\n+      const resMediaBlob = await MediaBlob.fromBlob(await res.blob());\n+      ret.data = resMediaBlob.data;\n+    }\n+\n+    ret.metadata = metadata;\n+    return ret;\n+  }\n+\n+  async mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    const blobMap = keys.map(this._fetch);\n+    return await Promise.all(blobMap);\n+  }\n+\n+  async mdelete(_keys: string[]): Promise<void> {\n+    this._notImplementedException();\n+  }\n+\n+  async mset(_keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    this._notImplementedException();\n+  }\n+\n+  async *yieldKeys(_prefix: string | undefined): AsyncGenerator<string> {\n+    this._notImplementedException();\n+    yield \"\";\n+  }\n+}\n+\n+/**\n+ * A blob \"store\" that works with data: URLs that will turn the URL into\n+ * a blob.\n+ */\n+export class DataBlobStore extends BlobStore {\n+  _notImplementedException() {\n+    throw new Error(\"Not implemented for DataBlobStore\");\n+  }\n+\n+  hasValidPath(path: string, _opts?: BlobStoreStoreOptions): Promise<boolean> {\n+    return super.hasValidPath(path, { pathPrefix: \"data:\" });\n+  }\n+\n+  _fetch(url: string): MediaBlob {\n+    return MediaBlob.fromDataUrl(url);\n+  }\n+\n+  async mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    const blobMap = keys.map(this._fetch);\n+    return blobMap;\n+  }\n+\n+  async mdelete(_keys: string[]): Promise<void> {\n+    this._notImplementedException();\n+  }\n+\n+  async mset(_keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    this._notImplementedException();\n+  }\n+\n+  async *yieldKeys(_prefix: string | undefined): AsyncGenerator<string> {\n+    this._notImplementedException();\n+    yield \"\";\n+  }\n+}\n+\n+export interface MediaManagerConfiguration {\n+  /**\n+   * A store that, given a common URI, returns the corresponding MediaBlob.\n+   * The returned MediaBlob may have a different URI.\n+   * In many cases, this will be a ReadThroughStore or something similar\n+   * that has a cached version of the MediaBlob, but also a way to get\n+   * a new (or refreshed) version.\n+   */\n+  store: BlobStore;\n+\n+  /**\n+   * BlobStores that can resolve a URL into the MediaBlob to save\n+   * in the canonical store. This list is evaluated in order.\n+   * If not provided, a default list (which involves a DataBlobStore\n+   * and a SimpleWebBlobStore) will be used.\n+   */\n+  resolvers?: BlobStore[];\n+}\n+\n+/**\n+ * Responsible for converting a URI (typically a web URL) into a MediaBlob.\n+ * Allows for aliasing / caching of the requested URI and what it resolves to.\n+ * This MediaBlob is expected to be usable to provide to an LLM, either\n+ * through the Base64 of the media or through a canonical URI that the LLM\n+ * supports.\n+ */\n+export class MediaManager {\n+  store: BlobStore;\n+\n+  resolvers: BlobStore[] | undefined;\n+\n+  constructor(config: MediaManagerConfiguration) {\n+    this.store = config.store;\n+    this.resolvers = config.resolvers;\n+  }\n+\n+  defaultResolvers(): BlobStore[] {\n+    return [new DataBlobStore({}), new SimpleWebBlobStore({})];\n+  }\n+\n+  async _isInvalid(blob: MediaBlob | undefined): Promise<boolean> {\n+    return typeof blob === \"undefined\";\n+  }\n+\n+  /**\n+   * Given the public URI, load what is at this URI and save it\n+   * in the store.\n+   * @param uri The URI to resolve using the resolver\n+   * @return A canonical MediaBlob for this URI\n+   */\n+  async _resolveAndSave(uri: string): Promise<MediaBlob | undefined> {\n+    let resolvedBlob: MediaBlob | undefined;\n+\n+    const resolvers = this.resolvers || this.defaultResolvers();\n+    for (let co = 0; co < resolvers.length; co += 1) {\n+      const resolver = resolvers[co];\n+      if (await resolver.hasValidPath(uri)) {\n+        resolvedBlob = await resolver.fetch(uri);\n+      }\n+    }\n+\n+    if (resolvedBlob) {\n+      return await this.store.store(resolvedBlob);\n+    } else {\n+      return new MediaBlob({});\n+    }\n+  }\n+\n+  async getMediaBlob(uri: string): Promise<MediaBlob | undefined> {\n+    const aliasBlob = await this.store.fetch(uri);\n+    const ret = (await this._isInvalid(aliasBlob))\n+      ? await this._resolveAndSave(uri)\n+      : (aliasBlob as MediaBlob);\n+    return ret;\n+  }\n+}",
          "libs/langchain-google-common/src/llms.ts": "@@ -21,13 +21,7 @@ import {\n   copyAIModelParams,\n   copyAndValidateModelParamsInto,\n } from \"./utils/common.js\";\n-import {\n-  chunkToString,\n-  messageContentToParts,\n-  safeResponseToBaseMessage,\n-  safeResponseToString,\n-  DefaultGeminiSafetyHandler,\n-} from \"./utils/gemini.js\";\n+import { DefaultGeminiSafetyHandler } from \"./utils/gemini.js\";\n import { ApiKeyGoogleAuth, GoogleAbstractedClient } from \"./auth.js\";\n import { ensureParams } from \"./utils/failed_handler.js\";\n import { ChatGoogleBase } from \"./chat_models.js\";\n@@ -39,11 +33,11 @@ class GoogleLLMConnection<AuthOptions> extends AbstractGoogleLLMConnection<\n   MessageContent,\n   AuthOptions\n > {\n-  formatContents(\n+  async formatContents(\n     input: MessageContent,\n     _parameters: GoogleAIModelParams\n-  ): GeminiContent[] {\n-    const parts = messageContentToParts(input);\n+  ): Promise<GeminiContent[]> {\n+    const parts = await this.api.messageContentToParts(input);\n     const contents: GeminiContent[] = [\n       {\n         role: \"user\", // Required by Vertex AI\n@@ -189,7 +183,10 @@ export abstract class GoogleBaseLLM<AuthOptions>\n   ): Promise<string> {\n     const parameters = copyAIModelParams(this, options);\n     const result = await this.connection.request(prompt, parameters, options);\n-    const ret = safeResponseToString(result, this.safetyHandler);\n+    const ret = this.connection.api.safeResponseToString(\n+      result,\n+      this.safetyHandler\n+    );\n     return ret;\n   }\n \n@@ -234,7 +231,7 @@ export abstract class GoogleBaseLLM<AuthOptions>\n     const proxyChat = this.createProxyChat();\n     try {\n       for await (const chunk of proxyChat._streamIterator(input, options)) {\n-        const stringValue = chunkToString(chunk);\n+        const stringValue = this.connection.api.chunkToString(chunk);\n         const generationChunk = new GenerationChunk({\n           text: stringValue,\n         });\n@@ -267,7 +264,10 @@ export abstract class GoogleBaseLLM<AuthOptions>\n       {},\n       options as BaseLanguageModelCallOptions\n     );\n-    const ret = safeResponseToBaseMessage(result, this.safetyHandler);\n+    const ret = this.connection.api.safeResponseToBaseMessage(\n+      result,\n+      this.safetyHandler\n+    );\n     return ret;\n   }\n ",
          "libs/langchain-google-common/src/tests/chat_models.test.ts": "@@ -9,6 +9,7 @@ import {\n   SystemMessage,\n   ToolMessage,\n } from \"@langchain/core/messages\";\n+import { InMemoryStore } from \"@langchain/core/stores\";\n \n import { z } from \"zod\";\n import { zodToJsonSchema } from \"zod-to-json-schema\";\n@@ -17,6 +18,12 @@ import { authOptions, MockClient, MockClientAuthInfo, mockId } from \"./mock.js\";\n import { GeminiTool, GoogleAIBaseLLMInput } from \"../types.js\";\n import { GoogleAbstractedClient } from \"../auth.js\";\n import { GoogleAISafetyError } from \"../utils/safety.js\";\n+import {\n+  BackedBlobStore,\n+  MediaBlob,\n+  MediaManager,\n+  ReadThroughBlobStore,\n+} from \"../experimental/utils/media_core.js\";\n import { removeAdditionalProperties } from \"../utils/zod_to_gemini_parameters.js\";\n \n class ChatGoogle extends ChatGoogleBase<MockClientAuthInfo> {\n@@ -502,10 +509,6 @@ describe(\"Mock ChatGoogle\", () => {\n     expect(caught).toEqual(true);\n   });\n \n-  /*\n-   * Images aren't supported (yet) by Gemini, but a one-round with\n-   * image should work ok.\n-   */\n   test(\"3. invoke - images\", async () => {\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     const record: Record<string, any> = {};\n@@ -517,7 +520,7 @@ describe(\"Mock ChatGoogle\", () => {\n     };\n     const model = new ChatGoogle({\n       authOptions,\n-      model: \"gemini-pro-vision\",\n+      model: \"gemini-1.5-flash\",\n     });\n \n     const message: MessageContentComplex[] = [\n@@ -552,6 +555,200 @@ describe(\"Mock ChatGoogle\", () => {\n     expect(result.content).toBe(\"A blue square.\");\n   });\n \n+  test(\"3. invoke - media - invalid\", async () => {\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    const record: Record<string, any> = {};\n+    const projectId = mockId();\n+    const authOptions: MockClientAuthInfo = {\n+      record,\n+      projectId,\n+      resultFile: \"chat-3-mock.json\",\n+    };\n+    const model = new ChatGoogle({\n+      authOptions,\n+      model: \"gemini-1.5-flash\",\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"mock://example.com/blue-box.png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    try {\n+      const result = await model.invoke(messages);\n+      expect(result).toBeUndefined();\n+    } catch (e) {\n+      expect((e as Error).message).toEqual(\"Invalid media content\");\n+    }\n+  });\n+\n+  test(\"3. invoke - media - no manager\", async () => {\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    const record: Record<string, any> = {};\n+    const projectId = mockId();\n+    const authOptions: MockClientAuthInfo = {\n+      record,\n+      projectId,\n+      resultFile: \"chat-3-mock.json\",\n+    };\n+    const model = new ChatGoogle({\n+      authOptions,\n+      model: \"gemini-1.5-flash\",\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"mock://example.com/blue-box.png\",\n+        mimeType: \"image/png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    const result = await model.invoke(messages);\n+\n+    console.log(JSON.stringify(record.opts, null, 1));\n+\n+    expect(record.opts).toHaveProperty(\"data\");\n+    expect(record.opts.data).toHaveProperty(\"contents\");\n+    expect(record.opts.data.contents).toHaveLength(1);\n+    expect(record.opts.data.contents[0]).toHaveProperty(\"parts\");\n+\n+    const parts = record?.opts?.data?.contents[0]?.parts;\n+    expect(parts).toHaveLength(2);\n+    expect(parts[0]).toHaveProperty(\"text\");\n+    expect(parts[1]).toHaveProperty(\"fileData\");\n+    expect(parts[1].fileData).toHaveProperty(\"mimeType\");\n+    expect(parts[1].fileData).toHaveProperty(\"fileUri\");\n+\n+    expect(result.content).toBe(\"A blue square.\");\n+  });\n+\n+  test(\"3. invoke - media - manager\", async () => {\n+    class MemStore extends InMemoryStore<MediaBlob> {\n+      get length() {\n+        return Object.keys(this.store).length;\n+      }\n+    }\n+\n+    const aliasMemory = new MemStore();\n+    const aliasStore = new BackedBlobStore({\n+      backingStore: aliasMemory,\n+      defaultFetchOptions: {\n+        actionIfBlobMissing: undefined,\n+      },\n+    });\n+    const canonicalMemory = new MemStore();\n+    const canonicalStore = new BackedBlobStore({\n+      backingStore: canonicalMemory,\n+      defaultStoreOptions: {\n+        pathPrefix: \"canonical://store/\",\n+        actionIfInvalid: \"prefixPath\",\n+      },\n+      defaultFetchOptions: {\n+        actionIfBlobMissing: undefined,\n+      },\n+    });\n+    const blobStore = new ReadThroughBlobStore({\n+      baseStore: aliasStore,\n+      backingStore: canonicalStore,\n+    });\n+    const resolverMemory = new MemStore();\n+    const resolver = new BackedBlobStore({\n+      backingStore: resolverMemory,\n+      defaultFetchOptions: {\n+        actionIfBlobMissing: \"emptyBlob\",\n+      },\n+    });\n+    const mediaManager = new MediaManager({\n+      store: blobStore,\n+      resolvers: [resolver],\n+    });\n+\n+    async function store(path: string, text: string): Promise<void> {\n+      const type = path.endsWith(\".png\") ? \"image/png\" : \"text/plain\";\n+      const blob = new MediaBlob({\n+        data: {\n+          value: text,\n+          type,\n+        },\n+        path,\n+      });\n+      await resolver.store(blob);\n+    }\n+    await store(\"resolve://host/foo\", \"fooing\");\n+    await store(\"resolve://host2/bar/baz\", \"barbazing\");\n+    await store(\"resolve://host/foo/blue-box.png\", \"png\");\n+\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    const record: Record<string, any> = {};\n+    const projectId = mockId();\n+    const authOptions: MockClientAuthInfo = {\n+      record,\n+      projectId,\n+      resultFile: \"chat-3-mock.json\",\n+    };\n+    const model = new ChatGoogle({\n+      authOptions,\n+      model: \"gemini-1.5-flash\",\n+      mediaManager,\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"resolve://host/foo/blue-box.png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    const result = await model.invoke(messages);\n+\n+    console.log(JSON.stringify(record.opts, null, 1));\n+\n+    expect(record.opts).toHaveProperty(\"data\");\n+    expect(record.opts.data).toHaveProperty(\"contents\");\n+    expect(record.opts.data.contents).toHaveLength(1);\n+    expect(record.opts.data.contents[0]).toHaveProperty(\"parts\");\n+\n+    const parts = record?.opts?.data?.contents[0]?.parts;\n+    expect(parts).toHaveLength(2);\n+    expect(parts[0]).toHaveProperty(\"text\");\n+    expect(parts[1]).toHaveProperty(\"fileData\");\n+    expect(parts[1].fileData).toHaveProperty(\"mimeType\");\n+    expect(parts[1].fileData.mimeType).toEqual(\"image/png\");\n+    expect(parts[1].fileData).toHaveProperty(\"fileUri\");\n+    expect(parts[1].fileData.fileUri).toEqual(\n+      \"canonical://store/host/foo/blue-box.png\"\n+    );\n+\n+    expect(result.content).toBe(\"A blue square.\");\n+  });\n+\n   test(\"4. Functions Bind - Gemini format request\", async () => {\n     // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     const record: Record<string, any> = {};",
          "libs/langchain-google-common/src/tests/utils.test.ts": "@@ -1,85 +1,419 @@\n /* eslint-disable @typescript-eslint/no-explicit-any */\n-import { expect, test } from \"@jest/globals\";\n+import { beforeEach, expect, test } from \"@jest/globals\";\n+import { InMemoryStore } from \"@langchain/core/stores\";\n+import { SerializedConstructor } from \"@langchain/core/load/serializable\";\n+import { load } from \"@langchain/core/load\";\n import { z } from \"zod\";\n import { zodToGeminiParameters } from \"../utils/zod_to_gemini_parameters.js\";\n+import {\n+  BackedBlobStore,\n+  BlobStore,\n+  MediaBlob,\n+  MediaManager,\n+  ReadThroughBlobStore,\n+  SimpleWebBlobStore,\n+} from \"../experimental/utils/media_core.js\";\n import { ReadableJsonStream } from \"../utils/stream.js\";\n \n-test(\"zodToGeminiParameters can convert zod schema to gemini schema\", () => {\n-  const zodSchema = z\n-    .object({\n-      operation: z\n-        .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n-        .describe(\"The type of operation to execute\"),\n-      number1: z.number().describe(\"The first number to operate on.\"),\n-      number2: z.number().describe(\"The second number to operate on.\"),\n-      childObject: z.object({}),\n-    })\n-    .describe(\"A simple calculator tool\");\n-\n-  const convertedSchema = zodToGeminiParameters(zodSchema);\n-\n-  expect(convertedSchema.type).toBe(\"object\");\n-  expect(convertedSchema.description).toBe(\"A simple calculator tool\");\n-  expect((convertedSchema as any).additionalProperties).toBeUndefined();\n-  expect(convertedSchema.properties).toEqual({\n-    operation: {\n-      type: \"string\",\n-      enum: [\"add\", \"subtract\", \"multiply\", \"divide\"],\n-      description: \"The type of operation to execute\",\n-    },\n-    number1: {\n-      type: \"number\",\n-      description: \"The first number to operate on.\",\n-    },\n-    number2: {\n-      type: \"number\",\n-      description: \"The second number to operate on.\",\n-    },\n-    childObject: {\n-      type: \"object\",\n-      properties: {},\n-    },\n+describe(\"zodToGeminiParameters\", () => {\n+  test(\"can convert zod schema to gemini schema\", () => {\n+    const zodSchema = z\n+      .object({\n+        operation: z\n+          .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n+          .describe(\"The type of operation to execute\"),\n+        number1: z.number().describe(\"The first number to operate on.\"),\n+        number2: z.number().describe(\"The second number to operate on.\"),\n+        childObject: z.object({}),\n+      })\n+      .describe(\"A simple calculator tool\");\n+\n+    const convertedSchema = zodToGeminiParameters(zodSchema);\n+\n+    expect(convertedSchema.type).toBe(\"object\");\n+    expect(convertedSchema.description).toBe(\"A simple calculator tool\");\n+    expect((convertedSchema as any).additionalProperties).toBeUndefined();\n+    expect(convertedSchema.properties).toEqual({\n+      operation: {\n+        type: \"string\",\n+        enum: [\"add\", \"subtract\", \"multiply\", \"divide\"],\n+        description: \"The type of operation to execute\",\n+      },\n+      number1: {\n+        type: \"number\",\n+        description: \"The first number to operate on.\",\n+      },\n+      number2: {\n+        type: \"number\",\n+        description: \"The second number to operate on.\",\n+      },\n+      childObject: {\n+        type: \"object\",\n+        properties: {},\n+      },\n+    });\n+    expect(convertedSchema.required).toEqual([\n+      \"operation\",\n+      \"number1\",\n+      \"number2\",\n+      \"childObject\",\n+    ]);\n+  });\n+\n+  test(\"removes additional properties from arrays\", () => {\n+    const zodSchema = z\n+      .object({\n+        people: z\n+          .object({\n+            name: z.string().describe(\"The name of a person\"),\n+          })\n+          .array()\n+          .describe(\"person elements\"),\n+      })\n+      .describe(\"A list of people\");\n+\n+    const convertedSchema = zodToGeminiParameters(zodSchema);\n+    expect(convertedSchema.type).toBe(\"object\");\n+    expect(convertedSchema.description).toBe(\"A list of people\");\n+    expect((convertedSchema as any).additionalProperties).toBeUndefined();\n+\n+    const peopleSchema = convertedSchema?.properties?.people;\n+    expect(peopleSchema).not.toBeUndefined();\n+\n+    if (peopleSchema !== undefined) {\n+      expect(peopleSchema.type).toBe(\"array\");\n+      expect((peopleSchema as any).additionalProperties).toBeUndefined();\n+      expect(peopleSchema.description).toBe(\"person elements\");\n+    }\n+\n+    const arrayItemsSchema = peopleSchema?.items;\n+    expect(arrayItemsSchema).not.toBeUndefined();\n+    if (arrayItemsSchema !== undefined) {\n+      expect(arrayItemsSchema.type).toBe(\"object\");\n+      expect((arrayItemsSchema as any).additionalProperties).toBeUndefined();\n+    }\n   });\n-  expect(convertedSchema.required).toEqual([\n-    \"operation\",\n-    \"number1\",\n-    \"number2\",\n-    \"childObject\",\n-  ]);\n });\n \n-test(\"zodToGeminiParameters removes additional properties from arrays\", () => {\n-  const zodSchema = z\n-    .object({\n-      people: z\n-        .object({\n-          name: z.string().describe(\"The name of a person\"),\n-        })\n-        .array()\n-        .describe(\"person elements\"),\n-    })\n-    .describe(\"A list of people\");\n-\n-  const convertedSchema = zodToGeminiParameters(zodSchema);\n-  expect(convertedSchema.type).toBe(\"object\");\n-  expect(convertedSchema.description).toBe(\"A list of people\");\n-  expect((convertedSchema as any).additionalProperties).toBeUndefined();\n-\n-  const peopleSchema = convertedSchema?.properties?.people;\n-  expect(peopleSchema).not.toBeUndefined();\n-\n-  if (peopleSchema !== undefined) {\n-    expect(peopleSchema.type).toBe(\"array\");\n-    expect((peopleSchema as any).additionalProperties).toBeUndefined();\n-    expect(peopleSchema.description).toBe(\"person elements\");\n-  }\n-\n-  const arrayItemsSchema = peopleSchema?.items;\n-  expect(arrayItemsSchema).not.toBeUndefined();\n-  if (arrayItemsSchema !== undefined) {\n-    expect(arrayItemsSchema.type).toBe(\"object\");\n-    expect((arrayItemsSchema as any).additionalProperties).toBeUndefined();\n-  }\n+describe(\"media core\", () => {\n+  test(\"MediaBlob plain\", async () => {\n+    const blob = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+    const mblob = await MediaBlob.fromBlob(blob);\n+    expect(mblob.dataType).toEqual(\"text/plain\");\n+    expect(mblob.mimetype).toEqual(\"text/plain\");\n+    expect(mblob.encoding).toEqual(\"utf-8\");\n+    expect(await mblob.asString()).toEqual(\"This is a test\");\n+  });\n+\n+  test(\"MediaBlob charset\", async () => {\n+    const blob = new Blob([\"This is a test\"], {\n+      type: \"text/plain; charset=US-ASCII\",\n+    });\n+    const mblob = await MediaBlob.fromBlob(blob);\n+    expect(mblob.dataType).toEqual(\"text/plain; charset=us-ascii\");\n+    expect(mblob.mimetype).toEqual(\"text/plain\");\n+    expect(mblob.encoding).toEqual(\"us-ascii\");\n+    expect(await mblob.asString()).toEqual(\"This is a test\");\n+  });\n+\n+  test(\"MediaBlob fromDataUrl\", async () => {\n+    const blobData = \"This is a test\";\n+    const blobMimeType = \"text/plain\";\n+    const blobDataType = `${blobMimeType}; charset=US-ASCII`;\n+    const blob = new Blob([blobData], {\n+      type: blobDataType,\n+    });\n+    const mblob = await MediaBlob.fromBlob(blob);\n+    const dataUrl = await mblob.asDataUrl();\n+    const dblob = MediaBlob.fromDataUrl(dataUrl);\n+    expect(await dblob.asString()).toEqual(blobData);\n+    expect(dblob.mimetype).toEqual(blobMimeType);\n+  });\n+\n+  test(\"MediaBlob serialize\", async () => {\n+    const blob = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+    const mblob = await MediaBlob.fromBlob(blob);\n+    console.log(\"serialize mblob\", mblob);\n+    const serialized = mblob.toJSON() as SerializedConstructor;\n+    console.log(\"serialized\", serialized);\n+    expect(serialized.kwargs).toHaveProperty(\"data\");\n+    expect(serialized.kwargs.data.value).toEqual(\"VGhpcyBpcyBhIHRlc3Q=\");\n+  });\n+\n+  test(\"MediaBlob deserialize\", async () => {\n+    const serialized: SerializedConstructor = {\n+      lc: 1,\n+      type: \"constructor\",\n+      id: [\n+        \"langchain\",\n+        \"google_common\",\n+        \"experimental\",\n+        \"utils\",\n+        \"media_core\",\n+        \"MediaBlob\",\n+      ],\n+      kwargs: {\n+        data: {\n+          value: \"VGhpcyBpcyBhIHRlc3Q=\",\n+          type: \"text/plain\",\n+        },\n+      },\n+    };\n+    const mblob: MediaBlob = await load(JSON.stringify(serialized), {\n+      importMap: {\n+        google_common__experimental__utils__media_core: await import(\n+          \"../experimental/utils/media_core.js\"\n+        ),\n+      },\n+    });\n+    console.log(\"deserialize mblob\", mblob);\n+    expect(mblob.dataType).toEqual(\"text/plain\");\n+    expect(await mblob.asString()).toEqual(\"This is a test\");\n+  });\n+\n+  test(\"SimpleWebBlobStore fetch\", async () => {\n+    const webStore = new SimpleWebBlobStore();\n+    const exampleBlob = await webStore.fetch(\"http://example.com/\");\n+    console.log(exampleBlob);\n+    expect(exampleBlob?.mimetype).toEqual(\"text/html\");\n+    expect(exampleBlob?.encoding).toEqual(\"utf-8\");\n+    expect(exampleBlob?.size).toBeGreaterThan(0);\n+    expect(exampleBlob?.metadata).toBeDefined();\n+    expect(exampleBlob?.metadata?.ok).toBeTruthy();\n+    expect(exampleBlob?.metadata?.status).toEqual(200);\n+  });\n+\n+  describe(\"BackedBlobStore\", () => {\n+    test(\"simple\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+      });\n+      const data = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+      const path = \"simple://foo\";\n+      const blob = await MediaBlob.fromBlob(data, { path });\n+      const storedBlob = await store.store(blob);\n+      expect(storedBlob).toBeDefined();\n+      const fetchedBlob = await store.fetch(path);\n+      expect(fetchedBlob).toBeDefined();\n+    });\n+\n+    test(\"missing undefined\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+      });\n+      const path = \"simple://foo\";\n+      const fetchedBlob = await store.fetch(path);\n+      expect(fetchedBlob).toBeUndefined();\n+    });\n+\n+    test(\"missing emptyBlob defaultConfig\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultFetchOptions: {\n+          actionIfBlobMissing: \"emptyBlob\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const fetchedBlob = await store.fetch(path);\n+      expect(fetchedBlob).toBeDefined();\n+      expect(fetchedBlob?.size).toEqual(0);\n+      expect(fetchedBlob?.path).toEqual(path);\n+    });\n+\n+    test(\"missing undefined fetch\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultFetchOptions: {\n+          actionIfBlobMissing: \"emptyBlob\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const fetchedBlob = await store.fetch(path, {\n+        actionIfBlobMissing: undefined,\n+      });\n+      expect(fetchedBlob).toBeUndefined();\n+    });\n+\n+    test(\"invalid undefined\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultStoreOptions: {\n+          pathPrefix: \"example://bar/\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const data = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+      const blob = await MediaBlob.fromBlob(data, { path });\n+      const storedBlob = await store.store(blob);\n+      expect(storedBlob).toBeUndefined();\n+    });\n+\n+    test(\"invalid ignore\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultStoreOptions: {\n+          actionIfInvalid: \"ignore\",\n+          pathPrefix: \"example://bar/\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const data = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+      const blob = await MediaBlob.fromBlob(data, { path });\n+      const storedBlob = await store.store(blob);\n+      expect(storedBlob).toBeDefined();\n+      expect(storedBlob?.path).toEqual(path);\n+      expect(storedBlob?.metadata).toBeUndefined();\n+    });\n+\n+    test(\"invalid prefixPath\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultStoreOptions: {\n+          actionIfInvalid: \"prefixPath\",\n+          pathPrefix: \"example://bar/\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const data = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+      const blob = await MediaBlob.fromBlob(data, { path });\n+      const storedBlob = await store.store(blob);\n+      expect(storedBlob?.path).toEqual(\"example://bar/foo\");\n+      expect(await storedBlob?.asString()).toEqual(\"This is a test\");\n+      expect(storedBlob?.metadata?.langchainOldPath).toEqual(path);\n+    });\n+\n+    test(\"invalid prefixUuid\", async () => {\n+      const backingStore = new InMemoryStore<MediaBlob>();\n+      const store = new BackedBlobStore({\n+        backingStore,\n+        defaultStoreOptions: {\n+          actionIfInvalid: \"prefixUuid4\",\n+          pathPrefix: \"example://bar/\",\n+        },\n+      });\n+      const path = \"simple://foo\";\n+      const data = new Blob([\"This is a test\"], { type: \"text/plain\" });\n+      const metadata = {\n+        alpha: \"one\",\n+        bravo: \"two\",\n+      };\n+      const blob = await MediaBlob.fromBlob(data, { path, metadata });\n+      const storedBlob = await store.store(blob);\n+      expect(storedBlob?.path).toMatch(\n+        /example:\\/\\/bar\\/[a-f0-9]{8}(-[a-f0-9]{4}){3}-[a-f0-9]{12}$/i\n+      );\n+      expect(storedBlob?.size).toEqual(14);\n+      expect(await storedBlob?.asString()).toEqual(\"This is a test\");\n+      expect(storedBlob?.metadata?.alpha).toEqual(\"one\");\n+      expect(storedBlob?.metadata?.langchainOldPath).toEqual(path);\n+    });\n+  });\n+\n+  describe(\"MediaManager\", () => {\n+    class MemStore extends InMemoryStore<MediaBlob> {\n+      get length() {\n+        return Object.keys(this.store).length;\n+      }\n+    }\n+\n+    let mediaManager: MediaManager;\n+    let aliasMemory: MemStore;\n+    let canonicalMemory: MemStore;\n+    let resolverMemory: MemStore;\n+\n+    async function store(\n+      blobStore: BlobStore,\n+      path: string,\n+      text: string\n+    ): Promise<void> {\n+      const data = new Blob([text], { type: \"text/plain\" });\n+      const blob = await MediaBlob.fromBlob(data, { path });\n+      await blobStore.store(blob);\n+    }\n+\n+    beforeEach(async () => {\n+      aliasMemory = new MemStore();\n+      const aliasStore = new BackedBlobStore({\n+        backingStore: aliasMemory,\n+        defaultFetchOptions: {\n+          actionIfBlobMissing: undefined,\n+        },\n+      });\n+      canonicalMemory = new MemStore();\n+      const canonicalStore = new BackedBlobStore({\n+        backingStore: canonicalMemory,\n+        defaultStoreOptions: {\n+          pathPrefix: \"canonical://store/\",\n+          actionIfInvalid: \"prefixPath\",\n+        },\n+        defaultFetchOptions: {\n+          actionIfBlobMissing: undefined,\n+        },\n+      });\n+      resolverMemory = new MemStore();\n+      const resolver = new BackedBlobStore({\n+        backingStore: resolverMemory,\n+        defaultFetchOptions: {\n+          actionIfBlobMissing: \"emptyBlob\",\n+        },\n+      });\n+      const mediaStore = new ReadThroughBlobStore({\n+        baseStore: aliasStore,\n+        backingStore: canonicalStore,\n+      });\n+      mediaManager = new MediaManager({\n+        store: mediaStore,\n+        resolvers: [resolver],\n+      });\n+      await store(resolver, \"resolve://host/foo\", \"fooing\");\n+      await store(resolver, \"resolve://host2/bar/baz\", \"barbazing\");\n+    });\n+\n+    test(\"environment\", async () => {\n+      expect(resolverMemory.length).toEqual(2);\n+      const fooBlob = await mediaManager.resolvers?.[0]?.fetch(\n+        \"resolve://host/foo\"\n+      );\n+      expect(await fooBlob?.asString()).toEqual(\"fooing\");\n+    });\n+\n+    test(\"simple\", async () => {\n+      const uri = \"resolve://host/foo\";\n+      const curi = \"canonical://store/host/foo\";\n+      const blob = await mediaManager.getMediaBlob(uri);\n+      expect(await blob?.asString()).toEqual(\"fooing\");\n+      expect(blob?.path).toEqual(curi);\n+\n+      // In the alias store,\n+      // we should be able to fetch it by the resolve uri, but the\n+      // path in the blob itself should be the canonical uri\n+      expect(aliasMemory.length).toEqual(1);\n+      const mediaStore: ReadThroughBlobStore =\n+        mediaManager.store as ReadThroughBlobStore;\n+      const aliasBlob = await mediaStore.baseStore.fetch(uri);\n+      expect(aliasBlob).toBeDefined();\n+      expect(aliasBlob?.path).toEqual(curi);\n+      expect(await aliasBlob?.asString()).toEqual(\"fooing\");\n+\n+      // For the canonical store,\n+      // fetching it by the resolve uri should fail\n+      // but fetching it by the canonical uri should succeed\n+      expect(canonicalMemory.length).toEqual(1);\n+      const canonicalBlobU = await mediaStore.backingStore.fetch(uri);\n+      expect(canonicalBlobU).toBeUndefined();\n+      const canonicalBlob = await mediaStore.backingStore.fetch(curi);\n+      expect(canonicalBlob).toBeDefined();\n+      expect(canonicalBlob?.path).toEqual(curi);\n+      expect(await canonicalBlob?.asString()).toEqual(\"fooing\");\n+    });\n+  });\n });\n \n function toUint8Array(data: string): Uint8Array {",
          "libs/langchain-google-common/src/types.ts": "@@ -4,6 +4,7 @@ import type {\n   BindToolsInput,\n } from \"@langchain/core/language_models/chat_models\";\n import type { JsonStream } from \"./utils/stream.js\";\n+import { MediaManager } from \"./experimental/utils/media_core.js\";\n \n /**\n  * Parameters needed to setup the client connection.\n@@ -147,7 +148,8 @@ export interface GoogleAIBaseLLMInput<AuthOptions>\n   extends BaseLLMParams,\n     GoogleConnectionParams<AuthOptions>,\n     GoogleAIModelParams,\n-    GoogleAISafetyParams {}\n+    GoogleAISafetyParams,\n+    GeminiAPIConfig {}\n \n export interface GoogleAIBaseLanguageModelCallOptions\n   extends BaseChatModelCallOptions,\n@@ -172,6 +174,10 @@ export interface GoogleResponse {\n   data: any;\n }\n \n+export interface GoogleRawResponse extends GoogleResponse {\n+  data: Blob;\n+}\n+\n export interface GeminiPartText {\n   text: string;\n }\n@@ -183,7 +189,6 @@ export interface GeminiPartInlineData {\n   };\n }\n \n-// Vertex AI only\n export interface GeminiPartFileData {\n   fileData: {\n     mimeType: string;\n@@ -342,3 +347,7 @@ export interface GeminiJsonSchemaDirty extends GeminiJsonSchema {\n   properties?: Record<string, GeminiJsonSchemaDirty>;\n   additionalProperties?: boolean;\n }\n+\n+export interface GeminiAPIConfig {\n+  mediaManager?: MediaManager;\n+}",
          "libs/langchain-google-common/src/utils/gemini.ts": "@@ -19,7 +19,6 @@ import {\n   ChatGeneration,\n   ChatGenerationChunk,\n   ChatResult,\n-  Generation,\n } from \"@langchain/core/outputs\";\n import { ToolCallChunk } from \"@langchain/core/messages/tool\";\n import type {\n@@ -34,8 +33,32 @@ import type {\n   GenerateContentResponseData,\n   GoogleAISafetyHandler,\n   GeminiPartFunctionCall,\n+  GeminiAPIConfig,\n } from \"../types.js\";\n import { GoogleAISafetyError } from \"./safety.js\";\n+import { MediaBlob } from \"../experimental/utils/media_core.js\";\n+\n+export interface FunctionCall {\n+  name: string;\n+  arguments: string;\n+}\n+\n+export interface ToolCall {\n+  id: string;\n+  type: \"function\";\n+  function: FunctionCall;\n+}\n+\n+export interface FunctionCallRaw {\n+  name: string;\n+  arguments: object;\n+}\n+\n+export interface ToolCallRaw {\n+  id: string;\n+  type: \"function\";\n+  function: FunctionCallRaw;\n+}\n \n const extractMimeType = (\n   str: string\n@@ -49,671 +72,686 @@ const extractMimeType = (\n   return null;\n };\n \n-function messageContentText(\n-  content: MessageContentText\n-): GeminiPartText | null {\n-  if (content?.text && content?.text.length > 0) {\n-    return {\n-      text: content.text,\n-    };\n-  } else {\n-    return null;\n+export function getGeminiAPI(config?: GeminiAPIConfig) {\n+  function messageContentText(\n+    content: MessageContentText\n+  ): GeminiPartText | null {\n+    if (content?.text && content?.text.length > 0) {\n+      return {\n+        text: content.text,\n+      };\n+    } else {\n+      return null;\n+    }\n   }\n-}\n \n-function messageContentImageUrl(\n-  content: MessageContentImageUrl\n-): GeminiPartInlineData | GeminiPartFileData {\n-  const url: string =\n-    typeof content.image_url === \"string\"\n-      ? content.image_url\n-      : content.image_url.url;\n-  if (!url) {\n-    throw new Error(\"Missing Image URL\");\n-  }\n+  function messageContentImageUrl(\n+    content: MessageContentImageUrl\n+  ): GeminiPartInlineData | GeminiPartFileData {\n+    const url: string =\n+      typeof content.image_url === \"string\"\n+        ? content.image_url\n+        : content.image_url.url;\n+    if (!url) {\n+      throw new Error(\"Missing Image URL\");\n+    }\n \n-  const mineTypeAndData = extractMimeType(url);\n-  if (mineTypeAndData) {\n-    return {\n-      inlineData: mineTypeAndData,\n-    };\n-  } else {\n-    // FIXME - need some way to get mime type\n-    return {\n-      fileData: {\n-        mimeType: \"image/png\",\n-        fileUri: url,\n-      },\n-    };\n+    const mineTypeAndData = extractMimeType(url);\n+    if (mineTypeAndData) {\n+      return {\n+        inlineData: mineTypeAndData,\n+      };\n+    } else {\n+      // FIXME - need some way to get mime type\n+      return {\n+        fileData: {\n+          mimeType: \"image/png\",\n+          fileUri: url,\n+        },\n+      };\n+    }\n   }\n-}\n \n-function messageContentMedia(\n-  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-  content: Record<string, any>\n-): GeminiPartInlineData | GeminiPartFileData {\n-  if (\"mimeType\" in content && \"data\" in content) {\n-    return {\n-      inlineData: {\n-        mimeType: content.mimeType,\n-        data: content.data,\n-      },\n-    };\n-  } else if (\"mimeType\" in content && \"fileUri\" in content) {\n+  async function blobToFileData(blob: MediaBlob): Promise<GeminiPartFileData> {\n     return {\n       fileData: {\n-        mimeType: content.mimeType,\n-        fileUri: content.fileUri,\n+        fileUri: blob.path!,\n+        mimeType: blob.mimetype,\n       },\n     };\n   }\n \n-  throw new Error(\"Invalid media content\");\n-}\n+  async function fileUriContentToBlob(\n+    uri: string\n+  ): Promise<MediaBlob | undefined> {\n+    return config?.mediaManager?.getMediaBlob(uri);\n+  }\n \n-export function messageContentToParts(content: MessageContent): GeminiPart[] {\n-  // Convert a string to a text type MessageContent if needed\n-  const messageContent: MessageContent =\n-    typeof content === \"string\"\n-      ? [\n-          {\n-            type: \"text\",\n-            text: content,\n-          },\n-        ]\n-      : content;\n-\n-  // eslint-disable-next-line array-callback-return\n-  const parts: GeminiPart[] = messageContent\n-    .map((content) => {\n-      switch (content.type) {\n-        case \"text\":\n-          if (\"text\" in content) {\n-            return messageContentText(content as MessageContentText);\n-          }\n-          break;\n-        case \"image_url\":\n-          if (\"image_url\" in content) {\n-            // Type guard for MessageContentImageUrl\n-            return messageContentImageUrl(content as MessageContentImageUrl);\n-          }\n-          break;\n-        case \"media\":\n-          return messageContentMedia(content);\n-        default:\n-          throw new Error(\n-            `Unsupported type received while converting message to message parts`\n-          );\n-      }\n-      throw new Error(\n-        `Cannot coerce \"${content.type}\" message part into a string.`\n-      );\n-    })\n-    .reduce((acc: GeminiPart[], val: GeminiPart | null | undefined) => {\n-      if (val) {\n-        return [...acc, val];\n-      } else {\n-        return acc;\n+  async function messageContentMedia(\n+    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    content: Record<string, any>\n+  ): Promise<GeminiPartInlineData | GeminiPartFileData> {\n+    if (\"mimeType\" in content && \"data\" in content) {\n+      return {\n+        inlineData: {\n+          mimeType: content.mimeType,\n+          data: content.data,\n+        },\n+      };\n+    } else if (\"mimeType\" in content && \"fileUri\" in content) {\n+      return {\n+        fileData: {\n+          mimeType: content.mimeType,\n+          fileUri: content.fileUri,\n+        },\n+      };\n+    } else {\n+      const uri = content.fileUri;\n+      const blob = await fileUriContentToBlob(uri);\n+      if (blob) {\n+        return await blobToFileData(blob);\n       }\n-    }, []);\n-\n-  return parts;\n-}\n+    }\n \n-function messageToolCallsToParts(toolCalls: ToolCall[]): GeminiPart[] {\n-  if (!toolCalls || toolCalls.length === 0) {\n-    return [];\n+    throw new Error(\"Invalid media content\");\n   }\n \n-  return toolCalls.map((tool: ToolCall) => {\n-    let args = {};\n-    if (tool?.function?.arguments) {\n-      const argStr = tool.function.arguments;\n-      args = JSON.parse(argStr);\n+  async function messageContentComplexToPart(\n+    content: MessageContentComplex\n+  ): Promise<GeminiPart | null> {\n+    switch (content.type) {\n+      case \"text\":\n+        if (\"text\" in content) {\n+          return messageContentText(content as MessageContentText);\n+        }\n+        break;\n+      case \"image_url\":\n+        if (\"image_url\" in content) {\n+          // Type guard for MessageContentImageUrl\n+          return messageContentImageUrl(content as MessageContentImageUrl);\n+        }\n+        break;\n+      case \"media\":\n+        return await messageContentMedia(content);\n+      default:\n+        throw new Error(\n+          `Unsupported type received while converting message to message parts`\n+        );\n     }\n-    return {\n-      functionCall: {\n-        name: tool.function.name,\n-        args,\n-      },\n-    };\n-  });\n-}\n+    throw new Error(\n+      `Cannot coerce \"${content.type}\" message part into a string.`\n+    );\n+  }\n+\n+  async function messageContentComplexToParts(\n+    content: MessageContentComplex[]\n+  ): Promise<(GeminiPart | null)[]> {\n+    const contents = content.map(messageContentComplexToPart);\n+    return Promise.all(contents);\n+  }\n \n-function messageKwargsToParts(kwargs: Record<string, unknown>): GeminiPart[] {\n-  const ret: GeminiPart[] = [];\n+  async function messageContentToParts(\n+    content: MessageContent\n+  ): Promise<GeminiPart[]> {\n+    // Convert a string to a text type MessageContent if needed\n+    const messageContent: MessageContentComplex[] =\n+      typeof content === \"string\"\n+        ? [\n+            {\n+              type: \"text\",\n+              text: content,\n+            },\n+          ]\n+        : content;\n+\n+    // Get all of the parts, even those that don't correctly resolve\n+    const allParts = await messageContentComplexToParts(messageContent);\n+\n+    // Remove any invalid parts\n+    const parts: GeminiPart[] = allParts.reduce(\n+      (acc: GeminiPart[], val: GeminiPart | null | undefined) => {\n+        if (val) {\n+          return [...acc, val];\n+        } else {\n+          return acc;\n+        }\n+      },\n+      []\n+    );\n \n-  if (kwargs?.tool_calls) {\n-    ret.push(...messageToolCallsToParts(kwargs.tool_calls as ToolCall[]));\n+    return parts;\n   }\n \n-  return ret;\n-}\n+  function messageToolCallsToParts(toolCalls: ToolCall[]): GeminiPart[] {\n+    if (!toolCalls || toolCalls.length === 0) {\n+      return [];\n+    }\n \n-function roleMessageToContent(\n-  role: GeminiRole,\n-  message: BaseMessage\n-): GeminiContent[] {\n-  const contentParts: GeminiPart[] = messageContentToParts(message.content);\n-  let toolParts: GeminiPart[];\n-  if (isAIMessage(message) && !!message.tool_calls?.length) {\n-    toolParts = message.tool_calls.map(\n-      (toolCall): GeminiPart => ({\n+    return toolCalls.map((tool: ToolCall) => {\n+      let args = {};\n+      if (tool?.function?.arguments) {\n+        const argStr = tool.function.arguments;\n+        args = JSON.parse(argStr);\n+      }\n+      return {\n         functionCall: {\n-          name: toolCall.name,\n-          args: toolCall.args,\n+          name: tool.function.name,\n+          args,\n         },\n-      })\n-    );\n-  } else {\n-    toolParts = messageKwargsToParts(message.additional_kwargs);\n-  }\n-  const parts: GeminiPart[] = [...contentParts, ...toolParts];\n-  return [\n-    {\n-      role,\n-      parts,\n-    },\n-  ];\n-}\n+      };\n+    });\n+  }\n \n-function systemMessageToContent(\n-  message: SystemMessage,\n-  useSystemInstruction: boolean\n-): GeminiContent[] {\n-  return useSystemInstruction\n-    ? roleMessageToContent(\"system\", message)\n-    : [\n-        ...roleMessageToContent(\"user\", message),\n-        ...roleMessageToContent(\"model\", new AIMessage(\"Ok\")),\n-      ];\n-}\n+  function messageKwargsToParts(kwargs: Record<string, unknown>): GeminiPart[] {\n+    const ret: GeminiPart[] = [];\n+\n+    if (kwargs?.tool_calls) {\n+      ret.push(...messageToolCallsToParts(kwargs.tool_calls as ToolCall[]));\n+    }\n \n-function toolMessageToContent(\n-  message: ToolMessage,\n-  prevMessage: BaseMessage\n-): GeminiContent[] {\n-  const contentStr =\n-    typeof message.content === \"string\"\n-      ? message.content\n-      : message.content.reduce(\n-          (acc: string, content: MessageContentComplex) => {\n-            if (content.type === \"text\") {\n-              return acc + content.text;\n-            } else {\n-              return acc;\n-            }\n+    return ret;\n+  }\n+\n+  async function roleMessageToContent(\n+    role: GeminiRole,\n+    message: BaseMessage\n+  ): Promise<GeminiContent[]> {\n+    const contentParts: GeminiPart[] = await messageContentToParts(\n+      message.content\n+    );\n+    let toolParts: GeminiPart[];\n+    if (isAIMessage(message) && !!message.tool_calls?.length) {\n+      toolParts = message.tool_calls.map(\n+        (toolCall): GeminiPart => ({\n+          functionCall: {\n+            name: toolCall.name,\n+            args: toolCall.args,\n           },\n-          \"\"\n-        );\n-  // Hacky :(\n-  const responseName =\n-    (isAIMessage(prevMessage) && !!prevMessage.tool_calls?.length\n-      ? prevMessage.tool_calls[0].name\n-      : prevMessage.name) ?? message.tool_call_id;\n-  try {\n-    const content = JSON.parse(contentStr);\n+        })\n+      );\n+    } else {\n+      toolParts = messageKwargsToParts(message.additional_kwargs);\n+    }\n+    const parts: GeminiPart[] = [...contentParts, ...toolParts];\n     return [\n       {\n-        role: \"function\",\n-        parts: [\n-          {\n-            functionResponse: {\n-              name: responseName,\n-              response: { content },\n-            },\n-          },\n-        ],\n+        role,\n+        parts,\n       },\n     ];\n-  } catch (_) {\n-    return [\n-      {\n-        role: \"function\",\n-        parts: [\n-          {\n-            functionResponse: {\n-              name: responseName,\n-              response: { content: contentStr },\n+  }\n+\n+  async function systemMessageToContent(\n+    message: SystemMessage,\n+    useSystemInstruction: boolean\n+  ): Promise<GeminiContent[]> {\n+    return useSystemInstruction\n+      ? roleMessageToContent(\"system\", message)\n+      : [\n+          ...(await roleMessageToContent(\"user\", message)),\n+          ...(await roleMessageToContent(\"model\", new AIMessage(\"Ok\"))),\n+        ];\n+  }\n+\n+  function toolMessageToContent(\n+    message: ToolMessage,\n+    prevMessage: BaseMessage\n+  ): GeminiContent[] {\n+    const contentStr =\n+      typeof message.content === \"string\"\n+        ? message.content\n+        : message.content.reduce(\n+            (acc: string, content: MessageContentComplex) => {\n+              if (content.type === \"text\") {\n+                return acc + content.text;\n+              } else {\n+                return acc;\n+              }\n             },\n-          },\n-        ],\n-      },\n-    ];\n+            \"\"\n+          );\n+    // Hacky :(\n+    const responseName =\n+      (isAIMessage(prevMessage) && !!prevMessage.tool_calls?.length\n+        ? prevMessage.tool_calls[0].name\n+        : prevMessage.name) ?? message.tool_call_id;\n+    try {\n+      const content = JSON.parse(contentStr);\n+      return [\n+        {\n+          role: \"function\",\n+          parts: [\n+            {\n+              functionResponse: {\n+                name: responseName,\n+                response: { content },\n+              },\n+            },\n+          ],\n+        },\n+      ];\n+    } catch (_) {\n+      return [\n+        {\n+          role: \"function\",\n+          parts: [\n+            {\n+              functionResponse: {\n+                name: responseName,\n+                response: { content: contentStr },\n+              },\n+            },\n+          ],\n+        },\n+      ];\n+    }\n   }\n-}\n \n-export function baseMessageToContent(\n-  message: BaseMessage,\n-  prevMessage: BaseMessage | undefined,\n-  useSystemInstruction: boolean\n-): GeminiContent[] {\n-  const type = message._getType();\n-  switch (type) {\n-    case \"system\":\n-      return systemMessageToContent(\n-        message as SystemMessage,\n-        useSystemInstruction\n-      );\n-    case \"human\":\n-      return roleMessageToContent(\"user\", message);\n-    case \"ai\":\n-      return roleMessageToContent(\"model\", message);\n-    case \"tool\":\n-      if (!prevMessage) {\n-        throw new Error(\n-          \"Tool messages cannot be the first message passed to the model.\"\n+  async function baseMessageToContent(\n+    message: BaseMessage,\n+    prevMessage: BaseMessage | undefined,\n+    useSystemInstruction: boolean\n+  ): Promise<GeminiContent[]> {\n+    const type = message._getType();\n+    switch (type) {\n+      case \"system\":\n+        return systemMessageToContent(\n+          message as SystemMessage,\n+          useSystemInstruction\n         );\n-      }\n-      return toolMessageToContent(message as ToolMessage, prevMessage);\n-    default:\n-      console.log(`Unsupported message type: ${type}`);\n-      return [];\n+      case \"human\":\n+        return roleMessageToContent(\"user\", message);\n+      case \"ai\":\n+        return roleMessageToContent(\"model\", message);\n+      case \"tool\":\n+        if (!prevMessage) {\n+          throw new Error(\n+            \"Tool messages cannot be the first message passed to the model.\"\n+          );\n+        }\n+        return toolMessageToContent(message as ToolMessage, prevMessage);\n+      default:\n+        console.log(`Unsupported message type: ${type}`);\n+        return [];\n+    }\n   }\n-}\n \n-function textPartToMessageContent(part: GeminiPartText): MessageContentText {\n-  return {\n-    type: \"text\",\n-    text: part.text,\n-  };\n-}\n+  function textPartToMessageContent(part: GeminiPartText): MessageContentText {\n+    return {\n+      type: \"text\",\n+      text: part.text,\n+    };\n+  }\n \n-function inlineDataPartToMessageContent(\n-  part: GeminiPartInlineData\n-): MessageContentImageUrl {\n-  return {\n-    type: \"image_url\",\n-    image_url: `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`,\n-  };\n-}\n+  function inlineDataPartToMessageContent(\n+    part: GeminiPartInlineData\n+  ): MessageContentImageUrl {\n+    return {\n+      type: \"image_url\",\n+      image_url: `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`,\n+    };\n+  }\n \n-function fileDataPartToMessageContent(\n-  part: GeminiPartFileData\n-): MessageContentImageUrl {\n-  return {\n-    type: \"image_url\",\n-    image_url: part.fileData.fileUri,\n-  };\n-}\n+  function fileDataPartToMessageContent(\n+    part: GeminiPartFileData\n+  ): MessageContentImageUrl {\n+    return {\n+      type: \"image_url\",\n+      image_url: part.fileData.fileUri,\n+    };\n+  }\n \n-export function partsToMessageContent(parts: GeminiPart[]): MessageContent {\n-  return parts\n-    .map((part) => {\n-      if (part === undefined || part === null) {\n-        return null;\n-      } else if (\"text\" in part) {\n-        return textPartToMessageContent(part);\n-      } else if (\"inlineData\" in part) {\n-        return inlineDataPartToMessageContent(part);\n-      } else if (\"fileData\" in part) {\n-        return fileDataPartToMessageContent(part);\n-      } else {\n-        return null;\n-      }\n-    })\n-    .reduce((acc, content) => {\n-      if (content) {\n-        acc.push(content);\n-      }\n-      return acc;\n-    }, [] as MessageContentComplex[]);\n-}\n+  function partsToMessageContent(parts: GeminiPart[]): MessageContent {\n+    return parts\n+      .map((part) => {\n+        if (part === undefined || part === null) {\n+          return null;\n+        } else if (\"text\" in part) {\n+          return textPartToMessageContent(part);\n+        } else if (\"inlineData\" in part) {\n+          return inlineDataPartToMessageContent(part);\n+        } else if (\"fileData\" in part) {\n+          return fileDataPartToMessageContent(part);\n+        } else {\n+          return null;\n+        }\n+      })\n+      .reduce((acc, content) => {\n+        if (content) {\n+          acc.push(content);\n+        }\n+        return acc;\n+      }, [] as MessageContentComplex[]);\n+  }\n \n-interface FunctionCall {\n-  name: string;\n-  arguments: string;\n-}\n+  function toolRawToTool(raw: ToolCallRaw): ToolCall {\n+    return {\n+      id: raw.id,\n+      type: raw.type,\n+      function: {\n+        name: raw.function.name,\n+        arguments: JSON.stringify(raw.function.arguments),\n+      },\n+    };\n+  }\n \n-interface ToolCall {\n-  id: string;\n-  type: \"function\";\n-  function: FunctionCall;\n-}\n+  function functionCallPartToToolRaw(\n+    part: GeminiPartFunctionCall\n+  ): ToolCallRaw {\n+    return {\n+      id: uuidv4().replace(/-/g, \"\"),\n+      type: \"function\",\n+      function: {\n+        name: part.functionCall.name,\n+        arguments: part.functionCall.args ?? {},\n+      },\n+    };\n+  }\n \n-interface FunctionCallRaw {\n-  name: string;\n-  arguments: object;\n-}\n+  function partsToToolsRaw(parts: GeminiPart[]): ToolCallRaw[] {\n+    return parts\n+      .map((part: GeminiPart) => {\n+        if (part === undefined || part === null) {\n+          return null;\n+        } else if (\"functionCall\" in part) {\n+          return functionCallPartToToolRaw(part);\n+        } else {\n+          return null;\n+        }\n+      })\n+      .reduce((acc, content) => {\n+        if (content) {\n+          acc.push(content);\n+        }\n+        return acc;\n+      }, [] as ToolCallRaw[]);\n+  }\n \n-interface ToolCallRaw {\n-  id: string;\n-  type: \"function\";\n-  function: FunctionCallRaw;\n-}\n+  function toolsRawToTools(raws: ToolCallRaw[]): ToolCall[] {\n+    return raws.map((raw) => toolRawToTool(raw));\n+  }\n \n-function toolRawToTool(raw: ToolCallRaw): ToolCall {\n-  return {\n-    id: raw.id,\n-    type: raw.type,\n-    function: {\n-      name: raw.function.name,\n-      arguments: JSON.stringify(raw.function.arguments),\n-    },\n-  };\n-}\n+  function responseToGenerateContentResponseData(\n+    response: GoogleLLMResponse\n+  ): GenerateContentResponseData {\n+    if (\"nextChunk\" in response.data) {\n+      throw new Error(\"Cannot convert Stream to GenerateContentResponseData\");\n+    } else if (Array.isArray(response.data)) {\n+      // Collapse the array of response data as if it was a single one\n+      return response.data.reduce(\n+        (\n+          acc: GenerateContentResponseData,\n+          val: GenerateContentResponseData\n+        ): GenerateContentResponseData => {\n+          // Add all the parts\n+          // FIXME: Handle other candidates?\n+          const valParts = val?.candidates?.[0]?.content?.parts ?? [];\n+          acc.candidates[0].content.parts.push(...valParts);\n+\n+          // FIXME: Merge promptFeedback and safety settings\n+          acc.promptFeedback = val.promptFeedback;\n+          return acc;\n+        }\n+      );\n+    } else {\n+      return response.data as GenerateContentResponseData;\n+    }\n+  }\n \n-function functionCallPartToToolRaw(part: GeminiPartFunctionCall): ToolCallRaw {\n-  return {\n-    id: uuidv4().replace(/-/g, \"\"),\n-    type: \"function\",\n-    function: {\n-      name: part.functionCall.name,\n-      arguments: part.functionCall.args ?? {},\n-    },\n-  };\n-}\n+  function responseToParts(response: GoogleLLMResponse): GeminiPart[] {\n+    const responseData = responseToGenerateContentResponseData(response);\n+    const parts = responseData?.candidates?.[0]?.content?.parts ?? [];\n+    return parts;\n+  }\n \n-export function partsToToolsRaw(parts: GeminiPart[]): ToolCallRaw[] {\n-  return parts\n-    .map((part: GeminiPart) => {\n-      if (part === undefined || part === null) {\n-        return null;\n-      } else if (\"functionCall\" in part) {\n-        return functionCallPartToToolRaw(part);\n-      } else {\n-        return null;\n-      }\n-    })\n-    .reduce((acc, content) => {\n-      if (content) {\n-        acc.push(content);\n-      }\n-      return acc;\n-    }, [] as ToolCallRaw[]);\n-}\n+  function partToText(part: GeminiPart): string {\n+    return \"text\" in part ? part.text : \"\";\n+  }\n \n-export function toolsRawToTools(raws: ToolCallRaw[]): ToolCall[] {\n-  return raws.map((raw) => toolRawToTool(raw));\n-}\n+  function responseToString(response: GoogleLLMResponse): string {\n+    const parts = responseToParts(response);\n+    const ret: string = parts.reduce((acc, part) => {\n+      const val = partToText(part);\n+      return acc + val;\n+    }, \"\");\n+    return ret;\n+  }\n \n-export function responseToGenerateContentResponseData(\n-  response: GoogleLLMResponse\n-): GenerateContentResponseData {\n-  if (\"nextChunk\" in response.data) {\n-    throw new Error(\"Cannot convert Stream to GenerateContentResponseData\");\n-  } else if (Array.isArray(response.data)) {\n-    // Collapse the array of response data as if it was a single one\n-    return response.data.reduce(\n-      (\n-        acc: GenerateContentResponseData,\n-        val: GenerateContentResponseData\n-      ): GenerateContentResponseData => {\n-        // Add all the parts\n-        // FIXME: Handle other candidates?\n-        const valParts = val?.candidates?.[0]?.content?.parts ?? [];\n-        acc.candidates[0].content.parts.push(...valParts);\n-\n-        // FIXME: Merge promptFeedback and safety settings\n-        acc.promptFeedback = val.promptFeedback;\n-        return acc;\n+  function safeResponseTo<RetType>(\n+    response: GoogleLLMResponse,\n+    safetyHandler: GoogleAISafetyHandler,\n+    responseTo: (response: GoogleLLMResponse) => RetType\n+  ): RetType {\n+    try {\n+      const safeResponse = safetyHandler.handle(response);\n+      return responseTo(safeResponse);\n+    } catch (xx) {\n+      // eslint-disable-next-line no-instanceof/no-instanceof\n+      if (xx instanceof GoogleAISafetyError) {\n+        const ret = responseTo(xx.response);\n+        xx.reply = ret;\n       }\n-    );\n-  } else {\n-    return response.data as GenerateContentResponseData;\n+      throw xx;\n+    }\n   }\n-}\n \n-export function responseToParts(response: GoogleLLMResponse): GeminiPart[] {\n-  const responseData = responseToGenerateContentResponseData(response);\n-  const parts = responseData?.candidates?.[0]?.content?.parts ?? [];\n-  return parts;\n-}\n-\n-export function partToText(part: GeminiPart): string {\n-  return \"text\" in part ? part.text : \"\";\n-}\n-\n-export function responseToString(response: GoogleLLMResponse): string {\n-  const parts = responseToParts(response);\n-  const ret: string = parts.reduce((acc, part) => {\n-    const val = partToText(part);\n-    return acc + val;\n-  }, \"\");\n-  return ret;\n-}\n+  function safeResponseToString(\n+    response: GoogleLLMResponse,\n+    safetyHandler: GoogleAISafetyHandler\n+  ): string {\n+    return safeResponseTo(response, safetyHandler, responseToString);\n+  }\n \n-function safeResponseTo<RetType>(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler,\n-  responseTo: (response: GoogleLLMResponse) => RetType\n-): RetType {\n-  try {\n-    const safeResponse = safetyHandler.handle(response);\n-    return responseTo(safeResponse);\n-  } catch (xx) {\n-    // eslint-disable-next-line no-instanceof/no-instanceof\n-    if (xx instanceof GoogleAISafetyError) {\n-      const ret = responseTo(xx.response);\n-      xx.reply = ret;\n+  function responseToGenerationInfo(response: GoogleLLMResponse) {\n+    if (!Array.isArray(response.data)) {\n+      return {};\n     }\n-    throw xx;\n+    const data = response.data[0];\n+    return {\n+      usage_metadata: {\n+        prompt_token_count: data.usageMetadata?.promptTokenCount,\n+        candidates_token_count: data.usageMetadata?.candidatesTokenCount,\n+        total_token_count: data.usageMetadata?.totalTokenCount,\n+      },\n+      safety_ratings: data.candidates[0]?.safetyRatings?.map((rating) => ({\n+        category: rating.category,\n+        probability: rating.probability,\n+        probability_score: rating.probabilityScore,\n+        severity: rating.severity,\n+        severity_score: rating.severityScore,\n+      })),\n+      finish_reason: data.candidates[0]?.finishReason,\n+    };\n   }\n-}\n-\n-export function safeResponseToString(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler\n-): string {\n-  return safeResponseTo(response, safetyHandler, responseToString);\n-}\n \n-export function responseToGenerationInfo(response: GoogleLLMResponse) {\n-  if (!Array.isArray(response.data)) {\n-    return {};\n+  function responseToChatGeneration(\n+    response: GoogleLLMResponse\n+  ): ChatGenerationChunk {\n+    return new ChatGenerationChunk({\n+      text: responseToString(response),\n+      message: partToMessageChunk(responseToParts(response)[0]),\n+      generationInfo: responseToGenerationInfo(response),\n+    });\n   }\n-  const data = response.data[0];\n-  return {\n-    usage_metadata: {\n-      prompt_token_count: data.usageMetadata?.promptTokenCount,\n-      candidates_token_count: data.usageMetadata?.candidatesTokenCount,\n-      total_token_count: data.usageMetadata?.totalTokenCount,\n-    },\n-    safety_ratings: data.candidates[0]?.safetyRatings?.map((rating) => ({\n-      category: rating.category,\n-      probability: rating.probability,\n-      probability_score: rating.probabilityScore,\n-      severity: rating.severity,\n-      severity_score: rating.severityScore,\n-    })),\n-    finish_reason: data.candidates[0]?.finishReason,\n-  };\n-}\n-\n-export function responseToGeneration(response: GoogleLLMResponse): Generation {\n-  return {\n-    text: responseToString(response),\n-    generationInfo: responseToGenerationInfo(response),\n-  };\n-}\n-\n-export function safeResponseToGeneration(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler\n-): Generation {\n-  return safeResponseTo(response, safetyHandler, responseToGeneration);\n-}\n \n-export function responseToChatGeneration(\n-  response: GoogleLLMResponse\n-): ChatGenerationChunk {\n-  return new ChatGenerationChunk({\n-    text: responseToString(response),\n-    message: partToMessageChunk(responseToParts(response)[0]),\n-    generationInfo: responseToGenerationInfo(response),\n-  });\n-}\n-\n-export function safeResponseToChatGeneration(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler\n-): ChatGenerationChunk {\n-  return safeResponseTo(response, safetyHandler, responseToChatGeneration);\n-}\n+  function safeResponseToChatGeneration(\n+    response: GoogleLLMResponse,\n+    safetyHandler: GoogleAISafetyHandler\n+  ): ChatGenerationChunk {\n+    return safeResponseTo(response, safetyHandler, responseToChatGeneration);\n+  }\n \n-export function chunkToString(chunk: BaseMessageChunk): string {\n-  if (chunk === null) {\n-    return \"\";\n-  } else if (typeof chunk.content === \"string\") {\n-    return chunk.content;\n-  } else if (chunk.content.length === 0) {\n-    return \"\";\n-  } else if (chunk.content[0].type === \"text\") {\n-    return chunk.content[0].text;\n-  } else {\n-    throw new Error(`Unexpected chunk: ${chunk}`);\n+  function chunkToString(chunk: BaseMessageChunk): string {\n+    if (chunk === null) {\n+      return \"\";\n+    } else if (typeof chunk.content === \"string\") {\n+      return chunk.content;\n+    } else if (chunk.content.length === 0) {\n+      return \"\";\n+    } else if (chunk.content[0].type === \"text\") {\n+      return chunk.content[0].text;\n+    } else {\n+      throw new Error(`Unexpected chunk: ${chunk}`);\n+    }\n   }\n-}\n \n-export function partToMessageChunk(part: GeminiPart): BaseMessageChunk {\n-  const fields = partsToBaseMessageChunkFields([part]);\n-  if (typeof fields.content === \"string\") {\n+  function partToMessageChunk(part: GeminiPart): BaseMessageChunk {\n+    const fields = partsToBaseMessageChunkFields([part]);\n+    if (typeof fields.content === \"string\") {\n+      return new AIMessageChunk(fields);\n+    } else if (fields.content.every((item) => item.type === \"text\")) {\n+      const newContent = fields.content\n+        .map((item) => (\"text\" in item ? item.text : \"\"))\n+        .join(\"\");\n+      return new AIMessageChunk({\n+        ...fields,\n+        content: newContent,\n+      });\n+    }\n     return new AIMessageChunk(fields);\n-  } else if (fields.content.every((item) => item.type === \"text\")) {\n-    const newContent = fields.content\n-      .map((item) => (\"text\" in item ? item.text : \"\"))\n-      .join(\"\");\n-    return new AIMessageChunk({\n-      ...fields,\n-      content: newContent,\n-    });\n   }\n-  return new AIMessageChunk(fields);\n-}\n \n-export function partToChatGeneration(part: GeminiPart): ChatGeneration {\n-  const message = partToMessageChunk(part);\n-  const text = partToText(part);\n-  return new ChatGenerationChunk({\n-    text,\n-    message,\n-  });\n-}\n+  function partToChatGeneration(part: GeminiPart): ChatGeneration {\n+    const message = partToMessageChunk(part);\n+    const text = partToText(part);\n+    return new ChatGenerationChunk({\n+      text,\n+      message,\n+    });\n+  }\n \n-export function responseToChatGenerations(\n-  response: GoogleLLMResponse\n-): ChatGeneration[] {\n-  const parts = responseToParts(response);\n-  let ret = parts.map((part) => partToChatGeneration(part));\n-  if (ret.every((item) => typeof item.message.content === \"string\")) {\n-    const combinedContent = ret.map((item) => item.message.content).join(\"\");\n-    const combinedText = ret.map((item) => item.text).join(\"\");\n-    const toolCallChunks: ToolCallChunk[] | undefined = ret[\n-      ret.length - 1\n-    ]?.message.additional_kwargs?.tool_calls?.map((toolCall, i) => ({\n-      name: toolCall.function.name,\n-      args: toolCall.function.arguments,\n-      id: toolCall.id,\n-      index: i,\n-      type: \"tool_call_chunk\",\n-    }));\n-    let usageMetadata: UsageMetadata | undefined;\n-    if (\"usageMetadata\" in response.data) {\n-      usageMetadata = {\n-        input_tokens: response.data.usageMetadata.promptTokenCount as number,\n-        output_tokens: response.data.usageMetadata\n-          .candidatesTokenCount as number,\n-        total_tokens: response.data.usageMetadata.totalTokenCount as number,\n-      };\n-    }\n-    ret = [\n-      new ChatGenerationChunk({\n-        message: new AIMessageChunk({\n-          content: combinedContent,\n-          additional_kwargs: ret[ret.length - 1]?.message.additional_kwargs,\n-          tool_call_chunks: toolCallChunks,\n-          usage_metadata: usageMetadata,\n+  function responseToChatGenerations(\n+    response: GoogleLLMResponse\n+  ): ChatGeneration[] {\n+    const parts = responseToParts(response);\n+    let ret = parts.map((part) => partToChatGeneration(part));\n+    if (ret.every((item) => typeof item.message.content === \"string\")) {\n+      const combinedContent = ret.map((item) => item.message.content).join(\"\");\n+      const combinedText = ret.map((item) => item.text).join(\"\");\n+      const toolCallChunks: ToolCallChunk[] | undefined = ret[\n+        ret.length - 1\n+      ]?.message.additional_kwargs?.tool_calls?.map((toolCall, i) => ({\n+        name: toolCall.function.name,\n+        args: toolCall.function.arguments,\n+        id: toolCall.id,\n+        index: i,\n+        type: \"tool_call_chunk\",\n+      }));\n+      let usageMetadata: UsageMetadata | undefined;\n+      if (\"usageMetadata\" in response.data) {\n+        usageMetadata = {\n+          input_tokens: response.data.usageMetadata.promptTokenCount as number,\n+          output_tokens: response.data.usageMetadata\n+            .candidatesTokenCount as number,\n+          total_tokens: response.data.usageMetadata.totalTokenCount as number,\n+        };\n+      }\n+      ret = [\n+        new ChatGenerationChunk({\n+          message: new AIMessageChunk({\n+            content: combinedContent,\n+            additional_kwargs: ret[ret.length - 1]?.message.additional_kwargs,\n+            tool_call_chunks: toolCallChunks,\n+            usage_metadata: usageMetadata,\n+          }),\n+          text: combinedText,\n+          generationInfo: ret[ret.length - 1].generationInfo,\n         }),\n-        text: combinedText,\n-        generationInfo: ret[ret.length - 1].generationInfo,\n-      }),\n-    ];\n+      ];\n+    }\n+    return ret;\n   }\n-  return ret;\n-}\n-\n-export function responseToBaseMessageFields(\n-  response: GoogleLLMResponse\n-): BaseMessageFields {\n-  const parts = responseToParts(response);\n-  return partsToBaseMessageChunkFields(parts);\n-}\n \n-export function partsToBaseMessageChunkFields(\n-  parts: GeminiPart[]\n-): AIMessageChunkFields {\n-  const fields: AIMessageChunkFields = {\n-    content: partsToMessageContent(parts),\n-    tool_call_chunks: [],\n-    tool_calls: [],\n-    invalid_tool_calls: [],\n-  };\n+  function responseToBaseMessageFields(\n+    response: GoogleLLMResponse\n+  ): BaseMessageFields {\n+    const parts = responseToParts(response);\n+    return partsToBaseMessageChunkFields(parts);\n+  }\n \n-  const rawTools = partsToToolsRaw(parts);\n-  if (rawTools.length > 0) {\n-    const tools = toolsRawToTools(rawTools);\n-    for (const tool of tools) {\n-      fields.tool_call_chunks?.push({\n-        name: tool.function.name,\n-        args: tool.function.arguments,\n-        id: tool.id,\n-        type: \"tool_call_chunk\",\n-      });\n+  function partsToBaseMessageChunkFields(\n+    parts: GeminiPart[]\n+  ): AIMessageChunkFields {\n+    const fields: AIMessageChunkFields = {\n+      content: partsToMessageContent(parts),\n+      tool_call_chunks: [],\n+      tool_calls: [],\n+      invalid_tool_calls: [],\n+    };\n \n-      try {\n-        fields.tool_calls?.push({\n-          name: tool.function.name,\n-          args: JSON.parse(tool.function.arguments),\n-          id: tool.id,\n-          type: \"tool_call\",\n-        });\n-        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-      } catch (e: any) {\n-        fields.invalid_tool_calls?.push({\n+    const rawTools = partsToToolsRaw(parts);\n+    if (rawTools.length > 0) {\n+      const tools = toolsRawToTools(rawTools);\n+      for (const tool of tools) {\n+        fields.tool_call_chunks?.push({\n           name: tool.function.name,\n           args: tool.function.arguments,\n           id: tool.id,\n-          error: e.message,\n-          type: \"invalid_tool_call\",\n+          type: \"tool_call_chunk\",\n         });\n+\n+        try {\n+          fields.tool_calls?.push({\n+            name: tool.function.name,\n+            args: JSON.parse(tool.function.arguments),\n+            id: tool.id,\n+          });\n+          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+        } catch (e: any) {\n+          fields.invalid_tool_calls?.push({\n+            name: tool.function.name,\n+            args: tool.function.arguments,\n+            id: tool.id,\n+            error: e.message,\n+            type: \"invalid_tool_call\",\n+          });\n+        }\n       }\n+      fields.additional_kwargs = {\n+        tool_calls: tools,\n+      };\n     }\n-    fields.additional_kwargs = {\n-      tool_calls: tools,\n-    };\n+    return fields;\n   }\n-  return fields;\n-}\n \n-export function responseToBaseMessage(\n-  response: GoogleLLMResponse\n-): BaseMessage {\n-  const fields = responseToBaseMessageFields(response);\n-  return new AIMessage(fields);\n-}\n+  function responseToBaseMessage(response: GoogleLLMResponse): BaseMessage {\n+    const fields = responseToBaseMessageFields(response);\n+    return new AIMessage(fields);\n+  }\n \n-export function safeResponseToBaseMessage(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler\n-): BaseMessage {\n-  return safeResponseTo(response, safetyHandler, responseToBaseMessage);\n-}\n+  function safeResponseToBaseMessage(\n+    response: GoogleLLMResponse,\n+    safetyHandler: GoogleAISafetyHandler\n+  ): BaseMessage {\n+    return safeResponseTo(response, safetyHandler, responseToBaseMessage);\n+  }\n+\n+  function responseToChatResult(response: GoogleLLMResponse): ChatResult {\n+    const generations = responseToChatGenerations(response);\n+    return {\n+      generations,\n+      llmOutput: responseToGenerationInfo(response),\n+    };\n+  }\n+\n+  function safeResponseToChatResult(\n+    response: GoogleLLMResponse,\n+    safetyHandler: GoogleAISafetyHandler\n+  ): ChatResult {\n+    return safeResponseTo(response, safetyHandler, responseToChatResult);\n+  }\n \n-export function responseToChatResult(response: GoogleLLMResponse): ChatResult {\n-  const generations = responseToChatGenerations(response);\n   return {\n-    generations,\n-    llmOutput: responseToGenerationInfo(response),\n+    messageContentToParts,\n+    baseMessageToContent,\n+    safeResponseToString,\n+    safeResponseToChatGeneration,\n+    chunkToString,\n+    safeResponseToBaseMessage,\n+    safeResponseToChatResult,\n   };\n }\n \n-export function safeResponseToChatResult(\n-  response: GoogleLLMResponse,\n-  safetyHandler: GoogleAISafetyHandler\n-): ChatResult {\n-  return safeResponseTo(response, safetyHandler, responseToChatResult);\n-}\n-\n export function validateGeminiParams(params: GoogleAIModelParams): void {\n   if (params.maxOutputTokens && params.maxOutputTokens < 0) {\n     throw new Error(\"`maxOutputTokens` must be a positive integer\");",
          "libs/langchain-google-gauth/src/index.ts": "@@ -1,3 +1,5 @@\n export * from \"./chat_models.js\";\n export * from \"./llms.js\";\n export * from \"./embeddings.js\";\n+\n+export * from \"./media.js\";",
          "libs/langchain-google-gauth/src/media.ts": "@@ -0,0 +1,31 @@\n+import { GoogleAbstractedClient } from \"@langchain/google-common\";\n+import {\n+  BlobStoreGoogleCloudStorageBase,\n+  BlobStoreGoogleCloudStorageBaseParams,\n+  BlobStoreAIStudioFileBase,\n+  BlobStoreAIStudioFileBaseParams,\n+} from \"@langchain/google-common/experimental/media\";\n+import { GoogleAuthOptions } from \"google-auth-library\";\n+import { GAuthClient } from \"./auth.js\";\n+\n+export interface BlobStoreGoogleCloudStorageParams\n+  extends BlobStoreGoogleCloudStorageBaseParams<GoogleAuthOptions> {}\n+\n+export class BlobStoreGoogleCloudStorage extends BlobStoreGoogleCloudStorageBase<GoogleAuthOptions> {\n+  buildClient(\n+    fields?: BlobStoreGoogleCloudStorageParams\n+  ): GoogleAbstractedClient {\n+    return new GAuthClient(fields);\n+  }\n+}\n+\n+export interface BlobStoreAIStudioFileParams\n+  extends BlobStoreAIStudioFileBaseParams<GoogleAuthOptions> {}\n+\n+export class BlobStoreAIStudioFile extends BlobStoreAIStudioFileBase<GoogleAuthOptions> {\n+  buildAbstractedClient(\n+    fields?: BlobStoreAIStudioFileParams\n+  ): GoogleAbstractedClient {\n+    return new GAuthClient(fields);\n+  }\n+}",
          "libs/langchain-google-gauth/src/tests/chat_models.int.test.ts": "@@ -0,0 +1,315 @@\n+import { test } from \"@jest/globals\";\n+import { BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\n+import { ChatPromptValue } from \"@langchain/core/prompt_values\";\n+import {\n+  AIMessage,\n+  AIMessageChunk,\n+  BaseMessage,\n+  BaseMessageChunk,\n+  BaseMessageLike,\n+  HumanMessage,\n+  HumanMessageChunk,\n+  MessageContentComplex,\n+  SystemMessage,\n+  ToolMessage,\n+} from \"@langchain/core/messages\";\n+import {\n+  BackedBlobStore,\n+  MediaBlob,\n+  MediaManager,\n+  ReadThroughBlobStore,\n+  SimpleWebBlobStore,\n+} from \"@langchain/google-common/experimental/utils/media_core\";\n+import { GoogleCloudStorageUri } from \"@langchain/google-common/experimental/media\";\n+import { InMemoryStore } from \"@langchain/core/stores\";\n+import { GeminiTool } from \"../types.js\";\n+import { ChatGoogle } from \"../chat_models.js\";\n+import { BlobStoreGoogleCloudStorage } from \"../media.js\";\n+\n+describe(\"GAuth Chat\", () => {\n+  test(\"invoke\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const res = await model.invoke(\"What is 1 + 1?\");\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+\n+      expect(typeof aiMessage.content).toBe(\"string\");\n+      const text = aiMessage.content as string;\n+      expect(text).toMatch(/(1 + 1 (equals|is|=) )?2.? ?/);\n+\n+      /*\n+      expect(aiMessage.content.length).toBeGreaterThan(0);\n+      expect(aiMessage.content[0]).toBeDefined();\n+      const content = aiMessage.content[0] as MessageContentComplex;\n+      expect(content).toHaveProperty(\"type\");\n+      expect(content.type).toEqual(\"text\");\n+\n+      const textContent = content as MessageContentText;\n+      expect(textContent.text).toBeDefined();\n+      expect(textContent.text).toEqual(\"2\");\n+      */\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test(\"generate\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const messages: BaseMessage[] = [\n+        new SystemMessage(\n+          \"You will reply to all requests to flip a coin with either H, indicating heads, or T, indicating tails.\"\n+        ),\n+        new HumanMessage(\"Flip it\"),\n+        new AIMessage(\"T\"),\n+        new HumanMessage(\"Flip the coin again\"),\n+      ];\n+      const res = await model.predictMessages(messages);\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+\n+      expect(typeof aiMessage.content).toBe(\"string\");\n+      const text = aiMessage.content as string;\n+      expect([\"H\", \"T\"]).toContainEqual(text);\n+\n+      /*\n+      expect(aiMessage.content.length).toBeGreaterThan(0);\n+      expect(aiMessage.content[0]).toBeDefined();\n+\n+      const content = aiMessage.content[0] as MessageContentComplex;\n+      expect(content).toHaveProperty(\"type\");\n+      expect(content.type).toEqual(\"text\");\n+\n+      const textContent = content as MessageContentText;\n+      expect(textContent.text).toBeDefined();\n+      expect([\"H\", \"T\"]).toContainEqual(textContent.text);\n+      */\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test(\"stream\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const input: BaseLanguageModelInput = new ChatPromptValue([\n+        new SystemMessage(\n+          \"You will reply to all requests to flip a coin with either H, indicating heads, or T, indicating tails.\"\n+        ),\n+        new HumanMessage(\"Flip it\"),\n+        new AIMessage(\"T\"),\n+        new HumanMessage(\"Flip the coin again\"),\n+      ]);\n+      const res = await model.stream(input);\n+      const resArray: BaseMessageChunk[] = [];\n+      for await (const chunk of res) {\n+        resArray.push(chunk);\n+      }\n+      expect(resArray).toBeDefined();\n+      expect(resArray.length).toBeGreaterThanOrEqual(1);\n+\n+      const lastChunk = resArray[resArray.length - 1];\n+      expect(lastChunk).toBeDefined();\n+      expect(lastChunk._getType()).toEqual(\"ai\");\n+      const aiChunk = lastChunk as AIMessageChunk;\n+      console.log(aiChunk);\n+\n+      console.log(JSON.stringify(resArray, null, 2));\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test(\"function\", async () => {\n+    const tools: GeminiTool[] = [\n+      {\n+        functionDeclarations: [\n+          {\n+            name: \"test\",\n+            description:\n+              \"Run a test with a specific name and get if it passed or failed\",\n+            parameters: {\n+              type: \"object\",\n+              properties: {\n+                testName: {\n+                  type: \"string\",\n+                  description: \"The name of the test that should be run.\",\n+                },\n+              },\n+              required: [\"testName\"],\n+            },\n+          },\n+        ],\n+      },\n+    ];\n+    const model = new ChatGoogle().bind({ tools });\n+    const result = await model.invoke(\"Run a test on the cobalt project\");\n+    expect(result).toHaveProperty(\"content\");\n+    expect(result.content).toBe(\"\");\n+    const args = result?.lc_kwargs?.additional_kwargs;\n+    expect(args).toBeDefined();\n+    expect(args).toHaveProperty(\"tool_calls\");\n+    expect(Array.isArray(args.tool_calls)).toBeTruthy();\n+    expect(args.tool_calls).toHaveLength(1);\n+    const call = args.tool_calls[0];\n+    expect(call).toHaveProperty(\"type\");\n+    expect(call.type).toBe(\"function\");\n+    expect(call).toHaveProperty(\"function\");\n+    const func = call.function;\n+    expect(func).toBeDefined();\n+    expect(func).toHaveProperty(\"name\");\n+    expect(func.name).toBe(\"test\");\n+    expect(func).toHaveProperty(\"arguments\");\n+    expect(typeof func.arguments).toBe(\"string\");\n+    expect(func.arguments.replaceAll(\"\\n\", \"\")).toBe('{\"testName\":\"cobalt\"}');\n+  });\n+\n+  test(\"function reply\", async () => {\n+    const tools: GeminiTool[] = [\n+      {\n+        functionDeclarations: [\n+          {\n+            name: \"test\",\n+            description:\n+              \"Run a test with a specific name and get if it passed or failed\",\n+            parameters: {\n+              type: \"object\",\n+              properties: {\n+                testName: {\n+                  type: \"string\",\n+                  description: \"The name of the test that should be run.\",\n+                },\n+              },\n+              required: [\"testName\"],\n+            },\n+          },\n+        ],\n+      },\n+    ];\n+    const model = new ChatGoogle().bind({ tools });\n+    const toolResult = {\n+      testPassed: true,\n+    };\n+    const messages: BaseMessageLike[] = [\n+      new HumanMessage(\"Run a test on the cobalt project.\"),\n+      new AIMessage(\"\", {\n+        tool_calls: [\n+          {\n+            id: \"test\",\n+            type: \"function\",\n+            function: {\n+              name: \"test\",\n+              arguments: '{\"testName\":\"cobalt\"}',\n+            },\n+          },\n+        ],\n+      }),\n+      new ToolMessage(JSON.stringify(toolResult), \"test\"),\n+    ];\n+    const res = await model.stream(messages);\n+    const resArray: BaseMessageChunk[] = [];\n+    for await (const chunk of res) {\n+      resArray.push(chunk);\n+    }\n+    console.log(JSON.stringify(resArray, null, 2));\n+  });\n+\n+  test(\"withStructuredOutput\", async () => {\n+    const tool = {\n+      name: \"get_weather\",\n+      description:\n+        \"Get the weather of a specific location and return the temperature in Celsius.\",\n+      parameters: {\n+        type: \"object\",\n+        properties: {\n+          location: {\n+            type: \"string\",\n+            description: \"The name of city to get the weather for.\",\n+          },\n+        },\n+        required: [\"location\"],\n+      },\n+    };\n+    const model = new ChatGoogle().withStructuredOutput(tool);\n+    const result = await model.invoke(\"What is the weather in Paris?\");\n+    expect(result).toHaveProperty(\"location\");\n+  });\n+\n+  test(\"media - fileData\", async () => {\n+    class MemStore extends InMemoryStore<MediaBlob> {\n+      get length() {\n+        return Object.keys(this.store).length;\n+      }\n+    }\n+    const aliasMemory = new MemStore();\n+    const aliasStore = new BackedBlobStore({\n+      backingStore: aliasMemory,\n+      defaultFetchOptions: {\n+        actionIfBlobMissing: undefined,\n+      },\n+    });\n+    const canonicalStore = new BlobStoreGoogleCloudStorage({\n+      uriPrefix: new GoogleCloudStorageUri(\"gs://test-langchainjs/mediatest/\"),\n+      defaultStoreOptions: {\n+        actionIfInvalid: \"prefixPath\",\n+      },\n+    });\n+    const blobStore = new ReadThroughBlobStore({\n+      baseStore: aliasStore,\n+      backingStore: canonicalStore,\n+    });\n+    const resolver = new SimpleWebBlobStore();\n+    const mediaManager = new MediaManager({\n+      store: blobStore,\n+      resolvers: [resolver],\n+    });\n+    const model = new ChatGoogle({\n+      modelName: \"gemini-1.5-flash\",\n+      mediaManager,\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"https://js.langchain.com/v0.2/img/brand/wordmark.png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    try {\n+      const res = await model.invoke(messages);\n+\n+      console.log(res);\n+\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+\n+      expect(typeof aiMessage.content).toBe(\"string\");\n+      const text = aiMessage.content as string;\n+      expect(text).toMatch(/LangChain/);\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+});",
          "libs/langchain-google-gauth/src/tests/media.int.test.ts": "@@ -0,0 +1,137 @@\n+import fs from \"fs/promises\";\n+import { test } from \"@jest/globals\";\n+import { GoogleCloudStorageUri } from \"@langchain/google-common/experimental/media\";\n+import { MediaBlob } from \"@langchain/google-common/experimental/utils/media_core\";\n+import {\n+  BlobStoreGoogleCloudStorage,\n+  BlobStoreGoogleCloudStorageParams,\n+} from \"../media.js\";\n+\n+describe(\"GAuth GCS store\", () => {\n+  test(\"save text no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/text/test-${Date.now()}-nm`;\n+    const content = \"This is a test\";\n+    const blob = await MediaBlob.fromBlob(\n+      new Blob([content], { type: \"text/plain\" }),\n+      {\n+        path: uri,\n+      }\n+    );\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(await storedBlob?.asString()).toEqual(content);\n+    expect(storedBlob?.mimetype).toEqual(\"text/plain\");\n+    expect(storedBlob?.metadata).not.toHaveProperty(\"metadata\");\n+    expect(storedBlob?.size).toEqual(content.length);\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"save text with-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/text/test-${Date.now()}-wm`;\n+    const content = \"This is a test\";\n+    const blob = await MediaBlob.fromBlob(\n+      new Blob([content], { type: \"text/plain\" }),\n+      {\n+        path: uri,\n+        metadata: {\n+          alpha: \"one\",\n+          bravo: \"two\",\n+        },\n+      }\n+    );\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(await storedBlob?.asString()).toEqual(content);\n+    expect(storedBlob?.mimetype).toEqual(\"text/plain\");\n+    expect(storedBlob?.metadata).toHaveProperty(\"metadata\");\n+    expect(storedBlob?.metadata?.metadata?.alpha).toEqual(\"one\");\n+    expect(storedBlob?.metadata?.metadata?.bravo).toEqual(\"two\");\n+    expect(storedBlob?.size).toEqual(content.length);\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"save image no-metadata\", async () => {\n+    const filename = `src/tests/data/blue-square.png`;\n+    const dataBuffer = await fs.readFile(filename);\n+    const data = new Blob([dataBuffer], { type: \"image/png\" });\n+\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/image/test-${Date.now()}-nm`;\n+    const blob = await MediaBlob.fromBlob(data, {\n+      path: uri,\n+    });\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(storedBlob?.size).toEqual(176);\n+    expect(storedBlob?.mimetype).toEqual(\"image/png\");\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get text no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/text/test-nm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(blob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(await blob?.asString()).toEqual(\"This is a test\");\n+    expect(blob?.mimetype).toEqual(\"text/plain\");\n+    expect(blob?.metadata).not.toHaveProperty(\"metadata\");\n+    expect(blob?.size).toEqual(14);\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get text with-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/text/test-wm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(blob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(await blob?.asString()).toEqual(\"This is a test\");\n+    expect(blob?.mimetype).toEqual(\"text/plain\");\n+    expect(blob?.metadata).toHaveProperty(\"metadata\");\n+    expect(blob?.metadata?.metadata?.alpha).toEqual(\"one\");\n+    expect(blob?.metadata?.metadata?.bravo).toEqual(\"two\");\n+    expect(blob?.size).toEqual(14);\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get image no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/image/test-nm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(storedBlob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(blob?.size).toEqual(176);\n+    expect(blob?.mimetype).toEqual(\"image/png\");\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+});",
          "libs/langchain-google-vertexai/src/tests/chat_models.int.test.ts": "@@ -9,16 +9,31 @@ import {\n   BaseMessageChunk,\n   BaseMessageLike,\n   HumanMessage,\n+  HumanMessageChunk,\n+  MessageContentComplex,\n   SystemMessage,\n   ToolMessage,\n } from \"@langchain/core/messages\";\n+import {\n+  BlobStoreGoogleCloudStorage,\n+  ChatGoogle,\n+} from \"@langchain/google-gauth\";\n import { tool } from \"@langchain/core/tools\";\n import { z } from \"zod\";\n import { concat } from \"@langchain/core/utils/stream\";\n+import {\n+  BackedBlobStore,\n+  MediaBlob,\n+  MediaManager,\n+  ReadThroughBlobStore,\n+  SimpleWebBlobStore,\n+} from \"@langchain/google-common/experimental/utils/media_core\";\n+import { GoogleCloudStorageUri } from \"@langchain/google-common/experimental/media\";\n import {\n   ChatPromptTemplate,\n   MessagesPlaceholder,\n } from \"@langchain/core/prompts\";\n+import { InMemoryStore } from \"@langchain/core/stores\";\n import { GeminiTool } from \"../types.js\";\n import { ChatVertexAI } from \"../chat_models.js\";\n \n@@ -196,6 +211,74 @@ describe(\"GAuth Chat\", () => {\n     const result = await model.invoke(\"What is the weather in Paris?\");\n     expect(result).toHaveProperty(\"location\");\n   });\n+\n+  test(\"media - fileData\", async () => {\n+    class MemStore extends InMemoryStore<MediaBlob> {\n+      get length() {\n+        return Object.keys(this.store).length;\n+      }\n+    }\n+    const aliasMemory = new MemStore();\n+    const aliasStore = new BackedBlobStore({\n+      backingStore: aliasMemory,\n+      defaultFetchOptions: {\n+        actionIfBlobMissing: undefined,\n+      },\n+    });\n+    const canonicalStore = new BlobStoreGoogleCloudStorage({\n+      uriPrefix: new GoogleCloudStorageUri(\"gs://test-langchainjs/mediatest/\"),\n+      defaultStoreOptions: {\n+        actionIfInvalid: \"prefixPath\",\n+      },\n+    });\n+    const blobStore = new ReadThroughBlobStore({\n+      baseStore: aliasStore,\n+      backingStore: canonicalStore,\n+    });\n+    const resolver = new SimpleWebBlobStore();\n+    const mediaManager = new MediaManager({\n+      store: blobStore,\n+      resolvers: [resolver],\n+    });\n+    const model = new ChatGoogle({\n+      modelName: \"gemini-1.5-flash\",\n+      mediaManager,\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"https://js.langchain.com/v0.2/img/brand/wordmark.png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    try {\n+      const res = await model.invoke(messages);\n+\n+      console.log(res);\n+\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+\n+      expect(typeof aiMessage.content).toBe(\"string\");\n+      const text = aiMessage.content as string;\n+      expect(text).toMatch(/LangChain/);\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n });\n \n test(\"Stream token count usage_metadata\", async () => {",
          "libs/langchain-google-webauth/package.json": "@@ -58,7 +58,8 @@\n     \"release-it\": \"^17.6.0\",\n     \"rollup\": \"^4.5.2\",\n     \"ts-jest\": \"^29.1.0\",\n-    \"typescript\": \"<5.2.0\"\n+    \"typescript\": \"<5.2.0\",\n+    \"zod\": \"^3.23.8\"\n   },\n   \"publishConfig\": {\n     \"access\": \"public\"",
          "libs/langchain-google-webauth/src/media.ts": "@@ -0,0 +1,33 @@\n+import {\n+  GoogleAbstractedClient,\n+  GoogleBaseLLMInput,\n+} from \"@langchain/google-common\";\n+import {\n+  BlobStoreAIStudioFileBase,\n+  BlobStoreAIStudioFileBaseParams,\n+  BlobStoreGoogleCloudStorageBase,\n+  BlobStoreGoogleCloudStorageBaseParams,\n+} from \"@langchain/google-common/experimental/media\";\n+import { WebGoogleAuth, WebGoogleAuthOptions } from \"./auth.js\";\n+\n+export interface BlobStoreGoogleCloudStorageParams\n+  extends BlobStoreGoogleCloudStorageBaseParams<WebGoogleAuthOptions> {}\n+\n+export class BlobStoreGoogleCloudStorage extends BlobStoreGoogleCloudStorageBase<WebGoogleAuthOptions> {\n+  buildClient(\n+    fields?: GoogleBaseLLMInput<WebGoogleAuthOptions>\n+  ): GoogleAbstractedClient {\n+    return new WebGoogleAuth(fields);\n+  }\n+}\n+\n+export interface BlobStoreAIStudioFileParams\n+  extends BlobStoreAIStudioFileBaseParams<WebGoogleAuthOptions> {}\n+\n+export class BlobStoreAIStudioFile extends BlobStoreAIStudioFileBase<WebGoogleAuthOptions> {\n+  buildAbstractedClient(\n+    fields?: BlobStoreAIStudioFileParams\n+  ): GoogleAbstractedClient {\n+    return new WebGoogleAuth(fields);\n+  }\n+}",
          "libs/langchain-google-webauth/src/tests/chat_models.int.test.ts": "@@ -0,0 +1,247 @@\n+/* eslint-disable import/no-extraneous-dependencies */\n+import { StructuredTool } from \"@langchain/core/tools\";\n+import { z } from \"zod\";\n+import { test } from \"@jest/globals\";\n+import {\n+  AIMessage,\n+  AIMessageChunk,\n+  BaseMessage,\n+  BaseMessageChunk,\n+  HumanMessage,\n+  HumanMessageChunk,\n+  MessageContentComplex,\n+  SystemMessage,\n+  ToolMessage,\n+} from \"@langchain/core/messages\";\n+import { BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\n+import { ChatPromptValue } from \"@langchain/core/prompt_values\";\n+import {\n+  MediaManager,\n+  SimpleWebBlobStore,\n+} from \"@langchain/google-common/experimental/utils/media_core\";\n+import { ChatGoogle } from \"../chat_models.js\";\n+import { BlobStoreAIStudioFile } from \"../media.js\";\n+\n+class WeatherTool extends StructuredTool {\n+  schema = z.object({\n+    locations: z\n+      .array(z.object({ name: z.string() }))\n+      .describe(\"The name of cities to get the weather for.\"),\n+  });\n+\n+  description =\n+    \"Get the weather of a specific location and return the temperature in Celsius.\";\n+\n+  name = \"get_weather\";\n+\n+  async _call(input: z.infer<typeof this.schema>) {\n+    console.log(`WeatherTool called with input: ${input}`);\n+    return `The weather in ${JSON.stringify(input.locations)} is 25°C`;\n+  }\n+}\n+\n+describe(\"Google APIKey Chat\", () => {\n+  test(\"invoke\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const res = await model.invoke(\"What is 1 + 1?\");\n+      console.log(res);\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      console.log(aiMessage);\n+      expect(aiMessage.content).toBeDefined();\n+      expect(aiMessage.content.length).toBeGreaterThan(0);\n+      expect(aiMessage.content[0]).toBeDefined();\n+\n+      // const content = aiMessage.content[0] as MessageContentComplex;\n+      // expect(content).toHaveProperty(\"type\");\n+      // expect(content.type).toEqual(\"text\");\n+\n+      // const textContent = content as MessageContentText;\n+      // expect(textContent.text).toBeDefined();\n+      // expect(textContent.text).toEqual(\"2\");\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test(\"generate\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const messages: BaseMessage[] = [\n+        new SystemMessage(\n+          \"You will reply to all requests to flip a coin with either H, indicating heads, or T, indicating tails.\"\n+        ),\n+        new HumanMessage(\"Flip it\"),\n+        new AIMessage(\"T\"),\n+        new HumanMessage(\"Flip the coin again\"),\n+      ];\n+      const res = await model.predictMessages(messages);\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+      expect(aiMessage.content.length).toBeGreaterThan(0);\n+      expect(aiMessage.content[0]).toBeDefined();\n+      console.log(aiMessage);\n+\n+      // const content = aiMessage.content[0] as MessageContentComplex;\n+      // expect(content).toHaveProperty(\"type\");\n+      // expect(content.type).toEqual(\"text\");\n+\n+      // const textContent = content as MessageContentText;\n+      // expect(textContent.text).toBeDefined();\n+      // expect([\"H\", \"T\"]).toContainEqual(textContent.text);\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test(\"stream\", async () => {\n+    const model = new ChatGoogle();\n+    try {\n+      const input: BaseLanguageModelInput = new ChatPromptValue([\n+        new SystemMessage(\n+          \"You will reply to all requests to flip a coin with either H, indicating heads, or T, indicating tails.\"\n+        ),\n+        new HumanMessage(\"Flip it\"),\n+        new AIMessage(\"T\"),\n+        new HumanMessage(\"Flip the coin again\"),\n+      ]);\n+      const res = await model.stream(input);\n+      const resArray: BaseMessageChunk[] = [];\n+      for await (const chunk of res) {\n+        resArray.push(chunk);\n+      }\n+      expect(resArray).toBeDefined();\n+      expect(resArray.length).toBeGreaterThanOrEqual(1);\n+\n+      const lastChunk = resArray[resArray.length - 1];\n+      expect(lastChunk).toBeDefined();\n+      expect(lastChunk._getType()).toEqual(\"ai\");\n+      const aiChunk = lastChunk as AIMessageChunk;\n+      console.log(aiChunk);\n+\n+      console.log(JSON.stringify(resArray, null, 2));\n+    } catch (e) {\n+      console.error(e);\n+      throw e;\n+    }\n+  });\n+\n+  test.skip(\"Tool call\", async () => {\n+    const chat = new ChatGoogle().bindTools([new WeatherTool()]);\n+    const res = await chat.invoke(\"What is the weather in SF and LA\");\n+    console.log(res);\n+    expect(res.tool_calls?.length).toEqual(1);\n+    expect(res.tool_calls?.[0].args).toEqual(\n+      JSON.parse(res.additional_kwargs.tool_calls?.[0].function.arguments ?? \"\")\n+    );\n+  });\n+\n+  test.skip(\"Few shotting with tool calls\", async () => {\n+    const chat = new ChatGoogle().bindTools([new WeatherTool()]);\n+    const res = await chat.invoke(\"What is the weather in SF\");\n+    console.log(res);\n+    const res2 = await chat.invoke([\n+      new HumanMessage(\"What is the weather in SF?\"),\n+      new AIMessage({\n+        content: \"\",\n+        tool_calls: [\n+          {\n+            id: \"12345\",\n+            name: \"get_current_weather\",\n+            args: {\n+              location: \"SF\",\n+            },\n+          },\n+        ],\n+      }),\n+      new ToolMessage({\n+        tool_call_id: \"12345\",\n+        content: \"It is currently 24 degrees with hail in SF.\",\n+      }),\n+      new AIMessage(\"It is currently 24 degrees in SF with hail in SF.\"),\n+      new HumanMessage(\"What did you say the weather was?\"),\n+    ]);\n+    console.log(res2);\n+    expect(res2.content).toContain(\"24\");\n+  });\n+\n+  test.skip(\"withStructuredOutput\", async () => {\n+    const tool = {\n+      name: \"get_weather\",\n+      description:\n+        \"Get the weather of a specific location and return the temperature in Celsius.\",\n+      parameters: {\n+        type: \"object\",\n+        properties: {\n+          location: {\n+            type: \"string\",\n+            description: \"The name of city to get the weather for.\",\n+          },\n+        },\n+        required: [\"location\"],\n+      },\n+    };\n+    const model = new ChatGoogle().withStructuredOutput(tool);\n+    const result = await model.invoke(\"What is the weather in Paris?\");\n+    expect(result).toHaveProperty(\"location\");\n+  });\n+\n+  test(\"media - fileData\", async () => {\n+    const canonicalStore = new BlobStoreAIStudioFile({});\n+    const resolver = new SimpleWebBlobStore();\n+    const mediaManager = new MediaManager({\n+      store: canonicalStore,\n+      resolvers: [resolver],\n+    });\n+    const model = new ChatGoogle({\n+      modelName: \"gemini-1.5-flash\",\n+      apiVersion: \"v1beta\",\n+      mediaManager,\n+    });\n+\n+    const message: MessageContentComplex[] = [\n+      {\n+        type: \"text\",\n+        text: \"What is in this image?\",\n+      },\n+      {\n+        type: \"media\",\n+        fileUri: \"https://js.langchain.com/v0.2/img/brand/wordmark.png\",\n+      },\n+    ];\n+\n+    const messages: BaseMessage[] = [\n+      new HumanMessageChunk({ content: message }),\n+    ];\n+\n+    try {\n+      const res = await model.invoke(messages);\n+\n+      // console.log(res);\n+\n+      expect(res).toBeDefined();\n+      expect(res._getType()).toEqual(\"ai\");\n+\n+      const aiMessage = res as AIMessageChunk;\n+      expect(aiMessage.content).toBeDefined();\n+\n+      expect(typeof aiMessage.content).toBe(\"string\");\n+      const text = aiMessage.content as string;\n+      expect(text).toMatch(/LangChain/);\n+\n+      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+    } catch (e: any) {\n+      console.error(e);\n+      console.error(JSON.stringify(e.details, null, 1));\n+      throw e;\n+    }\n+  });\n+});",
          "libs/langchain-google-webauth/src/tests/media.int.test.ts": "@@ -0,0 +1,201 @@\n+import fs from \"fs/promises\";\n+import { test } from \"@jest/globals\";\n+import { GoogleCloudStorageUri } from \"@langchain/google-common/experimental/media\";\n+import { MediaBlob } from \"@langchain/google-common/experimental/utils/media_core\";\n+import {\n+  BlobStoreAIStudioFile,\n+  BlobStoreGoogleCloudStorage,\n+  BlobStoreGoogleCloudStorageParams,\n+} from \"../media.js\";\n+\n+describe(\"Google Webauth GCS store\", () => {\n+  test(\"save text no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/text/test-${Date.now()}-nm`;\n+    const content = \"This is a test\";\n+    const blob = await MediaBlob.fromBlob(\n+      new Blob([content], { type: \"text/plain\" }),\n+      {\n+        path: uri,\n+      }\n+    );\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(await storedBlob?.asString()).toEqual(content);\n+    expect(storedBlob?.mimetype).toEqual(\"text/plain\");\n+    expect(storedBlob?.metadata).not.toHaveProperty(\"metadata\");\n+    expect(storedBlob?.size).toEqual(content.length);\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"save text with-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/text/test-${Date.now()}-wm`;\n+    const content = \"This is a test\";\n+    const blob = await MediaBlob.fromBlob(\n+      new Blob([content], { type: \"text/plain\" }),\n+      {\n+        path: uri,\n+        metadata: {\n+          alpha: \"one\",\n+          bravo: \"two\",\n+        },\n+      }\n+    );\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(await storedBlob?.asString()).toEqual(content);\n+    expect(storedBlob?.mimetype).toEqual(\"text/plain\");\n+    expect(storedBlob?.metadata).toHaveProperty(\"metadata\");\n+    expect(storedBlob?.metadata?.metadata?.alpha).toEqual(\"one\");\n+    expect(storedBlob?.metadata?.metadata?.bravo).toEqual(\"two\");\n+    expect(storedBlob?.size).toEqual(content.length);\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"save image no-metadata\", async () => {\n+    const filename = `src/tests/data/blue-square.png`;\n+    const dataBuffer = await fs.readFile(filename);\n+    const data = new Blob([dataBuffer], { type: \"image/png\" });\n+\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri = `gs://test-langchainjs/image/test-${Date.now()}-nm`;\n+    const blob = await MediaBlob.fromBlob(data, {\n+      path: uri,\n+    });\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const storedBlob = await blobStore.store(blob);\n+    // console.log(storedBlob);\n+    expect(storedBlob?.path).toEqual(uri);\n+    expect(storedBlob?.size).toEqual(176);\n+    expect(storedBlob?.mimetype).toEqual(\"image/png\");\n+    expect(storedBlob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get text no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/text/test-nm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(blob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(await blob?.asString()).toEqual(\"This is a test\");\n+    expect(blob?.mimetype).toEqual(\"text/plain\");\n+    expect(blob?.metadata).not.toHaveProperty(\"metadata\");\n+    expect(blob?.size).toEqual(14);\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get text with-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/text/test-wm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(blob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(await blob?.asString()).toEqual(\"This is a test\");\n+    expect(blob?.mimetype).toEqual(\"text/plain\");\n+    expect(blob?.metadata).toHaveProperty(\"metadata\");\n+    expect(blob?.metadata?.metadata?.alpha).toEqual(\"one\");\n+    expect(blob?.metadata?.metadata?.bravo).toEqual(\"two\");\n+    expect(blob?.size).toEqual(14);\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+\n+  test(\"get image no-metadata\", async () => {\n+    const uriPrefix = new GoogleCloudStorageUri(\"gs://test-langchainjs/\");\n+    const uri: string = \"gs://test-langchainjs/image/test-nm\";\n+    const config: BlobStoreGoogleCloudStorageParams = {\n+      uriPrefix,\n+    };\n+    const blobStore = new BlobStoreGoogleCloudStorage(config);\n+    const blob = await blobStore.fetch(uri);\n+    // console.log(storedBlob);\n+    expect(blob?.path).toEqual(uri);\n+    expect(blob?.size).toEqual(176);\n+    expect(blob?.mimetype).toEqual(\"image/png\");\n+    expect(blob?.metadata?.kind).toEqual(\"storage#object\");\n+  });\n+});\n+\n+describe(\"Google APIKey AIStudioBlobStore\", () => {\n+  test(\"save image no metadata\", async () => {\n+    const filename = `src/tests/data/blue-square.png`;\n+    const dataBuffer = await fs.readFile(filename);\n+    const data = new Blob([dataBuffer], { type: \"image/png\" });\n+    const blob = await MediaBlob.fromBlob(data, {\n+      path: filename,\n+    });\n+    const blobStore = new BlobStoreAIStudioFile();\n+    const storedBlob = await blobStore.store(blob);\n+    console.log(storedBlob);\n+\n+    // The blob itself is expected to have no data right now,\n+    // but this will hopefully change in the future.\n+    expect(storedBlob?.size).toEqual(0);\n+    expect(storedBlob?.dataType).toEqual(\"image/png\");\n+    expect(storedBlob?.metadata?.sizeBytes).toEqual(\"176\");\n+    expect(storedBlob?.metadata?.state).toEqual(\"ACTIVE\");\n+  });\n+\n+  test(\"save video with retry\", async () => {\n+    const filename = `src/tests/data/rainbow.mp4`;\n+    const dataBuffer = await fs.readFile(filename);\n+    const data = new Blob([dataBuffer], { type: \"video/mp4\" });\n+    const blob = await MediaBlob.fromBlob(data, {\n+      path: filename,\n+    });\n+    const blobStore = new BlobStoreAIStudioFile();\n+    const storedBlob = await blobStore.store(blob);\n+    console.log(storedBlob);\n+\n+    // The blob itself is expected to have no data right now,\n+    // but this will hopefully change in the future.\n+    expect(storedBlob?.size).toEqual(0);\n+    expect(storedBlob?.dataType).toEqual(\"video/mp4\");\n+    expect(storedBlob?.metadata?.sizeBytes).toEqual(\"1020253\");\n+    expect(storedBlob?.metadata?.state).toEqual(\"ACTIVE\");\n+    expect(storedBlob?.metadata?.videoMetadata?.videoDuration).toEqual(\"8s\");\n+  });\n+\n+  test(\"save video no retry\", async () => {\n+    const filename = `src/tests/data/rainbow.mp4`;\n+    const dataBuffer = await fs.readFile(filename);\n+    const data = new Blob([dataBuffer], { type: \"video/mp4\" });\n+    const blob = await MediaBlob.fromBlob(data, {\n+      path: filename,\n+    });\n+    const blobStore = new BlobStoreAIStudioFile({\n+      retryTime: -1,\n+    });\n+    const storedBlob = await blobStore.store(blob);\n+    console.log(storedBlob);\n+\n+    // The blob itself is expected to have no data right now,\n+    // but this will hopefully change in the future.\n+    expect(storedBlob?.size).toEqual(0);\n+    expect(storedBlob?.dataType).toEqual(\"video/mp4\");\n+    expect(storedBlob?.metadata?.sizeBytes).toEqual(\"1020253\");\n+    expect(storedBlob?.metadata?.state).toEqual(\"PROCESSING\");\n+    expect(storedBlob?.metadata?.videoMetadata).toBeUndefined();\n+  });\n+});",
          "libs/langchain-mongodb/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/mongodb\",\n-  \"version\": \"0.0.5\",\n+  \"version\": \"0.0.6\",\n   \"description\": \"Sample integration for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {",
          "libs/langchain-mongodb/src/storage.ts": "@@ -35,7 +35,7 @@ export interface MongoDBStoreInput {\n  * const store = new MongoDBStore({\n  *   collection,\n  * });\n- * \n+ *\n  * const docs = [\n  *   [uuidv4(), \"Dogs are tough.\"],\n  *   [uuidv4(), \"Cats are tough.\"],\n@@ -97,11 +97,18 @@ export class MongoDBStore extends BaseStore<string, Uint8Array> {\n       .toArray();\n \n     const encoder = new TextEncoder();\n-    return retrievedValues.map((value) => {\n-      if (!(\"value\" in value)) {\n+    const valueMap = new Map(\n+      retrievedValues.map((item) => [item[this.primaryKey], item])\n+    );\n+\n+    return prefixedKeys.map((prefixedKey) => {\n+      const value = valueMap.get(prefixedKey);\n+\n+      if (!value) {\n         return undefined;\n       }\n-      if (value === undefined || value === null) {\n+\n+      if (!(\"value\" in value)) {\n         return undefined;\n       } else if (typeof value.value === \"object\") {\n         return encoder.encode(JSON.stringify(value.value));",
          "libs/langchain-mongodb/src/tests/storage.int.test.ts": "@@ -43,18 +43,22 @@ test(\"MongoDBStore can set and retrieve\", async () => {\n       encoder.encode(doc[1]),\n     ]);\n     await store.mset(docsAsKVPairs);\n-    const retrievedDocs = (await store.mget(docs.map((doc) => doc[0]))).flatMap(\n-      (doc) => {\n-        if (doc !== undefined) {\n-          const decodedDoc = decoder.decode(doc);\n-          const parsedDoc = JSON.parse(decodedDoc);\n-          return [parsedDoc];\n-        }\n-        return [];\n-      }\n-    );\n-\n-    expect(retrievedDocs.sort()).toEqual(docs.map((doc) => doc[1]).sort());\n+\n+    const keysToRetrieve = docs.map((doc) => doc[0]);\n+    keysToRetrieve.unshift(\"nonexistent_key_0\");\n+    keysToRetrieve.push(\"nonexistent_key_3\");\n+\n+    const retrievedDocs = await store.mget(keysToRetrieve);\n+    expect(retrievedDocs.length).toBe(keysToRetrieve.length);\n+    // Check that the first item is undefined (nonexistent_key_0)\n+    expect(retrievedDocs[0]).toBeUndefined();\n+\n+    // Check that the second and third items match the original docs\n+    expect(decoder.decode(retrievedDocs[1])).toBe(docs[0][1]);\n+    expect(decoder.decode(retrievedDocs[2])).toBe(docs[1][1]);\n+\n+    // Check that the last item is undefined (nonexistent_key_1)\n+    expect(retrievedDocs[retrievedDocs.length - 1]).toBeUndefined();\n   } finally {\n     const keys = store.yieldKeys();\n     const yieldedKeys = [];\n@@ -104,7 +108,9 @@ test(\"MongoDBStore can delete\", async () => {\n \n     const retrievedDocs = await store.mget(docs.map((doc) => doc[0]));\n \n-    expect(retrievedDocs.length).toBe(0);\n+    expect(retrievedDocs.length).toBe(2);\n+    const everyValueUndefined = retrievedDocs.every((v) => v === undefined);\n+    expect(everyValueUndefined).toBe(true);\n   } finally {\n     const keys = store.yieldKeys();\n     const yieldedKeys = [];",
          "libs/langchain-openai/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"@langchain/openai\",\n-  \"version\": \"0.2.9\",\n+  \"version\": \"0.2.10\",\n   \"description\": \"OpenAI integrations for LangChain.js\",\n   \"type\": \"module\",\n   \"engines\": {\n@@ -37,7 +37,7 @@\n   \"dependencies\": {\n     \"@langchain/core\": \">=0.2.26 <0.3.0\",\n     \"js-tiktoken\": \"^1.0.12\",\n-    \"openai\": \"^4.55.0\",\n+    \"openai\": \"^4.57.3\",\n     \"zod\": \"^3.22.4\",\n     \"zod-to-json-schema\": \"^3.22.3\"\n   },",
          "libs/langchain-openai/src/chat_models.ts": "@@ -61,7 +61,6 @@ import type {\n   ResponseFormatJSONObject,\n   ResponseFormatJSONSchema,\n } from \"openai/resources/shared\";\n-import { ParsedChatCompletion } from \"openai/resources/beta/chat/completions.mjs\";\n import type {\n   AzureOpenAIInput,\n   OpenAICallOptions,\n@@ -1615,7 +1614,8 @@ export class ChatOpenAI<\n   async betaParsedCompletionWithRetry(\n     request: OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming,\n     options?: OpenAICoreRequestOptions\n-  ): Promise<ParsedChatCompletion<null>> {\n+    // Avoid relying importing a beta type with no official entrypoint\n+  ): Promise<ReturnType<OpenAIClient[\"beta\"][\"chat\"][\"completions\"][\"parse\"]>> {\n     const requestOptions = this._getClientOptions(options);\n     return this.caller.call(async () => {\n       try {",
          "libs/langchain-openai/src/tests/chat_models_structured_output.int.test.ts": "@@ -25,10 +25,8 @@ test(\"withStructuredOutput zod schema function calling\", async () => {\n   );\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    \"You are VERY bad at math and must always use a calculator.\",\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    [\"system\", \"You are VERY bad at math and must always use a calculator.\"],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -38,6 +36,41 @@ test(\"withStructuredOutput zod schema function calling\", async () => {\n   expect(\"number2\" in result).toBe(true);\n });\n \n+test(\"withStructuredOutput zod schema streaming\", async () => {\n+  const model = new ChatOpenAI({\n+    temperature: 0,\n+    modelName: \"gpt-4-turbo-preview\",\n+  });\n+\n+  const calculatorSchema = z.object({\n+    operation: z.enum([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n+    number1: z.number(),\n+    number2: z.number(),\n+  });\n+  const modelWithStructuredOutput = model.withStructuredOutput(\n+    calculatorSchema,\n+    {\n+      name: \"calculator\",\n+    }\n+  );\n+\n+  const prompt = ChatPromptTemplate.fromMessages([\n+    [\"system\", \"You are VERY bad at math and must always use a calculator.\"],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n+  ]);\n+  const chain = prompt.pipe(modelWithStructuredOutput);\n+  const stream = await chain.stream({});\n+  const chunks = [];\n+  for await (const chunk of stream) {\n+    chunks.push(chunk);\n+  }\n+  expect(chunks.length).toBeGreaterThan(1);\n+  const result = chunks.at(-1) ?? {};\n+  expect(\"operation\" in result).toBe(true);\n+  expect(\"number1\" in result).toBe(true);\n+  expect(\"number2\" in result).toBe(true);\n+});\n+\n test(\"withStructuredOutput zod schema JSON mode\", async () => {\n   const model = new ChatOpenAI({\n     temperature: 0,\n@@ -58,15 +91,16 @@ test(\"withStructuredOutput zod schema JSON mode\", async () => {\n   );\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    `You are VERY bad at math and must always use a calculator.\n+    [\n+      \"system\",\n+      `You are VERY bad at math and must always use a calculator.\n Respond with a JSON object containing three keys:\n 'operation': the type of operation to execute, either 'add', 'subtract', 'multiply' or 'divide',\n 'number1': the first number to operate on,\n 'number2': the second number to operate on.\n `,\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    ],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -93,10 +127,8 @@ test(\"withStructuredOutput JSON schema function calling\", async () => {\n   });\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    `You are VERY bad at math and must always use a calculator.`,\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    [\"system\", `You are VERY bad at math and must always use a calculator.`],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -123,10 +155,8 @@ test(\"withStructuredOutput OpenAI function definition function calling\", async (\n   });\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    `You are VERY bad at math and must always use a calculator.`,\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    [\"system\", `You are VERY bad at math and must always use a calculator.`],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -156,15 +186,16 @@ test(\"withStructuredOutput JSON schema JSON mode\", async () => {\n   );\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    `You are VERY bad at math and must always use a calculator.\n+    [\n+      \"system\",\n+      `You are VERY bad at math and must always use a calculator.\n Respond with a JSON object containing three keys:\n 'operation': the type of operation to execute, either 'add', 'subtract', 'multiply' or 'divide',\n 'number1': the first number to operate on,\n 'number2': the second number to operate on.\n `,\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    ],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -196,15 +227,16 @@ test(\"withStructuredOutput JSON schema\", async () => {\n   const modelWithStructuredOutput = model.withStructuredOutput(jsonSchema);\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    `You are VERY bad at math and must always use a calculator.\n+    [\n+      \"system\",\n+      `You are VERY bad at math and must always use a calculator.\n Respond with a JSON object containing three keys:\n 'operation': the type of operation to execute, either 'add', 'subtract', 'multiply' or 'divide',\n 'number1': the first number to operate on,\n 'number2': the second number to operate on.\n `,\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    ],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});\n@@ -234,10 +266,8 @@ test(\"withStructuredOutput includeRaw true\", async () => {\n   );\n \n   const prompt = ChatPromptTemplate.fromMessages([\n-    \"system\",\n-    \"You are VERY bad at math and must always use a calculator.\",\n-    \"human\",\n-    \"Please help me!! What is 2 + 2?\",\n+    [\"system\", \"You are VERY bad at math and must always use a calculator.\"],\n+    [\"human\", \"Please help me!! What is 2 + 2?\"],\n   ]);\n   const chain = prompt.pipe(modelWithStructuredOutput);\n   const result = await chain.invoke({});",
          "libs/langchain-openai/src/tools/dalle.ts": "@@ -80,6 +80,10 @@ export interface DallEAPIWrapperParams extends ToolParams {\n    * The organization to use\n    */\n   organization?: string;\n+  /**\n+   * The base URL of the OpenAI API.\n+   */\n+  baseUrl?: string;\n }\n \n /**\n@@ -141,6 +145,7 @@ export class DallEAPIWrapper extends Tool {\n       apiKey: openAIApiKey,\n       organization,\n       dangerouslyAllowBrowser: true,\n+      baseUrl: fields?.baseUrl,\n     };\n     this.client = new OpenAIClient(clientConfig);\n     this.model = fields?.model ?? fields?.modelName ?? this.model;",
          "yarn.lock": "@@ -286,6 +286,22 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n+\"@arcjet/redact-wasm@npm:1.0.0-alpha.24\":\n+  version: 1.0.0-alpha.24\n+  resolution: \"@arcjet/redact-wasm@npm:1.0.0-alpha.24\"\n+  checksum: b0cf2a5b8eec1fcacdc130c3fce963cfe2bdc786aeb21296409b8bcc0053204ea6c1d7a816e9adf999fb56c8f73aaa950dfbbb7bd628d06ea21fb868cc4b7e5c\n+  languageName: node\n+  linkType: hard\n+\n+\"@arcjet/redact@npm:^v1.0.0-alpha.23\":\n+  version: 1.0.0-alpha.24\n+  resolution: \"@arcjet/redact@npm:1.0.0-alpha.24\"\n+  dependencies:\n+    \"@arcjet/redact-wasm\": 1.0.0-alpha.24\n+  checksum: 06eedc5f65eb98ac3b3f2598981a3c4b7265f89a59b138dfb38c7e19e3c4342ccb5ddfc3a0ae94add47f931b095138794b8c88355003c27374c37ead02f2106c\n+  languageName: node\n+  linkType: hard\n+\n \"@aws-crypto/crc32@npm:3.0.0\":\n   version: 3.0.0\n   resolution: \"@aws-crypto/crc32@npm:3.0.0\"\n@@ -8457,24 +8473,17 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n-\"@cloudflare/ai@npm:1.0.47\":\n-  version: 1.0.47\n-  resolution: \"@cloudflare/ai@npm:1.0.47\"\n-  checksum: a9e371f789256bb5795c0cb60d975749b07d6cb50141b997da6b5c94380ddf18e851cba518e246414fcab4239d8f4438c5fe29172e6f06c136be3be19a3b65e0\n-  languageName: node\n-  linkType: hard\n-\n \"@cloudflare/workers-types@npm:^4.20230922.0\":\n   version: 4.20230922.0\n   resolution: \"@cloudflare/workers-types@npm:4.20230922.0\"\n   checksum: 629bab47cdbcb74e3c42fc9486f5186734b6dd734154cea7a0983ad83ee053b4fb1ae13ff618a7287612bc3b3d19ad72d6a34a84289a903623cb8a13af57596b\n   languageName: node\n   linkType: hard\n \n-\"@cloudflare/workers-types@npm:^4.20231218.0\":\n-  version: 4.20231218.0\n-  resolution: \"@cloudflare/workers-types@npm:4.20231218.0\"\n-  checksum: b7e50a76ee8e9d662227bbb74798b93b6102acc224f1071a9c99a9adb419ad0b3bdabf7561e7e1b4a320a6a4616badeecdfb1848fbdaada197c7b37d845b8774\n+\"@cloudflare/workers-types@npm:^4.20240909.0\":\n+  version: 4.20240909.0\n+  resolution: \"@cloudflare/workers-types@npm:4.20240909.0\"\n+  checksum: 82fe9b22510d6a23533830684018651bf8a679692cc487cf82d15816e4c91b95ca5e759a6702d6b1268cd93cda8f0b45f3bf9e6ac27e926112ed4f3c7ef65968\n   languageName: node\n   linkType: hard\n \n@@ -11097,8 +11106,7 @@ __metadata:\n   version: 0.0.0-use.local\n   resolution: \"@langchain/cloudflare@workspace:libs/langchain-cloudflare\"\n   dependencies:\n-    \"@cloudflare/ai\": 1.0.47\n-    \"@cloudflare/workers-types\": ^4.20231218.0\n+    \"@cloudflare/workers-types\": ^4.20240909.0\n     \"@jest/globals\": ^29.5.0\n     \"@langchain/core\": \">0.1.0 <0.3.0\"\n     \"@langchain/langgraph\": ~0.0.31\n@@ -11174,6 +11182,7 @@ __metadata:\n   version: 0.0.0-use.local\n   resolution: \"@langchain/community@workspace:libs/langchain-community\"\n   dependencies:\n+    \"@arcjet/redact\": ^v1.0.0-alpha.23\n     \"@aws-crypto/sha256-js\": ^5.0.0\n     \"@aws-sdk/client-bedrock-agent-runtime\": ^3.583.0\n     \"@aws-sdk/client-bedrock-runtime\": ^3.422.0\n@@ -11211,7 +11220,7 @@ __metadata:\n     \"@langchain/standard-tests\": 0.0.0\n     \"@layerup/layerup-security\": ^1.5.12\n     \"@mendable/firecrawl-js\": ^0.0.36\n-    \"@mlc-ai/web-llm\": 0.2.46\n+    \"@mlc-ai/web-llm\": \">=0.2.62 <0.3.0\"\n     \"@mozilla/readability\": ^0.4.4\n     \"@neondatabase/serverless\": ^0.9.1\n     \"@notionhq/client\": ^2.2.10\n@@ -11349,6 +11358,7 @@ __metadata:\n     zod: ^3.22.3\n     zod-to-json-schema: ^3.22.5\n   peerDependencies:\n+    \"@arcjet/redact\": ^v1.0.0-alpha.23\n     \"@aws-crypto/sha256-js\": ^5.0.0\n     \"@aws-sdk/client-bedrock-agent-runtime\": ^3.583.0\n     \"@aws-sdk/client-bedrock-runtime\": ^3.422.0\n@@ -11378,7 +11388,7 @@ __metadata:\n     \"@langchain/langgraph\": \"*\"\n     \"@layerup/layerup-security\": ^1.5.12\n     \"@mendable/firecrawl-js\": ^0.0.13\n-    \"@mlc-ai/web-llm\": 0.2.46\n+    \"@mlc-ai/web-llm\": \"*\"\n     \"@mozilla/readability\": \"*\"\n     \"@neondatabase/serverless\": \"*\"\n     \"@notionhq/client\": ^2.2.10\n@@ -11470,6 +11480,8 @@ __metadata:\n     youtube-transcript: ^1.0.6\n     youtubei.js: ^9.1.0\n   peerDependenciesMeta:\n+    \"@arcjet/redact\":\n+      optional: true\n     \"@aws-crypto/sha256-js\":\n       optional: true\n     \"@aws-sdk/client-bedrock-agent-runtime\":\n@@ -11984,6 +11996,7 @@ __metadata:\n     ts-jest: ^29.1.0\n     typescript: <5.2.0\n     web-auth-library: ^1.0.3\n+    zod: ^3.23.8\n   languageName: unknown\n   linkType: soft\n \n@@ -12300,7 +12313,7 @@ __metadata:\n     jest: ^29.5.0\n     jest-environment-node: ^29.6.4\n     js-tiktoken: ^1.0.12\n-    openai: ^4.55.0\n+    openai: ^4.57.3\n     prettier: ^2.8.3\n     release-it: ^17.6.0\n     rimraf: ^5.0.1\n@@ -12776,12 +12789,12 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n-\"@mlc-ai/web-llm@npm:0.2.46\":\n-  version: 0.2.46\n-  resolution: \"@mlc-ai/web-llm@npm:0.2.46\"\n+\"@mlc-ai/web-llm@npm:>=0.2.62 <0.3.0\":\n+  version: 0.2.62\n+  resolution: \"@mlc-ai/web-llm@npm:0.2.62\"\n   dependencies:\n     loglevel: ^1.9.1\n-  checksum: 09c83a45d7f9351ae492d8704fe580868d0b46b640eca232ebc76d552f2ffad031c9c504a0d29f69122029478af270eeeda0800e7fb032b00c11dc1632e1ae11\n+  checksum: 7ccb0842e3fe83156406e61e0172f2f07115c77669b37729499f52c120cc33acd000fc4619b30fb67ca7712246c9722af65e4fe208c2f4546e2882297d53293c\n   languageName: node\n   linkType: hard\n \n@@ -19003,6 +19016,13 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n+\"@types/qs@npm:^6.9.15\":\n+  version: 6.9.15\n+  resolution: \"@types/qs@npm:6.9.15\"\n+  checksum: 97d8208c2b82013b618e7a9fc14df6bd40a73e1385ac479b6896bafc7949a46201c15f42afd06e86a05e914f146f495f606b6fb65610cc60cf2e0ff743ec38a2\n+  languageName: node\n+  linkType: hard\n+\n \"@types/range-parser@npm:*\":\n   version: 1.2.4\n   resolution: \"@types/range-parser@npm:1.2.4\"\n@@ -32192,7 +32212,7 @@ __metadata:\n     \"@aws-sdk/types\": ^3.357.0\n     \"@azure/storage-blob\": ^12.15.0\n     \"@browserbasehq/sdk\": ^1.1.5\n-    \"@cloudflare/workers-types\": ^4.20230922.0\n+    \"@cloudflare/workers-types\": ^4.20240909.0\n     \"@faker-js/faker\": ^7.6.0\n     \"@gomomento/sdk\": ^1.51.1\n     \"@gomomento/sdk-core\": ^1.51.1\n@@ -35228,25 +35248,27 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n-\"openai@npm:^4.55.0\":\n-  version: 4.55.0\n-  resolution: \"openai@npm:4.55.0\"\n+\"openai@npm:^4.57.3\":\n+  version: 4.57.3\n+  resolution: \"openai@npm:4.57.3\"\n   dependencies:\n     \"@types/node\": ^18.11.18\n     \"@types/node-fetch\": ^2.6.4\n+    \"@types/qs\": ^6.9.15\n     abort-controller: ^3.0.0\n     agentkeepalive: ^4.2.1\n     form-data-encoder: 1.7.2\n     formdata-node: ^4.3.2\n     node-fetch: ^2.6.7\n+    qs: ^6.10.3\n   peerDependencies:\n     zod: ^3.23.8\n   peerDependenciesMeta:\n     zod:\n       optional: true\n   bin:\n     openai: bin/cli\n-  checksum: b2b1daa976516262e08e182ee982976a1dc615eebd250bbd71f4122740ebeeb207a20af6d35c718b67f1c3457196b524667a0c7fa417ab4e119020b5c1f5cd74\n+  checksum: 6e8cef99975af5fd8e9a06685f05396a6fabecda38bd77fa62db4b7ea9bdfa0b4c762c5f74e99e42212af81f74f50748c5034bf78c9abcf74cc6eb984f3dcffa\n   languageName: node\n   linkType: hard\n \n@@ -37361,6 +37383,15 @@ __metadata:\n   languageName: node\n   linkType: hard\n \n+\"qs@npm:^6.10.3\":\n+  version: 6.13.0\n+  resolution: \"qs@npm:6.13.0\"\n+  dependencies:\n+    side-channel: ^1.0.6\n+  checksum: e9404dc0fc2849245107108ce9ec2766cde3be1b271de0bf1021d049dc5b98d1a2901e67b431ac5509f865420a7ed80b7acb3980099fe1c118a1c5d2e1432ad8\n+  languageName: node\n+  linkType: hard\n+\n \"querystringify@npm:^2.1.1\":\n   version: 2.2.0\n   resolution: \"querystringify@npm:2.2.0\""
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N",
      "score": 6.5
    },
    "cwes": [
      {
        "cwe_id": "CWE-22",
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')"
      },
      {
        "cwe_id": "CWE-29",
        "name": "Path Traversal: '..filename'"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "hinthornw",
          "id": 13333726,
          "node_id": "MDQ6VXNlcjEzMzMzNzI2",
          "avatar_url": "https://avatars.githubusercontent.com/u/13333726?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/hinthornw",
          "html_url": "https://github.com/hinthornw",
          "followers_url": "https://api.github.com/users/hinthornw/followers",
          "following_url": "https://api.github.com/users/hinthornw/following{/other_user}",
          "gists_url": "https://api.github.com/users/hinthornw/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/hinthornw/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/hinthornw/subscriptions",
          "organizations_url": "https://api.github.com/users/hinthornw/orgs",
          "repos_url": "https://api.github.com/users/hinthornw/repos",
          "events_url": "https://api.github.com/users/hinthornw/events{/privacy}",
          "received_events_url": "https://api.github.com/users/hinthornw/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N",
        "score": 6.5
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:L/VI:L/VA:N/SC:N/SI:N/SA:N",
        "score": 6.9
      }
    },
    "epss": {
      "percentage": 0.00125,
      "percentile": 0.48072
    },
    "cve_description": "A path traversal vulnerability exists in the `getFullPath` method of langchain-ai/langchainjs version 0.2.5. This vulnerability allows attackers to save files anywhere in the filesystem, overwrite existing text files, read `.txt` files, and delete files. The vulnerability is exploited through the `setFileContent`, `getParsedFile`, and `mdelete` methods, which do not properly sanitize user input."
  },
  {
    "ghsa_id": "GHSA-3phv-83cj-p8p7",
    "cve_id": "CVE-2020-26309",
    "url": "https://api.github.com/advisories/GHSA-3phv-83cj-p8p7",
    "html_url": "https://github.com/advisories/GHSA-3phv-83cj-p8p7",
    "summary": "nope-validator Regular Expression Denial of Service vulnerability",
    "description": "Nope is a JavaScript validator. Versions 0.11.3 and prior contain one or more regular expressions that are vulnerable to Regular Expression Denial of Service (ReDoS). This vulnerability is fixed in 0.12.1.",
    "type": "reviewed",
    "severity": "medium",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/ftonato/nope-validator",
    "identifiers": [
      {
        "value": "GHSA-3phv-83cj-p8p7",
        "type": "GHSA"
      },
      {
        "value": "CVE-2020-26309",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2020-26309",
      "https://github.com/ftonato/nope-validator/issues/352",
      "https://securitylab.github.com/advisories/GHSL-2020-303-redos-nope-validator",
      "https://github.com/ftonato/nope-validator/commit/4564b7444dcd92769e5c5b80420469c9f18b7a05#diff-9c399c46fa266bcf2be2704fbb369181726959e148e95ab548a32ef9ca9e7d47R1",
      "https://github.com/ftonato/nope-validator/commit/c8af9f93abe8f4786f8f69d2b0518f8ca3652f44",
      "https://github.com/advisories/GHSA-3phv-83cj-p8p7"
    ],
    "published_at": "2024-10-26T21:30:46Z",
    "updated_at": "2024-10-28T14:56:39Z",
    "github_reviewed_at": "2024-10-28T14:56:39Z",
    "nvd_published_at": "2024-10-26T21:15:14Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "nope-validator"
        },
        "vulnerable_version_range": "< 0.12.1",
        "first_patched_version": "0.12.1",
        "vulnerable_functions": [],
        "vulnerable_version": "0.12.0",
        "patches": {
          ".github/workflows/codeql-analysis.yml": "@@ -0,0 +1,34 @@\n+name: \"CodeQL\"\n+\n+on:\n+  push:\n+    branches: [ master ]\n+  pull_request:\n+    branches: [ master ]\n+  schedule:\n+    - cron: '20 0 * * 0'\n+\n+jobs:\n+  analyze:\n+    name: Analyze\n+    runs-on: ubuntu-latest\n+\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        language: [ 'javascript' ]\n+\n+    steps:\n+    - name: Checkout repository\n+      uses: actions/checkout@v2\n+\n+    - name: Initialize CodeQL\n+      uses: github/codeql-action/init@v1\n+      with:\n+        languages: ${{ matrix.language }}\n+\n+    - name: Autobuild\n+      uses: github/codeql-action/autobuild@v1\n+\n+    - name: Perform CodeQL Analysis\n+      uses: github/codeql-action/analyze@v1",
          "CHANGELOG": "@@ -1,3 +1,8 @@\n+0.12.1\n+\n+Fixes 🔨\n+- URL and e-mail regex security issues\n+\n 0.12.0\n \n Added 🚀",
          "package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"nope-validator\",\n-  \"version\": \"0.12.0\",\n+  \"version\": \"0.12.1\",\n   \"main\": \"lib/umd/index.js\",\n   \"module\": \"lib/es2015/index.js\",\n   \"types\": \"lib/umd/index.d.ts\",",
          "src/__tests__/NopeString.spec.ts": "@@ -7,9 +7,7 @@ describe('#NopeString', () => {\n     });\n \n     it('should return an error message for an invalid entry', () => {\n-      expect(Nope.string().regex(/abc/i, 'errorMessage').validate('http:google.com')).toBe(\n-        'errorMessage',\n-      );\n+      expect(Nope.string().regex(/abc/i, 'errorMessage').validate('defg')).toBe('errorMessage');\n     });\n \n     it('should return undefined for an valid entry', () => {\n@@ -23,11 +21,29 @@ describe('#NopeString', () => {\n     });\n \n     it('should return an error message for an invalid URL', () => {\n-      expect(Nope.string().url().validate('http:google.com')).toBe('Input is not a valid url');\n+      const invalidUrls = [\n+        'http://:google.com',\n+        'http://',\n+        // 'http:///a',\n+        'http://foo.bar/foo(bar)baz quux',\n+      ];\n+      for (const url of invalidUrls) {\n+        expect(Nope.string().url().validate(url)).toBe('Input is not a valid url');\n+      }\n     });\n \n     it('should return undefined for an valid URL', () => {\n-      expect(Nope.string().url('urlErrorMessage').validate('https://google.com')).toBe(undefined);\n+      const validUrls = [\n+        'https://github.com/bvego/nope-validator/commit/4564b7444dcd92769e5c5b80420469c9f18b7a05?branch=4564b7444dcd92769e5c5b80420469c9f18b7a05&diff=split',\n+        'https://google.com',\n+        'https://google.com?asd=123',\n+        'https://google.com/123',\n+        'https://google.com/123/456?q=42',\n+      ];\n+\n+      for (const url of validUrls) {\n+        expect(Nope.string().url('urlErrorMessage').validate(url)).toBe(undefined);\n+      }\n     });\n   });\n \n@@ -37,15 +53,21 @@ describe('#NopeString', () => {\n     });\n \n     it('should return an error message for an invalid email', () => {\n-      expect(Nope.string().email().validate('bruno.vegogmail.com')).toBe(\n-        'Input is not a valid email',\n-      );\n+      const ns = () => Nope.string();\n+\n+      const ERR_MSG = 'err';\n+\n+      expect(ns().email(ERR_MSG).validate('bruno.vegogmail.com')).toBe(ERR_MSG);\n+      expect(ns().email(ERR_MSG).validate('bruno.vego.gmail.com')).toBe(ERR_MSG);\n+      expect(ns().email(ERR_MSG).validate('bruno.vego@gmail.com@')).toBe(ERR_MSG);\n     });\n \n     it('should return undefined for an valid email', () => {\n-      expect(Nope.string().email('emailErrorMessage').validate('bruno.vego@gmail.com')).toBe(\n-        undefined,\n-      );\n+      const ns = () => Nope.string();\n+\n+      expect(ns().email().validate('bruno.vego@gmail.com')).toBe(undefined);\n+      expect(ns().email().validate('random-guy@google.com')).toBe(undefined);\n+      expect(ns().email().validate('random-guy+test@google.com')).toBe(undefined);\n     });\n   });\n ",
          "src/consts.ts": "@@ -1,3 +1,3 @@\n-export const emailRegex = /^(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])$/i;\n+export const emailRegex = /^(([^<>()[\\]\\.,;:\\s@\\\"]+(\\.[^<>()[\\]\\.,;:\\s@\\\"]+)*)|(\\\".+\\\"))@(([^<>()[\\]\\.,;:\\s@\\\"]+\\.)+[^<>()[\\]\\.,;:\\s@\\\"]{2,})$/i;\n \n-export const urlRegex = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i;\n+export const urlRegex = /^([a-z][a-z0-9\\*\\-\\.]*):\\/\\/(?:(?:(?:[\\w\\.\\-\\+!$&'\\(\\)*\\+,;=]|%[0-9a-f]{2})+:)*(?:[\\w\\.\\-\\+%!$&'\\(\\)*\\+,;=]|%[0-9a-f]{2})+@)?(?:(?:[a-z0-9\\-\\.]|%[0-9a-f]{2})+|(?:\\[(?:[0-9a-f]{0,4}:)*(?:[0-9a-f]{0,4})\\]))(?::[0-9]+)?(?:[\\/|\\?](?:[\\w#!:\\.\\?\\+=&@!$'~*,;\\/\\(\\)\\[\\]\\-]|%[0-9a-f]{2})*)?$/i;"
        }
      }
    ],
    "cvss": {
      "vector_string": null,
      "score": null
    },
    "cwes": [
      {
        "cwe_id": "CWE-1333",
        "name": "Inefficient Regular Expression Complexity"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": null,
        "score": 0
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:N/VI:N/VA:H/SC:N/SI:N/SA:N/E:U/U:Green",
        "score": 6.6
      }
    },
    "epss": {
      "percentage": 0.00043,
      "percentile": 0.09833
    },
    "cve_description": "Validate.js provides a declarative way of validating javascript objects. Versions 0.11.3 and prior contain one or more regular expressions that are vulnerable to Regular Expression Denial of Service (ReDoS). As of time of publication, it is unknown if any patches are available."
  },
  {
    "ghsa_id": "GHSA-fq9m-v26v-2m4f",
    "cve_id": "CVE-2024-21537",
    "url": "https://api.github.com/advisories/GHSA-fq9m-v26v-2m4f",
    "html_url": "https://github.com/advisories/GHSA-fq9m-v26v-2m4f",
    "summary": "lilconfig Code Injection vulnerability",
    "description": "Versions of the package lilconfig from 3.1.0 and before 3.1.1 are vulnerable to Arbitrary Code Execution due to the insecure usage of eval in the dynamicImport function. An attacker can exploit this vulnerability by passing a malicious input through the defaultLoaders function.",
    "type": "reviewed",
    "severity": "high",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/antonk52/lilconfig",
    "identifiers": [
      {
        "value": "GHSA-fq9m-v26v-2m4f",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-21537",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-21537",
      "https://github.com/antonk52/lilconfig/pull/48",
      "https://github.com/antonk52/lilconfig/commit/2c68a1ab8764fc74acc46771e1ad39ab07a9b0a7",
      "https://github.com/antonk52/lilconfig/releases/tag/v3.1.1",
      "https://security.snyk.io/vuln/SNYK-JS-LILCONFIG-6263789",
      "https://github.com/advisories/GHSA-fq9m-v26v-2m4f"
    ],
    "published_at": "2024-10-31T06:30:45Z",
    "updated_at": "2024-11-01T21:39:33Z",
    "github_reviewed_at": "2024-11-01T21:39:32Z",
    "nvd_published_at": "2024-10-31T05:15:04Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "lilconfig"
        },
        "vulnerable_version_range": ">= 3.1.0, < 3.1.1",
        "first_patched_version": "3.1.1",
        "vulnerable_functions": [],
        "vulnerable_version": "3.1.0",
        "patches": {
          ".github/workflows/nodejs.yaml": "@@ -4,7 +4,7 @@ on: [push]\n \n jobs:\n \n-  build:\n+  types:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n@@ -17,8 +17,8 @@ jobs:\n         node-version: ${{ matrix.node-version }}\n     - name: install dependencies\n       run: npm ci\n-    - name: build\n-      run: npm run build\n+    - name: types\n+      run: npm run types\n \n   lint:\n     runs-on: ubuntu-latest\n@@ -66,7 +66,7 @@ jobs:\n     - name: install dependencies\n       run: npm ci\n     - name: old node tests\n-      run: npx ts-node src/spec/old-node-tests.uvu.ts\n+      run: node src/spec/old-node-tests.uvu.js\n \n   coveralls:\n     runs-on: ubuntu-latest",
          "eslint.config.js": "@@ -1,24 +1,14 @@\n const eslint = require('@eslint/js');\n-const tseslint = require('typescript-eslint');\n+const prettierPlugin = require('eslint-plugin-prettier/recommended');\n \n-module.exports = tseslint.config(\n-    eslint.configs.recommended,\n-    ...tseslint.configs.recommended,\n-    {\n-        rules: {\n-            '@typescript-eslint/no-explicit-any': 'off',\n-            '@typescript-eslint/ban-ts-comment': [\n-                'error',\n-                {\n-                    'ts-expect-error': 'allow-with-description',\n-                    'ts-ignore': true,\n-                    'ts-nocheck': true,\n-                    'ts-check': false,\n-                    minimumDescriptionLength: 3,\n-                },\n-            ],\n-            'no-constant-condition': 'off',\n-            'prefer-const': 'error',\n-        },\n-    },\n-);\n+module.exports = [\n+\teslint.configs.recommended,\n+\tprettierPlugin,\n+\t{\n+\t\trules: {\n+\t\t\t'no-undef': 'off',\n+\t\t\t'no-constant-condition': 'off',\n+\t\t\t'prefer-const': 'error',\n+\t\t},\n+\t},\n+];",
          "jest.config.js": "@@ -1,16 +1,15 @@\n module.exports = {\n-    roots: ['<rootDir>/src'],\n-    preset: 'ts-jest',\n-    moduleNameMapper: {\n-        '^@/(.*)$': '<rootDir>/src/$1',\n-    },\n-    collectCoverageFrom: ['./src/index.ts'],\n-    coverageThreshold: {\n-        global: {\n-            branches: 97,\n-            functions: 99,\n-            lines: 99,\n-            statements: 99,\n-        },\n-    },\n+\troots: ['<rootDir>/src'],\n+\tmoduleNameMapper: {\n+\t\t'^@/(.*)$': '<rootDir>/src/$1',\n+\t},\n+\tcollectCoverageFrom: ['./src/index.js'],\n+\tcoverageThreshold: {\n+\t\tglobal: {\n+\t\t\tbranches: 92,\n+\t\t\tfunctions: 99,\n+\t\t\tlines: 99,\n+\t\t\tstatements: 99,\n+\t\t},\n+\t},\n };",
          "package.json": "@@ -1,49 +1,44 @@\n {\n-    \"name\": \"lilconfig\",\n-    \"version\": \"3.1.0\",\n-    \"description\": \"A zero-dependency alternative to cosmiconfig\",\n-    \"main\": \"dist/index.js\",\n-    \"types\": \"dist/index.d.ts\",\n-    \"scripts\": {\n-        \"prebuild\": \"npm run clean\",\n-        \"build\": \"tsc --declaration\",\n-        \"postbuild\": \"du -h ./dist/*\",\n-        \"clean\": \"rm -rf ./dist\",\n-        \"test\": \"NODE_OPTIONS=--experimental-vm-modules ./node_modules/.bin/jest --coverage\",\n-        \"lint\": \"eslint ./src/*.ts\"\n-    },\n-    \"keywords\": [\n-        \"cosmiconfig\",\n-        \"config\",\n-        \"configuration\",\n-        \"search\"\n-    ],\n-    \"files\": [\n-        \"dist/*\"\n-    ],\n-    \"repository\": {\n-        \"type\": \"git\",\n-        \"url\": \"https://github.com/antonk52/lilconfig\"\n-    },\n-    \"bugs\": \"https://github.com/antonk52/lilconfig/issues\",\n-    \"author\": \"antonk52\",\n-    \"license\": \"MIT\",\n-    \"devDependencies\": {\n-        \"@types/jest\": \"^29.5.12\",\n-        \"@types/node\": \"^14.18.63\",\n-        \"cosmiconfig\": \"^8.3.6\",\n-        \"eslint\": \"^8.56.0\",\n-        \"eslint-config-prettier\": \"^9.1.0\",\n-        \"eslint-plugin-prettier\": \"^5.1.3\",\n-        \"jest\": \"^29.7.0\",\n-        \"prettier\": \"^3.2.5\",\n-        \"ts-jest\": \"29.1.2\",\n-        \"typescript\": \"^5.3.3\",\n-        \"typescript-eslint\": \"^7.0.1\",\n-        \"uvu\": \"^0.5.6\"\n-    },\n-    \"funding\": \"https://github.com/sponsors/antonk52\",\n-    \"engines\": {\n-        \"node\": \">=14\"\n-    }\n+  \"name\": \"lilconfig\",\n+  \"version\": \"3.1.1\",\n+  \"description\": \"A zero-dependency alternative to cosmiconfig\",\n+  \"main\": \"src/index.js\",\n+  \"types\": \"src/index.d.ts\",\n+  \"scripts\": {\n+    \"test\": \"NODE_OPTIONS=--experimental-vm-modules ./node_modules/.bin/jest --coverage\",\n+    \"lint\": \"eslint ./src\",\n+    \"types\": \"tsc\"\n+  },\n+  \"keywords\": [\n+    \"cosmiconfig\",\n+    \"config\",\n+    \"configuration\",\n+    \"search\"\n+  ],\n+  \"files\": [\n+    \"src/index.*\"\n+  ],\n+  \"repository\": {\n+    \"type\": \"git\",\n+    \"url\": \"https://github.com/antonk52/lilconfig\"\n+  },\n+  \"bugs\": \"https://github.com/antonk52/lilconfig/issues\",\n+  \"author\": \"antonk52\",\n+  \"license\": \"MIT\",\n+  \"devDependencies\": {\n+    \"@types/jest\": \"^29.5.12\",\n+    \"@types/node\": \"^14.18.63\",\n+    \"cosmiconfig\": \"^8.3.6\",\n+    \"eslint\": \"^8.56.0\",\n+    \"eslint-config-prettier\": \"^9.1.0\",\n+    \"eslint-plugin-prettier\": \"^5.1.3\",\n+    \"jest\": \"^29.7.0\",\n+    \"prettier\": \"^3.2.5\",\n+    \"typescript\": \"^5.3.3\",\n+    \"uvu\": \"^0.5.6\"\n+  },\n+  \"funding\": \"https://github.com/sponsors/antonk52\",\n+  \"engines\": {\n+    \"node\": \">=14\"\n+  }\n }",
          "prettier.config.js": "@@ -1,7 +1,8 @@\n+/** @type {import('prettier').Options} */\n module.exports = {\n-    arrowParens: 'avoid',\n-    bracketSpacing: false,\n-    singleQuote: true,\n-    tabWidth: 4,\n-    trailingComma: 'all',\n+\tuseTabs: true,\n+\tarrowParens: 'avoid',\n+\tbracketSpacing: false,\n+\tsingleQuote: true,\n+\ttrailingComma: 'all',\n };",
          "src/index.d.ts": "@@ -0,0 +1,54 @@\n+export type LilconfigResult = null | {\n+\tfilepath: string;\n+\tconfig: any;\n+\tisEmpty?: boolean;\n+};\n+interface OptionsBase {\n+\tcache?: boolean;\n+\tstopDir?: string;\n+\tsearchPlaces?: string[];\n+\tignoreEmptySearchPlaces?: boolean;\n+\tpackageProp?: string | string[];\n+}\n+export type Transform =\n+\t| TransformSync\n+\t| ((result: LilconfigResult) => Promise<LilconfigResult>);\n+export type TransformSync = (result: LilconfigResult) => LilconfigResult;\n+type LoaderResult = any;\n+export type LoaderSync = (filepath: string, content: string) => LoaderResult;\n+export type Loader =\n+\t| LoaderSync\n+\t| ((filepath: string, content: string) => Promise<LoaderResult>);\n+export type Loaders = Record<string, Loader>;\n+export type LoadersSync = Record<string, LoaderSync>;\n+export interface Options extends OptionsBase {\n+\tloaders?: Loaders;\n+\ttransform?: Transform;\n+}\n+export interface OptionsSync extends OptionsBase {\n+\tloaders?: LoadersSync;\n+\ttransform?: TransformSync;\n+}\n+export declare const defaultLoadersSync: LoadersSync;\n+export declare const defaultLoaders: Loaders;\n+type ClearCaches = {\n+\tclearLoadCache: () => void;\n+\tclearSearchCache: () => void;\n+\tclearCaches: () => void;\n+};\n+type AsyncSearcher = {\n+\tsearch(searchFrom?: string): Promise<LilconfigResult>;\n+\tload(filepath: string): Promise<LilconfigResult>;\n+} & ClearCaches;\n+export declare function lilconfig(\n+\tname: string,\n+\toptions?: Partial<Options>,\n+): AsyncSearcher;\n+type SyncSearcher = {\n+\tsearch(searchFrom?: string): LilconfigResult;\n+\tload(filepath: string): LilconfigResult;\n+} & ClearCaches;\n+export declare function lilconfigSync(\n+\tname: string,\n+\toptions?: OptionsSync,\n+): SyncSearcher;",
          "src/index.js": "@@ -0,0 +1,455 @@\n+// @ts-check\n+const path = require('path');\n+const fs = require('fs');\n+const os = require('os');\n+\n+const fsReadFileAsync = fs.promises.readFile;\n+\n+/** @type {(name: string, sync: boolean) => string[]} */\n+function getDefaultSearchPlaces(name, sync) {\n+\treturn [\n+\t\t'package.json',\n+\t\t`.${name}rc.json`,\n+\t\t`.${name}rc.js`,\n+\t\t`.${name}rc.cjs`,\n+\t\t...(sync ? [] : [`.${name}rc.mjs`]),\n+\t\t`.config/${name}rc`,\n+\t\t`.config/${name}rc.json`,\n+\t\t`.config/${name}rc.js`,\n+\t\t`.config/${name}rc.cjs`,\n+\t\t...(sync ? [] : [`.config/${name}rc.mjs`]),\n+\t\t`${name}.config.js`,\n+\t\t`${name}.config.cjs`,\n+\t\t...(sync ? [] : [`${name}.config.mjs`]),\n+\t];\n+}\n+\n+/**\n+ * @type {(p: string) => string}\n+ *\n+ * see #17\n+ * On *nix, if cwd is not under homedir,\n+ * the last path will be '', ('/build' -> '')\n+ * but it should be '/' actually.\n+ * And on Windows, this will never happen. ('C:\\build' -> 'C:')\n+ */\n+function parentDir(p) {\n+\treturn path.dirname(p) || path.sep;\n+}\n+\n+/** @type {import('./index').LoaderSync} */\n+const jsonLoader = (_, content) => JSON.parse(content);\n+/** @type {import('./index').LoadersSync} */\n+const defaultLoadersSync = Object.freeze({\n+\t'.js': require,\n+\t'.json': require,\n+\t'.cjs': require,\n+\tnoExt: jsonLoader,\n+});\n+module.exports.defaultLoadersSync = defaultLoadersSync;\n+\n+/** @type {import('./index').Loader} */\n+const dynamicImport = async id => {\n+\ttry {\n+\t\tconst mod = await import(id);\n+\n+\t\treturn mod.default;\n+\t} catch (e) {\n+\t\ttry {\n+\t\t\treturn require(id);\n+\t\t} catch (/** @type {any} */ requireE) {\n+\t\t\tif (\n+\t\t\t\trequireE.code === 'ERR_REQUIRE_ESM' ||\n+\t\t\t\t(requireE instanceof SyntaxError &&\n+\t\t\t\t\trequireE\n+\t\t\t\t\t\t.toString()\n+\t\t\t\t\t\t.includes('Cannot use import statement outside a module'))\n+\t\t\t) {\n+\t\t\t\tthrow e;\n+\t\t\t}\n+\t\t\tthrow requireE;\n+\t\t}\n+\t}\n+};\n+\n+/** @type {import('./index').Loaders} */\n+const defaultLoaders = Object.freeze({\n+\t'.js': dynamicImport,\n+\t'.mjs': dynamicImport,\n+\t'.cjs': dynamicImport,\n+\t'.json': jsonLoader,\n+\tnoExt: jsonLoader,\n+});\n+module.exports.defaultLoaders = defaultLoaders;\n+\n+/**\n+ * @param {string} name\n+ * @param {import('./index').Options | import('./index').OptionsSync} options\n+ * @param {boolean} sync\n+ * @returns {Required<import('./index').Options | import('./index').OptionsSync>}\n+ */\n+function getOptions(name, options, sync) {\n+\t/** @type {Required<import('./index').Options>} */\n+\tconst conf = {\n+\t\tstopDir: os.homedir(),\n+\t\tsearchPlaces: getDefaultSearchPlaces(name, sync),\n+\t\tignoreEmptySearchPlaces: true,\n+\t\tcache: true,\n+\t\ttransform: x => x,\n+\t\tpackageProp: [name],\n+\t\t...options,\n+\t\tloaders: {\n+\t\t\t...(sync ? defaultLoadersSync : defaultLoaders),\n+\t\t\t...options.loaders,\n+\t\t},\n+\t};\n+\tconf.searchPlaces.forEach(place => {\n+\t\tconst key = path.extname(place) || 'noExt';\n+\t\tconst loader = conf.loaders[key];\n+\t\tif (!loader) {\n+\t\t\tthrow new Error(`Missing loader for extension \"${place}\"`);\n+\t\t}\n+\n+\t\tif (typeof loader !== 'function') {\n+\t\t\tthrow new Error(\n+\t\t\t\t`Loader for extension \"${place}\" is not a function: Received ${typeof loader}.`,\n+\t\t\t);\n+\t\t}\n+\t});\n+\n+\treturn conf;\n+}\n+\n+/** @type {(props: string | string[], obj: Record<string, any>) => unknown} */\n+function getPackageProp(props, obj) {\n+\tif (typeof props === 'string' && props in obj) return obj[props];\n+\treturn (\n+\t\t(Array.isArray(props) ? props : props.split('.')).reduce(\n+\t\t\t(acc, prop) => (acc === undefined ? acc : acc[prop]),\n+\t\t\tobj,\n+\t\t) || null\n+\t);\n+}\n+\n+/** @param {string} filepath */\n+function validateFilePath(filepath) {\n+\tif (!filepath) throw new Error('load must pass a non-empty string');\n+}\n+\n+/** @type {(loader: import('./index').Loader, ext: string) => void} */\n+function validateLoader(loader, ext) {\n+\tif (!loader) throw new Error(`No loader specified for extension \"${ext}\"`);\n+\tif (typeof loader !== 'function') throw new Error('loader is not a function');\n+}\n+\n+/** @type {(enableCache: boolean) => <T>(c: Map<string, T>, filepath: string, res: T) => T} */\n+const makeEmplace = enableCache => (c, filepath, res) => {\n+\tif (enableCache) c.set(filepath, res);\n+\treturn res;\n+};\n+\n+/** @type {import('./index').lilconfig} */\n+module.exports.lilconfig = function lilconfig(name, options) {\n+\tconst {\n+\t\tignoreEmptySearchPlaces,\n+\t\tloaders,\n+\t\tpackageProp,\n+\t\tsearchPlaces,\n+\t\tstopDir,\n+\t\ttransform,\n+\t\tcache,\n+\t} = getOptions(name, options ?? {}, false);\n+\tconst searchCache = new Map();\n+\tconst loadCache = new Map();\n+\tconst emplace = makeEmplace(cache);\n+\n+\treturn {\n+\t\tasync search(searchFrom = process.cwd()) {\n+\t\t\t/** @type {import('./index').LilconfigResult} */\n+\t\t\tconst result = {\n+\t\t\t\tconfig: null,\n+\t\t\t\tfilepath: '',\n+\t\t\t};\n+\n+\t\t\t/** @type {Set<string>} */\n+\t\t\tconst visited = new Set();\n+\t\t\tlet dir = searchFrom;\n+\t\t\tdirLoop: while (true) {\n+\t\t\t\tif (cache) {\n+\t\t\t\t\tconst r = searchCache.get(dir);\n+\t\t\t\t\tif (r !== undefined) {\n+\t\t\t\t\t\tfor (const p of visited) searchCache.set(p, r);\n+\t\t\t\t\t\treturn r;\n+\t\t\t\t\t}\n+\t\t\t\t\tvisited.add(dir);\n+\t\t\t\t}\n+\n+\t\t\t\tfor (const searchPlace of searchPlaces) {\n+\t\t\t\t\tconst filepath = path.join(dir, searchPlace);\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tawait fs.promises.access(filepath);\n+\t\t\t\t\t} catch {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\tconst content = String(await fsReadFileAsync(filepath));\n+\t\t\t\t\tconst loaderKey = path.extname(searchPlace) || 'noExt';\n+\t\t\t\t\tconst loader = loaders[loaderKey];\n+\n+\t\t\t\t\t// handle package.json\n+\t\t\t\t\tif (searchPlace === 'package.json') {\n+\t\t\t\t\t\tconst pkg = await loader(filepath, content);\n+\t\t\t\t\t\tconst maybeConfig = getPackageProp(packageProp, pkg);\n+\t\t\t\t\t\tif (maybeConfig != null) {\n+\t\t\t\t\t\t\tresult.config = maybeConfig;\n+\t\t\t\t\t\t\tresult.filepath = filepath;\n+\t\t\t\t\t\t\tbreak dirLoop;\n+\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\t// handle other type of configs\n+\t\t\t\t\tconst isEmpty = content.trim() === '';\n+\t\t\t\t\tif (isEmpty && ignoreEmptySearchPlaces) continue;\n+\n+\t\t\t\t\tif (isEmpty) {\n+\t\t\t\t\t\tresult.isEmpty = true;\n+\t\t\t\t\t\tresult.config = undefined;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvalidateLoader(loader, loaderKey);\n+\t\t\t\t\t\tresult.config = await loader(filepath, content);\n+\t\t\t\t\t}\n+\t\t\t\t\tresult.filepath = filepath;\n+\t\t\t\t\tbreak dirLoop;\n+\t\t\t\t}\n+\t\t\t\tif (dir === stopDir || dir === parentDir(dir)) break dirLoop;\n+\t\t\t\tdir = parentDir(dir);\n+\t\t\t}\n+\n+\t\t\tconst transformed =\n+\t\t\t\t// not found\n+\t\t\t\tresult.filepath === '' && result.config === null\n+\t\t\t\t\t? transform(null)\n+\t\t\t\t\t: transform(result);\n+\n+\t\t\tif (cache) {\n+\t\t\t\tfor (const p of visited) searchCache.set(p, transformed);\n+\t\t\t}\n+\n+\t\t\treturn transformed;\n+\t\t},\n+\t\tasync load(filepath) {\n+\t\t\tvalidateFilePath(filepath);\n+\t\t\tconst absPath = path.resolve(process.cwd(), filepath);\n+\t\t\tif (cache && loadCache.has(absPath)) {\n+\t\t\t\treturn loadCache.get(absPath);\n+\t\t\t}\n+\t\t\tconst {base, ext} = path.parse(absPath);\n+\t\t\tconst loaderKey = ext || 'noExt';\n+\t\t\tconst loader = loaders[loaderKey];\n+\t\t\tvalidateLoader(loader, loaderKey);\n+\t\t\tconst content = String(await fsReadFileAsync(absPath));\n+\n+\t\t\tif (base === 'package.json') {\n+\t\t\t\tconst pkg = await loader(absPath, content);\n+\t\t\t\treturn emplace(\n+\t\t\t\t\tloadCache,\n+\t\t\t\t\tabsPath,\n+\t\t\t\t\ttransform({\n+\t\t\t\t\t\tconfig: getPackageProp(packageProp, pkg),\n+\t\t\t\t\t\tfilepath: absPath,\n+\t\t\t\t\t}),\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t/** @type {import('./index').LilconfigResult} */\n+\t\t\tconst result = {\n+\t\t\t\tconfig: null,\n+\t\t\t\tfilepath: absPath,\n+\t\t\t};\n+\t\t\t// handle other type of configs\n+\t\t\tconst isEmpty = content.trim() === '';\n+\t\t\tif (isEmpty && ignoreEmptySearchPlaces)\n+\t\t\t\treturn emplace(\n+\t\t\t\t\tloadCache,\n+\t\t\t\t\tabsPath,\n+\t\t\t\t\ttransform({\n+\t\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\t\tfilepath: absPath,\n+\t\t\t\t\t\tisEmpty: true,\n+\t\t\t\t\t}),\n+\t\t\t\t);\n+\n+\t\t\t// cosmiconfig returns undefined for empty files\n+\t\t\tresult.config = isEmpty ? undefined : await loader(absPath, content);\n+\n+\t\t\treturn emplace(\n+\t\t\t\tloadCache,\n+\t\t\t\tabsPath,\n+\t\t\t\ttransform(isEmpty ? {...result, isEmpty, config: undefined} : result),\n+\t\t\t);\n+\t\t},\n+\t\tclearLoadCache() {\n+\t\t\tif (cache) loadCache.clear();\n+\t\t},\n+\t\tclearSearchCache() {\n+\t\t\tif (cache) searchCache.clear();\n+\t\t},\n+\t\tclearCaches() {\n+\t\t\tif (cache) {\n+\t\t\t\tloadCache.clear();\n+\t\t\t\tsearchCache.clear();\n+\t\t\t}\n+\t\t},\n+\t};\n+};\n+\n+/** @type {import('./index').lilconfigSync} */\n+module.exports.lilconfigSync = function lilconfigSync(name, options) {\n+\tconst {\n+\t\tignoreEmptySearchPlaces,\n+\t\tloaders,\n+\t\tpackageProp,\n+\t\tsearchPlaces,\n+\t\tstopDir,\n+\t\ttransform,\n+\t\tcache,\n+\t} = getOptions(name, options ?? {}, true);\n+\tconst searchCache = new Map();\n+\tconst loadCache = new Map();\n+\tconst emplace = makeEmplace(cache);\n+\n+\treturn {\n+\t\tsearch(searchFrom = process.cwd()) {\n+\t\t\t/** @type {import('./index').LilconfigResult} */\n+\t\t\tconst result = {\n+\t\t\t\tconfig: null,\n+\t\t\t\tfilepath: '',\n+\t\t\t};\n+\n+\t\t\t/** @type {Set<string>} */\n+\t\t\tconst visited = new Set();\n+\t\t\tlet dir = searchFrom;\n+\t\t\tdirLoop: while (true) {\n+\t\t\t\tif (cache) {\n+\t\t\t\t\tconst r = searchCache.get(dir);\n+\t\t\t\t\tif (r !== undefined) {\n+\t\t\t\t\t\tfor (const p of visited) searchCache.set(p, r);\n+\t\t\t\t\t\treturn r;\n+\t\t\t\t\t}\n+\t\t\t\t\tvisited.add(dir);\n+\t\t\t\t}\n+\n+\t\t\t\tfor (const searchPlace of searchPlaces) {\n+\t\t\t\t\tconst filepath = path.join(dir, searchPlace);\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfs.accessSync(filepath);\n+\t\t\t\t\t} catch {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\tconst loaderKey = path.extname(searchPlace) || 'noExt';\n+\t\t\t\t\tconst loader = loaders[loaderKey];\n+\t\t\t\t\tconst content = String(fs.readFileSync(filepath));\n+\n+\t\t\t\t\t// handle package.json\n+\t\t\t\t\tif (searchPlace === 'package.json') {\n+\t\t\t\t\t\tconst pkg = loader(filepath, content);\n+\t\t\t\t\t\tconst maybeConfig = getPackageProp(packageProp, pkg);\n+\t\t\t\t\t\tif (maybeConfig != null) {\n+\t\t\t\t\t\t\tresult.config = maybeConfig;\n+\t\t\t\t\t\t\tresult.filepath = filepath;\n+\t\t\t\t\t\t\tbreak dirLoop;\n+\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\t// handle other type of configs\n+\t\t\t\t\tconst isEmpty = content.trim() === '';\n+\t\t\t\t\tif (isEmpty && ignoreEmptySearchPlaces) continue;\n+\n+\t\t\t\t\tif (isEmpty) {\n+\t\t\t\t\t\tresult.isEmpty = true;\n+\t\t\t\t\t\tresult.config = undefined;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvalidateLoader(loader, loaderKey);\n+\t\t\t\t\t\tresult.config = loader(filepath, content);\n+\t\t\t\t\t}\n+\t\t\t\t\tresult.filepath = filepath;\n+\t\t\t\t\tbreak dirLoop;\n+\t\t\t\t}\n+\t\t\t\tif (dir === stopDir || dir === parentDir(dir)) break dirLoop;\n+\t\t\t\tdir = parentDir(dir);\n+\t\t\t}\n+\n+\t\t\tconst transformed =\n+\t\t\t\t// not found\n+\t\t\t\tresult.filepath === '' && result.config === null\n+\t\t\t\t\t? transform(null)\n+\t\t\t\t\t: transform(result);\n+\n+\t\t\tif (cache) {\n+\t\t\t\tfor (const p of visited) searchCache.set(p, transformed);\n+\t\t\t}\n+\n+\t\t\treturn transformed;\n+\t\t},\n+\t\tload(filepath) {\n+\t\t\tvalidateFilePath(filepath);\n+\t\t\tconst absPath = path.resolve(process.cwd(), filepath);\n+\t\t\tif (cache && loadCache.has(absPath)) {\n+\t\t\t\treturn loadCache.get(absPath);\n+\t\t\t}\n+\t\t\tconst {base, ext} = path.parse(absPath);\n+\t\t\tconst loaderKey = ext || 'noExt';\n+\t\t\tconst loader = loaders[loaderKey];\n+\t\t\tvalidateLoader(loader, loaderKey);\n+\n+\t\t\tconst content = String(fs.readFileSync(absPath));\n+\n+\t\t\tif (base === 'package.json') {\n+\t\t\t\tconst pkg = loader(absPath, content);\n+\t\t\t\treturn transform({\n+\t\t\t\t\tconfig: getPackageProp(packageProp, pkg),\n+\t\t\t\t\tfilepath: absPath,\n+\t\t\t\t});\n+\t\t\t}\n+\t\t\tconst result = {\n+\t\t\t\tconfig: null,\n+\t\t\t\tfilepath: absPath,\n+\t\t\t};\n+\t\t\t// handle other type of configs\n+\t\t\tconst isEmpty = content.trim() === '';\n+\t\t\tif (isEmpty && ignoreEmptySearchPlaces)\n+\t\t\t\treturn emplace(\n+\t\t\t\t\tloadCache,\n+\t\t\t\t\tabsPath,\n+\t\t\t\t\ttransform({\n+\t\t\t\t\t\tfilepath: absPath,\n+\t\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\t\tisEmpty: true,\n+\t\t\t\t\t}),\n+\t\t\t\t);\n+\n+\t\t\t// cosmiconfig returns undefined for empty files\n+\t\t\tresult.config = isEmpty ? undefined : loader(absPath, content);\n+\n+\t\t\treturn emplace(\n+\t\t\t\tloadCache,\n+\t\t\t\tabsPath,\n+\t\t\t\ttransform(isEmpty ? {...result, isEmpty, config: undefined} : result),\n+\t\t\t);\n+\t\t},\n+\t\tclearLoadCache() {\n+\t\t\tif (cache) loadCache.clear();\n+\t\t},\n+\t\tclearSearchCache() {\n+\t\t\tif (cache) searchCache.clear();\n+\t\t},\n+\t\tclearCaches() {\n+\t\t\tif (cache) {\n+\t\t\t\tloadCache.clear();\n+\t\t\t\tsearchCache.clear();\n+\t\t\t}\n+\t\t},\n+\t};\n+};",
          "src/index.ts": "@@ -1,522 +0,0 @@\n-import * as path from 'path';\n-import * as fs from 'fs';\n-import * as os from 'os';\n-\n-const fsReadFileAsync = fs.promises.readFile;\n-\n-export type LilconfigResult = null | {\n-    filepath: string;\n-    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-    config: any;\n-    isEmpty?: boolean;\n-};\n-\n-interface OptionsBase {\n-    cache?: boolean;\n-    stopDir?: string;\n-    searchPlaces?: string[];\n-    ignoreEmptySearchPlaces?: boolean;\n-    packageProp?: string | string[];\n-}\n-\n-export type Transform =\n-    | TransformSync\n-    | ((result: LilconfigResult) => Promise<LilconfigResult>);\n-export type TransformSync = (result: LilconfigResult) => LilconfigResult;\n-// eslint-disable-next-line @typescript-eslint/no-explicit-any\n-type LoaderResult = any;\n-export type LoaderSync = (filepath: string, content: string) => LoaderResult;\n-export type Loader =\n-    | LoaderSync\n-    | ((filepath: string, content: string) => Promise<LoaderResult>);\n-export type Loaders = Record<string, Loader>;\n-export type LoadersSync = Record<string, LoaderSync>;\n-\n-export interface Options extends OptionsBase {\n-    loaders?: Loaders;\n-    transform?: Transform;\n-}\n-\n-export interface OptionsSync extends OptionsBase {\n-    loaders?: LoadersSync;\n-    transform?: TransformSync;\n-}\n-\n-function getDefaultSearchPlaces(name: string, sync: boolean): string[] {\n-    return [\n-        'package.json',\n-        `.${name}rc.json`,\n-        `.${name}rc.js`,\n-        `.${name}rc.cjs`,\n-        ...(sync ? [] : [`.${name}rc.mjs`]),\n-        `.config/${name}rc`,\n-        `.config/${name}rc.json`,\n-        `.config/${name}rc.js`,\n-        `.config/${name}rc.cjs`,\n-        ...(sync ? [] : [`.config/${name}rc.mjs`]),\n-        `${name}.config.js`,\n-        `${name}.config.cjs`,\n-        ...(sync ? [] : [`${name}.config.mjs`]),\n-    ];\n-}\n-\n-/**\n- * @see #17\n- * On *nix, if cwd is not under homedir,\n- * the last path will be '', ('/build' -> '')\n- * but it should be '/' actually.\n- * And on Windows, this will never happen. ('C:\\build' -> 'C:')\n- */\n-function parentDir(p: string): string {\n-    return path.dirname(p) || path.sep;\n-}\n-\n-const jsonLoader: LoaderSync = (_, content) => JSON.parse(content);\n-export const defaultLoadersSync: LoadersSync = Object.freeze({\n-    '.js': require,\n-    '.json': require,\n-    '.cjs': require,\n-    noExt: jsonLoader,\n-});\n-\n-const dynamicImport = async (id: string) => {\n-    try {\n-        // to preserve CJS output but keep dynamic import as is\n-        // https://github.com/microsoft/TypeScript/issues/43329#issuecomment-922544562\n-        const mod = await eval(`import('${id}')`);\n-\n-        return mod.default;\n-    } catch (e) {\n-        try {\n-            return require(id);\n-        } catch (requireE: any) {\n-            if (\n-                requireE.code === 'ERR_REQUIRE_ESM' ||\n-                (requireE instanceof SyntaxError &&\n-                    requireE\n-                        .toString()\n-                        .includes(\n-                            'Cannot use import statement outside a module',\n-                        ))\n-            ) {\n-                throw e;\n-            }\n-            throw requireE;\n-        }\n-    }\n-};\n-\n-export const defaultLoaders: Loaders = Object.freeze({\n-    '.js': dynamicImport,\n-    '.mjs': dynamicImport,\n-    '.cjs': dynamicImport,\n-    '.json': jsonLoader,\n-    noExt: jsonLoader,\n-});\n-\n-function getOptions(\n-    name: string,\n-    options: OptionsSync,\n-    sync: true,\n-): Required<OptionsSync>;\n-function getOptions(\n-    name: string,\n-    options: Options,\n-    sync: false,\n-): Required<Options>;\n-function getOptions(\n-    name: string,\n-    options: Options | OptionsSync,\n-    sync: boolean,\n-): Required<Options | OptionsSync> {\n-    const conf: Required<Options> = {\n-        stopDir: os.homedir(),\n-        searchPlaces: getDefaultSearchPlaces(name, sync),\n-        ignoreEmptySearchPlaces: true,\n-        cache: true,\n-        transform: (x: LilconfigResult): LilconfigResult => x,\n-        packageProp: [name],\n-        ...options,\n-        loaders: {\n-            ...(sync ? defaultLoadersSync : defaultLoaders),\n-            ...options.loaders,\n-        },\n-    };\n-    conf.searchPlaces.forEach(place => {\n-        const key = path.extname(place) || 'noExt';\n-        const loader = conf.loaders[key];\n-        if (!loader) {\n-            throw new Error(`Missing loader for extension \"${place}\"`);\n-        }\n-\n-        if (typeof loader !== 'function') {\n-            throw new Error(\n-                `Loader for extension \"${place}\" is not a function: Received ${typeof loader}.`,\n-            );\n-        }\n-    });\n-\n-    return conf;\n-}\n-\n-function getPackageProp(\n-    props: string | string[],\n-    obj: Record<string, unknown>,\n-): unknown {\n-    if (typeof props === 'string' && props in obj) return obj[props];\n-    return (\n-        (Array.isArray(props) ? props : props.split('.')).reduce(\n-            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n-            (acc: any, prop): unknown => (acc === undefined ? acc : acc[prop]),\n-            obj,\n-        ) || null\n-    );\n-}\n-\n-function validateFilePath(filepath: string): void {\n-    if (!filepath) throw new Error('load must pass a non-empty string');\n-}\n-\n-function validateLoader(loader: Loader, ext: string): void | never {\n-    if (!loader) throw new Error(`No loader specified for extension \"${ext}\"`);\n-    if (typeof loader !== 'function')\n-        throw new Error('loader is not a function');\n-}\n-\n-type ClearCaches = {\n-    clearLoadCache: () => void;\n-    clearSearchCache: () => void;\n-    clearCaches: () => void;\n-};\n-\n-const makeEmplace =\n-    <T extends LilconfigResult | Promise<LilconfigResult>>(\n-        enableCache: boolean,\n-    ) =>\n-    (c: Map<string, T>, filepath: string, res: T): T => {\n-        if (enableCache) c.set(filepath, res);\n-        return res;\n-    };\n-\n-type AsyncSearcher = {\n-    search(searchFrom?: string): Promise<LilconfigResult>;\n-    load(filepath: string): Promise<LilconfigResult>;\n-} & ClearCaches;\n-\n-export function lilconfig(\n-    name: string,\n-    options?: Partial<Options>,\n-): AsyncSearcher {\n-    const {\n-        ignoreEmptySearchPlaces,\n-        loaders,\n-        packageProp,\n-        searchPlaces,\n-        stopDir,\n-        transform,\n-        cache,\n-    } = getOptions(name, options ?? {}, false);\n-    type R = LilconfigResult | Promise<LilconfigResult>;\n-    const searchCache = new Map<string, R>();\n-    const loadCache = new Map<string, R>();\n-    const emplace = makeEmplace<R>(cache);\n-\n-    return {\n-        async search(searchFrom = process.cwd()): Promise<LilconfigResult> {\n-            const result: LilconfigResult = {\n-                config: null,\n-                filepath: '',\n-            };\n-\n-            const visited: Set<string> = new Set();\n-            let dir = searchFrom;\n-            dirLoop: while (true) {\n-                if (cache) {\n-                    const r = searchCache.get(dir);\n-                    if (r !== undefined) {\n-                        for (const p of visited) searchCache.set(p, r);\n-                        return r;\n-                    }\n-                    visited.add(dir);\n-                }\n-\n-                for (const searchPlace of searchPlaces) {\n-                    const filepath = path.join(dir, searchPlace);\n-                    try {\n-                        await fs.promises.access(filepath);\n-                    } catch {\n-                        continue;\n-                    }\n-                    const content = String(await fsReadFileAsync(filepath));\n-                    const loaderKey = path.extname(searchPlace) || 'noExt';\n-                    const loader = loaders[loaderKey];\n-\n-                    // handle package.json\n-                    if (searchPlace === 'package.json') {\n-                        const pkg = await loader(filepath, content);\n-                        const maybeConfig = getPackageProp(packageProp, pkg);\n-                        if (maybeConfig != null) {\n-                            result.config = maybeConfig;\n-                            result.filepath = filepath;\n-                            break dirLoop;\n-                        }\n-\n-                        continue;\n-                    }\n-\n-                    // handle other type of configs\n-                    const isEmpty = content.trim() === '';\n-                    if (isEmpty && ignoreEmptySearchPlaces) continue;\n-\n-                    if (isEmpty) {\n-                        result.isEmpty = true;\n-                        result.config = undefined;\n-                    } else {\n-                        validateLoader(loader, loaderKey);\n-                        result.config = await loader(filepath, content);\n-                    }\n-                    result.filepath = filepath;\n-                    break dirLoop;\n-                }\n-                if (dir === stopDir || dir === parentDir(dir)) break dirLoop;\n-                dir = parentDir(dir);\n-            }\n-\n-            const transformed =\n-                // not found\n-                result.filepath === '' && result.config === null\n-                    ? transform(null)\n-                    : transform(result);\n-\n-            if (cache) {\n-                for (const p of visited) searchCache.set(p, transformed);\n-            }\n-\n-            return transformed;\n-        },\n-        async load(filepath: string): Promise<LilconfigResult> {\n-            validateFilePath(filepath);\n-            const absPath = path.resolve(process.cwd(), filepath);\n-            if (cache && loadCache.has(absPath)) {\n-                return loadCache.get(absPath) as LilconfigResult;\n-            }\n-            const {base, ext} = path.parse(absPath);\n-            const loaderKey = ext || 'noExt';\n-            const loader = loaders[loaderKey];\n-            validateLoader(loader, loaderKey);\n-            const content = String(await fsReadFileAsync(absPath));\n-\n-            if (base === 'package.json') {\n-                const pkg = await loader(absPath, content);\n-                return emplace(\n-                    loadCache,\n-                    absPath,\n-                    transform({\n-                        config: getPackageProp(packageProp, pkg),\n-                        filepath: absPath,\n-                    }),\n-                );\n-            }\n-            const result: LilconfigResult = {\n-                config: null,\n-                filepath: absPath,\n-            };\n-            // handle other type of configs\n-            const isEmpty = content.trim() === '';\n-            if (isEmpty && ignoreEmptySearchPlaces)\n-                return emplace(\n-                    loadCache,\n-                    absPath,\n-                    transform({\n-                        config: undefined,\n-                        filepath: absPath,\n-                        isEmpty: true,\n-                    }),\n-                );\n-\n-            // cosmiconfig returns undefined for empty files\n-            result.config = isEmpty\n-                ? undefined\n-                : await loader(absPath, content);\n-\n-            return emplace(\n-                loadCache,\n-                absPath,\n-                transform(\n-                    isEmpty ? {...result, isEmpty, config: undefined} : result,\n-                ),\n-            );\n-        },\n-        clearLoadCache() {\n-            if (cache) loadCache.clear();\n-        },\n-        clearSearchCache() {\n-            if (cache) searchCache.clear();\n-        },\n-        clearCaches() {\n-            if (cache) {\n-                loadCache.clear();\n-                searchCache.clear();\n-            }\n-        },\n-    };\n-}\n-\n-type SyncSearcher = {\n-    search(searchFrom?: string): LilconfigResult;\n-    load(filepath: string): LilconfigResult;\n-} & ClearCaches;\n-\n-export function lilconfigSync(\n-    name: string,\n-    options?: OptionsSync,\n-): SyncSearcher {\n-    const {\n-        ignoreEmptySearchPlaces,\n-        loaders,\n-        packageProp,\n-        searchPlaces,\n-        stopDir,\n-        transform,\n-        cache,\n-    } = getOptions(name, options ?? {}, true);\n-    type R = LilconfigResult;\n-    const searchCache = new Map<string, R>();\n-    const loadCache = new Map<string, R>();\n-    const emplace = makeEmplace<R>(cache);\n-\n-    return {\n-        search(searchFrom = process.cwd()): LilconfigResult {\n-            const result: LilconfigResult = {\n-                config: null,\n-                filepath: '',\n-            };\n-\n-            const visited: Set<string> = new Set();\n-            let dir = searchFrom;\n-            dirLoop: while (true) {\n-                if (cache) {\n-                    const r = searchCache.get(dir);\n-                    if (r !== undefined) {\n-                        for (const p of visited) searchCache.set(p, r);\n-                        return r;\n-                    }\n-                    visited.add(dir);\n-                }\n-\n-                for (const searchPlace of searchPlaces) {\n-                    const filepath = path.join(dir, searchPlace);\n-                    try {\n-                        fs.accessSync(filepath);\n-                    } catch {\n-                        continue;\n-                    }\n-                    const loaderKey = path.extname(searchPlace) || 'noExt';\n-                    const loader = loaders[loaderKey];\n-                    const content = String(fs.readFileSync(filepath));\n-\n-                    // handle package.json\n-                    if (searchPlace === 'package.json') {\n-                        const pkg = loader(filepath, content);\n-                        const maybeConfig = getPackageProp(packageProp, pkg);\n-                        if (maybeConfig != null) {\n-                            result.config = maybeConfig;\n-                            result.filepath = filepath;\n-                            break dirLoop;\n-                        }\n-\n-                        continue;\n-                    }\n-\n-                    // handle other type of configs\n-                    const isEmpty = content.trim() === '';\n-                    if (isEmpty && ignoreEmptySearchPlaces) continue;\n-\n-                    if (isEmpty) {\n-                        result.isEmpty = true;\n-                        result.config = undefined;\n-                    } else {\n-                        validateLoader(loader, loaderKey);\n-                        result.config = loader(filepath, content);\n-                    }\n-                    result.filepath = filepath;\n-                    break dirLoop;\n-                }\n-                if (dir === stopDir || dir === parentDir(dir)) break dirLoop;\n-                dir = parentDir(dir);\n-            }\n-\n-            const transformed =\n-                // not found\n-                result.filepath === '' && result.config === null\n-                    ? transform(null)\n-                    : transform(result);\n-\n-            if (cache) {\n-                for (const p of visited) searchCache.set(p, transformed);\n-            }\n-\n-            return transformed;\n-        },\n-        load(filepath: string): LilconfigResult {\n-            validateFilePath(filepath);\n-            const absPath = path.resolve(process.cwd(), filepath);\n-            if (cache && loadCache.has(absPath)) {\n-                return loadCache.get(absPath) as LilconfigResult;\n-            }\n-            const {base, ext} = path.parse(absPath);\n-            const loaderKey = ext || 'noExt';\n-            const loader = loaders[loaderKey];\n-            validateLoader(loader, loaderKey);\n-\n-            const content = String(fs.readFileSync(absPath));\n-\n-            if (base === 'package.json') {\n-                const pkg = loader(absPath, content);\n-                return transform({\n-                    config: getPackageProp(packageProp, pkg),\n-                    filepath: absPath,\n-                });\n-            }\n-            const result: LilconfigResult = {\n-                config: null,\n-                filepath: absPath,\n-            };\n-            // handle other type of configs\n-            const isEmpty = content.trim() === '';\n-            if (isEmpty && ignoreEmptySearchPlaces)\n-                return emplace(\n-                    loadCache,\n-                    absPath,\n-                    transform({\n-                        filepath: absPath,\n-                        config: undefined,\n-                        isEmpty: true,\n-                    }),\n-                );\n-\n-            // cosmiconfig returns undefined for empty files\n-            result.config = isEmpty ? undefined : loader(absPath, content);\n-\n-            return emplace(\n-                loadCache,\n-                absPath,\n-                transform(\n-                    isEmpty ? {...result, isEmpty, config: undefined} : result,\n-                ),\n-            );\n-        },\n-        clearLoadCache() {\n-            if (cache) loadCache.clear();\n-        },\n-        clearSearchCache() {\n-            if (cache) searchCache.clear();\n-        },\n-        clearCaches() {\n-            if (cache) {\n-                loadCache.clear();\n-                searchCache.clear();\n-            }\n-        },\n-    };\n-}",
          "src/spec/cjs-project/cjs.config.cjs": "@@ -1,3 +1,3 @@\n module.exports = {\n-    cjs: true,\n+\tcjs: true,\n };",
          "src/spec/cjs-project/cjs.config.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    cjs: true,\n+\tcjs: true,\n };",
          "src/spec/cjs-project/cjs.config.mjs": "@@ -1,3 +1,3 @@\n export default {\n-    esm: true,\n+\tesm: true,\n };",
          "src/spec/esm-project/cjs.config.mjs": "@@ -1,3 +1,3 @@\n module.exports = {\n-    iShouldBeESMbutIAmCJS: true,\n+\tiShouldBeESMbutIAmCJS: true,\n };",
          "src/spec/esm-project/esm.config.cjs": "@@ -1,3 +1,3 @@\n module.exports = {\n-    cjs: true,\n+\tcjs: true,\n };",
          "src/spec/esm-project/esm.config.js": "@@ -1,3 +1,3 @@\n export default {\n-    esm: true,\n+\tesm: true,\n };",
          "src/spec/esm-project/esm.config.mjs": "@@ -1,3 +1,3 @@\n export default {\n-    esm: true,\n+\tesm: true,\n };",
          "src/spec/index.spec.js": "@@ -0,0 +1,1874 @@\n+// @ts-check\n+const path = require('path');\n+const fs = require('fs');\n+const {lilconfig, lilconfigSync} = require('..');\n+const {cosmiconfig, cosmiconfigSync} = require('cosmiconfig');\n+const {transpileModule} = require('typescript');\n+\n+/**\n+ * Mocking fs solely to test the root directory filepath\n+ */\n+jest.mock('fs', () => {\n+\tconst fs = jest.requireActual('fs');\n+\n+\treturn {\n+\t\t...fs,\n+\t\tpromises: {\n+\t\t\t...fs.promises,\n+\t\t\treadFile: jest.fn(fs.promises.readFile),\n+\t\t\taccess: jest.fn(fs.promises.access),\n+\t\t},\n+\t\taccessSync: jest.fn(fs.accessSync),\n+\t\treadFileSync: jest.fn(fs.readFileSync),\n+\t};\n+});\n+\n+beforeEach(() => {\n+\tjest.clearAllMocks();\n+});\n+\n+const isNodeV20orNewer = parseInt(process.versions.node, 10) >= 20;\n+\n+describe('options', () => {\n+\tconst dirname = path.join(__dirname, 'load');\n+\n+\tdescribe('loaders', () => {\n+\t\t/** @type {import('../index').LoaderSync} */\n+\t\tconst tsLoader = (_, content) => {\n+\t\t\tconst res = transpileModule(content, {}).outputText;\n+\t\t\treturn eval(res);\n+\t\t};\n+\n+\t\tdescribe('ts-loader', () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.ts');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.ts': tsLoader,\n+\t\t\t\t},\n+\t\t\t};\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\ttypescript: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst result = lilconfigSync('test-app', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\t\t\t\tconst ccResult = cosmiconfigSync('test-app', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst result = await lilconfig('test-app', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('async loaders', () => {\n+\t\t\tconst config = {data: 42};\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.js': async () => config,\n+\n+\t\t\t\t\t/** @type {import('../index').LoaderSync} */\n+\t\t\t\t\tnoExt: (_, content) => content,\n+\t\t\t\t},\n+\t\t\t};\n+\n+\t\t\tit('async load', async () => {\n+\t\t\t\tconst filepath = path.join(__dirname, 'load', 'test-app.js');\n+\n+\t\t\t\tconst result = await lilconfig('test-app', options).load(filepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(filepath);\n+\n+\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t});\n+\n+\t\t\tit('async search', async () => {\n+\t\t\t\tconst searchPath = path.join(__dirname, 'search');\n+\t\t\t\tconst filepath = path.join(searchPath, 'test-app.config.js');\n+\n+\t\t\t\tconst result = await lilconfig('test-app', options).search(searchPath);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\tsearchPath,\n+\t\t\t\t);\n+\n+\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t});\n+\n+\t\t\tdescribe('esm-project', () => {\n+\t\t\t\tit('async search js', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.js');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['esm.config.js'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {esm: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\n+\t\t\t\tit('async search mjs', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.mjs');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['esm.config.mjs'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {esm: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\n+\t\t\t\tit('async search cjs', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.cjs');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['esm.config.cjs'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {cjs: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\t\t\t\tit('throws for using cjs instead of esm in esm project', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'esm-project', 'cjs.config.mjs');\n+\n+\t\t\t\t\tconst searcher = lilconfig('test-app', {});\n+\n+\t\t\t\t\tconst err = await searcher.load(filepath).catch(e => e);\n+\t\t\t\t\texpect(err.toString()).toMatch('module is not defined');\n+\t\t\t\t\t// TODO test for cosmiconfig\n+\t\t\t\t\t// cosmiconfig added this in v9.0.0\n+\t\t\t\t\t// but also some breaking changes\n+\t\t\t\t});\n+\t\t\t});\n+\n+\t\t\tdescribe('cjs-project', () => {\n+\t\t\t\tit('async search js', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.js');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['cjs.config.js'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {cjs: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\n+\t\t\t\tit('async search mjs', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.mjs');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['cjs.config.mjs'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {esm: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\n+\t\t\t\tit('async search cjs', async () => {\n+\t\t\t\t\tconst stopDir = __dirname;\n+\t\t\t\t\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.cjs');\n+\t\t\t\t\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\t\t\t\t\tconst options = {\n+\t\t\t\t\t\tsearchPlaces: ['cjs.config.cjs'],\n+\t\t\t\t\t\tstopDir,\n+\t\t\t\t\t};\n+\n+\t\t\t\t\tconst config = {cjs: true};\n+\n+\t\t\t\t\tconst result = await lilconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\t\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).search(\n+\t\t\t\t\t\tsearchFrom,\n+\t\t\t\t\t);\n+\n+\t\t\t\t\texpect(result).toEqual({config, filepath});\n+\t\t\t\t\texpect(ccResult).toEqual({config, filepath});\n+\t\t\t\t});\n+\t\t\t});\n+\n+\t\t\tit('async noExt', async () => {\n+\t\t\t\tconst searchPath = path.join(__dirname, 'search');\n+\t\t\t\tconst filepath = path.join(searchPath, 'noExtension');\n+\t\t\t\tconst opts = {\n+\t\t\t\t\t...options,\n+\t\t\t\t\tsearchPlaces: ['noExtension'],\n+\t\t\t\t};\n+\n+\t\t\t\tconst result = await lilconfig('noExtension', opts).search(searchPath);\n+\t\t\t\tconst ccResult = await cosmiconfig('noExtension', opts).search(\n+\t\t\t\t\tsearchPath,\n+\t\t\t\t);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tconfig: 'this file has no extension\\n',\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\n+\t\t\tit('sync noExt', () => {\n+\t\t\t\tconst searchPath = path.join(__dirname, 'search');\n+\t\t\t\tconst filepath = path.join(searchPath, 'noExtension');\n+\t\t\t\tconst opts = {\n+\t\t\t\t\t...options,\n+\t\t\t\t\tsearchPlaces: ['noExtension'],\n+\t\t\t\t};\n+\n+\t\t\t\tconst result = lilconfigSync('noExtension', opts).search(searchPath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('noExtension', opts).search(\n+\t\t\t\t\tsearchPath,\n+\t\t\t\t);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tconfig: 'this file has no extension\\n',\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\t});\n+\n+\tdescribe('transform', () => {\n+\t\t/** @type {import('../index').TransformSync} */\n+\t\tconst transform = result => {\n+\t\t\tif (result == null) return null;\n+\t\t\treturn {\n+\t\t\t\t...result,\n+\t\t\t\tconfig: {\n+\t\t\t\t\t...result.config,\n+\t\t\t\t\ttransformed: true,\n+\t\t\t\t},\n+\t\t\t};\n+\t\t};\n+\t\tconst filepath = path.join(dirname, 'test-app.js');\n+\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\tconst options = {\n+\t\t\ttransform,\n+\t\t};\n+\t\tconst expected = {\n+\t\t\tconfig: {\n+\t\t\t\tjsTest: true,\n+\t\t\t\ttransformed: true,\n+\t\t\t},\n+\t\t\tfilepath,\n+\t\t};\n+\n+\t\tit('sync', () => {\n+\t\t\tconst result = lilconfigSync('test-app', options).load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t\tit('async', async () => {\n+\t\t\tconst result = await lilconfig('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t});\n+\n+\tdescribe('ignoreEmptySearchPlaces', () => {\n+\t\tconst dirname = path.join(__dirname, 'load');\n+\t\tconst filepath = path.join(dirname, 'test-empty.js');\n+\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\tdescribe('ignores by default', () => {\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst result = lilconfigSync('test-app').load(relativeFilepath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tisEmpty: true,\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst result = await lilconfig('test-app').load(relativeFilepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app').load(relativeFilepath);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tisEmpty: true,\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('ignores when true', () => {\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst options = {\n+\t\t\t\t\tignoreEmptySearchPlaces: true,\n+\t\t\t\t};\n+\t\t\t\tconst result = lilconfigSync('test-app', options).load(filepath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('test-app', options).load(filepath);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tisEmpty: true,\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst options = {\n+\t\t\t\t\tignoreEmptySearchPlaces: true,\n+\t\t\t\t};\n+\t\t\t\tconst result = await lilconfig('test-app', options).load(filepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(filepath);\n+\n+\t\t\t\tconst expected = {\n+\t\t\t\t\tconfig: undefined,\n+\t\t\t\t\tfilepath,\n+\t\t\t\t\tisEmpty: true,\n+\t\t\t\t};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('doesnt ignore when false', () => {\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst options = {\n+\t\t\t\t\tignoreEmptySearchPlaces: false,\n+\t\t\t\t};\n+\t\t\t\tconst result = lilconfigSync('test-app', options).load(filepath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('test-app', options).load(filepath);\n+\n+\t\t\t\tconst expected = {config: undefined, filepath, isEmpty: true};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst options = {\n+\t\t\t\t\tignoreEmptySearchPlaces: false,\n+\t\t\t\t};\n+\t\t\t\tconst result = await lilconfig('test-app', options).load(filepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(filepath);\n+\n+\t\t\t\tconst expected = {config: undefined, filepath, isEmpty: true};\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\t});\n+\n+\tit('stopDir', () => {\n+\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\tconst searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n+\n+\t\tconst result = lilconfigSync('non-existent', {stopDir}).search(searchFrom);\n+\t\tconst ccResult = cosmiconfigSync('non-existent', {stopDir}).search(\n+\t\t\tsearchFrom,\n+\t\t);\n+\n+\t\tconst expected = null;\n+\n+\t\texpect(result).toEqual(expected);\n+\t\texpect(ccResult).toEqual(expected);\n+\t});\n+\n+\tit('searchPlaces', () => {\n+\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\tconst searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n+\t\tconst searchPlaces = ['searchPlaces.conf.js', 'searchPlaces-noExt'];\n+\n+\t\tconst options = {\n+\t\t\tstopDir,\n+\t\t\tsearchPlaces,\n+\t\t};\n+\n+\t\tconst result = lilconfigSync('doesnt-matter', options).search(searchFrom);\n+\t\tconst ccResult = cosmiconfigSync('doesnt-matter', options).search(\n+\t\t\tsearchFrom,\n+\t\t);\n+\n+\t\tconst expected = {\n+\t\t\tconfig: {\n+\t\t\t\tsearchPlacesWorks: true,\n+\t\t\t},\n+\t\t\tfilepath: path.join(\n+\t\t\t\t__dirname,\n+\t\t\t\t'search',\n+\t\t\t\t'a',\n+\t\t\t\t'b',\n+\t\t\t\t'searchPlaces.conf.js',\n+\t\t\t),\n+\t\t};\n+\n+\t\texpect(result).toEqual(expected);\n+\t\texpect(ccResult).toEqual(expected);\n+\t});\n+\n+\tdescribe('cache', () => {\n+\t\t// running all checks in one to avoid resetting cache for fs.promises.access\n+\t\tdescribe('enabled(default)', () => {\n+\t\t\tit('async search()', async () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchFrom = path.join(stopDir, 'a', 'b', 'c');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfig('cached', {\n+\t\t\t\t\tcache: true,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst fsLookUps = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.promises.access.mock.calls.length;\n+\n+\t\t\t\texpect(fsLookUps()).toBe(0);\n+\n+\t\t\t\t// per one search\n+\t\t\t\t// for unexisting\n+\t\t\t\t// (search + a + b + c) * times searchPlaces\n+\n+\t\t\t\t// for existing\n+\t\t\t\t// (search + a + b + c) * (times searchPlaces - **first** matched)\n+\t\t\t\tconst expectedFsLookUps = 7;\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = await searcher.search(searchFrom);\n+\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\n+\t\t\t\t// subsequant search reads from cache\n+\t\t\t\tconst result2 = await searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\t\t\t\texpect(result).toEqual(result2);\n+\n+\t\t\t\t// searching a subpath reuses cache\n+\t\t\t\tconst result3 = await searcher.search(path.join(stopDir, 'a'));\n+\t\t\t\tconst result4 = await searcher.search(path.join(stopDir, 'a', 'b'));\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\t\t\t\texpect(result2).toEqual(result3);\n+\t\t\t\texpect(result3).toEqual(result4);\n+\n+\t\t\t\t// calling clearCaches empties search cache\n+\t\t\t\tsearcher.clearCaches();\n+\n+\t\t\t\t// emptied all caches, should perform new lookups\n+\t\t\t\tconst result5 = await searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 2);\n+\t\t\t\texpect(result4).toEqual(result5);\n+\t\t\t\t// different references\n+\t\t\t\texpect(result4 === result5).toEqual(false);\n+\n+\t\t\t\tsearcher.clearSearchCache();\n+\t\t\t\tconst result6 = await searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 3);\n+\t\t\t\texpect(result5).toEqual(result6);\n+\t\t\t\t// different references\n+\t\t\t\texpect(result5 === result6).toEqual(false);\n+\n+\t\t\t\t// clearLoadCache does not clear search cache\n+\t\t\t\tsearcher.clearLoadCache();\n+\t\t\t\tconst result7 = await searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 3);\n+\t\t\t\texpect(result6).toEqual(result7);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result6 === result7).toEqual(true);\n+\n+\t\t\t\t// searching a superset path will access fs until it hits a known path\n+\t\t\t\tconst result8 = await searcher.search(path.join(searchFrom, 'd'));\n+\t\t\t\texpect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n+\t\t\t\texpect(result7).toEqual(result8);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result7 === result8).toEqual(true);\n+\n+\t\t\t\t// repeated searches do not cause extra fs calls\n+\t\t\t\tconst result9 = await searcher.search(path.join(searchFrom, 'd'));\n+\t\t\t\texpect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n+\t\t\t\texpect(result8).toEqual(result9);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result8 === result9).toEqual(true);\n+\t\t\t});\n+\n+\t\t\tit('sync search()', () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchFrom = path.join(stopDir, 'a', 'b', 'c');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfigSync('cached', {\n+\t\t\t\t\tcache: true,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst fsLookUps = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.accessSync.mock.calls.length;\n+\n+\t\t\t\texpect(fsLookUps()).toBe(0);\n+\n+\t\t\t\t// per one search\n+\t\t\t\t// for unexisting\n+\t\t\t\t// (search + a + b + c) * times searchPlaces\n+\n+\t\t\t\t// for existing\n+\t\t\t\t// (search + a + b + c) * (times searchPlaces - **first** matched)\n+\t\t\t\tconst expectedFsLookUps = 7;\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = searcher.search(searchFrom);\n+\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\n+\t\t\t\t// subsequant search reads from cache\n+\t\t\t\tconst result2 = searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\t\t\t\texpect(result).toEqual(result2);\n+\n+\t\t\t\t// searching a subpath reuses cache\n+\t\t\t\tconst result3 = searcher.search(path.join(stopDir, 'a'));\n+\t\t\t\tconst result4 = searcher.search(path.join(stopDir, 'a', 'b'));\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\t\t\t\texpect(result2).toEqual(result3);\n+\t\t\t\texpect(result3).toEqual(result4);\n+\n+\t\t\t\t// calling clearCaches empties search cache\n+\t\t\t\tsearcher.clearCaches();\n+\n+\t\t\t\t// emptied all caches, should perform new lookups\n+\t\t\t\tconst result5 = searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 2);\n+\t\t\t\texpect(result4).toEqual(result5);\n+\t\t\t\t// different references\n+\t\t\t\texpect(result4 === result5).toEqual(false);\n+\n+\t\t\t\tsearcher.clearSearchCache();\n+\t\t\t\tconst result6 = searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 3);\n+\t\t\t\texpect(result5).toEqual(result6);\n+\t\t\t\t// different references\n+\t\t\t\texpect(result5 === result6).toEqual(false);\n+\n+\t\t\t\t// clearLoadCache does not clear search cache\n+\t\t\t\tsearcher.clearLoadCache();\n+\t\t\t\tconst result7 = searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 3);\n+\t\t\t\texpect(result6).toEqual(result7);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result6 === result7).toEqual(true);\n+\n+\t\t\t\t// searching a superset path will access fs until it hits a known path\n+\t\t\t\tconst result8 = searcher.search(path.join(searchFrom, 'd'));\n+\t\t\t\texpect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n+\t\t\t\texpect(result7).toEqual(result8);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result7 === result8).toEqual(true);\n+\n+\t\t\t\t// repeated searches do not cause extra fs calls\n+\t\t\t\tconst result9 = searcher.search(path.join(searchFrom, 'd'));\n+\t\t\t\texpect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n+\t\t\t\texpect(result8).toEqual(result9);\n+\t\t\t\t// same references\n+\t\t\t\texpect(result8 === result9).toEqual(true);\n+\t\t\t});\n+\n+\t\t\tit('async load()', async () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfig('cached', {\n+\t\t\t\t\tcache: true,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst existingFile = path.join(stopDir, 'cached.config.js');\n+\t\t\t\tconst fsReadFileCalls = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.promises.readFile.mock.calls.length;\n+\n+\t\t\t\texpect(fsReadFileCalls()).toBe(0);\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\n+\t\t\t\t// subsequant load reads from cache\n+\t\t\t\tconst result2 = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\t\t\t\texpect(result).toEqual(result2);\n+\t\t\t\t// same reference\n+\t\t\t\texpect(result === result2).toEqual(true);\n+\n+\t\t\t\t// calling clearCaches empties search cache\n+\t\t\t\tsearcher.clearCaches();\n+\t\t\t\tconst result3 = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(2);\n+\t\t\t\texpect(result2).toEqual(result3);\n+\t\t\t\t// different reference\n+\t\t\t\texpect(result2 === result3).toEqual(false);\n+\n+\t\t\t\tsearcher.clearLoadCache();\n+\t\t\t\tconst result4 = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(3);\n+\t\t\t\texpect(result3).toEqual(result4);\n+\t\t\t\t// different reference\n+\t\t\t\texpect(result3 === result4).toEqual(false);\n+\n+\t\t\t\t// clearLoadCache does not clear search cache\n+\t\t\t\tsearcher.clearSearchCache();\n+\t\t\t\tconst result5 = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(3);\n+\t\t\t\texpect(result4).toEqual(result5);\n+\t\t\t\t// same reference\n+\t\t\t\texpect(result4 === result5).toEqual(true);\n+\t\t\t});\n+\n+\t\t\tit('sync load()', () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfigSync('cached', {\n+\t\t\t\t\tcache: true,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst existingFile = path.join(stopDir, 'cached.config.js');\n+\t\t\t\tconst fsReadFileCalls = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.readFileSync.mock.calls.length;\n+\n+\t\t\t\texpect(fsReadFileCalls()).toBe(0);\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\n+\t\t\t\t// subsequant load reads from cache\n+\t\t\t\tconst result2 = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\t\t\t\texpect(result).toEqual(result2);\n+\t\t\t\t// same reference\n+\t\t\t\texpect(result === result2).toEqual(true);\n+\n+\t\t\t\t// calling clearCaches empties search cache\n+\t\t\t\tsearcher.clearCaches();\n+\t\t\t\tconst result3 = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(2);\n+\t\t\t\texpect(result2).toEqual(result3);\n+\t\t\t\t// different reference\n+\t\t\t\texpect(result2 === result3).toEqual(false);\n+\n+\t\t\t\tsearcher.clearLoadCache();\n+\t\t\t\tconst result4 = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(3);\n+\t\t\t\texpect(result3).toEqual(result4);\n+\t\t\t\t// different reference\n+\t\t\t\texpect(result3 === result4).toEqual(false);\n+\n+\t\t\t\t// clearLoadCache does not clear search cache\n+\t\t\t\tsearcher.clearSearchCache();\n+\t\t\t\tconst result5 = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(3);\n+\t\t\t\texpect(result4).toEqual(result5);\n+\t\t\t\t// same reference\n+\t\t\t\texpect(result4 === result5).toEqual(true);\n+\t\t\t});\n+\t\t});\n+\t\tdescribe('disabled', () => {\n+\t\t\tit('async search()', async () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchFrom = path.join(stopDir, 'a', 'b', 'c');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfig('cached', {\n+\t\t\t\t\tcache: false,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst fsLookUps = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.promises.access.mock.calls.length;\n+\n+\t\t\t\texpect(fsLookUps()).toBe(0);\n+\n+\t\t\t\tconst expectedFsLookUps = 7;\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = await searcher.search(searchFrom);\n+\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\n+\t\t\t\t// subsequant search reads from cache\n+\t\t\t\tconst result2 = await searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 2);\n+\t\t\t\texpect(result).toEqual(result2);\n+\n+\t\t\t\texpect(result2 === result).toBe(false);\n+\t\t\t});\n+\n+\t\t\tit('sync search()', () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchFrom = path.join(stopDir, 'a', 'b', 'c');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfigSync('cached', {\n+\t\t\t\t\tcache: false,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst fsLookUps = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.accessSync.mock.calls.length;\n+\n+\t\t\t\texpect(fsLookUps()).toBe(0);\n+\n+\t\t\t\tconst expectedFsLookUps = 7;\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = searcher.search(searchFrom);\n+\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps);\n+\n+\t\t\t\t// subsequent search reads from cache\n+\t\t\t\tconst result2 = searcher.search(searchFrom);\n+\t\t\t\texpect(fsLookUps()).toBe(expectedFsLookUps * 2);\n+\t\t\t\texpect(result).toEqual(result2);\n+\n+\t\t\t\texpect(result2 === result).toBe(false);\n+\t\t\t});\n+\n+\t\t\tit('async load()', async () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfig('cached', {\n+\t\t\t\t\tcache: false,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst existingFile = path.join(stopDir, 'cached.config.js');\n+\t\t\t\tconst fsReadFileCalls = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.promises.readFile.mock.calls.length;\n+\n+\t\t\t\texpect(fsReadFileCalls()).toBe(0);\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\n+\t\t\t\t// subsequant load reads from cache\n+\t\t\t\tconst result2 = await searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(2);\n+\t\t\t\texpect(result).toEqual(result2);\n+\t\t\t\t// different reference\n+\t\t\t\texpect(result === result2).toEqual(false);\n+\t\t\t});\n+\n+\t\t\tit('sync load()', () => {\n+\t\t\t\tconst stopDir = path.join(__dirname, 'search');\n+\t\t\t\tconst searchPlaces = ['cached.config.js', 'package.json'];\n+\t\t\t\tconst searcher = lilconfigSync('cached', {\n+\t\t\t\t\tcache: false,\n+\t\t\t\t\tstopDir,\n+\t\t\t\t\tsearchPlaces,\n+\t\t\t\t});\n+\t\t\t\tconst existingFile = path.join(stopDir, 'cached.config.js');\n+\t\t\t\tconst fsReadFileCalls = () =>\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.readFileSync.mock.calls.length;\n+\n+\t\t\t\texpect(fsReadFileCalls()).toBe(0);\n+\n+\t\t\t\t// initial search populates cache\n+\t\t\t\tconst result = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(1);\n+\n+\t\t\t\t// subsequant load reads from cache\n+\t\t\t\tconst result2 = searcher.load(existingFile);\n+\t\t\t\texpect(fsReadFileCalls()).toBe(2);\n+\t\t\t\texpect(result).toEqual(result2);\n+\t\t\t\t// differnt reference\n+\t\t\t\texpect(result === result2).toEqual(false);\n+\t\t\t});\n+\t\t});\n+\t});\n+\n+\tdescribe('packageProp', () => {\n+\t\tdescribe('plain property string', () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst options = {packageProp: 'foo'};\n+\t\t\tconst filepath = path.join(dirname, 'package.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tinsideFoo: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst result = lilconfigSync('foo', options).load(relativeFilepath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('foo', options).load(relativeFilepath);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst result = await lilconfig('foo', options).load(relativeFilepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('foo', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('array of strings', () => {\n+\t\t\tconst filepath = path.join(__dirname, 'search', 'a', 'package.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst options = {\n+\t\t\t\tpackageProp: 'bar.baz',\n+\t\t\t\tstopDir: path.join(__dirname, 'search'),\n+\t\t\t};\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tinsideBarBaz: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst result = lilconfigSync('foo', options).load(relativeFilepath);\n+\t\t\t\tconst ccResult = cosmiconfigSync('foo', options).load(relativeFilepath);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst result = await lilconfig('foo', options).load(relativeFilepath);\n+\t\t\t\tconst ccResult = await cosmiconfig('foo', options).load(\n+\t\t\t\t\trelativeFilepath,\n+\t\t\t\t);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('string[] with null in the middle', () => {\n+\t\t\tconst searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n+\t\t\tconst options = {\n+\t\t\t\tpackageProp: 'bar.baz',\n+\t\t\t\tstopDir: path.join(__dirname, 'search'),\n+\t\t\t};\n+\t\t\t/**\n+\t\t\t * cosmiconfig throws when there is `null` value in the chain of package prop keys\n+\t\t\t */\n+\n+\t\t\tconst expectedMessage =\n+\t\t\t\tparseInt(process.version.slice(1), 10) > 14\n+\t\t\t\t\t? \"Cannot read properties of null (reading 'baz')\"\n+\t\t\t\t\t: \"Cannot read property 'baz' of null\";\n+\n+\t\t\tit('sync', () => {\n+\t\t\t\texpect(() => {\n+\t\t\t\t\tlilconfigSync('foo', options).search(searchFrom);\n+\t\t\t\t}).toThrowError(expectedMessage);\n+\t\t\t\texpect(() => {\n+\t\t\t\t\tcosmiconfigSync('foo', options).search(searchFrom);\n+\t\t\t\t}).toThrowError(expectedMessage);\n+\t\t\t});\n+\t\t\tit('async', async () => {\n+\t\t\t\texpect(\n+\t\t\t\t\tlilconfig('foo', options).search(searchFrom),\n+\t\t\t\t).rejects.toThrowError(expectedMessage);\n+\t\t\t\texpect(\n+\t\t\t\t\tcosmiconfig('foo', options).search(searchFrom),\n+\t\t\t\t).rejects.toThrowError(expectedMessage);\n+\t\t\t});\n+\t\t});\n+\n+\t\tdescribe('string[] with result', () => {\n+\t\t\tconst searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n+\t\t\tconst options = {\n+\t\t\t\tpackageProp: 'zoo.foo',\n+\t\t\t\tstopDir: path.join(__dirname, 'search'),\n+\t\t\t};\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tinsideZooFoo: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath: path.join(__dirname, 'search', 'a', 'package.json'),\n+\t\t\t};\n+\n+\t\t\tit('sync', () => {\n+\t\t\t\tconst result = lilconfigSync('foo', options).search(searchFrom);\n+\t\t\t\tconst ccResult = cosmiconfigSync('foo', options).search(searchFrom);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t\tit('async', async () => {\n+\t\t\t\tconst result = await lilconfig('foo', options).search(searchFrom);\n+\t\t\t\tconst ccResult = await cosmiconfig('foo', options).search(searchFrom);\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t});\n+\t});\n+});\n+describe('lilconfigSync', () => {\n+\tdescribe('load', () => {\n+\t\tconst dirname = path.join(__dirname, 'load');\n+\n+\t\tit('existing js file', () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.js');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst result = lilconfigSync('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('existing cjs file', () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.cjs');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst result = lilconfigSync('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('existing json file', () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst result = lilconfigSync('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsonTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('no extension json file', () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-noExt-json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst result = lilconfigSync('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {noExtJsonFile: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('package.json', () => {\n+\t\t\tconst filepath = path.join(dirname, 'package.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst options = {};\n+\t\t\tconst result = lilconfigSync('test-app', options).load(relativeFilepath);\n+\t\t\tconst ccResult = cosmiconfigSync('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tcustomThingHere: 'is-configured',\n+\t\t\t\t},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t});\n+\n+\tdescribe('search', () => {\n+\t\tconst dirname = path.join(__dirname, 'search');\n+\n+\t\tit('default for searchFrom', () => {\n+\t\t\tconst result = lilconfigSync('non-existent').search();\n+\t\t\tconst ccResult = cosmiconfigSync('non-existent').search();\n+\n+\t\t\tconst expected = null;\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('checks in hidden .config dir', () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst result = lilconfigSync('hidden').search(searchFrom);\n+\t\t\tconst ccResult = cosmiconfigSync('hidden').search(searchFrom);\n+\n+\t\t\tconst expected = {hidden: true};\n+\n+\t\t\texpect(result?.config).toEqual(expected);\n+\t\t\texpect(ccResult?.config).toEqual(expected);\n+\t\t});\n+\n+\t\tif (process.platform !== 'win32') {\n+\t\t\tit('default for searchFrom till root directory', () => {\n+\t\t\t\tconst options = {stopDir: '/'};\n+\t\t\t\tconst result = lilconfigSync('non-existent', options).search();\n+\t\t\t\texpect(\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.accessSync.mock.calls.slice(-10),\n+\t\t\t\t).toEqual([\n+\t\t\t\t\t['/package.json'],\n+\t\t\t\t\t['/.non-existentrc.json'],\n+\t\t\t\t\t['/.non-existentrc.js'],\n+\t\t\t\t\t['/.non-existentrc.cjs'],\n+\t\t\t\t\t['/.config/non-existentrc'],\n+\t\t\t\t\t['/.config/non-existentrc.json'],\n+\t\t\t\t\t['/.config/non-existentrc.js'],\n+\t\t\t\t\t['/.config/non-existentrc.cjs'],\n+\t\t\t\t\t['/non-existent.config.js'],\n+\t\t\t\t\t['/non-existent.config.cjs'],\n+\t\t\t\t]);\n+\t\t\t\tconst ccResult = cosmiconfigSync('non-existent', options).search();\n+\n+\t\t\t\tconst expected = null;\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t}\n+\n+\t\tit('provided searchFrom', () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t};\n+\n+\t\t\tconst result = lilconfigSync('non-existent', options).search(searchFrom);\n+\t\t\tconst ccResult = cosmiconfigSync('non-existent', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = null;\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('treating empty configs', () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t};\n+\n+\t\t\tconst result = lilconfigSync('maybeEmpty', options).search(searchFrom);\n+\t\t\tconst ccResult = cosmiconfigSync('maybeEmpty', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tnotSoEmpty: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath: path.join(dirname, 'a', 'maybeEmpty.config.js'),\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('treating empty configs with ignoreEmptySearchPlaces off', () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t\tignoreEmptySearchPlaces: false,\n+\t\t\t};\n+\n+\t\t\tconst result = lilconfigSync('maybeEmpty', options).search(searchFrom);\n+\t\t\tconst ccResult = cosmiconfigSync('maybeEmpty', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: undefined,\n+\t\t\t\tfilepath: path.join(dirname, 'a', 'b', 'maybeEmpty.config.js'),\n+\t\t\t\tisEmpty: true,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t});\n+\n+\tdescribe('when to throw', () => {\n+\t\tit('loader throws', () => {\n+\t\t\tconst dirname = path.join(__dirname, 'search');\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tclass LoaderError extends Error {}\n+\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.js'() {\n+\t\t\t\t\t\tthrow new LoaderError();\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t};\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('maybeEmpty', options).search(searchFrom);\n+\t\t\t}).toThrowError(LoaderError);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('maybeEmpty', options).search(searchFrom);\n+\t\t\t}).toThrowError(LoaderError);\n+\t\t});\n+\n+\t\tit('non existing file', () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'nope.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(`ENOENT: no such file or directory, open '${filepath}'`);\n+\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(`ENOENT: no such file or directory, open '${filepath}'`);\n+\t\t});\n+\n+\t\tit('throws for invalid json', () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'test-invalid.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\t/**\n+\t\t\t * throws but less elegant\n+\t\t\t */\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(\n+\t\t\t\tisNodeV20orNewer\n+\t\t\t\t\t? `Expected ',' or '}' after property value in JSON at position 22`\n+\t\t\t\t\t: 'Unexpected token / in JSON at position 22',\n+\t\t\t);\n+\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(`JSON Error in ${filepath}:`);\n+\t\t});\n+\n+\t\tit('throws for provided filepath that does not exist', () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'i-do-no-exist.js');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app', {}).load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app', {}).load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('no loader specified for the search place', () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'config.coffee');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst errMsg = 'No loader specified for extension \".coffee\"';\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('loader is not a function', () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'config.coffee');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.coffee': true,\n+\t\t\t\t},\n+\t\t\t};\n+\n+\t\t\tconst errMsg = 'loader is not a function';\n+\n+\t\t\texpect(() => {\n+\t\t\t\t// @ts-expect-error: unit test is literally for this purpose\n+\t\t\t\tlilconfigSync('test-app', options).load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\t// @ts-ignore: unit test is literally for this purpose\n+\t\t\t\tcosmiconfigSync('test-app', options).load(relativeFilepath);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('no extension loader throws for unparsable file', () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'test-noExt-nonParsable');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(\n+\t\t\t\tisNodeV20orNewer\n+\t\t\t\t\t? `Unexpected token 'h', \"hobbies:\\n- \"Reading\\n`\n+\t\t\t\t\t: 'Unexpected token # in JSON at position 2',\n+\t\t\t);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app').load(relativeFilepath);\n+\t\t\t}).toThrowError(`YAML Error in ${filepath}`);\n+\t\t});\n+\n+\t\tit('throws for empty strings passed to load', () => {\n+\t\t\tconst errMsg = 'load must pass a non-empty string';\n+\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('test-app').load('');\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('test-app').load('');\n+\t\t\t}).toThrowError('EISDIR: illegal operation on a directory, read');\n+\t\t});\n+\n+\t\tit('throws when provided searchPlace has no loader', () => {\n+\t\t\tconst errMsg = 'Missing loader for extension \"file.coffee\"';\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync('foo', {\n+\t\t\t\t\tsearchPlaces: ['file.coffee'],\n+\t\t\t\t});\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync('foo', {\n+\t\t\t\t\tsearchPlaces: ['file.coffee'],\n+\t\t\t\t});\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('throws when a loader for a searchPlace is not a function', () => {\n+\t\t\tconst errMsg =\n+\t\t\t\t'Loader for extension \"file.js\" is not a function: Received object.';\n+\t\t\tconst options = {\n+\t\t\t\tsearchPlaces: ['file.js'],\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.js': {},\n+\t\t\t\t},\n+\t\t\t};\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-expect-error: unit test is literally for this purpose\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-ignore: needed for jest\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('throws for searchPlaces with no extension', () => {\n+\t\t\tconst errMsg =\n+\t\t\t\t'Loader for extension \"file\" is not a function: Received object.';\n+\t\t\tconst options = {\n+\t\t\t\tsearchPlaces: ['file'],\n+\t\t\t\tloaders: {\n+\t\t\t\t\tnoExt: {},\n+\t\t\t\t},\n+\t\t\t};\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfigSync(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-expect-error: unit test is literally for this purpose\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfigSync(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-ignore: needed for jest\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\t});\n+});\n+\n+describe('lilconfig', () => {\n+\tdescribe('load', () => {\n+\t\tconst dirname = path.join(__dirname, 'load');\n+\n+\t\tit('existing js file', async () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.js');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst result = await lilconfig('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = await cosmiconfig('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('existing cjs file', async () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.cjs');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst result = await lilconfig('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = await cosmiconfig('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('existing json file', async () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-app.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst result = await lilconfig('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = await cosmiconfig('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {jsonTest: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('no extension json file', async () => {\n+\t\t\tconst filepath = path.join(dirname, 'test-noExt-json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst result = await lilconfig('test-app').load(relativeFilepath);\n+\t\t\tconst ccResult = await cosmiconfig('test-app').load(relativeFilepath);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {noExtJsonFile: true},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(result).toEqual(ccResult);\n+\t\t});\n+\n+\t\tit('package.json', async () => {\n+\t\t\tconst filepath = path.join(dirname, 'package.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst options = {};\n+\t\t\tconst result = await lilconfig('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\t\t\tconst ccResult = await cosmiconfig('test-app', options).load(\n+\t\t\t\trelativeFilepath,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tcustomThingHere: 'is-configured',\n+\t\t\t\t},\n+\t\t\t\tfilepath,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t});\n+\n+\tdescribe('search', () => {\n+\t\tconst dirname = path.join(__dirname, 'search');\n+\n+\t\tit('returns null when no config found', async () => {\n+\t\t\tconst result = await lilconfig('non-existent').search();\n+\t\t\tconst ccResult = await cosmiconfig('non-existent').search();\n+\n+\t\t\tconst expected = null;\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('checks in hidden .config dir', async () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst result = await lilconfig('hidden').search(searchFrom);\n+\t\t\tconst ccResult = await cosmiconfig('hidden').search(searchFrom);\n+\n+\t\t\tconst expected = {hidden: true};\n+\n+\t\t\texpect(result?.config).toEqual(expected);\n+\t\t\texpect(ccResult?.config).toEqual(expected);\n+\t\t});\n+\n+\t\tif (process.platform !== 'win32') {\n+\t\t\tit('searches root directory correctly', async () => {\n+\t\t\t\tconst options = {stopDir: '/'};\n+\t\t\t\tconst result = await lilconfig('non-existent', options).search();\n+\t\t\t\texpect(\n+\t\t\t\t\t// @ts-expect-error\n+\t\t\t\t\tfs.promises.access.mock.calls.slice(-13),\n+\t\t\t\t).toEqual([\n+\t\t\t\t\t['/package.json'],\n+\t\t\t\t\t['/.non-existentrc.json'],\n+\t\t\t\t\t['/.non-existentrc.js'],\n+\t\t\t\t\t['/.non-existentrc.cjs'],\n+\t\t\t\t\t['/.non-existentrc.mjs'],\n+\t\t\t\t\t['/.config/non-existentrc'],\n+\t\t\t\t\t['/.config/non-existentrc.json'],\n+\t\t\t\t\t['/.config/non-existentrc.js'],\n+\t\t\t\t\t['/.config/non-existentrc.cjs'],\n+\t\t\t\t\t['/.config/non-existentrc.mjs'],\n+\t\t\t\t\t['/non-existent.config.js'],\n+\t\t\t\t\t['/non-existent.config.cjs'],\n+\t\t\t\t\t['/non-existent.config.mjs'],\n+\t\t\t\t]);\n+\t\t\t\tconst ccResult = await cosmiconfig('non-existent', options).search();\n+\n+\t\t\t\tconst expected = null;\n+\n+\t\t\t\texpect(result).toEqual(expected);\n+\t\t\t\texpect(ccResult).toEqual(expected);\n+\t\t\t});\n+\t\t}\n+\n+\t\tit('provided searchFrom', async () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t};\n+\n+\t\t\tconst result = await lilconfig('non-existent', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\t\t\tconst ccResult = await cosmiconfig('non-existent', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = null;\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('treating empty configs', async () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t};\n+\n+\t\t\tconst result = await lilconfig('maybeEmpty', options).search(searchFrom);\n+\t\t\tconst ccResult = await cosmiconfig('maybeEmpty', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: {\n+\t\t\t\t\tnotSoEmpty: true,\n+\t\t\t\t},\n+\t\t\t\tfilepath: path.join(dirname, 'a', 'maybeEmpty.config.js'),\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\n+\t\tit('treating empty configs with ignoreEmptySearchPlaces off', async () => {\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tconst options = {\n+\t\t\t\tstopDir: dirname,\n+\t\t\t\tignoreEmptySearchPlaces: false,\n+\t\t\t};\n+\n+\t\t\tconst result = await lilconfig('maybeEmpty', options).search(searchFrom);\n+\t\t\tconst ccResult = await cosmiconfig('maybeEmpty', options).search(\n+\t\t\t\tsearchFrom,\n+\t\t\t);\n+\n+\t\t\tconst expected = {\n+\t\t\t\tconfig: undefined,\n+\t\t\t\tfilepath: path.join(dirname, 'a', 'b', 'maybeEmpty.config.js'),\n+\t\t\t\tisEmpty: true,\n+\t\t\t};\n+\n+\t\t\texpect(result).toEqual(expected);\n+\t\t\texpect(ccResult).toEqual(expected);\n+\t\t});\n+\t});\n+\n+\tdescribe('when to throw', () => {\n+\t\tit('loader throws', async () => {\n+\t\t\tconst dirname = path.join(__dirname, 'search');\n+\t\t\tconst searchFrom = path.join(dirname, 'a', 'b', 'c');\n+\n+\t\t\tclass LoaderError extends Error {}\n+\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.js': () => {\n+\t\t\t\t\t\tthrow new LoaderError();\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t};\n+\n+\t\t\tconst result = await lilconfig('maybeEmpty', options)\n+\t\t\t\t.search(searchFrom)\n+\t\t\t\t.catch(x => x);\n+\t\t\tconst ccResult = await cosmiconfig('maybeEmpty', options)\n+\t\t\t\t.search(searchFrom)\n+\t\t\t\t.catch(x => x);\n+\n+\t\t\texpect(result instanceof LoaderError).toBeTruthy();\n+\t\t\texpect(ccResult instanceof LoaderError).toBeTruthy();\n+\t\t});\n+\n+\t\tit('non existing file', async () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'nope.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n+\n+\t\t\texpect(lilconfig('test-app').load(relativeFilepath)).rejects.toThrowError(\n+\t\t\t\terrMsg,\n+\t\t\t);\n+\n+\t\t\texpect(\n+\t\t\t\tcosmiconfig('test-app').load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('throws for invalid json', async () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'test-invalid.json');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\t/**\n+\t\t\t * throws but less elegant\n+\t\t\t */\n+\t\t\texpect(lilconfig('test-app').load(relativeFilepath)).rejects.toThrowError(\n+\t\t\t\tisNodeV20orNewer\n+\t\t\t\t\t? `Expected ',' or '}' after property value in JSON at position 22`\n+\t\t\t\t\t: 'Unexpected token / in JSON at position 22',\n+\t\t\t);\n+\n+\t\t\texpect(\n+\t\t\t\tcosmiconfig('test-app').load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(`JSON Error in ${filepath}:`);\n+\t\t});\n+\n+\t\tit('throws for provided filepath that does not exist', async () => {\n+\t\t\tconst dirname = path.join(__dirname, 'load');\n+\t\t\tconst filepath = path.join(dirname, 'i-do-no-exist.js');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n+\n+\t\t\texpect(\n+\t\t\t\tlilconfig('test-app', {}).load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t\texpect(\n+\t\t\t\tcosmiconfig('test-app', {}).load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('no loader specified for the search place', async () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'config.coffee');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tconst errMsg = 'No loader specified for extension \".coffee\"';\n+\n+\t\t\texpect(lilconfig('test-app').load(relativeFilepath)).rejects.toThrowError(\n+\t\t\t\terrMsg,\n+\t\t\t);\n+\t\t\texpect(\n+\t\t\t\tcosmiconfig('test-app').load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('loader is not a function', async () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'config.coffee');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\t\t\tconst options = {\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.coffee': true,\n+\t\t\t\t},\n+\t\t\t};\n+\n+\t\t\tconst errMsg = 'loader is not a function';\n+\n+\t\t\texpect(\n+\t\t\t\tlilconfig(\n+\t\t\t\t\t'test-app',\n+\t\t\t\t\t// @ts-expect-error: for unit test purpose\n+\t\t\t\t\toptions,\n+\t\t\t\t).load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t\texpect(\n+\t\t\t\t// @ts-ignore: required for jest, but not ts used in editor\n+\t\t\t\tcosmiconfig('test-app', options).load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('no extension loader throws for unparsable file', async () => {\n+\t\t\tconst filepath = path.join(__dirname, 'load', 'test-noExt-nonParsable');\n+\t\t\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\n+\t\t\tawait expect(\n+\t\t\t\tlilconfig('test-app').load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(\n+\t\t\t\tisNodeV20orNewer\n+\t\t\t\t\t? `Unexpected token 'h', \"hobbies:\\n- \"Reading\\n\" is not valid JSON`\n+\t\t\t\t\t: 'Unexpected token h in JSON at position 0',\n+\t\t\t);\n+\t\t\tawait expect(\n+\t\t\t\tcosmiconfig('test-app').load(relativeFilepath),\n+\t\t\t).rejects.toThrowError(`YAML Error in ${filepath}`);\n+\t\t});\n+\n+\t\tit('throws for empty strings passed to load', async () => {\n+\t\t\tconst errMsg = 'load must pass a non-empty string';\n+\n+\t\t\texpect(lilconfig('test-app').load('')).rejects.toThrowError(errMsg);\n+\t\t\texpect(cosmiconfig('test-app').load('')).rejects.toThrowError(\n+\t\t\t\t'EISDIR: illegal operation on a directory, read',\n+\t\t\t);\n+\t\t});\n+\n+\t\tit('throws when provided searchPlace has no loader', () => {\n+\t\t\tconst errMsg = 'Missing loader for extension \"file.coffee\"';\n+\t\t\texpect(() =>\n+\t\t\t\tlilconfig('foo', {\n+\t\t\t\t\tsearchPlaces: ['file.coffee'],\n+\t\t\t\t}),\n+\t\t\t).toThrowError(errMsg);\n+\t\t\texpect(() =>\n+\t\t\t\tcosmiconfig('foo', {\n+\t\t\t\t\tsearchPlaces: ['file.coffee'],\n+\t\t\t\t}),\n+\t\t\t).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('throws when a loader for a searchPlace is not a function', () => {\n+\t\t\tconst errMsg =\n+\t\t\t\t'Loader for extension \"file.js\" is not a function: Received object';\n+\t\t\tconst options = {\n+\t\t\t\tsearchPlaces: ['file.js'],\n+\t\t\t\tloaders: {\n+\t\t\t\t\t'.js': {},\n+\t\t\t\t},\n+\t\t\t};\n+\t\t\texpect(() =>\n+\t\t\t\tlilconfig(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-expect-error: for unit test purpose\n+\t\t\t\t\toptions,\n+\t\t\t\t),\n+\t\t\t).toThrowError(errMsg);\n+\t\t\texpect(() =>\n+\t\t\t\tcosmiconfig(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-ignore: needed for jest\n+\t\t\t\t\toptions,\n+\t\t\t\t),\n+\t\t\t).toThrowError(errMsg);\n+\t\t});\n+\n+\t\tit('throws for searchPlaces with no extension', () => {\n+\t\t\tconst errMsg =\n+\t\t\t\t'Loader for extension \"file\" is not a function: Received object.';\n+\t\t\tconst options = {\n+\t\t\t\tsearchPlaces: ['file'],\n+\t\t\t\tloaders: {\n+\t\t\t\t\tnoExt: {},\n+\t\t\t\t},\n+\t\t\t};\n+\t\t\texpect(() => {\n+\t\t\t\tlilconfig(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-expect-error: for unit test purpose\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t\texpect(() => {\n+\t\t\t\tcosmiconfig(\n+\t\t\t\t\t'foo',\n+\t\t\t\t\t// @ts-ignore: needed for jest, but not editor\n+\t\t\t\t\toptions,\n+\t\t\t\t);\n+\t\t\t}).toThrowError(errMsg);\n+\t\t});\n+\t});\n+});\n+\n+describe('npm package api', () => {\n+\tit('exports the same things as cosmiconfig', () => {\n+\t\tconst lc = require('../index');\n+\t\tconst cc = require('cosmiconfig');\n+\n+\t\texpect(typeof lc.defaultLoaders).toEqual(typeof cc.defaultLoaders);\n+\t\texpect(typeof lc.defaultLoadersSync).toEqual(typeof cc.defaultLoadersSync);\n+\t\t// @ts-expect-error: not in types\n+\t\texpect(typeof lc.defaultLoadersAsync).toEqual(\n+\t\t\t// @ts-expect-error: not in types\n+\t\t\ttypeof cc.defaultLoadersAsync,\n+\t\t);\n+\n+\t\tconst lcExplorerSyncKeys = Object.keys(lc.lilconfigSync('foo'));\n+\t\tconst ccExplorerSyncKeys = Object.keys(cc.cosmiconfigSync('foo'));\n+\n+\t\texpect(lcExplorerSyncKeys).toEqual(ccExplorerSyncKeys);\n+\n+\t\t/* eslint-disable no-unused-vars */\n+\t\tconst omitKnownDifferKeys = ({\n+\t\t\tlilconfig,\n+\t\t\tlilconfigSync,\n+\t\t\tcosmiconfig,\n+\t\t\tcosmiconfigSync,\n+\t\t\tmetaSearchPlaces,\n+\t\t\t...rest\n+\t\t}) => rest;\n+\t\t/* eslint-enable no-unused-vars */\n+\n+\t\t// @ts-expect-error: not in types\n+\t\texpect(Object.keys(omitKnownDifferKeys(lc)).sort()).toEqual(\n+\t\t\t// @ts-expect-error: not in types\n+\t\t\tObject.keys(omitKnownDifferKeys(cc)).sort(),\n+\t\t);\n+\t});\n+});",
          "src/spec/index.spec.ts": "@@ -1,2038 +0,0 @@\n-/* eslint-disable @typescript-eslint/ban-ts-comment */\n-import * as path from 'path';\n-import * as fs from 'fs';\n-import {lilconfig, lilconfigSync, LoaderSync, TransformSync} from '..';\n-import {cosmiconfig, cosmiconfigSync} from 'cosmiconfig';\n-import {transpileModule} from 'typescript';\n-\n-/**\n- * Mocking fs solely to test the root directory filepath\n- */\n-jest.mock('fs', () => {\n-    const fs = jest.requireActual<typeof import('fs')>('fs');\n-\n-    return {\n-        ...fs,\n-        promises: {\n-            ...fs.promises,\n-            readFile: jest.fn(fs.promises.readFile),\n-            access: jest.fn(fs.promises.access),\n-        },\n-        accessSync: jest.fn(fs.accessSync),\n-        readFileSync: jest.fn(fs.readFileSync),\n-    };\n-});\n-\n-beforeEach(() => {\n-    jest.clearAllMocks();\n-});\n-\n-const isNodeV20orNewer = parseInt(process.versions.node, 10) >= 20;\n-\n-describe('options', () => {\n-    const dirname = path.join(__dirname, 'load');\n-\n-    describe('loaders', () => {\n-        const tsLoader: LoaderSync = (_, content) => {\n-            const res = transpileModule(content, {}).outputText;\n-            return eval(res);\n-        };\n-\n-        describe('ts-loader', () => {\n-            const filepath = path.join(dirname, 'test-app.ts');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const options = {\n-                loaders: {\n-                    '.ts': tsLoader,\n-                },\n-            };\n-            const expected = {\n-                config: {\n-                    typescript: true,\n-                },\n-                filepath,\n-            };\n-\n-            it('sync', () => {\n-                const result = lilconfigSync('test-app', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = cosmiconfigSync('test-app', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-\n-            it('async', async () => {\n-                const result = await lilconfig('test-app', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = await cosmiconfig('test-app', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-\n-        describe('async loaders', () => {\n-            const config = {data: 42};\n-            const options = {\n-                loaders: {\n-                    '.js': async () => config,\n-                    noExt: (_: string, content: string) => content,\n-                },\n-            };\n-\n-            it('async load', async () => {\n-                const filepath = path.join(__dirname, 'load', 'test-app.js');\n-\n-                const result = await lilconfig('test-app', options).load(\n-                    filepath,\n-                );\n-                const ccResult = await cosmiconfig('test-app', options).load(\n-                    filepath,\n-                );\n-\n-                expect(result).toEqual({config, filepath});\n-                expect(ccResult).toEqual({config, filepath});\n-            });\n-\n-            it('async search', async () => {\n-                const searchPath = path.join(__dirname, 'search');\n-                const filepath = path.join(searchPath, 'test-app.config.js');\n-\n-                const result = await lilconfig('test-app', options).search(\n-                    searchPath,\n-                );\n-                const ccResult = await cosmiconfig('test-app', options).search(\n-                    searchPath,\n-                );\n-\n-                expect(result).toEqual({config, filepath});\n-                expect(ccResult).toEqual({config, filepath});\n-            });\n-\n-            describe('esm-project', () => {\n-                it('async search js', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'esm.config.js',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['esm.config.js'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {esm: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-\n-                it('async search mjs', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'esm.config.mjs',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['esm.config.mjs'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {esm: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-\n-                it('async search cjs', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'esm.config.cjs',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['esm.config.cjs'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {cjs: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-                it('throws for using cjs instead of esm in esm project', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'esm-project',\n-                        'cjs.config.mjs',\n-                    );\n-\n-                    const searcher = lilconfig('test-app', {});\n-\n-                    const err = await searcher.load(filepath).catch(e => e);\n-                    expect(err.toString()).toMatch('module is not defined');\n-                    // TODO test for cosmiconfig\n-                    // cosmiconfig added this in v9.0.0\n-                    // but also some breaking changes\n-                });\n-            });\n-\n-            describe('cjs-project', () => {\n-                it('async search js', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'cjs.config.js',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['cjs.config.js'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {cjs: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-\n-                it('async search mjs', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'cjs.config.mjs',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['cjs.config.mjs'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {esm: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-\n-                it('async search cjs', async () => {\n-                    const stopDir = __dirname;\n-                    const filepath = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'cjs.config.cjs',\n-                    );\n-                    const searchFrom = path.join(\n-                        stopDir,\n-                        'cjs-project',\n-                        'a',\n-                        'b',\n-                        'c',\n-                    );\n-\n-                    const options = {\n-                        searchPlaces: ['cjs.config.cjs'],\n-                        stopDir,\n-                    };\n-\n-                    const config = {cjs: true};\n-\n-                    const result = await lilconfig('test-app', options).search(\n-                        searchFrom,\n-                    );\n-                    const ccResult = await cosmiconfig(\n-                        'test-app',\n-                        options,\n-                    ).search(searchFrom);\n-\n-                    expect(result).toEqual({config, filepath});\n-                    expect(ccResult).toEqual({config, filepath});\n-                });\n-            });\n-\n-            it('async noExt', async () => {\n-                const searchPath = path.join(__dirname, 'search');\n-                const filepath = path.join(searchPath, 'noExtension');\n-                const opts = {\n-                    ...options,\n-                    searchPlaces: ['noExtension'],\n-                };\n-\n-                const result = await lilconfig('noExtension', opts).search(\n-                    searchPath,\n-                );\n-                const ccResult = await cosmiconfig('noExtension', opts).search(\n-                    searchPath,\n-                );\n-\n-                const expected = {\n-                    filepath,\n-                    config: 'this file has no extension\\n',\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-\n-            it('sync noExt', () => {\n-                const searchPath = path.join(__dirname, 'search');\n-                const filepath = path.join(searchPath, 'noExtension');\n-                const opts = {\n-                    ...options,\n-                    searchPlaces: ['noExtension'],\n-                };\n-\n-                const result = lilconfigSync('noExtension', opts).search(\n-                    searchPath,\n-                );\n-                const ccResult = cosmiconfigSync('noExtension', opts).search(\n-                    searchPath,\n-                );\n-\n-                const expected = {\n-                    filepath,\n-                    config: 'this file has no extension\\n',\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-    });\n-\n-    describe('transform', () => {\n-        const transform: TransformSync = result => {\n-            if (result == null) return null;\n-            return {\n-                ...result,\n-                config: {\n-                    ...result.config,\n-                    transformed: true,\n-                },\n-            };\n-        };\n-        const filepath = path.join(dirname, 'test-app.js');\n-        const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-        const options = {\n-            transform,\n-        };\n-        const expected = {\n-            config: {\n-                jsTest: true,\n-                transformed: true,\n-            },\n-            filepath,\n-        };\n-\n-        it('sync', () => {\n-            const result = lilconfigSync('test-app', options).load(\n-                relativeFilepath,\n-            );\n-            const ccResult = cosmiconfigSync('test-app', options).load(\n-                relativeFilepath,\n-            );\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-        it('async', async () => {\n-            const result = await lilconfig('test-app', options).load(\n-                relativeFilepath,\n-            );\n-            const ccResult = await cosmiconfig('test-app', options).load(\n-                relativeFilepath,\n-            );\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-    });\n-\n-    describe('ignoreEmptySearchPlaces', () => {\n-        const dirname = path.join(__dirname, 'load');\n-        const filepath = path.join(dirname, 'test-empty.js');\n-        const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-        describe('ignores by default', () => {\n-            it('sync', () => {\n-                const result = lilconfigSync('test-app').load(relativeFilepath);\n-                const ccResult =\n-                    cosmiconfigSync('test-app').load(relativeFilepath);\n-\n-                const expected = {\n-                    config: undefined,\n-                    filepath,\n-                    isEmpty: true,\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-\n-            it('async', async () => {\n-                const result = await lilconfig('test-app').load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = await cosmiconfig('test-app').load(\n-                    relativeFilepath,\n-                );\n-\n-                const expected = {\n-                    config: undefined,\n-                    filepath,\n-                    isEmpty: true,\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-\n-        describe('ignores when true', () => {\n-            it('sync', () => {\n-                const options = {\n-                    ignoreEmptySearchPlaces: true,\n-                };\n-                const result = lilconfigSync('test-app', options).load(\n-                    filepath,\n-                );\n-                const ccResult = cosmiconfigSync('test-app', options).load(\n-                    filepath,\n-                );\n-\n-                const expected = {\n-                    config: undefined,\n-                    filepath,\n-                    isEmpty: true,\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-\n-            it('async', async () => {\n-                const options = {\n-                    ignoreEmptySearchPlaces: true,\n-                };\n-                const result = await lilconfig('test-app', options).load(\n-                    filepath,\n-                );\n-                const ccResult = await cosmiconfig('test-app', options).load(\n-                    filepath,\n-                );\n-\n-                const expected = {\n-                    config: undefined,\n-                    filepath,\n-                    isEmpty: true,\n-                };\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-\n-        describe('doesnt ignore when false', () => {\n-            it('sync', () => {\n-                const options = {\n-                    ignoreEmptySearchPlaces: false,\n-                };\n-                const result = lilconfigSync('test-app', options).load(\n-                    filepath,\n-                );\n-                const ccResult = cosmiconfigSync('test-app', options).load(\n-                    filepath,\n-                );\n-\n-                const expected = {config: undefined, filepath, isEmpty: true};\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-\n-            it('async', async () => {\n-                const options = {\n-                    ignoreEmptySearchPlaces: false,\n-                };\n-                const result = await lilconfig('test-app', options).load(\n-                    filepath,\n-                );\n-                const ccResult = await cosmiconfig('test-app', options).load(\n-                    filepath,\n-                );\n-\n-                const expected = {config: undefined, filepath, isEmpty: true};\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-    });\n-\n-    it('stopDir', () => {\n-        const stopDir = path.join(__dirname, 'search');\n-        const searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n-\n-        const result = lilconfigSync('non-existent', {stopDir}).search(\n-            searchFrom,\n-        );\n-        const ccResult = cosmiconfigSync('non-existent', {stopDir}).search(\n-            searchFrom,\n-        );\n-\n-        const expected = null;\n-\n-        expect(result).toEqual(expected);\n-        expect(ccResult).toEqual(expected);\n-    });\n-\n-    it('searchPlaces', () => {\n-        const stopDir = path.join(__dirname, 'search');\n-        const searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n-        const searchPlaces = ['searchPlaces.conf.js', 'searchPlaces-noExt'];\n-\n-        const options = {\n-            stopDir,\n-            searchPlaces,\n-        };\n-\n-        const result = lilconfigSync('doesnt-matter', options).search(\n-            searchFrom,\n-        );\n-        const ccResult = cosmiconfigSync('doesnt-matter', options).search(\n-            searchFrom,\n-        );\n-\n-        const expected = {\n-            config: {\n-                searchPlacesWorks: true,\n-            },\n-            filepath: path.join(\n-                __dirname,\n-                'search',\n-                'a',\n-                'b',\n-                'searchPlaces.conf.js',\n-            ),\n-        };\n-\n-        expect(result).toEqual(expected);\n-        expect(ccResult).toEqual(expected);\n-    });\n-\n-    describe('cache', () => {\n-        // running all checks in one to avoid resetting cache for fs.promises.access\n-        describe('enabled(default)', () => {\n-            it('async search()', async () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchFrom = path.join(stopDir, 'a', 'b', 'c');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfig('cached', {\n-                    cache: true,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const fsLookUps = () =>\n-                    (fs.promises.access as jest.Mock).mock.calls.length;\n-\n-                expect(fsLookUps()).toBe(0);\n-\n-                // per one search\n-                // for unexisting\n-                // (search + a + b + c) * times searchPlaces\n-\n-                // for existing\n-                // (search + a + b + c) * (times searchPlaces - **first** matched)\n-                const expectedFsLookUps = 7;\n-\n-                // initial search populates cache\n-                const result = await searcher.search(searchFrom);\n-\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-\n-                // subsequant search reads from cache\n-                const result2 = await searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-                expect(result).toEqual(result2);\n-\n-                // searching a subpath reuses cache\n-                const result3 = await searcher.search(path.join(stopDir, 'a'));\n-                const result4 = await searcher.search(\n-                    path.join(stopDir, 'a', 'b'),\n-                );\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-                expect(result2).toEqual(result3);\n-                expect(result3).toEqual(result4);\n-\n-                // calling clearCaches empties search cache\n-                searcher.clearCaches();\n-\n-                // emptied all caches, should perform new lookups\n-                const result5 = await searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 2);\n-                expect(result4).toEqual(result5);\n-                // different references\n-                expect(result4 === result5).toEqual(false);\n-\n-                searcher.clearSearchCache();\n-                const result6 = await searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 3);\n-                expect(result5).toEqual(result6);\n-                // different references\n-                expect(result5 === result6).toEqual(false);\n-\n-                // clearLoadCache does not clear search cache\n-                searcher.clearLoadCache();\n-                const result7 = await searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 3);\n-                expect(result6).toEqual(result7);\n-                // same references\n-                expect(result6 === result7).toEqual(true);\n-\n-                // searching a superset path will access fs until it hits a known path\n-                const result8 = await searcher.search(\n-                    path.join(searchFrom, 'd'),\n-                );\n-                expect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n-                expect(result7).toEqual(result8);\n-                // same references\n-                expect(result7 === result8).toEqual(true);\n-\n-                // repeated searches do not cause extra fs calls\n-                const result9 = await searcher.search(\n-                    path.join(searchFrom, 'd'),\n-                );\n-                expect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n-                expect(result8).toEqual(result9);\n-                // same references\n-                expect(result8 === result9).toEqual(true);\n-            });\n-\n-            it('sync search()', () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchFrom = path.join(stopDir, 'a', 'b', 'c');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfigSync('cached', {\n-                    cache: true,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const fsLookUps = () =>\n-                    (fs.accessSync as jest.Mock).mock.calls.length;\n-\n-                expect(fsLookUps()).toBe(0);\n-\n-                // per one search\n-                // for unexisting\n-                // (search + a + b + c) * times searchPlaces\n-\n-                // for existing\n-                // (search + a + b + c) * (times searchPlaces - **first** matched)\n-                const expectedFsLookUps = 7;\n-\n-                // initial search populates cache\n-                const result = searcher.search(searchFrom);\n-\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-\n-                // subsequant search reads from cache\n-                const result2 = searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-                expect(result).toEqual(result2);\n-\n-                // searching a subpath reuses cache\n-                const result3 = searcher.search(path.join(stopDir, 'a'));\n-                const result4 = searcher.search(path.join(stopDir, 'a', 'b'));\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-                expect(result2).toEqual(result3);\n-                expect(result3).toEqual(result4);\n-\n-                // calling clearCaches empties search cache\n-                searcher.clearCaches();\n-\n-                // emptied all caches, should perform new lookups\n-                const result5 = searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 2);\n-                expect(result4).toEqual(result5);\n-                // different references\n-                expect(result4 === result5).toEqual(false);\n-\n-                searcher.clearSearchCache();\n-                const result6 = searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 3);\n-                expect(result5).toEqual(result6);\n-                // different references\n-                expect(result5 === result6).toEqual(false);\n-\n-                // clearLoadCache does not clear search cache\n-                searcher.clearLoadCache();\n-                const result7 = searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 3);\n-                expect(result6).toEqual(result7);\n-                // same references\n-                expect(result6 === result7).toEqual(true);\n-\n-                // searching a superset path will access fs until it hits a known path\n-                const result8 = searcher.search(path.join(searchFrom, 'd'));\n-                expect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n-                expect(result7).toEqual(result8);\n-                // same references\n-                expect(result7 === result8).toEqual(true);\n-\n-                // repeated searches do not cause extra fs calls\n-                const result9 = searcher.search(path.join(searchFrom, 'd'));\n-                expect(fsLookUps()).toBe(3 * expectedFsLookUps + 2);\n-                expect(result8).toEqual(result9);\n-                // same references\n-                expect(result8 === result9).toEqual(true);\n-            });\n-\n-            it('async load()', async () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfig('cached', {\n-                    cache: true,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const existingFile = path.join(stopDir, 'cached.config.js');\n-                const fsReadFileCalls = () =>\n-                    (fs.promises.readFile as jest.Mock).mock.calls.length;\n-\n-                expect(fsReadFileCalls()).toBe(0);\n-\n-                // initial search populates cache\n-                const result = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-\n-                // subsequant load reads from cache\n-                const result2 = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-                expect(result).toEqual(result2);\n-                // same reference\n-                expect(result === result2).toEqual(true);\n-\n-                // calling clearCaches empties search cache\n-                searcher.clearCaches();\n-                const result3 = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(2);\n-                expect(result2).toEqual(result3);\n-                // different reference\n-                expect(result2 === result3).toEqual(false);\n-\n-                searcher.clearLoadCache();\n-                const result4 = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(3);\n-                expect(result3).toEqual(result4);\n-                // different reference\n-                expect(result3 === result4).toEqual(false);\n-\n-                // clearLoadCache does not clear search cache\n-                searcher.clearSearchCache();\n-                const result5 = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(3);\n-                expect(result4).toEqual(result5);\n-                // same reference\n-                expect(result4 === result5).toEqual(true);\n-            });\n-\n-            it('sync load()', () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfigSync('cached', {\n-                    cache: true,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const existingFile = path.join(stopDir, 'cached.config.js');\n-                const fsReadFileCalls = () =>\n-                    (fs.readFileSync as jest.Mock).mock.calls.length;\n-\n-                expect(fsReadFileCalls()).toBe(0);\n-\n-                // initial search populates cache\n-                const result = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-\n-                // subsequant load reads from cache\n-                const result2 = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-                expect(result).toEqual(result2);\n-                // same reference\n-                expect(result === result2).toEqual(true);\n-\n-                // calling clearCaches empties search cache\n-                searcher.clearCaches();\n-                const result3 = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(2);\n-                expect(result2).toEqual(result3);\n-                // different reference\n-                expect(result2 === result3).toEqual(false);\n-\n-                searcher.clearLoadCache();\n-                const result4 = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(3);\n-                expect(result3).toEqual(result4);\n-                // different reference\n-                expect(result3 === result4).toEqual(false);\n-\n-                // clearLoadCache does not clear search cache\n-                searcher.clearSearchCache();\n-                const result5 = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(3);\n-                expect(result4).toEqual(result5);\n-                // same reference\n-                expect(result4 === result5).toEqual(true);\n-            });\n-        });\n-        describe('disabled', () => {\n-            it('async search()', async () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchFrom = path.join(stopDir, 'a', 'b', 'c');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfig('cached', {\n-                    cache: false,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const fsLookUps = () =>\n-                    (fs.promises.access as jest.Mock).mock.calls.length;\n-\n-                expect(fsLookUps()).toBe(0);\n-\n-                const expectedFsLookUps = 7;\n-\n-                // initial search populates cache\n-                const result = await searcher.search(searchFrom);\n-\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-\n-                // subsequant search reads from cache\n-                const result2 = await searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 2);\n-                expect(result).toEqual(result2);\n-\n-                expect(result2 === result).toBe(false);\n-            });\n-\n-            it('sync search()', () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchFrom = path.join(stopDir, 'a', 'b', 'c');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfigSync('cached', {\n-                    cache: false,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const fsLookUps = () =>\n-                    (fs.accessSync as jest.Mock).mock.calls.length;\n-\n-                expect(fsLookUps()).toBe(0);\n-\n-                const expectedFsLookUps = 7;\n-\n-                // initial search populates cache\n-                const result = searcher.search(searchFrom);\n-\n-                expect(fsLookUps()).toBe(expectedFsLookUps);\n-\n-                // subsequent search reads from cache\n-                const result2 = searcher.search(searchFrom);\n-                expect(fsLookUps()).toBe(expectedFsLookUps * 2);\n-                expect(result).toEqual(result2);\n-\n-                expect(result2 === result).toBe(false);\n-            });\n-\n-            it('async load()', async () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfig('cached', {\n-                    cache: false,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const existingFile = path.join(stopDir, 'cached.config.js');\n-                const fsReadFileCalls = () =>\n-                    (fs.promises.readFile as jest.Mock).mock.calls.length;\n-\n-                expect(fsReadFileCalls()).toBe(0);\n-\n-                // initial search populates cache\n-                const result = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-\n-                // subsequant load reads from cache\n-                const result2 = await searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(2);\n-                expect(result).toEqual(result2);\n-                // different reference\n-                expect(result === result2).toEqual(false);\n-            });\n-\n-            it('sync load()', () => {\n-                const stopDir = path.join(__dirname, 'search');\n-                const searchPlaces = ['cached.config.js', 'package.json'];\n-                const searcher = lilconfigSync('cached', {\n-                    cache: false,\n-                    stopDir,\n-                    searchPlaces,\n-                });\n-                const existingFile = path.join(stopDir, 'cached.config.js');\n-                const fsReadFileCalls = () =>\n-                    (fs.readFileSync as jest.Mock).mock.calls.length;\n-\n-                expect(fsReadFileCalls()).toBe(0);\n-\n-                // initial search populates cache\n-                const result = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(1);\n-\n-                // subsequant load reads from cache\n-                const result2 = searcher.load(existingFile);\n-                expect(fsReadFileCalls()).toBe(2);\n-                expect(result).toEqual(result2);\n-                // differnt reference\n-                expect(result === result2).toEqual(false);\n-            });\n-        });\n-    });\n-\n-    describe('packageProp', () => {\n-        describe('plain property string', () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const options = {packageProp: 'foo'};\n-            const filepath = path.join(dirname, 'package.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const expected = {\n-                config: {\n-                    insideFoo: true,\n-                },\n-                filepath,\n-            };\n-\n-            it('sync', () => {\n-                const result = lilconfigSync('foo', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = cosmiconfigSync('foo', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-            it('async', async () => {\n-                const result = await lilconfig('foo', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = await cosmiconfig('foo', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-\n-        describe('array of strings', () => {\n-            const filepath = path.join(\n-                __dirname,\n-                'search',\n-                'a',\n-                'package.json',\n-            );\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const options = {\n-                packageProp: 'bar.baz',\n-                stopDir: path.join(__dirname, 'search'),\n-            };\n-            const expected = {\n-                config: {\n-                    insideBarBaz: true,\n-                },\n-                filepath,\n-            };\n-\n-            it('sync', () => {\n-                const result = lilconfigSync('foo', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = cosmiconfigSync('foo', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-            it('async', async () => {\n-                const result = await lilconfig('foo', options).load(\n-                    relativeFilepath,\n-                );\n-                const ccResult = await cosmiconfig('foo', options).load(\n-                    relativeFilepath,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-\n-        describe('string[] with null in the middle', () => {\n-            const searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n-            const options = {\n-                packageProp: 'bar.baz',\n-                stopDir: path.join(__dirname, 'search'),\n-            };\n-            /**\n-             * cosmiconfig throws when there is `null` value in the chain of package prop keys\n-             */\n-\n-            const expectedMessage =\n-                parseInt(process.version.slice(1), 10) > 14\n-                    ? \"Cannot read properties of null (reading 'baz')\"\n-                    : \"Cannot read property 'baz' of null\";\n-\n-            it('sync', () => {\n-                expect(() => {\n-                    lilconfigSync('foo', options).search(searchFrom);\n-                }).toThrowError(expectedMessage);\n-                expect(() => {\n-                    cosmiconfigSync('foo', options).search(searchFrom);\n-                }).toThrowError(expectedMessage);\n-            });\n-            it('async', async () => {\n-                expect(\n-                    lilconfig('foo', options).search(searchFrom),\n-                ).rejects.toThrowError(expectedMessage);\n-                expect(\n-                    cosmiconfig('foo', options).search(searchFrom),\n-                ).rejects.toThrowError(expectedMessage);\n-            });\n-        });\n-\n-        describe('string[] with result', () => {\n-            const searchFrom = path.join(__dirname, 'search', 'a', 'b', 'c');\n-            const options = {\n-                packageProp: 'zoo.foo',\n-                stopDir: path.join(__dirname, 'search'),\n-            };\n-            const expected = {\n-                config: {\n-                    insideZooFoo: true,\n-                },\n-                filepath: path.join(__dirname, 'search', 'a', 'package.json'),\n-            };\n-\n-            it('sync', () => {\n-                const result = lilconfigSync('foo', options).search(searchFrom);\n-                const ccResult = cosmiconfigSync('foo', options).search(\n-                    searchFrom,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-            it('async', async () => {\n-                const result = await lilconfig('foo', options).search(\n-                    searchFrom,\n-                );\n-                const ccResult = await cosmiconfig('foo', options).search(\n-                    searchFrom,\n-                );\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        });\n-    });\n-});\n-describe('lilconfigSync', () => {\n-    describe('load', () => {\n-        const dirname = path.join(__dirname, 'load');\n-\n-        it('existing js file', () => {\n-            const filepath = path.join(dirname, 'test-app.js');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const result = lilconfigSync('test-app').load(relativeFilepath);\n-            const ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n-\n-            const expected = {\n-                config: {jsTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('existing cjs file', () => {\n-            const filepath = path.join(dirname, 'test-app.cjs');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const result = lilconfigSync('test-app').load(relativeFilepath);\n-            const ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n-\n-            const expected = {\n-                config: {jsTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('existing json file', () => {\n-            const filepath = path.join(dirname, 'test-app.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const result = lilconfigSync('test-app').load(relativeFilepath);\n-            const ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n-\n-            const expected = {\n-                config: {jsonTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('no extension json file', () => {\n-            const filepath = path.join(dirname, 'test-noExt-json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const result = lilconfigSync('test-app').load(relativeFilepath);\n-            const ccResult = cosmiconfigSync('test-app').load(relativeFilepath);\n-\n-            const expected = {\n-                config: {noExtJsonFile: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('package.json', () => {\n-            const filepath = path.join(dirname, 'package.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const options = {};\n-            const result = lilconfigSync('test-app', options).load(\n-                relativeFilepath,\n-            );\n-            const ccResult = cosmiconfigSync('test-app', options).load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {\n-                    customThingHere: 'is-configured',\n-                },\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-    });\n-\n-    describe('search', () => {\n-        const dirname = path.join(__dirname, 'search');\n-\n-        it('default for searchFrom', () => {\n-            const result = lilconfigSync('non-existent').search();\n-            const ccResult = cosmiconfigSync('non-existent').search();\n-\n-            const expected = null;\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('checks in hidden .config dir', () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const result = lilconfigSync('hidden').search(searchFrom);\n-            const ccResult = cosmiconfigSync('hidden').search(searchFrom);\n-\n-            const expected = {hidden: true};\n-\n-            expect(result?.config).toEqual(expected);\n-            expect(ccResult?.config).toEqual(expected);\n-        });\n-\n-        if (process.platform !== 'win32') {\n-            it('default for searchFrom till root directory', () => {\n-                const options = {stopDir: '/'};\n-                const result = lilconfigSync('non-existent', options).search();\n-                expect(\n-                    (fs.accessSync as jest.Mock).mock.calls.slice(-10),\n-                ).toEqual([\n-                    ['/package.json'],\n-                    ['/.non-existentrc.json'],\n-                    ['/.non-existentrc.js'],\n-                    ['/.non-existentrc.cjs'],\n-                    ['/.config/non-existentrc'],\n-                    ['/.config/non-existentrc.json'],\n-                    ['/.config/non-existentrc.js'],\n-                    ['/.config/non-existentrc.cjs'],\n-                    ['/non-existent.config.js'],\n-                    ['/non-existent.config.cjs'],\n-                ]);\n-                const ccResult = cosmiconfigSync(\n-                    'non-existent',\n-                    options,\n-                ).search();\n-\n-                const expected = null;\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        }\n-\n-        it('provided searchFrom', () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-            };\n-\n-            const result = lilconfigSync('non-existent', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = cosmiconfigSync('non-existent', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = null;\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('treating empty configs', () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-            };\n-\n-            const result = lilconfigSync('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = cosmiconfigSync('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = {\n-                config: {\n-                    notSoEmpty: true,\n-                },\n-                filepath: path.join(dirname, 'a', 'maybeEmpty.config.js'),\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('treating empty configs with ignoreEmptySearchPlaces off', () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-                ignoreEmptySearchPlaces: false,\n-            };\n-\n-            const result = lilconfigSync('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = cosmiconfigSync('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = {\n-                config: undefined,\n-                filepath: path.join(dirname, 'a', 'b', 'maybeEmpty.config.js'),\n-                isEmpty: true,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-    });\n-\n-    describe('when to throw', () => {\n-        it('loader throws', () => {\n-            const dirname = path.join(__dirname, 'search');\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            class LoaderError extends Error {}\n-\n-            const options = {\n-                loaders: {\n-                    '.js'(): void {\n-                        throw new LoaderError();\n-                    },\n-                },\n-            };\n-\n-            expect(() => {\n-                lilconfigSync('maybeEmpty', options).search(searchFrom);\n-            }).toThrowError(LoaderError);\n-            expect(() => {\n-                cosmiconfigSync('maybeEmpty', options).search(searchFrom);\n-            }).toThrowError(LoaderError);\n-        });\n-\n-        it('non existing file', () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'nope.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            expect(() => {\n-                lilconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(\n-                `ENOENT: no such file or directory, open '${filepath}'`,\n-            );\n-\n-            expect(() => {\n-                cosmiconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(\n-                `ENOENT: no such file or directory, open '${filepath}'`,\n-            );\n-        });\n-\n-        it('throws for invalid json', () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'test-invalid.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            /**\n-             * throws but less elegant\n-             */\n-            expect(() => {\n-                lilconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(\n-                isNodeV20orNewer\n-                    ? `Expected ',' or '}' after property value in JSON at position 22`\n-                    : 'Unexpected token / in JSON at position 22',\n-            );\n-\n-            expect(() => {\n-                cosmiconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(`JSON Error in ${filepath}:`);\n-        });\n-\n-        it('throws for provided filepath that does not exist', () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'i-do-no-exist.js');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n-\n-            expect(() => {\n-                lilconfigSync('test-app', {}).load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync('test-app', {}).load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-        });\n-\n-        it('no loader specified for the search place', () => {\n-            const filepath = path.join(__dirname, 'load', 'config.coffee');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const errMsg = 'No loader specified for extension \".coffee\"';\n-\n-            expect(() => {\n-                lilconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-        });\n-\n-        it('loader is not a function', () => {\n-            const filepath = path.join(__dirname, 'load', 'config.coffee');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const options = {\n-                loaders: {\n-                    '.coffee': true,\n-                },\n-            };\n-\n-            const errMsg = 'loader is not a function';\n-\n-            expect(() => {\n-                // @ts-expect-error: unit test is literally for this purpose\n-                lilconfigSync('test-app', options).load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                // @ts-ignore: unit test is literally for this purpose\n-                cosmiconfigSync('test-app', options).load(relativeFilepath);\n-            }).toThrowError(errMsg);\n-        });\n-\n-        it('no extension loader throws for unparsable file', () => {\n-            const filepath = path.join(\n-                __dirname,\n-                'load',\n-                'test-noExt-nonParsable',\n-            );\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            expect(() => {\n-                lilconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(\n-                isNodeV20orNewer\n-                    ? `Unexpected token 'h', \\\"hobbies:\\n- \"Reading\\n`\n-                    : 'Unexpected token # in JSON at position 2',\n-            );\n-            expect(() => {\n-                cosmiconfigSync('test-app').load(relativeFilepath);\n-            }).toThrowError(`YAML Error in ${filepath}`);\n-        });\n-\n-        it('throws for empty strings passed to load', () => {\n-            const errMsg = 'load must pass a non-empty string';\n-\n-            expect(() => {\n-                lilconfigSync('test-app').load('');\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync('test-app').load('');\n-            }).toThrowError('EISDIR: illegal operation on a directory, read');\n-        });\n-\n-        it('throws when provided searchPlace has no loader', () => {\n-            const errMsg = 'Missing loader for extension \"file.coffee\"';\n-            expect(() => {\n-                lilconfigSync('foo', {\n-                    searchPlaces: ['file.coffee'],\n-                });\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync('foo', {\n-                    searchPlaces: ['file.coffee'],\n-                });\n-            }).toThrowError(errMsg);\n-        });\n-\n-        it('throws when a loader for a searchPlace is not a function', () => {\n-            const errMsg =\n-                'Loader for extension \"file.js\" is not a function: Received object.';\n-            const options = {\n-                searchPlaces: ['file.js'],\n-                loaders: {\n-                    '.js': {},\n-                },\n-            };\n-            expect(() => {\n-                lilconfigSync(\n-                    'foo',\n-                    // @ts-expect-error: unit test is literally for this purpose\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync(\n-                    'foo',\n-                    // @ts-ignore: needed for jest\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-        });\n-\n-        it('throws for searchPlaces with no extension', () => {\n-            const errMsg =\n-                'Loader for extension \"file\" is not a function: Received object.';\n-            const options = {\n-                searchPlaces: ['file'],\n-                loaders: {\n-                    noExt: {},\n-                },\n-            };\n-            expect(() => {\n-                lilconfigSync(\n-                    'foo',\n-                    // @ts-expect-error: unit test is literally for this purpose\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfigSync(\n-                    'foo',\n-                    // @ts-ignore: needed for jest\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-        });\n-    });\n-});\n-\n-describe('lilconfig', () => {\n-    describe('load', () => {\n-        const dirname = path.join(__dirname, 'load');\n-\n-        it('existing js file', async () => {\n-            const filepath = path.join(dirname, 'test-app.js');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const result = await lilconfig('test-app').load(relativeFilepath);\n-            const ccResult = await cosmiconfig('test-app').load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {jsTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('existing cjs file', async () => {\n-            const filepath = path.join(dirname, 'test-app.cjs');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const result = await lilconfig('test-app').load(relativeFilepath);\n-            const ccResult = await cosmiconfig('test-app').load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {jsTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('existing json file', async () => {\n-            const filepath = path.join(dirname, 'test-app.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const result = await lilconfig('test-app').load(relativeFilepath);\n-            const ccResult = await cosmiconfig('test-app').load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {jsonTest: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('no extension json file', async () => {\n-            const filepath = path.join(dirname, 'test-noExt-json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const result = await lilconfig('test-app').load(relativeFilepath);\n-            const ccResult = await cosmiconfig('test-app').load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {noExtJsonFile: true},\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(result).toEqual(ccResult);\n-        });\n-\n-        it('package.json', async () => {\n-            const filepath = path.join(dirname, 'package.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const options = {};\n-            const result = await lilconfig('test-app', options).load(\n-                relativeFilepath,\n-            );\n-            const ccResult = await cosmiconfig('test-app', options).load(\n-                relativeFilepath,\n-            );\n-\n-            const expected = {\n-                config: {\n-                    customThingHere: 'is-configured',\n-                },\n-                filepath,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-    });\n-\n-    describe('search', () => {\n-        const dirname = path.join(__dirname, 'search');\n-\n-        it('returns null when no config found', async () => {\n-            const result = await lilconfig('non-existent').search();\n-            const ccResult = await cosmiconfig('non-existent').search();\n-\n-            const expected = null;\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('checks in hidden .config dir', async () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const result = await lilconfig('hidden').search(searchFrom);\n-            const ccResult = await cosmiconfig('hidden').search(searchFrom);\n-\n-            const expected = {hidden: true};\n-\n-            expect(result?.config).toEqual(expected);\n-            expect(ccResult?.config).toEqual(expected);\n-        });\n-\n-        if (process.platform !== 'win32') {\n-            it('searches root directory correctly', async () => {\n-                const options = {stopDir: '/'};\n-                const result = await lilconfig(\n-                    'non-existent',\n-                    options,\n-                ).search();\n-                expect(\n-                    (fs.promises.access as jest.Mock).mock.calls.slice(-13),\n-                ).toEqual([\n-                    ['/package.json'],\n-                    ['/.non-existentrc.json'],\n-                    ['/.non-existentrc.js'],\n-                    ['/.non-existentrc.cjs'],\n-                    ['/.non-existentrc.mjs'],\n-                    ['/.config/non-existentrc'],\n-                    ['/.config/non-existentrc.json'],\n-                    ['/.config/non-existentrc.js'],\n-                    ['/.config/non-existentrc.cjs'],\n-                    ['/.config/non-existentrc.mjs'],\n-                    ['/non-existent.config.js'],\n-                    ['/non-existent.config.cjs'],\n-                    ['/non-existent.config.mjs'],\n-                ]);\n-                const ccResult = await cosmiconfig(\n-                    'non-existent',\n-                    options,\n-                ).search();\n-\n-                const expected = null;\n-\n-                expect(result).toEqual(expected);\n-                expect(ccResult).toEqual(expected);\n-            });\n-        }\n-\n-        it('provided searchFrom', async () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-            };\n-\n-            const result = await lilconfig('non-existent', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = await cosmiconfig('non-existent', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = null;\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('treating empty configs', async () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-            };\n-\n-            const result = await lilconfig('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = await cosmiconfig('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = {\n-                config: {\n-                    notSoEmpty: true,\n-                },\n-                filepath: path.join(dirname, 'a', 'maybeEmpty.config.js'),\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-\n-        it('treating empty configs with ignoreEmptySearchPlaces off', async () => {\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            const options = {\n-                stopDir: dirname,\n-                ignoreEmptySearchPlaces: false,\n-            };\n-\n-            const result = await lilconfig('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-            const ccResult = await cosmiconfig('maybeEmpty', options).search(\n-                searchFrom,\n-            );\n-\n-            const expected = {\n-                config: undefined,\n-                filepath: path.join(dirname, 'a', 'b', 'maybeEmpty.config.js'),\n-                isEmpty: true,\n-            };\n-\n-            expect(result).toEqual(expected);\n-            expect(ccResult).toEqual(expected);\n-        });\n-    });\n-\n-    describe('when to throw', () => {\n-        it('loader throws', async () => {\n-            const dirname = path.join(__dirname, 'search');\n-            const searchFrom = path.join(dirname, 'a', 'b', 'c');\n-\n-            class LoaderError extends Error {}\n-\n-            const options = {\n-                loaders: {\n-                    '.js': (): void => {\n-                        throw new LoaderError();\n-                    },\n-                },\n-            };\n-\n-            const result = await lilconfig('maybeEmpty', options)\n-                .search(searchFrom)\n-                .catch(x => x);\n-            const ccResult = await cosmiconfig('maybeEmpty', options)\n-                .search(searchFrom)\n-                .catch((x: unknown) => x);\n-\n-            expect(result instanceof LoaderError).toBeTruthy();\n-            expect(ccResult instanceof LoaderError).toBeTruthy();\n-        });\n-\n-        it('non existing file', async () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'nope.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n-\n-            expect(\n-                lilconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-\n-            expect(\n-                cosmiconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-        });\n-\n-        it('throws for invalid json', async () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'test-invalid.json');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            /**\n-             * throws but less elegant\n-             */\n-            expect(\n-                lilconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(\n-                isNodeV20orNewer\n-                    ? `Expected ',' or '}' after property value in JSON at position 22`\n-                    : 'Unexpected token / in JSON at position 22',\n-            );\n-\n-            expect(\n-                cosmiconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(`JSON Error in ${filepath}:`);\n-        });\n-\n-        it('throws for provided filepath that does not exist', async () => {\n-            const dirname = path.join(__dirname, 'load');\n-            const filepath = path.join(dirname, 'i-do-no-exist.js');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const errMsg = `ENOENT: no such file or directory, open '${filepath}'`;\n-\n-            expect(\n-                lilconfig('test-app', {}).load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-            expect(\n-                cosmiconfig('test-app', {}).load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-        });\n-\n-        it('no loader specified for the search place', async () => {\n-            const filepath = path.join(__dirname, 'load', 'config.coffee');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            const errMsg = 'No loader specified for extension \".coffee\"';\n-\n-            expect(\n-                lilconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-            expect(\n-                cosmiconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-        });\n-\n-        it('loader is not a function', async () => {\n-            const filepath = path.join(__dirname, 'load', 'config.coffee');\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-            const options = {\n-                loaders: {\n-                    '.coffee': true,\n-                },\n-            };\n-\n-            const errMsg = 'loader is not a function';\n-\n-            expect(\n-                lilconfig(\n-                    'test-app',\n-                    // @ts-expect-error: for unit test purpose\n-                    options,\n-                ).load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-            expect(\n-                // @ts-ignore: required for jest, but not ts used in editor\n-                cosmiconfig('test-app', options).load(relativeFilepath),\n-            ).rejects.toThrowError(errMsg);\n-        });\n-\n-        it('no extension loader throws for unparsable file', async () => {\n-            const filepath = path.join(\n-                __dirname,\n-                'load',\n-                'test-noExt-nonParsable',\n-            );\n-            const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-\n-            await expect(\n-                lilconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(\n-                isNodeV20orNewer\n-                    ? `Unexpected token 'h', \"hobbies:\\n- \\\"Reading\\n\" is not valid JSON`\n-                    : 'Unexpected token h in JSON at position 0',\n-            );\n-            await expect(\n-                cosmiconfig('test-app').load(relativeFilepath),\n-            ).rejects.toThrowError(`YAML Error in ${filepath}`);\n-        });\n-\n-        it('throws for empty strings passed to load', async () => {\n-            const errMsg = 'load must pass a non-empty string';\n-\n-            expect(lilconfig('test-app').load('')).rejects.toThrowError(errMsg);\n-            expect(cosmiconfig('test-app').load('')).rejects.toThrowError(\n-                'EISDIR: illegal operation on a directory, read',\n-            );\n-        });\n-\n-        it('throws when provided searchPlace has no loader', () => {\n-            const errMsg = 'Missing loader for extension \"file.coffee\"';\n-            expect(() =>\n-                lilconfig('foo', {\n-                    searchPlaces: ['file.coffee'],\n-                }),\n-            ).toThrowError(errMsg);\n-            expect(() =>\n-                cosmiconfig('foo', {\n-                    searchPlaces: ['file.coffee'],\n-                }),\n-            ).toThrowError(errMsg);\n-        });\n-\n-        it('throws when a loader for a searchPlace is not a function', () => {\n-            const errMsg =\n-                'Loader for extension \"file.js\" is not a function: Received object';\n-            const options = {\n-                searchPlaces: ['file.js'],\n-                loaders: {\n-                    '.js': {},\n-                },\n-            };\n-            expect(() =>\n-                lilconfig(\n-                    'foo',\n-                    // @ts-expect-error: for unit test purpose\n-                    options,\n-                ),\n-            ).toThrowError(errMsg);\n-            expect(() =>\n-                cosmiconfig(\n-                    'foo',\n-                    // @ts-ignore: needed for jest\n-                    options,\n-                ),\n-            ).toThrowError(errMsg);\n-        });\n-\n-        it('throws for searchPlaces with no extension', () => {\n-            const errMsg =\n-                'Loader for extension \"file\" is not a function: Received object.';\n-            const options = {\n-                searchPlaces: ['file'],\n-                loaders: {\n-                    noExt: {},\n-                },\n-            };\n-            expect(() => {\n-                lilconfig(\n-                    'foo',\n-                    // @ts-expect-error: for unit test purpose\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-            expect(() => {\n-                cosmiconfig(\n-                    'foo',\n-                    // @ts-ignore: needed for jest, but not editor\n-                    options,\n-                );\n-            }).toThrowError(errMsg);\n-        });\n-    });\n-});\n-\n-describe('npm package api', () => {\n-    it('exports the same things as cosmiconfig', () => {\n-        // eslint-disable-next-line @typescript-eslint/no-var-requires\n-        const lc = require('../index');\n-        // eslint-disable-next-line @typescript-eslint/no-var-requires\n-        const cc = require('cosmiconfig');\n-\n-        expect(typeof lc.defaultLoaders).toEqual(typeof cc.defaultLoaders);\n-        expect(typeof lc.defaultLoadersAsync).toEqual(\n-            typeof cc.defaultLoadersAsync,\n-        );\n-\n-        const lcExplorerSyncKeys = Object.keys(lc.lilconfigSync('foo'));\n-        const ccExplorerSyncKeys = Object.keys(cc.cosmiconfigSync('foo'));\n-\n-        expect(lcExplorerSyncKeys).toEqual(ccExplorerSyncKeys);\n-\n-        /* eslint-disable @typescript-eslint/no-unused-vars */\n-        const omitKnownDifferKeys = ({\n-            lilconfig,\n-            lilconfigSync,\n-            cosmiconfig,\n-            cosmiconfigSync,\n-            metaSearchPlaces,\n-            ...rest\n-        }: {\n-            [key: string]: unknown;\n-        }) => rest;\n-        /* eslint-enable @typescript-eslint/no-unused-vars */\n-\n-        expect(Object.keys(omitKnownDifferKeys(lc)).sort()).toEqual(\n-            Object.keys(omitKnownDifferKeys(cc)).sort(),\n-        );\n-    });\n-});",
          "src/spec/load/test-app.cjs": "@@ -1,3 +1,3 @@\n module.exports = {\n-    jsTest: true,\n+\tjsTest: true,\n };",
          "src/spec/load/test-app.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    jsTest: true,\n+\tjsTest: true,\n };",
          "src/spec/load/test-app.ts": "@@ -1,3 +1,3 @@\n export default {\n-    typescript: true,\n+\ttypescript: true,\n };",
          "src/spec/old-node-tests.uvu.js": "@@ -0,0 +1,181 @@\n+const uvu = require('uvu');\n+const assert = require('assert');\n+const path = require('path');\n+const {lilconfig, lilconfigSync} = require('..');\n+const {cosmiconfig, cosmiconfigSync} = require('cosmiconfig');\n+const {transpileModule} = require('typescript');\n+\n+const dirname = path.join(__dirname, 'load');\n+/** @type {import('../index').LoaderSync} */\n+const tsLoader = (_, content) => {\n+\tconst res = transpileModule(content, {}).outputText;\n+\treturn eval(res);\n+};\n+\n+const tsLoaderSuit = uvu.suite('ts-loader');\n+\n+tsLoaderSuit('sync', () => {\n+\tconst filepath = path.join(dirname, 'test-app.ts');\n+\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\tconst options = {\n+\t\tloaders: {\n+\t\t\t'.ts': tsLoader,\n+\t\t},\n+\t};\n+\tconst expected = {\n+\t\tconfig: {\n+\t\t\ttypescript: true,\n+\t\t},\n+\t\tfilepath,\n+\t};\n+\tconst result = lilconfigSync('test-app', options).load(relativeFilepath);\n+\tconst ccResult = cosmiconfigSync('test-app', options).load(relativeFilepath);\n+\n+\tassert.deepStrictEqual(result, expected);\n+\tassert.deepStrictEqual(ccResult, expected);\n+});\n+\n+tsLoaderSuit('async', async () => {\n+\tconst filepath = path.join(dirname, 'test-app.ts');\n+\tconst relativeFilepath = filepath.slice(process.cwd().length + 1);\n+\tconst options = {\n+\t\tloaders: {\n+\t\t\t'.ts': tsLoader,\n+\t\t},\n+\t};\n+\tconst expected = {\n+\t\tconfig: {\n+\t\t\ttypescript: true,\n+\t\t},\n+\t\tfilepath,\n+\t};\n+\tconst result = await lilconfig('test-app', options).load(relativeFilepath);\n+\tconst ccResult = await cosmiconfig('test-app', options).load(\n+\t\trelativeFilepath,\n+\t);\n+\n+\tassert.deepStrictEqual(result, expected);\n+\tassert.deepStrictEqual(ccResult, expected);\n+});\n+\n+tsLoaderSuit.run();\n+\n+const esmProjectSuit = uvu.suite('esm-project');\n+esmProjectSuit('async search js', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.js');\n+\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['esm.config.js'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {esm: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+esmProjectSuit('async search mjs', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.mjs');\n+\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['esm.config.mjs'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {esm: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+esmProjectSuit('async search cjs', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'esm-project', 'esm.config.cjs');\n+\tconst searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['esm.config.cjs'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {cjs: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+esmProjectSuit.run();\n+\n+const cjsProjectSuit = uvu.suite('cjs-project');\n+cjsProjectSuit('async search js', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.js');\n+\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['cjs.config.js'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {cjs: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+cjsProjectSuit('async search mjs', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.mjs');\n+\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['cjs.config.mjs'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {esm: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+cjsProjectSuit('async search cjs', async () => {\n+\tconst stopDir = __dirname;\n+\tconst filepath = path.join(stopDir, 'cjs-project', 'cjs.config.cjs');\n+\tconst searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n+\n+\tconst options = {\n+\t\tsearchPlaces: ['cjs.config.cjs'],\n+\t\tstopDir,\n+\t};\n+\n+\tconst config = {cjs: true};\n+\n+\tconst result = await lilconfig('test-app', options).search(searchFrom);\n+\tconst ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n+\n+\tassert.deepStrictEqual(result, {config, filepath});\n+\tassert.deepStrictEqual(ccResult, {config, filepath});\n+});\n+\n+cjsProjectSuit.run();",
          "src/spec/old-node-tests.uvu.ts": "@@ -1,182 +0,0 @@\n-import * as uvu from 'uvu';\n-import * as assert from 'assert';\n-import * as path from 'path';\n-import {lilconfig, lilconfigSync, LoaderSync, TransformSync} from '..';\n-import {cosmiconfig, cosmiconfigSync} from 'cosmiconfig';\n-import {transpileModule} from 'typescript';\n-\n-const dirname = path.join(__dirname, 'load');\n-const tsLoader: LoaderSync = (_, content) => {\n-    const res = transpileModule(content, {}).outputText;\n-    return eval(res);\n-};\n-\n-const tsLoaderSuit = uvu.suite('ts-loader');\n-\n-tsLoaderSuit('sync', () => {\n-    const filepath = path.join(dirname, 'test-app.ts');\n-    const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-    const options = {\n-        loaders: {\n-            '.ts': tsLoader,\n-        },\n-    };\n-    const expected = {\n-        config: {\n-            typescript: true,\n-        },\n-        filepath,\n-    };\n-    const result = lilconfigSync('test-app', options).load(relativeFilepath);\n-    const ccResult = cosmiconfigSync('test-app', options).load(\n-        relativeFilepath,\n-    );\n-\n-    assert.deepStrictEqual(result, expected);\n-    assert.deepStrictEqual(ccResult, expected);\n-});\n-\n-tsLoaderSuit('async', async () => {\n-    const filepath = path.join(dirname, 'test-app.ts');\n-    const relativeFilepath = filepath.slice(process.cwd().length + 1);\n-    const options = {\n-        loaders: {\n-            '.ts': tsLoader,\n-        },\n-    };\n-    const expected = {\n-        config: {\n-            typescript: true,\n-        },\n-        filepath,\n-    };\n-    const result = await lilconfig('test-app', options).load(relativeFilepath);\n-    const ccResult = await cosmiconfig('test-app', options).load(\n-        relativeFilepath,\n-    );\n-\n-    assert.deepStrictEqual(result, expected);\n-    assert.deepStrictEqual(ccResult, expected);\n-});\n-\n-tsLoaderSuit.run();\n-\n-const esmProjectSuit = uvu.suite('esm-project');\n-esmProjectSuit('async search js', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'esm-project', 'esm.config.js');\n-    const searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['esm.config.js'],\n-        stopDir,\n-    };\n-\n-    const config = {esm: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-esmProjectSuit('async search mjs', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'esm-project', 'esm.config.mjs');\n-    const searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['esm.config.mjs'],\n-        stopDir,\n-    };\n-\n-    const config = {esm: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-esmProjectSuit('async search cjs', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'esm-project', 'esm.config.cjs');\n-    const searchFrom = path.join(stopDir, 'esm-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['esm.config.cjs'],\n-        stopDir,\n-    };\n-\n-    const config = {cjs: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-esmProjectSuit.run();\n-\n-const cjsProjectSuit = uvu.suite('cjs-project');\n-cjsProjectSuit('async search js', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'cjs-project', 'cjs.config.js');\n-    const searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['cjs.config.js'],\n-        stopDir,\n-    };\n-\n-    const config = {cjs: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-cjsProjectSuit('async search mjs', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'cjs-project', 'cjs.config.mjs');\n-    const searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['cjs.config.mjs'],\n-        stopDir,\n-    };\n-\n-    const config = {esm: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-cjsProjectSuit('async search cjs', async () => {\n-    const stopDir = __dirname;\n-    const filepath = path.join(stopDir, 'cjs-project', 'cjs.config.cjs');\n-    const searchFrom = path.join(stopDir, 'cjs-project', 'a', 'b', 'c');\n-\n-    const options = {\n-        searchPlaces: ['cjs.config.cjs'],\n-        stopDir,\n-    };\n-\n-    const config = {cjs: true};\n-\n-    const result = await lilconfig('test-app', options).search(searchFrom);\n-    const ccResult = await cosmiconfig('test-app', options).search(searchFrom);\n-\n-    assert.deepStrictEqual(result, {config, filepath});\n-    assert.deepStrictEqual(ccResult, {config, filepath});\n-});\n-\n-cjsProjectSuit.run();",
          "src/spec/search/a/b/searchPlaces.conf.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    searchPlacesWorks: true,\n+\tsearchPlacesWorks: true,\n };",
          "src/spec/search/a/maybeEmpty.config.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    notSoEmpty: true,\n+\tnotSoEmpty: true,\n };",
          "src/spec/search/cached.config.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    iWasCached: true,\n+\tiWasCached: true,\n };",
          "src/spec/search/test-app.config.js": "@@ -1,3 +1,3 @@\n module.exports = {\n-    stopped: true,\n+\tstopped: true,\n };",
          "tsconfig.json": "@@ -1,6 +1,8 @@\n {\n     \"compilerOptions\": {\n         \"allowJs\": true,\n+        \"checkJs\": true,\n+        \"noEmit\": true,\n         \"alwaysStrict\": true,\n         \"esModuleInterop\": false,\n         \"lib\": [\"es2018\"],\n@@ -19,6 +21,6 @@\n         \"outDir\": \"./dist\",\n         \"baseUrl\": \"./src\"\n     },\n-    \"include\": [\"src/**/*.ts\"],\n+    \"include\": [\"src/**/*.ts\", \"src/**/*.js\"],\n     \"exclude\": [\"src/**/spec\", \"./package.json\"]\n }"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H",
      "score": 8.8
    },
    "cwes": [
      {
        "cwe_id": "CWE-94",
        "name": "Improper Control of Generation of Code ('Code Injection')"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H",
        "score": 8.8
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:L/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N",
        "score": 8.7
      }
    },
    "epss": {
      "percentage": 0.00045,
      "percentile": 0.16664
    },
    "cve_description": "Versions of the package lilconfig from 3.1.0 and before 3.1.1 are vulnerable to Arbitrary Code Execution due to the insecure usage of eval in the dynamicImport function. An attacker can exploit this vulnerability by passing a malicious input through the defaultLoaders function."
  },
  {
    "ghsa_id": "GHSA-qqqw-gm93-qf6m",
    "cve_id": "CVE-2024-48964",
    "url": "https://api.github.com/advisories/GHSA-qqqw-gm93-qf6m",
    "html_url": "https://github.com/advisories/GHSA-qqqw-gm93-qf6m",
    "summary": "OS Command Injection in Snyk gradle plugin",
    "description": "The Snyk gradle plugin is vulnerable to Code Injection when scanning an untrusted Gradle project. The vulnerability can be triggered if Snyk test is run inside the untrusted project due to the improper handling of the current working directory name. Snyk recommends only scanning trusted projects.",
    "type": "reviewed",
    "severity": "high",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/snyk/snyk-gradle-plugin",
    "identifiers": [
      {
        "value": "GHSA-qqqw-gm93-qf6m",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-48964",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-48964",
      "https://github.com/snyk/snyk-gradle-plugin/commit/2f5ee7579f00660282dd161a0b79690f4a9c865d",
      "https://github.com/advisories/GHSA-qqqw-gm93-qf6m"
    ],
    "published_at": "2024-10-23T21:30:28Z",
    "updated_at": "2024-10-30T16:12:37Z",
    "github_reviewed_at": "2024-10-23T21:41:02Z",
    "nvd_published_at": "2024-10-23T19:15:19Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "snyk-gradle-plugin"
        },
        "vulnerable_version_range": "< 4.5.0",
        "first_patched_version": "4.5.0",
        "vulnerable_functions": [],
        "vulnerable_version": "4.4.0",
        "patches": {
          "lib/index.ts": "@@ -396,11 +396,13 @@ function getVersionBuildInfo(\n export async function getGradleVersion(\n   root: string,\n   command: string,\n+  args?: string[],\n ): Promise<string> {\n   debugLog('`gradle -v` command run: ' + command);\n   let gradleVersionOutput = '[COULD NOT RUN gradle -v]';\n+  const completeArgs = args ? args.concat(['-v']) : ['-v'];\n   try {\n-    gradleVersionOutput = await subProcess.execute(command, ['-v'], {\n+    gradleVersionOutput = await subProcess.execute(command, completeArgs, {\n       cwd: root,\n     });\n   } catch (_) {\n@@ -409,6 +411,22 @@ export async function getGradleVersion(\n   return gradleVersionOutput;\n }\n \n+export function generateWrapperProcessArgs(\n+  commandPath: string,\n+  args: string[],\n+): { command: string; args: string[] } {\n+  let parseArgs: string[] = [];\n+  let command = commandPath;\n+  const isWinLocal = /^win/.test(os.platform());\n+  if (isWinLocal && command !== 'gradle') {\n+    command = 'cmd.exe';\n+    parseArgs.push('/c');\n+    parseArgs.push(commandPath);\n+  }\n+  parseArgs = parseArgs.concat(args);\n+  return { command, args: parseArgs };\n+}\n+\n async function getAllDepsWithPlugin(\n   root: string,\n   targetFile: string,\n@@ -427,12 +445,15 @@ async function getAllDepsWithPlugin(\n     gradleVersion,\n   );\n \n-  const fullCommandText = 'gradle command: ' + command + ' ' + args.join(' ');\n-  debugLog('Executing ' + fullCommandText);\n+  const { command: wrapperedCommand, args: wrapperArgs } =\n+    generateWrapperProcessArgs(command, args);\n \n+  const fullCommandText =\n+    'gradle command: ' + wrapperedCommand + ' ' + wrapperArgs.join(' ');\n+  debugLog('Executing ' + fullCommandText);\n   const stdoutText = await subProcess.execute(\n-    command,\n-    args,\n+    wrapperedCommand,\n+    wrapperArgs,\n     { cwd: root },\n     printIfEcho,\n   );\n@@ -467,7 +488,13 @@ async function getAllDeps(\n   snykHttpClient: SnykHttpClient,\n ): Promise<JsonDepsScriptResult> {\n   const command = getCommand(root, targetFile);\n-  const gradleVersion = await getGradleVersion(root, command);\n+  const { command: wrapperedCommand, args: wrapperArgs } =\n+    generateWrapperProcessArgs(command, []);\n+  const gradleVersion = await getGradleVersion(\n+    root,\n+    wrapperedCommand,\n+    wrapperArgs,\n+  );\n   if (gradleVersion.match(/Gradle 1/)) {\n     throw new Error('Gradle 1.x is not supported');\n   }\n@@ -636,7 +663,6 @@ function toCamelCase(input: string) {\n \n function getCommand(root: string, targetFile: string) {\n   const isWinLocal = /^win/.test(os.platform()); // local check, can be stubbed in tests\n-  const quotLocal = isWinLocal ? '\"' : \"'\";\n   const wrapperScript = isWinLocal ? 'gradlew.bat' : './gradlew';\n   // try to find a sibling wrapper script first\n   let pathToWrapper = path.resolve(\n@@ -645,12 +671,12 @@ function getCommand(root: string, targetFile: string) {\n     wrapperScript,\n   );\n   if (fs.existsSync(pathToWrapper)) {\n-    return quotLocal + pathToWrapper + quotLocal;\n+    return pathToWrapper;\n   }\n   // now try to find a wrapper in the root\n   pathToWrapper = path.resolve(root, wrapperScript);\n   if (fs.existsSync(pathToWrapper)) {\n-    return quotLocal + pathToWrapper + quotLocal;\n+    return pathToWrapper;\n   }\n   return 'gradle';\n }",
          "lib/sub-process.ts": "@@ -1,6 +1,6 @@\n import * as childProcess from 'child_process';\n import debugModule = require('debug');\n-import { quoteAll } from 'shescape';\n+import { escapeAll } from 'shescape';\n \n const debugLogging = debugModule('snyk-gradle-plugin');\n \n@@ -12,7 +12,7 @@ export function execute(\n   perLineCallback?: (s: string) => Promise<void>,\n ): Promise<string> {\n   const spawnOptions: childProcess.SpawnOptions = {\n-    shell: true,\n+    shell: false,\n     env: { ...process.env },\n   };\n   if (options?.cwd) {\n@@ -22,7 +22,7 @@ export function execute(\n     spawnOptions.env = { ...process.env, ...options.env };\n   }\n \n-  args = quoteAll(args, spawnOptions);\n+  args = escapeAll(args, spawnOptions);\n \n   // Before spawning an external process, we look if we need to restore the system proxy configuration,\n   // which overides the cli internal proxy configuration.",
          "test/fixtures-with-wrappers/with-lock-file/dep-graph.json": "@@ -144,4 +144,4 @@\n       }\n     ]\n   }\n-}\n+}\n\\ No newline at end of file",
          "test/system/darwin.test.ts": "@@ -34,7 +34,7 @@ test('darwin without wrapper invokes gradle directly', async () => {\n test('darwin with wrapper invokes wrapper script', async () => {\n   await expect(inspect(rootWithWrapper, 'build.gradle')).rejects.toThrow();\n   expect(subProcessExecSpy.mock.calls[0][0]).toBe(\n-    \"'\" + path.join(rootWithWrapper, 'gradlew') + \"'\",\n+    path.join(rootWithWrapper, 'gradlew'),\n   );\n });\n \n@@ -43,6 +43,6 @@ test('darwin with wrapper in root invokes wrapper script', async () => {\n     inspect(subWithWrapper, path.join('app', 'build.gradle')),\n   ).rejects.toThrow();\n   expect(subProcessExecSpy.mock.calls[0][0]).toBe(\n-    \"'\" + path.join(subWithWrapper, 'gradlew') + \"'\",\n+    path.join(subWithWrapper, 'gradlew'),\n   );\n });",
          "test/system/plugin.test.ts": "@@ -2,7 +2,8 @@ import * as fs from 'fs';\n import * as path from 'path';\n import * as depGraphLib from '@snyk/dep-graph';\n import { fixtureDir } from '../common';\n-import { inspect } from '../../lib';\n+import { generateWrapperProcessArgs, inspect } from '../../lib';\n+import * as os from 'os';\n \n const rootNoWrapper = fixtureDir('no wrapper');\n const withInitScript = fixtureDir('with-init-script');\n@@ -212,3 +213,17 @@ test('repeated transitive lines not pruned if verbose graph', async () => {\n   });\n   expect(result.dependencyGraph?.equals(expected)).toBe(true);\n });\n+\n+test('generateWrapperProcessArgs should return gradle is wrapper is not used', () => {\n+  const result = generateWrapperProcessArgs('gradle', ['-v']);\n+  expect(result).toEqual({ command: 'gradle', args: ['-v'] });\n+});\n+test('generateWrapperProcessArgs should return wrapped command is os is windows ', () => {\n+  const platformMock = jest.spyOn(os, 'platform');\n+  platformMock.mockReturnValue('win32');\n+  const result = generateWrapperProcessArgs('foo/bar/gradlew.bat', ['-v']);\n+  expect(result).toEqual({\n+    command: 'cmd.exe',\n+    args: ['/c', 'foo/bar/gradlew.bat', '-v'],\n+  });\n+});",
          "test/system/windows.test.ts": "@@ -9,6 +9,7 @@ const rootWithWrapper = fixtureDir('with-wrapper');\n const subWithWrapper = fixtureDir('with-wrapper-in-root');\n let subProcessExecSpy;\n let platformMock;\n+const isWinLocal = /^win/.test(os.platform());\n \n beforeAll(() => {\n   platformMock = jest.spyOn(os, 'platform');\n@@ -24,23 +25,31 @@ afterAll(() => {\n afterEach(() => {\n   jest.clearAllMocks();\n });\n+if (isWinLocal) {\n+  test('windows with wrapper in root invokes wrapper bat', async () => {\n+    await expect(\n+      inspect(subWithWrapper, path.join('app', 'build.gradle')),\n+    ).rejects.toThrow();\n+    expect(subProcessExecSpy.mock.calls[0]).toEqual([\n+      'cmd.exe',\n+      ['/c', `${subWithWrapper}\\\\gradlew.bat`, '-v'],\n+      {\n+        cwd: `${subWithWrapper}`,\n+      },\n+    ]);\n+  });\n \n-test('windows with wrapper in root invokes wrapper bat', async () => {\n-  await expect(\n-    inspect(subWithWrapper, path.join('app', 'build.gradle')),\n-  ).rejects.toThrow();\n-  expect(subProcessExecSpy.mock.calls[0][0]).toBe(\n-    '\"' + path.join(subWithWrapper, 'gradlew.bat') + '\"',\n-  );\n-});\n-\n-test('windows with wrapper invokes wrapper bat', async () => {\n-  await expect(inspect(rootWithWrapper, 'build.gradle')).rejects.toThrow();\n-  expect(subProcessExecSpy.mock.calls[0][0]).toBe(\n-    '\"' + path.join(rootWithWrapper, 'gradlew.bat') + '\"',\n-  );\n-});\n-\n+  test('windows with wrapper invokes wrapper bat', async () => {\n+    await expect(inspect(rootWithWrapper, 'build.gradle')).rejects.toThrow();\n+    expect(subProcessExecSpy.mock.calls[0]).toEqual([\n+      'cmd.exe',\n+      ['/c', `${rootWithWrapper}\\\\gradlew.bat`, '-v'],\n+      {\n+        cwd: `${rootWithWrapper}`,\n+      },\n+    ]);\n+  });\n+}\n test('windows without wrapper invokes gradle directly', async () => {\n   await expect(inspect(rootNoWrapper, 'build.gradle')).rejects.toThrow();\n   expect(subProcessExecSpy.mock.calls[0][0]).toBe('gradle');"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H",
      "score": 7.5
    },
    "cwes": [
      {
        "cwe_id": "CWE-78",
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')"
      },
      {
        "cwe_id": "CWE-94",
        "name": "Improper Control of Generation of Code ('Code Injection')"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H",
        "score": 7.5
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:P/PR:N/UI:A/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N/E:X/CR:X/IR:X/AR:X/MAV:X/MAC:X/MAT:X/MPR:X/MUI:X/MVC:X/MVI:X/MVA:X/MSC:X/MSI:X/MSA:X/S:X/AU:X/R:X/V:X/RE:X/U:X",
        "score": 7.5
      }
    },
    "epss": {
      "percentage": 0.00077,
      "percentile": 0.34068
    },
    "cve_description": "The package Snyk CLI before 1.1294.0 is vulnerable to Code Injection when scanning an untrusted Gradle project. The vulnerability can be triggered if Snyk test is run inside the untrusted project due to the improper handling of the current working directory name. Snyk recommends only scanning trusted projects."
  },
  {
    "ghsa_id": "GHSA-69f9-h8f9-7vjf",
    "cve_id": "CVE-2024-48963",
    "url": "https://api.github.com/advisories/GHSA-69f9-h8f9-7vjf",
    "html_url": "https://github.com/advisories/GHSA-69f9-h8f9-7vjf",
    "summary": "OS Command Injection in Snyk php plugin",
    "description": "The Snyk php plugin is vulnerable to Code Injection when scanning an untrusted PHP project. The vulnerability can be triggered if Snyk test is run inside the untrusted project due to the improper handling of the current working directory name. Snyk recommends only scanning trusted projects.",
    "type": "reviewed",
    "severity": "high",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/snyk/snyk-php-plugin",
    "identifiers": [
      {
        "value": "GHSA-69f9-h8f9-7vjf",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-48963",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-48963",
      "https://github.com/snyk/snyk-php-plugin/releases/tag/v1.10.0",
      "https://github.com/snyk/snyk-php-plugin/commit/9189f093b94f9ce51672f6919ffbc98171fd66d4",
      "https://github.com/advisories/GHSA-69f9-h8f9-7vjf"
    ],
    "published_at": "2024-10-23T21:30:28Z",
    "updated_at": "2024-10-23T21:40:15Z",
    "github_reviewed_at": "2024-10-23T21:40:12Z",
    "nvd_published_at": "2024-10-23T19:15:19Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "snyk-php-plugin"
        },
        "vulnerable_version_range": "< 1.10.0",
        "first_patched_version": "1.10.0",
        "vulnerable_functions": [],
        "vulnerable_version": "1.9.4",
        "patches": {
          ".circleci/config.yml": "@@ -1,7 +1,8 @@\n version: 2.1\n \n orbs:\n-  win: circleci/windows@2.4.0\n+  win: circleci/windows@5.0.0\n+  prodsec: snyk/prodsec-orb@1\n \n defaults: &defaults\n   parameters:\n@@ -16,6 +17,16 @@ windows_defaults: &windows_defaults\n   executor:\n     name: win/default\n \n+test_matrix_unix: &test_matrix_unix\n+  matrix:\n+    parameters:\n+      node_version: [ '16.18', '18.18', '20.9' ]\n+\n+test_matrix_win: &test_matrix_win\n+  matrix:\n+    parameters:\n+      node_version: [ '16', '18', '20' ]\n+\n commands:\n   install_deps:\n     description: Install dependencies\n@@ -47,10 +58,25 @@ commands:\n           command: npm --version\n \n jobs:\n+  security-scans:\n+    resource_class: small\n+    <<: *defaults\n+    docker:\n+      - image: cimg/node:<< parameters.node_version >>\n+    steps:\n+      - checkout\n+      - install_deps\n+      - show_node_version\n+      - prodsec/security_scans:\n+          mode: auto\n+          release-branch: master\n+          open-source-additional-arguments: --exclude=test\n+          iac-scan: disabled\n+\n   lint:\n     <<: *defaults\n     docker:\n-      - image: circleci/node:<< parameters.node_version >>\n+      - image: cimg/node:<< parameters.node_version >>\n     steps:\n       - checkout\n       - install_deps\n@@ -76,7 +102,7 @@ jobs:\n   test-unix:\n     <<: *defaults\n     docker:\n-      - image: circleci/node:<< parameters.node_version >>\n+      - image: cimg/node:<< parameters.node_version >>\n     steps:\n       - checkout\n       - install_deps\n@@ -88,10 +114,11 @@ jobs:\n   release:\n     <<: *defaults\n     docker:\n-      - image: circleci/node:<< parameters.node_version >>\n+      - image: cimg/node:<< parameters.node_version >>\n     resource_class: small\n     steps:\n       - checkout\n+      - run: sudo npm i -g semantic-release@22 @semantic-release/exec pkg\n       - install_deps\n       - run:\n           name: Publish to GitHub\n@@ -101,40 +128,37 @@ workflows:\n   version: 2\n   test_and_release:\n     jobs:\n-    - lint:\n-        name: Lint\n-        context: nodejs-install\n-        node_version: \"10\"\n+    - prodsec/secrets-scan:\n+        name: Scan repository for secrets\n+        context:\n+          - snyk-bot-slack\n+        channel: os-team-managed-alerts\n         filters:\n           branches:\n             ignore:\n               - master\n \n-    # UNIX tests\n-    - test-unix:\n-        name: Unix Tests for Node=12\n-        context: nodejs-install\n-        node_version: \"12\"\n-        requires:\n-          - Lint\n-        filters:\n-          branches:\n-            ignore:\n-              - master\n-    - test-unix:\n-        name: Unix Tests for Node=10\n+    - security-scans:\n+        name: Security Scans\n+        node_version: \"20.18\"\n+        context:\n+          - open_source-managed\n+          - nodejs-install\n+\n+    - lint:\n+        name: Lint\n         context: nodejs-install\n-        node_version: \"10\"\n-        requires:\n-          - Lint\n+        node_version: \"20.18\"\n         filters:\n           branches:\n             ignore:\n               - master\n+\n+    # UNIX tests\n     - test-unix:\n-        name: Unix Tests for Node=8\n+        name: Test OS=Unix Node=<<matrix.node_version>>\n         context: nodejs-install\n-        node_version: \"8\"\n+        <<: *test_matrix_unix\n         requires:\n           - Lint\n         filters:\n@@ -144,29 +168,9 @@ workflows:\n \n     # Windows tests\n     - test-windows:\n-        name: Windows Tests for Node=12\n-        context: nodejs-install\n-        node_version: \"12.0.0\"\n-        requires:\n-          - Lint\n-        filters:\n-          branches:\n-            ignore:\n-              - master\n-    - test-windows:\n-        name: Windows Tests for Node=10\n-        context: nodejs-install\n-        node_version: \"10.21.0\"\n-        requires:\n-          - Lint\n-        filters:\n-          branches:\n-            ignore:\n-              - master\n-    - test-windows:\n-        name: Windows Tests for Node=8\n+        name: Test OS=Windows Node=<<matrix.node_version>>\n         context: nodejs-install\n-        node_version: \"8.17.0\"\n+        <<: *test_matrix_win\n         requires:\n           - Lint\n         filters:\n@@ -178,7 +182,7 @@ workflows:\n     - release:\n         name: Release\n         context: nodejs-app-release\n-        node_version: \"14.17.0\"\n+        node_version: \"20.18\"\n         filters:\n           branches:\n             only:",
          ".github/CODEOWNERS": "@@ -1 +1 @@\n-* @snyk/loki\n+* @snyk/os-managed",
          ".pre-commit-config.yaml": "@@ -0,0 +1,5 @@\n+repos:\n+  - repo: https://github.com/gitleaks/gitleaks\n+    rev: v8.16.1\n+    hooks:\n+      - id: gitleaks",
          "catalog-info.yaml": "@@ -0,0 +1,11 @@\n+apiVersion: backstage.io/v1alpha1\n+kind: Component\n+metadata:\n+  name: snyk-php-plugin\n+  annotations:\n+    github.com/project-slug: snyk/snyk-php-plugin\n+    github.com/team-slug: snyk/os-managed\n+spec:\n+  type: snyk-cli-plugin\n+  lifecycle: \"-\"\n+  owner: os-managed",
          "lib/composer-cmds.ts": "@@ -1,15 +1,17 @@\n import * as path from 'path';\n import * as childProcess from 'child_process';\n+export const composerCmd = {command: 'composer', args: ['--version']};\n+export const composerShowCmd = {command: 'composer', args: ['show', '-p']};\n+export const pharCmd = {command: `php`, args:[`${path.resolve(path.resolve() + '/composer.phar')}`, 'show', '-p', '--format=json']\n+};\n \n-export const composerCmd = 'composer --version';\n-export const composerShowCmd = 'composer show -p';\n-export const pharCmd = `php ${path.resolve(path.resolve() + '/composer.phar')} show -p --format=json`;\n-\n-export function cmdReturnsOk(cmd): boolean {\n-  return cmd && childProcess.spawnSync(cmd, { shell: true }).status === 0;\n+export function cmdReturnsOk(cmd, args: string[] = []): boolean {\n+  const spawnOptions: childProcess.SpawnOptions = { shell: false };\n+  return cmd && childProcess.spawnSync(cmd, args,spawnOptions).status === 0;\n }\n \n // run a cmd in a specific folder and it's result should be there\n-export function execWithResult(cmd, basePath): string {\n-  return childProcess.execSync(cmd, { cwd: basePath }).toString();\n+export function execWithResult(cmd, basePath, args: string[] = []): string {\n+  const spawnOptions: childProcess.SpawnOptions ={ cwd: basePath, shell: false }\n+  return childProcess.spawnSync(cmd, args, spawnOptions).toString();\n }",
          "lib/system-deps.ts": "@@ -9,25 +9,25 @@ function isSet(variable): boolean {\n }\n \n export function systemDeps(basePath: string, options: PhpOptions): SystemPackages {\n-  const composerOk = isSet(options.composerIsFine) ? options.composerIsFine : cmds.cmdReturnsOk(cmds.composerCmd);\n+  const composerOk = isSet(options.composerIsFine) ? options.composerIsFine : cmds.cmdReturnsOk(cmds.composerCmd.command,cmds.composerCmd.args);\n   const composerPharOk = isSet(options.composerPharIsFine) ?\n-    options.composerPharIsFine : cmds.cmdReturnsOk(cmds.pharCmd);\n+    options.composerPharIsFine : cmds.cmdReturnsOk(cmds.pharCmd.command, cmds.pharCmd.args);\n \n   let finalVersionsObj = {};\n \n   if (options.systemVersions && (Object.keys(options.systemVersions).length > 0)) {\n     // give first preference to a stub\n     finalVersionsObj = options.systemVersions;\n   } else if (composerOk) {\n-    const lines = cmds.execWithResult(cmds.composerShowCmd, basePath).split(os.EOL);\n+    const lines = cmds.execWithResult(cmds.composerShowCmd.command, basePath, cmds.composerShowCmd.args).split(os.EOL);\n     lines.forEach((line) => {\n       const [part1, part2] = line.split(/\\s+/);\n       if (part2) {\n         finalVersionsObj[part1] = part2;\n       }\n     });\n   } else if (composerPharOk) {\n-    const output = cmds.execWithResult(cmds.pharCmd, basePath);\n+    const output = cmds.execWithResult(cmds.pharCmd.command, basePath, cmds.pharCmd.args);\n     const versionsObj = JSON.parse(output).platform;\n     versionsObj.forEach(({name, version}) => {\n       finalVersionsObj[name] = version;"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H",
      "score": 7.5
    },
    "cwes": [
      {
        "cwe_id": "CWE-78",
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H",
        "score": 7.5
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:P/PR:N/UI:A/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N/E:X/CR:X/IR:X/AR:X/MAV:X/MAC:X/MAT:X/MPR:X/MUI:X/MVC:X/MVI:X/MVA:X/MSC:X/MSI:X/MSA:X/S:X/AU:X/R:X/V:X/RE:X/U:X",
        "score": 7.5
      }
    },
    "epss": {
      "percentage": 0.0009,
      "percentile": 0.39373
    },
    "cve_description": "The package Snyk CLI before 1.1294.0 is vulnerable to Code Injection when scanning an untrusted PHP project. The vulnerability can be triggered if Snyk test is run inside the untrusted project due to the improper handling of the current working directory name. Snyk recommends only scanning trusted projects."
  },
  {
    "ghsa_id": "GHSA-m4gq-x24j-jpmf",
    "cve_id": null,
    "url": "https://api.github.com/advisories/GHSA-m4gq-x24j-jpmf",
    "html_url": "https://github.com/advisories/GHSA-m4gq-x24j-jpmf",
    "summary": "Prototype pollution vulnerability found in Mermaid's bundled version of DOMPurify",
    "description": "The following bundled files within the Mermaid NPM package contain a bundled version of DOMPurify that is vulnerable to https://github.com/cure53/DOMPurify/security/advisories/GHSA-mmhx-hmjr-r674, potentially resulting in an XSS attack.\n\nThis affects the built:\n\n- `dist/mermaid.min.js`\n- `dist/mermaid.js`\n- `dist/mermaid.esm.mjs`\n- `dist/mermaid.esm.min.mjs`\n\nThis will also affect users that use the above files via a CDN link, e.g. `https://cdn.jsdelivr.net/npm/mermaid@10.9.2/dist/mermaid.min.js`\n\n**Users that use the default NPM export of `mermaid`, e.g. `import mermaid from 'mermaid'`, or the `dist/mermaid.core.mjs` file, do not use this bundled version of DOMPurify, and can easily update using their package manager with something like `npm audit fix`.**\n\n### Patches\n\n- `develop` branch: 6c785c93166c151d27d328ddf68a13d9d65adc00\n- backport to v10: 92a07ffe40aab2769dd1c3431b4eb5beac282b34",
    "type": "reviewed",
    "severity": "high",
    "repository_advisory_url": "https://api.github.com/repos/mermaid-js/mermaid/security-advisories/GHSA-m4gq-x24j-jpmf",
    "source_code_location": "https://github.com/mermaid-js/mermaid",
    "identifiers": [
      {
        "value": "GHSA-m4gq-x24j-jpmf",
        "type": "GHSA"
      }
    ],
    "references": [
      "https://github.com/cure53/DOMPurify/security/advisories/GHSA-mmhx-hmjr-r674",
      "https://github.com/mermaid-js/mermaid/security/advisories/GHSA-m4gq-x24j-jpmf",
      "https://github.com/mermaid-js/mermaid/commit/6c785c93166c151d27d328ddf68a13d9d65adc00",
      "https://github.com/mermaid-js/mermaid/commit/92a07ffe40aab2769dd1c3431b4eb5beac282b34",
      "https://github.com/advisories/GHSA-m4gq-x24j-jpmf"
    ],
    "published_at": "2024-10-22T18:17:02Z",
    "updated_at": "2024-10-23T16:58:48Z",
    "github_reviewed_at": "2024-10-22T18:17:02Z",
    "nvd_published_at": null,
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "mermaid"
        },
        "vulnerable_version_range": "<= 10.9.2",
        "first_patched_version": "10.9.3",
        "vulnerable_functions": [],
        "vulnerable_version": "10.9.2",
        "patches": {
          "packages/mermaid/package.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"mermaid\",\n-  \"version\": \"10.9.2\",\n+  \"version\": \"10.9.3\",\n   \"description\": \"Markdown-ish syntax for generating flowcharts, sequence diagrams, class diagrams, gantt charts and git graphs.\",\n   \"type\": \"module\",\n   \"module\": \"./dist/mermaid.core.mjs\",",
          "packages/mermaid/src/diagrams/block/blockDB.ts": "@@ -20,7 +20,7 @@ const config = getConfig();\n \n let classes = {} as Record<string, ClassDef>;\n \n-const sanitizeText = (txt:string) => common.sanitizeText(txt, config);\n+const sanitizeText = (txt: string) => common.sanitizeText(txt, config);\n \n /**\n  * Called when the parser comes across a (style) class definition\n@@ -93,7 +93,7 @@ const populateBlockDatabase = (_blockList: Block[] | Block[][], parent: Block):\n   const children = [];\n   for (const block of blockList) {\n     if (block.label) {\n-        block.label = sanitizeText(block.label);\n+      block.label = sanitizeText(block.label);\n     }\n     if (block.type === 'classDef') {\n       addStyleClass(block.id, block.css);",
          "pnpm-lock.yaml": "@@ -228,13 +228,13 @@ importers:\n         version: 1.11.10\n       dompurify:\n         specifier: ^3.0.5 <3.1.7\n-        version: 3.0.9\n+        version: 3.1.6\n       elkjs:\n         specifier: ^0.9.0\n         version: 0.9.2\n       katex:\n         specifier: ^0.16.9\n-        version: 0.16.9\n+        version: 0.16.11\n       khroma:\n         specifier: ^2.0.0\n         version: 2.1.0\n@@ -8833,8 +8833,8 @@ packages:\n       domelementtype: 2.3.0\n     dev: true\n \n-  /dompurify@3.0.9:\n-    resolution: {integrity: sha512-uyb4NDIvQ3hRn6NiC+SIFaP4mJ/MdXlvtunaqK9Bn6dD3RuB/1S/gasEjDHD8eiaqdSael2vBv+hOs7Y+jhYOQ==}\n+  /dompurify@3.1.6:\n+    resolution: {integrity: sha512-cTOAhc36AalkjtBpfG6O8JimdTMWNXjiePT2xQH/ppBGi/4uIpmj8eKyIkMJErXWARyINV/sB38yf8JCLF5pbQ==}\n     dev: false\n \n   /domutils@3.1.0:\n@@ -12081,8 +12081,8 @@ packages:\n     engines: {node: '>=12.20'}\n     dev: true\n \n-  /katex@0.16.9:\n-    resolution: {integrity: sha512-fsSYjWS0EEOwvy81j3vRA8TEAhQhKiqO+FQaKWp0m39qwOzHVBgAUBIXWj1pB+O2W3fIpNa6Y9KSKCVbfPhyAQ==}\n+  /katex@0.16.11:\n+    resolution: {integrity: sha512-RQrI8rlHY92OLf3rho/Ts8i/XvjgguEjOkO1BEXcU3N8BqPpSzBNwV/G0Ukr+P/l3ivvJUE/Fa/CwbS6HesGNQ==}\n     hasBin: true\n     dependencies:\n       commander: 8.3.0"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:H/A:L",
      "score": 7
    },
    "cwes": [
      {
        "cwe_id": "CWE-1321",
        "name": "Improperly Controlled Modification of Object Prototype Attributes ('Prototype Pollution')"
      },
      {
        "cwe_id": "CWE-1395",
        "name": "Dependency on Vulnerable Third-Party Component"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "aloisklink",
          "id": 19716675,
          "node_id": "MDQ6VXNlcjE5NzE2Njc1",
          "avatar_url": "https://avatars.githubusercontent.com/u/19716675?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/aloisklink",
          "html_url": "https://github.com/aloisklink",
          "followers_url": "https://api.github.com/users/aloisklink/followers",
          "following_url": "https://api.github.com/users/aloisklink/following{/other_user}",
          "gists_url": "https://api.github.com/users/aloisklink/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/aloisklink/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/aloisklink/subscriptions",
          "organizations_url": "https://api.github.com/users/aloisklink/orgs",
          "repos_url": "https://api.github.com/users/aloisklink/repos",
          "events_url": "https://api.github.com/users/aloisklink/events{/privacy}",
          "received_events_url": "https://api.github.com/users/aloisklink/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "remediation_developer"
      },
      {
        "user": {
          "login": "sidharthv96",
          "id": 10703445,
          "node_id": "MDQ6VXNlcjEwNzAzNDQ1",
          "avatar_url": "https://avatars.githubusercontent.com/u/10703445?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/sidharthv96",
          "html_url": "https://github.com/sidharthv96",
          "followers_url": "https://api.github.com/users/sidharthv96/followers",
          "following_url": "https://api.github.com/users/sidharthv96/following{/other_user}",
          "gists_url": "https://api.github.com/users/sidharthv96/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/sidharthv96/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/sidharthv96/subscriptions",
          "organizations_url": "https://api.github.com/users/sidharthv96/orgs",
          "repos_url": "https://api.github.com/users/sidharthv96/repos",
          "events_url": "https://api.github.com/users/sidharthv96/events{/privacy}",
          "received_events_url": "https://api.github.com/users/sidharthv96/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "remediation_reviewer"
      },
      {
        "user": {
          "login": "ashishjain0512",
          "id": 16836093,
          "node_id": "MDQ6VXNlcjE2ODM2MDkz",
          "avatar_url": "https://avatars.githubusercontent.com/u/16836093?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/ashishjain0512",
          "html_url": "https://github.com/ashishjain0512",
          "followers_url": "https://api.github.com/users/ashishjain0512/followers",
          "following_url": "https://api.github.com/users/ashishjain0512/following{/other_user}",
          "gists_url": "https://api.github.com/users/ashishjain0512/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/ashishjain0512/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/ashishjain0512/subscriptions",
          "organizations_url": "https://api.github.com/users/ashishjain0512/orgs",
          "repos_url": "https://api.github.com/users/ashishjain0512/repos",
          "events_url": "https://api.github.com/users/ashishjain0512/events{/privacy}",
          "received_events_url": "https://api.github.com/users/ashishjain0512/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "remediation_reviewer"
      },
      {
        "user": {
          "login": "mlevy-parasoft",
          "id": 127135619,
          "node_id": "U_kgDOB5Pvgw",
          "avatar_url": "https://avatars.githubusercontent.com/u/127135619?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/mlevy-parasoft",
          "html_url": "https://github.com/mlevy-parasoft",
          "followers_url": "https://api.github.com/users/mlevy-parasoft/followers",
          "following_url": "https://api.github.com/users/mlevy-parasoft/following{/other_user}",
          "gists_url": "https://api.github.com/users/mlevy-parasoft/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/mlevy-parasoft/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/mlevy-parasoft/subscriptions",
          "organizations_url": "https://api.github.com/users/mlevy-parasoft/orgs",
          "repos_url": "https://api.github.com/users/mlevy-parasoft/repos",
          "events_url": "https://api.github.com/users/mlevy-parasoft/events{/privacy}",
          "received_events_url": "https://api.github.com/users/mlevy-parasoft/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "reporter"
      },
      {
        "user": {
          "login": "byt3n33dl3",
          "id": 151133481,
          "node_id": "U_kgDOCQIdKQ",
          "avatar_url": "https://avatars.githubusercontent.com/u/151133481?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/byt3n33dl3",
          "html_url": "https://github.com/byt3n33dl3",
          "followers_url": "https://api.github.com/users/byt3n33dl3/followers",
          "following_url": "https://api.github.com/users/byt3n33dl3/following{/other_user}",
          "gists_url": "https://api.github.com/users/byt3n33dl3/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/byt3n33dl3/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/byt3n33dl3/subscriptions",
          "organizations_url": "https://api.github.com/users/byt3n33dl3/orgs",
          "repos_url": "https://api.github.com/users/byt3n33dl3/repos",
          "events_url": "https://api.github.com/users/byt3n33dl3/events{/privacy}",
          "received_events_url": "https://api.github.com/users/byt3n33dl3/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:H/A:L",
        "score": 7
      },
      "cvss_v4": {
        "vector_string": null,
        "score": 0
      }
    },
    "cve_description": ""
  },
  {
    "ghsa_id": "GHSA-96g7-g7g9-jxw8",
    "cve_id": "CVE-2024-51757",
    "url": "https://api.github.com/advisories/GHSA-96g7-g7g9-jxw8",
    "html_url": "https://github.com/advisories/GHSA-96g7-g7g9-jxw8",
    "summary": "happy-dom allows for server side code to be executed by a <script> tag",
    "description": "### Impact\nConsumers of the NPM package `happy-dom`\n\n### Patches\nThe security vulnerability has been patched in v15.10.2\n\n### Workarounds\nNo easy workarounds to my knowledge\n\n### References\n[#1585](https://github.com/capricorn86/happy-dom/issues/1585)\n",
    "type": "reviewed",
    "severity": "critical",
    "repository_advisory_url": "https://api.github.com/repos/capricorn86/happy-dom/security-advisories/GHSA-96g7-g7g9-jxw8",
    "source_code_location": "https://github.com/capricorn86/happy-dom",
    "identifiers": [
      {
        "value": "GHSA-96g7-g7g9-jxw8",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-51757",
        "type": "CVE"
      }
    ],
    "references": [
      "https://github.com/capricorn86/happy-dom/security/advisories/GHSA-96g7-g7g9-jxw8",
      "https://github.com/capricorn86/happy-dom/issues/1585",
      "https://github.com/capricorn86/happy-dom/pull/1586",
      "https://github.com/capricorn86/happy-dom/commit/5ee0b1676d4ce20cc2a70d1c9c8d6f1e3f57efac",
      "https://github.com/capricorn86/happy-dom/commit/d23834c232f1cf5519c9418b073f1dcec6b2f0fd",
      "https://github.com/capricorn86/happy-dom/releases/tag/v15.10.2",
      "https://nvd.nist.gov/vuln/detail/CVE-2024-51757",
      "https://github.com/advisories/GHSA-96g7-g7g9-jxw8"
    ],
    "published_at": "2024-11-06T15:27:50Z",
    "updated_at": "2024-11-06T23:39:30Z",
    "github_reviewed_at": "2024-11-06T15:27:50Z",
    "nvd_published_at": "2024-11-06T20:15:06Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "happy-dom"
        },
        "vulnerable_version_range": "< 15.10.2",
        "first_patched_version": "15.10.2",
        "vulnerable_functions": [],
        "vulnerable_version": "15.10.1",
        "patches": {
          "packages/happy-dom/src/fetch/utilities/SyncFetchScriptBuilder.ts": "@@ -44,7 +44,9 @@ export default class SyncFetchScriptBuilder {\n \t\t\t\t\t\t\t\t\tnull,\n \t\t\t\t\t\t\t\t\t4\n \t\t\t\t\t\t\t\t)};\n-                const request = sendRequest(\\`${request.url.href}\\`, options, (incomingMessage) => {\n+                const request = sendRequest(${JSON.stringify(\n+\t\t\t\t\t\t\t\t\trequest.url.href\n+\t\t\t\t\t\t\t\t)}, options, (incomingMessage) => {\n                     let data = Buffer.alloc(0);\n                     incomingMessage.on('data', (chunk) => {\n                         data = Buffer.concat([data, Buffer.from(chunk)]);",
          "packages/happy-dom/test/fetch/SyncFetch.test.ts": "@@ -252,8 +252,7 @@ describe('SyncFetch', () => {\n \t\tit('Should not allow to inject code into scripts executed using child_process.execFileSync().', () => {\n \t\t\tbrowserFrame.url = 'https://localhost:8080/';\n \n-\t\t\tconst url =\n-\t\t\t\t\"https://localhost:8080/`+require('child_process').execSync('id')+`/'+require('child_process').execSync('id')+'\";\n+\t\t\tconst url = `https://localhost:8080/\\`+require('child_process').execSync('id')+\\`/'+require('child_process').execSync('id')+'/?key=\"+require('child_process').execSync('id')+\"`;\n \t\t\tconst responseText = 'test';\n \n \t\t\tmockModule('child_process', {\n@@ -267,7 +266,7 @@ describe('SyncFetch', () => {\n \t\t\t\t\texpect(args[1]).toBe(\n \t\t\t\t\t\tSyncFetchScriptBuilder.getScript({\n \t\t\t\t\t\t\turl: new URL(\n-\t\t\t\t\t\t\t\t\"https://localhost:8080/%60+require('child_process').execSync('id')+%60/'+require('child_process').execSync('id')+'\"\n+\t\t\t\t\t\t\t\t`https://localhost:8080/\\`+require('child_process').execSync('id')+\\`/'+require('child_process').execSync('id')+'/?key=\"+require('child_process').execSync('id')+\"`\n \t\t\t\t\t\t\t),\n \t\t\t\t\t\t\tmethod: 'GET',\n \t\t\t\t\t\t\theaders: {\n@@ -280,11 +279,9 @@ describe('SyncFetch', () => {\n \t\t\t\t\t\t\tbody: null\n \t\t\t\t\t\t})\n \t\t\t\t\t);\n-\t\t\t\t\t// new URL() will convert ` into %60\n-\t\t\t\t\t// By using ` for the URL string within the script, we can prevent the script from being injected\n \t\t\t\t\texpect(\n \t\t\t\t\t\targs[1].includes(\n-\t\t\t\t\t\t\t`\\`https://localhost:8080/%60+require('child_process').execSync('id')+%60/'+require('child_process').execSync('id')+'\\``\n+\t\t\t\t\t\t\t`\"https://localhost:8080/%60+require('child_process').execSync('id')+%60/'+require('child_process').execSync('id')+'/?key=%22+require(%27child_process%27).execSync(%27id%27)+%22\"`\n \t\t\t\t\t\t)\n \t\t\t\t\t).toBe(true);\n \t\t\t\t\texpect(options).toEqual({"
        }
      }
    ],
    "cvss": {
      "vector_string": null,
      "score": null
    },
    "cwes": [
      {
        "cwe_id": "CWE-79",
        "name": "Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "kevin-mizu",
          "id": 48991194,
          "node_id": "MDQ6VXNlcjQ4OTkxMTk0",
          "avatar_url": "https://avatars.githubusercontent.com/u/48991194?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/kevin-mizu",
          "html_url": "https://github.com/kevin-mizu",
          "followers_url": "https://api.github.com/users/kevin-mizu/followers",
          "following_url": "https://api.github.com/users/kevin-mizu/following{/other_user}",
          "gists_url": "https://api.github.com/users/kevin-mizu/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/kevin-mizu/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/kevin-mizu/subscriptions",
          "organizations_url": "https://api.github.com/users/kevin-mizu/orgs",
          "repos_url": "https://api.github.com/users/kevin-mizu/repos",
          "events_url": "https://api.github.com/users/kevin-mizu/events{/privacy}",
          "received_events_url": "https://api.github.com/users/kevin-mizu/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "reporter"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": null,
        "score": 0
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N",
        "score": 9.3
      }
    },
    "epss": {
      "percentage": 0.00044,
      "percentile": 0.11325
    },
    "cve_description": "happy-dom is a JavaScript implementation of a web browser without its graphical user interface. Versions of happy-dom prior to 15.10.2 may execute code on the host via a script tag. This would execute code in the user context of happy-dom. Users are advised to upgrade to version 15.10.2. There are no known workarounds for this vulnerability."
  },
  {
    "ghsa_id": "GHSA-p3vf-v8qc-cwcr",
    "cve_id": "CVE-2024-48910",
    "url": "https://api.github.com/advisories/GHSA-p3vf-v8qc-cwcr",
    "html_url": "https://github.com/advisories/GHSA-p3vf-v8qc-cwcr",
    "summary": "DOMPurify vulnerable to tampering by prototype polution",
    "description": "dompurify was vulnerable to prototype pollution\n\nFixed by https://github.com/cure53/DOMPurify/commit/d1dd0374caef2b4c56c3bd09fe1988c3479166dc",
    "type": "reviewed",
    "severity": "critical",
    "repository_advisory_url": "https://api.github.com/repos/cure53/DOMPurify/security-advisories/GHSA-p3vf-v8qc-cwcr",
    "source_code_location": "https://github.com/cure53/DOMPurify",
    "identifiers": [
      {
        "value": "GHSA-p3vf-v8qc-cwcr",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-48910",
        "type": "CVE"
      }
    ],
    "references": [
      "https://github.com/cure53/DOMPurify/security/advisories/GHSA-p3vf-v8qc-cwcr",
      "https://github.com/cure53/DOMPurify/commit/d1dd0374caef2b4c56c3bd09fe1988c3479166dc",
      "https://nvd.nist.gov/vuln/detail/CVE-2024-48910",
      "https://github.com/advisories/GHSA-p3vf-v8qc-cwcr"
    ],
    "published_at": "2024-10-31T14:23:34Z",
    "updated_at": "2024-11-01T09:17:33Z",
    "github_reviewed_at": "2024-10-31T14:23:34Z",
    "nvd_published_at": "2024-10-31T15:15:15Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "dompurify"
        },
        "vulnerable_version_range": "< 2.4.2",
        "first_patched_version": "2.4.2",
        "vulnerable_functions": [],
        "vulnerable_version": "2.4.1",
        "patches": {
          "README.md": "@@ -6,7 +6,7 @@\n \n DOMPurify is a DOM-only, super-fast, uber-tolerant XSS sanitizer for HTML, MathML and SVG.\n \n-It's also very simple to use and get started with. DOMPurify was [started in February 2014](https://github.com/cure53/DOMPurify/commit/a630922616927373485e0e787ab19e73e3691b2b) and, meanwhile, has reached version 2.4.1.\n+It's also very simple to use and get started with. DOMPurify was [started in February 2014](https://github.com/cure53/DOMPurify/commit/a630922616927373485e0e787ab19e73e3691b2b) and, meanwhile, has reached version 2.4.2.\n \n DOMPurify is written in JavaScript and works in all modern browsers (Safari (10+), Opera (15+), Internet Explorer (10+), Edge, Firefox and Chrome - as well as almost anything else using Blink or WebKit). It doesn't break on MSIE6 or other legacy browsers. It either uses [a fall-back](#what-about-older-browsers-like-msie8) or simply does nothing.\n \n@@ -244,7 +244,7 @@ var clean = DOMPurify.sanitize(\n         CUSTOM_ELEMENT_HANDLING: {\n             tagNameCheck: /^foo-/, // allow all tags starting with \"foo-\"\n             attributeNameCheck: /baz/, // allow all attributes containing \"baz\"\n-            allowCustomizedBuiltInElements: false, // customized built-ins are allowed\n+            allowCustomizedBuiltInElements: true, // customized built-ins are allowed\n         },\n     }\n ); // <foo-bar baz=\"foobar\"></foo-bar><div is=\"\"></div>\n@@ -409,7 +409,7 @@ Feature releases will not be announced to this list.\n \n Many people helped and help DOMPurify become what it is and need to be acknowledged here!\n \n-[JGraph 💸](https://github.com/jgraph), [GitHub 💸](https://github.com/github), [CynegeticIO 💸](https://github.com/CynegeticIO), [Sentry 💸](https://github.com/getsentry), [jarrodldavis 💸](https://github.com/jarrodldavis), [GrantGryczan](https://github.com/GrantGryczan), [Lowdefy 💸](https://twitter.com/lowdefy), [granlem ](https://twitter.com/MaximeVeit), [oreoshake ](https://github.com/oreoshake), [dcramer 💸](https://github.com/dcramer),[tdeekens ❤️](https://github.com/tdeekens), [peernohell ❤️](https://github.com/peernohell), [is2ei](https://github.com/is2ei), [SoheilKhodayari](https://github.com/SoheilKhodayari), [franktopel](https://github.com/franktopel), [NateScarlet](https://github.com/NateScarlet), [neilj](https://github.com/neilj), [fhemberger](https://github.com/fhemberger), [Joris-van-der-Wel](https://github.com/Joris-van-der-Wel), [ydaniv](https://github.com/ydaniv), [terjanq](https://twitter.com/terjanq), [filedescriptor](https://github.com/filedescriptor), [ConradIrwin](https://github.com/ConradIrwin), [gibson042](https://github.com/gibson042), [choumx](https://github.com/choumx), [0xSobky](https://github.com/0xSobky), [styfle](https://github.com/styfle), [koto](https://github.com/koto), [tlau88](https://github.com/tlau88), [strugee](https://github.com/strugee), [oparoz](https://github.com/oparoz), [mathiasbynens](https://github.com/mathiasbynens), [edg2s](https://github.com/edg2s), [dnkolegov](https://github.com/dnkolegov), [dhardtke](https://github.com/dhardtke), [wirehead](https://github.com/wirehead), [thorn0](https://github.com/thorn0), [styu](https://github.com/styu), [mozfreddyb](https://github.com/mozfreddyb), [mikesamuel](https://github.com/mikesamuel), [jorangreef](https://github.com/jorangreef), [jimmyhchan](https://github.com/jimmyhchan), [jameydeorio](https://github.com/jameydeorio), [jameskraus](https://github.com/jameskraus), [hyderali](https://github.com/hyderali), [hansottowirtz](https://github.com/hansottowirtz), [hackvertor](https://github.com/hackvertor), [freddyb](https://github.com/freddyb), [flavorjones](https://github.com/flavorjones), [djfarrelly](https://github.com/djfarrelly), [devd](https://github.com/devd), [camerondunford](https://github.com/camerondunford), [buu700](https://github.com/buu700), [buildog](https://github.com/buildog), [alabiaga](https://github.com/alabiaga), [Vector919](https://github.com/Vector919), [Robbert](https://github.com/Robbert), [GreLI](https://github.com/GreLI), [FuzzySockets](https://github.com/FuzzySockets), [ArtemBernatskyy](https://github.com/ArtemBernatskyy), [@garethheyes](https://twitter.com/garethheyes), [@shafigullin](https://twitter.com/shafigullin), [@mmrupp](https://twitter.com/mmrupp), [@irsdl](https://twitter.com/irsdl),[ShikariSenpai](https://github.com/ShikariSenpai), [ansjdnakjdnajkd](https://github.com/ansjdnakjdnajkd), [@asutherland](https://twitter.com/asutherland), [@mathias](https://twitter.com/mathias), [@cgvwzq](https://twitter.com/cgvwzq), [@robbertatwork](https://twitter.com/robbertatwork), [@giutro](https://twitter.com/giutro), [@CmdEngineer\\_](https://twitter.com/CmdEngineer_), [@avr4mit](https://twitter.com/avr4mit) and especially [@securitymb ❤️](https://twitter.com/securitymb) & [@masatokinugawa ❤️](https://twitter.com/masatokinugawa)\n+[JGraph 💸](https://github.com/jgraph), [GitHub 💸](https://github.com/github), [CynegeticIO 💸](https://github.com/CynegeticIO), [Sentry 💸](https://github.com/getsentry), [jarrodldavis 💸](https://github.com/jarrodldavis), [kevin_mizu](https://twitter.com/kevin_mizu), [GrantGryczan](https://github.com/GrantGryczan), [Lowdefy 💸](https://twitter.com/lowdefy), [granlem ](https://twitter.com/MaximeVeit), [oreoshake ](https://github.com/oreoshake), [dcramer 💸](https://github.com/dcramer),[tdeekens ❤️](https://github.com/tdeekens), [peernohell ❤️](https://github.com/peernohell), [is2ei](https://github.com/is2ei), [SoheilKhodayari](https://github.com/SoheilKhodayari), [franktopel](https://github.com/franktopel), [NateScarlet](https://github.com/NateScarlet), [neilj](https://github.com/neilj), [fhemberger](https://github.com/fhemberger), [Joris-van-der-Wel](https://github.com/Joris-van-der-Wel), [ydaniv](https://github.com/ydaniv), [terjanq](https://twitter.com/terjanq), [filedescriptor](https://github.com/filedescriptor), [ConradIrwin](https://github.com/ConradIrwin), [gibson042](https://github.com/gibson042), [choumx](https://github.com/choumx), [0xSobky](https://github.com/0xSobky), [styfle](https://github.com/styfle), [koto](https://github.com/koto), [tlau88](https://github.com/tlau88), [strugee](https://github.com/strugee), [oparoz](https://github.com/oparoz), [mathiasbynens](https://github.com/mathiasbynens), [edg2s](https://github.com/edg2s), [dnkolegov](https://github.com/dnkolegov), [dhardtke](https://github.com/dhardtke), [wirehead](https://github.com/wirehead), [thorn0](https://github.com/thorn0), [styu](https://github.com/styu), [mozfreddyb](https://github.com/mozfreddyb), [mikesamuel](https://github.com/mikesamuel), [jorangreef](https://github.com/jorangreef), [jimmyhchan](https://github.com/jimmyhchan), [jameydeorio](https://github.com/jameydeorio), [jameskraus](https://github.com/jameskraus), [hyderali](https://github.com/hyderali), [hansottowirtz](https://github.com/hansottowirtz), [hackvertor](https://github.com/hackvertor), [freddyb](https://github.com/freddyb), [flavorjones](https://github.com/flavorjones), [djfarrelly](https://github.com/djfarrelly), [devd](https://github.com/devd), [camerondunford](https://github.com/camerondunford), [buu700](https://github.com/buu700), [buildog](https://github.com/buildog), [alabiaga](https://github.com/alabiaga), [Vector919](https://github.com/Vector919), [Robbert](https://github.com/Robbert), [GreLI](https://github.com/GreLI), [FuzzySockets](https://github.com/FuzzySockets), [ArtemBernatskyy](https://github.com/ArtemBernatskyy), [@garethheyes](https://twitter.com/garethheyes), [@shafigullin](https://twitter.com/shafigullin), [@mmrupp](https://twitter.com/mmrupp), [@irsdl](https://twitter.com/irsdl),[ShikariSenpai](https://github.com/ShikariSenpai), [ansjdnakjdnajkd](https://github.com/ansjdnakjdnajkd), [@asutherland](https://twitter.com/asutherland), [@mathias](https://twitter.com/mathias), [@cgvwzq](https://twitter.com/cgvwzq), [@robbertatwork](https://twitter.com/robbertatwork), [@giutro](https://twitter.com/giutro), [@CmdEngineer\\_](https://twitter.com/CmdEngineer_), [@avr4mit](https://twitter.com/avr4mit) and especially [@securitymb ❤️](https://twitter.com/securitymb) & [@masatokinugawa ❤️](https://twitter.com/masatokinugawa)\n \n ## Testing powered by\n ",
          "bower.json": "@@ -1,6 +1,6 @@\n {\n   \"name\": \"DOMPurify\",\n-  \"version\": \"2.4.1\",\n+  \"version\": \"2.4.2\",\n   \"homepage\": \"https://github.com/cure53/DOMPurify\",\n   \"author\": \"Cure53 <info@cure53.de>\",\n   \"description\": \"A DOM-only, super-fast, uber-tolerant XSS sanitizer for HTML, MathML and SVG\",",
          "dist/purify.cjs.js": "@@ -1,4 +1,4 @@\n-/*! @license DOMPurify 2.4.1 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.1/LICENSE */\n+/*! @license DOMPurify 2.4.2 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.2/LICENSE */\n \n 'use strict';\n \n@@ -192,7 +192,7 @@ function clone(object) {\n   var property;\n \n   for (property in object) {\n-    if (apply(hasOwnProperty, object, [property])) {\n+    if (apply(hasOwnProperty, object, [property]) === true) {\n       newObject[property] = object[property];\n     }\n   }\n@@ -324,7 +324,7 @@ function createDOMPurify() {\n    */\n \n \n-  DOMPurify.version = '2.4.1';\n+  DOMPurify.version = '2.4.2';\n   /**\n    * Array of elements that DOMPurify removed during sanitation.\n    * Empty if nothing was removed.\n@@ -949,7 +949,7 @@ function createDOMPurify() {\n       doc = implementation.createDocument(NAMESPACE, 'template', null);\n \n       try {\n-        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? '' : dirtyPayload;\n+        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n       } catch (_) {// Syntax error if dirtyPayload is invalid xml\n       }\n     }",
          "dist/purify.es.js": "@@ -1,4 +1,4 @@\n-/*! @license DOMPurify 2.4.1 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.1/LICENSE */\n+/*! @license DOMPurify 2.4.2 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.2/LICENSE */\n \n function _typeof(obj) {\n   \"@babel/helpers - typeof\";\n@@ -190,7 +190,7 @@ function clone(object) {\n   var property;\n \n   for (property in object) {\n-    if (apply(hasOwnProperty, object, [property])) {\n+    if (apply(hasOwnProperty, object, [property]) === true) {\n       newObject[property] = object[property];\n     }\n   }\n@@ -322,7 +322,7 @@ function createDOMPurify() {\n    */\n \n \n-  DOMPurify.version = '2.4.1';\n+  DOMPurify.version = '2.4.2';\n   /**\n    * Array of elements that DOMPurify removed during sanitation.\n    * Empty if nothing was removed.\n@@ -947,7 +947,7 @@ function createDOMPurify() {\n       doc = implementation.createDocument(NAMESPACE, 'template', null);\n \n       try {\n-        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? '' : dirtyPayload;\n+        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n       } catch (_) {// Syntax error if dirtyPayload is invalid xml\n       }\n     }",
          "dist/purify.js": "@@ -1,4 +1,4 @@\n-/*! @license DOMPurify 2.4.1 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.1/LICENSE */\n+/*! @license DOMPurify 2.4.2 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.2/LICENSE */\n \n (function (global, factory) {\n   typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() :\n@@ -196,7 +196,7 @@\n     var property;\n \n     for (property in object) {\n-      if (apply(hasOwnProperty, object, [property])) {\n+      if (apply(hasOwnProperty, object, [property]) === true) {\n         newObject[property] = object[property];\n       }\n     }\n@@ -328,7 +328,7 @@\n      */\n \n \n-    DOMPurify.version = '2.4.1';\n+    DOMPurify.version = '2.4.2';\n     /**\n      * Array of elements that DOMPurify removed during sanitation.\n      * Empty if nothing was removed.\n@@ -953,7 +953,7 @@\n         doc = implementation.createDocument(NAMESPACE, 'template', null);\n \n         try {\n-          doc.documentElement.innerHTML = IS_EMPTY_INPUT ? '' : dirtyPayload;\n+          doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n         } catch (_) {// Syntax error if dirtyPayload is invalid xml\n         }\n       }",
          "dist/purify.min.js": "@@ -1,3 +1,3 @@\n-/*! @license DOMPurify 2.4.1 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.1/LICENSE */\n-!function(e,t){\"object\"==typeof exports&&\"undefined\"!=typeof module?module.exports=t():\"function\"==typeof define&&define.amd?define(t):(e=\"undefined\"!=typeof globalThis?globalThis:e||self).DOMPurify=t()}(this,(function(){\"use strict\";function e(t){return e=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e},e(t)}function t(e,n){return t=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e},t(e,n)}function n(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}function r(e,o,a){return r=n()?Reflect.construct:function(e,n,r){var o=[null];o.push.apply(o,n);var a=new(Function.bind.apply(e,o));return r&&t(a,r.prototype),a},r.apply(null,arguments)}function o(e){return function(e){if(Array.isArray(e))return a(e)}(e)||function(e){if(\"undefined\"!=typeof Symbol&&null!=e[Symbol.iterator]||null!=e[\"@@iterator\"])return Array.from(e)}(e)||function(e,t){if(!e)return;if(\"string\"==typeof e)return a(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);\"Object\"===n&&e.constructor&&(n=e.constructor.name);if(\"Map\"===n||\"Set\"===n)return Array.from(e);if(\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return a(e,t)}(e)||function(){throw new TypeError(\"Invalid attempt to spread non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}()}function a(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=new Array(t);n<t;n++)r[n]=e[n];return r}var i=Object.hasOwnProperty,l=Object.setPrototypeOf,c=Object.isFrozen,u=Object.getPrototypeOf,s=Object.getOwnPropertyDescriptor,m=Object.freeze,f=Object.seal,p=Object.create,d=\"undefined\"!=typeof Reflect&&Reflect,h=d.apply,g=d.construct;h||(h=function(e,t,n){return e.apply(t,n)}),m||(m=function(e){return e}),f||(f=function(e){return e}),g||(g=function(e,t){return r(e,o(t))});var y,b=O(Array.prototype.forEach),v=O(Array.prototype.pop),T=O(Array.prototype.push),N=O(String.prototype.toLowerCase),A=O(String.prototype.toString),E=O(String.prototype.match),w=O(String.prototype.replace),S=O(String.prototype.indexOf),x=O(String.prototype.trim),_=O(RegExp.prototype.test),k=(y=TypeError,function(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return g(y,t)});function O(e){return function(t){for(var n=arguments.length,r=new Array(n>1?n-1:0),o=1;o<n;o++)r[o-1]=arguments[o];return h(e,t,r)}}function D(e,t,n){n=n||N,l&&l(e,null);for(var r=t.length;r--;){var o=t[r];if(\"string\"==typeof o){var a=n(o);a!==o&&(c(t)||(t[r]=a),o=a)}e[o]=!0}return e}function L(e){var t,n=p(null);for(t in e)h(i,e,[t])&&(n[t]=e[t]);return n}function R(e,t){for(;null!==e;){var n=s(e,t);if(n){if(n.get)return O(n.get);if(\"function\"==typeof n.value)return O(n.value)}e=u(e)}return function(e){return console.warn(\"fallback value for\",e),null}}var M=m([\"a\",\"abbr\",\"acronym\",\"address\",\"area\",\"article\",\"aside\",\"audio\",\"b\",\"bdi\",\"bdo\",\"big\",\"blink\",\"blockquote\",\"body\",\"br\",\"button\",\"canvas\",\"caption\",\"center\",\"cite\",\"code\",\"col\",\"colgroup\",\"content\",\"data\",\"datalist\",\"dd\",\"decorator\",\"del\",\"details\",\"dfn\",\"dialog\",\"dir\",\"div\",\"dl\",\"dt\",\"element\",\"em\",\"fieldset\",\"figcaption\",\"figure\",\"font\",\"footer\",\"form\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"head\",\"header\",\"hgroup\",\"hr\",\"html\",\"i\",\"img\",\"input\",\"ins\",\"kbd\",\"label\",\"legend\",\"li\",\"main\",\"map\",\"mark\",\"marquee\",\"menu\",\"menuitem\",\"meter\",\"nav\",\"nobr\",\"ol\",\"optgroup\",\"option\",\"output\",\"p\",\"picture\",\"pre\",\"progress\",\"q\",\"rp\",\"rt\",\"ruby\",\"s\",\"samp\",\"section\",\"select\",\"shadow\",\"small\",\"source\",\"spacer\",\"span\",\"strike\",\"strong\",\"style\",\"sub\",\"summary\",\"sup\",\"table\",\"tbody\",\"td\",\"template\",\"textarea\",\"tfoot\",\"th\",\"thead\",\"time\",\"tr\",\"track\",\"tt\",\"u\",\"ul\",\"var\",\"video\",\"wbr\"]),C=m([\"svg\",\"a\",\"altglyph\",\"altglyphdef\",\"altglyphitem\",\"animatecolor\",\"animatemotion\",\"animatetransform\",\"circle\",\"clippath\",\"defs\",\"desc\",\"ellipse\",\"filter\",\"font\",\"g\",\"glyph\",\"glyphref\",\"hkern\",\"image\",\"line\",\"lineargradient\",\"marker\",\"mask\",\"metadata\",\"mpath\",\"path\",\"pattern\",\"polygon\",\"polyline\",\"radialgradient\",\"rect\",\"stop\",\"style\",\"switch\",\"symbol\",\"text\",\"textpath\",\"title\",\"tref\",\"tspan\",\"view\",\"vkern\"]),I=m([\"feBlend\",\"feColorMatrix\",\"feComponentTransfer\",\"feComposite\",\"feConvolveMatrix\",\"feDiffuseLighting\",\"feDisplacementMap\",\"feDistantLight\",\"feFlood\",\"feFuncA\",\"feFuncB\",\"feFuncG\",\"feFuncR\",\"feGaussianBlur\",\"feImage\",\"feMerge\",\"feMergeNode\",\"feMorphology\",\"feOffset\",\"fePointLight\",\"feSpecularLighting\",\"feSpotLight\",\"feTile\",\"feTurbulence\"]),F=m([\"animate\",\"color-profile\",\"cursor\",\"discard\",\"fedropshadow\",\"font-face\",\"font-face-format\",\"font-face-name\",\"font-face-src\",\"font-face-uri\",\"foreignobject\",\"hatch\",\"hatchpath\",\"mesh\",\"meshgradient\",\"meshpatch\",\"meshrow\",\"missing-glyph\",\"script\",\"set\",\"solidcolor\",\"unknown\",\"use\"]),U=m([\"math\",\"menclose\",\"merror\",\"mfenced\",\"mfrac\",\"mglyph\",\"mi\",\"mlabeledtr\",\"mmultiscripts\",\"mn\",\"mo\",\"mover\",\"mpadded\",\"mphantom\",\"mroot\",\"mrow\",\"ms\",\"mspace\",\"msqrt\",\"mstyle\",\"msub\",\"msup\",\"msubsup\",\"mtable\",\"mtd\",\"mtext\",\"mtr\",\"munder\",\"munderover\"]),H=m([\"maction\",\"maligngroup\",\"malignmark\",\"mlongdiv\",\"mscarries\",\"mscarry\",\"msgroup\",\"mstack\",\"msline\",\"msrow\",\"semantics\",\"annotation\",\"annotation-xml\",\"mprescripts\",\"none\"]),z=m([\"#text\"]),P=m([\"accept\",\"action\",\"align\",\"alt\",\"autocapitalize\",\"autocomplete\",\"autopictureinpicture\",\"autoplay\",\"background\",\"bgcolor\",\"border\",\"capture\",\"cellpadding\",\"cellspacing\",\"checked\",\"cite\",\"class\",\"clear\",\"color\",\"cols\",\"colspan\",\"controls\",\"controlslist\",\"coords\",\"crossorigin\",\"datetime\",\"decoding\",\"default\",\"dir\",\"disabled\",\"disablepictureinpicture\",\"disableremoteplayback\",\"download\",\"draggable\",\"enctype\",\"enterkeyhint\",\"face\",\"for\",\"headers\",\"height\",\"hidden\",\"high\",\"href\",\"hreflang\",\"id\",\"inputmode\",\"integrity\",\"ismap\",\"kind\",\"label\",\"lang\",\"list\",\"loading\",\"loop\",\"low\",\"max\",\"maxlength\",\"media\",\"method\",\"min\",\"minlength\",\"multiple\",\"muted\",\"name\",\"nonce\",\"noshade\",\"novalidate\",\"nowrap\",\"open\",\"optimum\",\"pattern\",\"placeholder\",\"playsinline\",\"poster\",\"preload\",\"pubdate\",\"radiogroup\",\"readonly\",\"rel\",\"required\",\"rev\",\"reversed\",\"role\",\"rows\",\"rowspan\",\"spellcheck\",\"scope\",\"selected\",\"shape\",\"size\",\"sizes\",\"span\",\"srclang\",\"start\",\"src\",\"srcset\",\"step\",\"style\",\"summary\",\"tabindex\",\"title\",\"translate\",\"type\",\"usemap\",\"valign\",\"value\",\"width\",\"xmlns\",\"slot\"]),j=m([\"accent-height\",\"accumulate\",\"additive\",\"alignment-baseline\",\"ascent\",\"attributename\",\"attributetype\",\"azimuth\",\"basefrequency\",\"baseline-shift\",\"begin\",\"bias\",\"by\",\"class\",\"clip\",\"clippathunits\",\"clip-path\",\"clip-rule\",\"color\",\"color-interpolation\",\"color-interpolation-filters\",\"color-profile\",\"color-rendering\",\"cx\",\"cy\",\"d\",\"dx\",\"dy\",\"diffuseconstant\",\"direction\",\"display\",\"divisor\",\"dur\",\"edgemode\",\"elevation\",\"end\",\"fill\",\"fill-opacity\",\"fill-rule\",\"filter\",\"filterunits\",\"flood-color\",\"flood-opacity\",\"font-family\",\"font-size\",\"font-size-adjust\",\"font-stretch\",\"font-style\",\"font-variant\",\"font-weight\",\"fx\",\"fy\",\"g1\",\"g2\",\"glyph-name\",\"glyphref\",\"gradientunits\",\"gradienttransform\",\"height\",\"href\",\"id\",\"image-rendering\",\"in\",\"in2\",\"k\",\"k1\",\"k2\",\"k3\",\"k4\",\"kerning\",\"keypoints\",\"keysplines\",\"keytimes\",\"lang\",\"lengthadjust\",\"letter-spacing\",\"kernelmatrix\",\"kernelunitlength\",\"lighting-color\",\"local\",\"marker-end\",\"marker-mid\",\"marker-start\",\"markerheight\",\"markerunits\",\"markerwidth\",\"maskcontentunits\",\"maskunits\",\"max\",\"mask\",\"media\",\"method\",\"mode\",\"min\",\"name\",\"numoctaves\",\"offset\",\"operator\",\"opacity\",\"order\",\"orient\",\"orientation\",\"origin\",\"overflow\",\"paint-order\",\"path\",\"pathlength\",\"patterncontentunits\",\"patterntransform\",\"patternunits\",\"points\",\"preservealpha\",\"preserveaspectratio\",\"primitiveunits\",\"r\",\"rx\",\"ry\",\"radius\",\"refx\",\"refy\",\"repeatcount\",\"repeatdur\",\"restart\",\"result\",\"rotate\",\"scale\",\"seed\",\"shape-rendering\",\"specularconstant\",\"specularexponent\",\"spreadmethod\",\"startoffset\",\"stddeviation\",\"stitchtiles\",\"stop-color\",\"stop-opacity\",\"stroke-dasharray\",\"stroke-dashoffset\",\"stroke-linecap\",\"stroke-linejoin\",\"stroke-miterlimit\",\"stroke-opacity\",\"stroke\",\"stroke-width\",\"style\",\"surfacescale\",\"systemlanguage\",\"tabindex\",\"targetx\",\"targety\",\"transform\",\"transform-origin\",\"text-anchor\",\"text-decoration\",\"text-rendering\",\"textlength\",\"type\",\"u1\",\"u2\",\"unicode\",\"values\",\"viewbox\",\"visibility\",\"version\",\"vert-adv-y\",\"vert-origin-x\",\"vert-origin-y\",\"width\",\"word-spacing\",\"wrap\",\"writing-mode\",\"xchannelselector\",\"ychannelselector\",\"x\",\"x1\",\"x2\",\"xmlns\",\"y\",\"y1\",\"y2\",\"z\",\"zoomandpan\"]),B=m([\"accent\",\"accentunder\",\"align\",\"bevelled\",\"close\",\"columnsalign\",\"columnlines\",\"columnspan\",\"denomalign\",\"depth\",\"dir\",\"display\",\"displaystyle\",\"encoding\",\"fence\",\"frame\",\"height\",\"href\",\"id\",\"largeop\",\"length\",\"linethickness\",\"lspace\",\"lquote\",\"mathbackground\",\"mathcolor\",\"mathsize\",\"mathvariant\",\"maxsize\",\"minsize\",\"movablelimits\",\"notation\",\"numalign\",\"open\",\"rowalign\",\"rowlines\",\"rowspacing\",\"rowspan\",\"rspace\",\"rquote\",\"scriptlevel\",\"scriptminsize\",\"scriptsizemultiplier\",\"selection\",\"separator\",\"separators\",\"stretchy\",\"subscriptshift\",\"supscriptshift\",\"symmetric\",\"voffset\",\"width\",\"xmlns\"]),G=m([\"xlink:href\",\"xml:id\",\"xlink:title\",\"xml:space\",\"xmlns:xlink\"]),W=f(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm),q=f(/<%[\\w\\W]*|[\\w\\W]*%>/gm),Y=f(/\\${[\\w\\W]*}/gm),$=f(/^data-[\\-\\w.\\u00B7-\\uFFFF]/),K=f(/^aria-[\\-\\w]+$/),V=f(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i),X=f(/^(?:\\w+script|data):/i),Z=f(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g),J=f(/^html$/i),Q=function(){return\"undefined\"==typeof window?null:window},ee=function(t,n){if(\"object\"!==e(t)||\"function\"!=typeof t.createPolicy)return null;var r=null,o=\"data-tt-policy-suffix\";n.currentScript&&n.currentScript.hasAttribute(o)&&(r=n.currentScript.getAttribute(o));var a=\"dompurify\"+(r?\"#\"+r:\"\");try{return t.createPolicy(a,{createHTML:function(e){return e},createScriptURL:function(e){return e}})}catch(e){return console.warn(\"TrustedTypes policy \"+a+\" could not be created.\"),null}};var te=function t(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Q(),r=function(e){return t(e)};if(r.version=\"2.4.1\",r.removed=[],!n||!n.document||9!==n.document.nodeType)return r.isSupported=!1,r;var a=n.document,i=n.document,l=n.DocumentFragment,c=n.HTMLTemplateElement,u=n.Node,s=n.Element,f=n.NodeFilter,p=n.NamedNodeMap,d=void 0===p?n.NamedNodeMap||n.MozNamedAttrMap:p,h=n.HTMLFormElement,g=n.DOMParser,y=n.trustedTypes,O=s.prototype,te=R(O,\"cloneNode\"),ne=R(O,\"nextSibling\"),re=R(O,\"childNodes\"),oe=R(O,\"parentNode\");if(\"function\"==typeof c){var ae=i.createElement(\"template\");ae.content&&ae.content.ownerDocument&&(i=ae.content.ownerDocument)}var ie=ee(y,a),le=ie?ie.createHTML(\"\"):\"\",ce=i,ue=ce.implementation,se=ce.createNodeIterator,me=ce.createDocumentFragment,fe=ce.getElementsByTagName,pe=a.importNode,de={};try{de=L(i).documentMode?i.documentMode:{}}catch(e){}var he={};r.isSupported=\"function\"==typeof oe&&ue&&void 0!==ue.createHTMLDocument&&9!==de;var ge,ye,be=W,ve=q,Te=Y,Ne=$,Ae=K,Ee=X,we=Z,Se=V,xe=null,_e=D({},[].concat(o(M),o(C),o(I),o(U),o(z))),ke=null,Oe=D({},[].concat(o(P),o(j),o(B),o(G))),De=Object.seal(Object.create(null,{tagNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},attributeNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},allowCustomizedBuiltInElements:{writable:!0,configurable:!1,enumerable:!0,value:!1}})),Le=null,Re=null,Me=!0,Ce=!0,Ie=!1,Fe=!1,Ue=!1,He=!1,ze=!1,Pe=!1,je=!1,Be=!1,Ge=!0,We=!1,qe=\"user-content-\",Ye=!0,$e=!1,Ke={},Ve=null,Xe=D({},[\"annotation-xml\",\"audio\",\"colgroup\",\"desc\",\"foreignobject\",\"head\",\"iframe\",\"math\",\"mi\",\"mn\",\"mo\",\"ms\",\"mtext\",\"noembed\",\"noframes\",\"noscript\",\"plaintext\",\"script\",\"style\",\"svg\",\"template\",\"thead\",\"title\",\"video\",\"xmp\"]),Ze=null,Je=D({},[\"audio\",\"video\",\"img\",\"source\",\"image\",\"track\"]),Qe=null,et=D({},[\"alt\",\"class\",\"for\",\"id\",\"label\",\"name\",\"pattern\",\"placeholder\",\"role\",\"summary\",\"title\",\"value\",\"style\",\"xmlns\"]),tt=\"http://www.w3.org/1998/Math/MathML\",nt=\"http://www.w3.org/2000/svg\",rt=\"http://www.w3.org/1999/xhtml\",ot=rt,at=!1,it=null,lt=D({},[tt,nt,rt],A),ct=[\"application/xhtml+xml\",\"text/html\"],ut=\"text/html\",st=null,mt=i.createElement(\"form\"),ft=function(e){return e instanceof RegExp||e instanceof Function},pt=function(t){st&&st===t||(t&&\"object\"===e(t)||(t={}),t=L(t),ge=ge=-1===ct.indexOf(t.PARSER_MEDIA_TYPE)?ut:t.PARSER_MEDIA_TYPE,ye=\"application/xhtml+xml\"===ge?A:N,xe=\"ALLOWED_TAGS\"in t?D({},t.ALLOWED_TAGS,ye):_e,ke=\"ALLOWED_ATTR\"in t?D({},t.ALLOWED_ATTR,ye):Oe,it=\"ALLOWED_NAMESPACES\"in t?D({},t.ALLOWED_NAMESPACES,A):lt,Qe=\"ADD_URI_SAFE_ATTR\"in t?D(L(et),t.ADD_URI_SAFE_ATTR,ye):et,Ze=\"ADD_DATA_URI_TAGS\"in t?D(L(Je),t.ADD_DATA_URI_TAGS,ye):Je,Ve=\"FORBID_CONTENTS\"in t?D({},t.FORBID_CONTENTS,ye):Xe,Le=\"FORBID_TAGS\"in t?D({},t.FORBID_TAGS,ye):{},Re=\"FORBID_ATTR\"in t?D({},t.FORBID_ATTR,ye):{},Ke=\"USE_PROFILES\"in t&&t.USE_PROFILES,Me=!1!==t.ALLOW_ARIA_ATTR,Ce=!1!==t.ALLOW_DATA_ATTR,Ie=t.ALLOW_UNKNOWN_PROTOCOLS||!1,Fe=t.SAFE_FOR_TEMPLATES||!1,Ue=t.WHOLE_DOCUMENT||!1,Pe=t.RETURN_DOM||!1,je=t.RETURN_DOM_FRAGMENT||!1,Be=t.RETURN_TRUSTED_TYPE||!1,ze=t.FORCE_BODY||!1,Ge=!1!==t.SANITIZE_DOM,We=t.SANITIZE_NAMED_PROPS||!1,Ye=!1!==t.KEEP_CONTENT,$e=t.IN_PLACE||!1,Se=t.ALLOWED_URI_REGEXP||Se,ot=t.NAMESPACE||rt,t.CUSTOM_ELEMENT_HANDLING&&ft(t.CUSTOM_ELEMENT_HANDLING.tagNameCheck)&&(De.tagNameCheck=t.CUSTOM_ELEMENT_HANDLING.tagNameCheck),t.CUSTOM_ELEMENT_HANDLING&&ft(t.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)&&(De.attributeNameCheck=t.CUSTOM_ELEMENT_HANDLING.attributeNameCheck),t.CUSTOM_ELEMENT_HANDLING&&\"boolean\"==typeof t.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements&&(De.allowCustomizedBuiltInElements=t.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements),Fe&&(Ce=!1),je&&(Pe=!0),Ke&&(xe=D({},o(z)),ke=[],!0===Ke.html&&(D(xe,M),D(ke,P)),!0===Ke.svg&&(D(xe,C),D(ke,j),D(ke,G)),!0===Ke.svgFilters&&(D(xe,I),D(ke,j),D(ke,G)),!0===Ke.mathMl&&(D(xe,U),D(ke,B),D(ke,G))),t.ADD_TAGS&&(xe===_e&&(xe=L(xe)),D(xe,t.ADD_TAGS,ye)),t.ADD_ATTR&&(ke===Oe&&(ke=L(ke)),D(ke,t.ADD_ATTR,ye)),t.ADD_URI_SAFE_ATTR&&D(Qe,t.ADD_URI_SAFE_ATTR,ye),t.FORBID_CONTENTS&&(Ve===Xe&&(Ve=L(Ve)),D(Ve,t.FORBID_CONTENTS,ye)),Ye&&(xe[\"#text\"]=!0),Ue&&D(xe,[\"html\",\"head\",\"body\"]),xe.table&&(D(xe,[\"tbody\"]),delete Le.tbody),m&&m(t),st=t)},dt=D({},[\"mi\",\"mo\",\"mn\",\"ms\",\"mtext\"]),ht=D({},[\"foreignobject\",\"desc\",\"title\",\"annotation-xml\"]),gt=D({},[\"title\",\"style\",\"font\",\"a\",\"script\"]),yt=D({},C);D(yt,I),D(yt,F);var bt=D({},U);D(bt,H);var vt=function(e){var t=oe(e);t&&t.tagName||(t={namespaceURI:ot,tagName:\"template\"});var n=N(e.tagName),r=N(t.tagName);return!!it[e.namespaceURI]&&(e.namespaceURI===nt?t.namespaceURI===rt?\"svg\"===n:t.namespaceURI===tt?\"svg\"===n&&(\"annotation-xml\"===r||dt[r]):Boolean(yt[n]):e.namespaceURI===tt?t.namespaceURI===rt?\"math\"===n:t.namespaceURI===nt?\"math\"===n&&ht[r]:Boolean(bt[n]):e.namespaceURI===rt?!(t.namespaceURI===nt&&!ht[r])&&(!(t.namespaceURI===tt&&!dt[r])&&(!bt[n]&&(gt[n]||!yt[n]))):!(\"application/xhtml+xml\"!==ge||!it[e.namespaceURI]))},Tt=function(e){T(r.removed,{element:e});try{e.parentNode.removeChild(e)}catch(t){try{e.outerHTML=le}catch(t){e.remove()}}},Nt=function(e,t){try{T(r.removed,{attribute:t.getAttributeNode(e),from:t})}catch(e){T(r.removed,{attribute:null,from:t})}if(t.removeAttribute(e),\"is\"===e&&!ke[e])if(Pe||je)try{Tt(t)}catch(e){}else try{t.setAttribute(e,\"\")}catch(e){}},At=function(e){var t,n;if(ze)e=\"<remove></remove>\"+e;else{var r=E(e,/^[\\r\\n\\t ]+/);n=r&&r[0]}\"application/xhtml+xml\"===ge&&ot===rt&&(e='<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>'+e+\"</body></html>\");var o=ie?ie.createHTML(e):e;if(ot===rt)try{t=(new g).parseFromString(o,ge)}catch(e){}if(!t||!t.documentElement){t=ue.createDocument(ot,\"template\",null);try{t.documentElement.innerHTML=at?\"\":o}catch(e){}}var a=t.body||t.documentElement;return e&&n&&a.insertBefore(i.createTextNode(n),a.childNodes[0]||null),ot===rt?fe.call(t,Ue?\"html\":\"body\")[0]:Ue?t.documentElement:a},Et=function(e){return se.call(e.ownerDocument||e,e,f.SHOW_ELEMENT|f.SHOW_COMMENT|f.SHOW_TEXT,null,!1)},wt=function(e){return e instanceof h&&(\"string\"!=typeof e.nodeName||\"string\"!=typeof e.textContent||\"function\"!=typeof e.removeChild||!(e.attributes instanceof d)||\"function\"!=typeof e.removeAttribute||\"function\"!=typeof e.setAttribute||\"string\"!=typeof e.namespaceURI||\"function\"!=typeof e.insertBefore||\"function\"!=typeof e.hasChildNodes)},St=function(t){return\"object\"===e(u)?t instanceof u:t&&\"object\"===e(t)&&\"number\"==typeof t.nodeType&&\"string\"==typeof t.nodeName},xt=function(e,t,n){he[e]&&b(he[e],(function(e){e.call(r,t,n,st)}))},_t=function(e){var t;if(xt(\"beforeSanitizeElements\",e,null),wt(e))return Tt(e),!0;if(_(/[\\u0080-\\uFFFF]/,e.nodeName))return Tt(e),!0;var n=ye(e.nodeName);if(xt(\"uponSanitizeElement\",e,{tagName:n,allowedTags:xe}),e.hasChildNodes()&&!St(e.firstElementChild)&&(!St(e.content)||!St(e.content.firstElementChild))&&_(/<[/\\w]/g,e.innerHTML)&&_(/<[/\\w]/g,e.textContent))return Tt(e),!0;if(\"select\"===n&&_(/<template/i,e.innerHTML))return Tt(e),!0;if(!xe[n]||Le[n]){if(!Le[n]&&Ot(n)){if(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,n))return!1;if(De.tagNameCheck instanceof Function&&De.tagNameCheck(n))return!1}if(Ye&&!Ve[n]){var o=oe(e)||e.parentNode,a=re(e)||e.childNodes;if(a&&o)for(var i=a.length-1;i>=0;--i)o.insertBefore(te(a[i],!0),ne(e))}return Tt(e),!0}return e instanceof s&&!vt(e)?(Tt(e),!0):\"noscript\"!==n&&\"noembed\"!==n||!_(/<\\/no(script|embed)/i,e.innerHTML)?(Fe&&3===e.nodeType&&(t=e.textContent,t=w(t,be,\" \"),t=w(t,ve,\" \"),t=w(t,Te,\" \"),e.textContent!==t&&(T(r.removed,{element:e.cloneNode()}),e.textContent=t)),xt(\"afterSanitizeElements\",e,null),!1):(Tt(e),!0)},kt=function(e,t,n){if(Ge&&(\"id\"===t||\"name\"===t)&&(n in i||n in mt))return!1;if(Ce&&!Re[t]&&_(Ne,t));else if(Me&&_(Ae,t));else if(!ke[t]||Re[t]){if(!(Ot(e)&&(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,e)||De.tagNameCheck instanceof Function&&De.tagNameCheck(e))&&(De.attributeNameCheck instanceof RegExp&&_(De.attributeNameCheck,t)||De.attributeNameCheck instanceof Function&&De.attributeNameCheck(t))||\"is\"===t&&De.allowCustomizedBuiltInElements&&(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,n)||De.tagNameCheck instanceof Function&&De.tagNameCheck(n))))return!1}else if(Qe[t]);else if(_(Se,w(n,we,\"\")));else if(\"src\"!==t&&\"xlink:href\"!==t&&\"href\"!==t||\"script\"===e||0!==S(n,\"data:\")||!Ze[e]){if(Ie&&!_(Ee,w(n,we,\"\")));else if(n)return!1}else;return!0},Ot=function(e){return e.indexOf(\"-\")>0},Dt=function(t){var n,o,a,i;xt(\"beforeSanitizeAttributes\",t,null);var l=t.attributes;if(l){var c={attrName:\"\",attrValue:\"\",keepAttr:!0,allowedAttributes:ke};for(i=l.length;i--;){var u=n=l[i],s=u.name,m=u.namespaceURI;if(o=\"value\"===s?n.value:x(n.value),a=ye(s),c.attrName=a,c.attrValue=o,c.keepAttr=!0,c.forceKeepAttr=void 0,xt(\"uponSanitizeAttribute\",t,c),o=c.attrValue,!c.forceKeepAttr&&(Nt(s,t),c.keepAttr))if(_(/\\/>/i,o))Nt(s,t);else{Fe&&(o=w(o,be,\" \"),o=w(o,ve,\" \"),o=w(o,Te,\" \"));var f=ye(t.nodeName);if(kt(f,a,o)){if(!We||\"id\"!==a&&\"name\"!==a||(Nt(s,t),o=qe+o),ie&&\"object\"===e(y)&&\"function\"==typeof y.getAttributeType)if(m);else switch(y.getAttributeType(f,a)){case\"TrustedHTML\":o=ie.createHTML(o);break;case\"TrustedScriptURL\":o=ie.createScriptURL(o)}try{m?t.setAttributeNS(m,s,o):t.setAttribute(s,o),v(r.removed)}catch(e){}}}}xt(\"afterSanitizeAttributes\",t,null)}},Lt=function e(t){var n,r=Et(t);for(xt(\"beforeSanitizeShadowDOM\",t,null);n=r.nextNode();)xt(\"uponSanitizeShadowNode\",n,null),_t(n)||(n.content instanceof l&&e(n.content),Dt(n));xt(\"afterSanitizeShadowDOM\",t,null)};return r.sanitize=function(t){var o,i,c,s,m,f=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if((at=!t)&&(t=\"\\x3c!--\\x3e\"),\"string\"!=typeof t&&!St(t)){if(\"function\"!=typeof t.toString)throw k(\"toString is not a function\");if(\"string\"!=typeof(t=t.toString()))throw k(\"dirty is not a string, aborting\")}if(!r.isSupported){if(\"object\"===e(n.toStaticHTML)||\"function\"==typeof n.toStaticHTML){if(\"string\"==typeof t)return n.toStaticHTML(t);if(St(t))return n.toStaticHTML(t.outerHTML)}return t}if(He||pt(f),r.removed=[],\"string\"==typeof t&&($e=!1),$e){if(t.nodeName){var p=ye(t.nodeName);if(!xe[p]||Le[p])throw k(\"root node is forbidden and cannot be sanitized in-place\")}}else if(t instanceof u)1===(i=(o=At(\"\\x3c!----\\x3e\")).ownerDocument.importNode(t,!0)).nodeType&&\"BODY\"===i.nodeName||\"HTML\"===i.nodeName?o=i:o.appendChild(i);else{if(!Pe&&!Fe&&!Ue&&-1===t.indexOf(\"<\"))return ie&&Be?ie.createHTML(t):t;if(!(o=At(t)))return Pe?null:Be?le:\"\"}o&&ze&&Tt(o.firstChild);for(var d=Et($e?t:o);c=d.nextNode();)3===c.nodeType&&c===s||_t(c)||(c.content instanceof l&&Lt(c.content),Dt(c),s=c);if(s=null,$e)return t;if(Pe){if(je)for(m=me.call(o.ownerDocument);o.firstChild;)m.appendChild(o.firstChild);else m=o;return ke.shadowroot&&(m=pe.call(a,m,!0)),m}var h=Ue?o.outerHTML:o.innerHTML;return Ue&&xe[\"!doctype\"]&&o.ownerDocument&&o.ownerDocument.doctype&&o.ownerDocument.doctype.name&&_(J,o.ownerDocument.doctype.name)&&(h=\"<!DOCTYPE \"+o.ownerDocument.doctype.name+\">\\n\"+h),Fe&&(h=w(h,be,\" \"),h=w(h,ve,\" \"),h=w(h,Te,\" \")),ie&&Be?ie.createHTML(h):h},r.setConfig=function(e){pt(e),He=!0},r.clearConfig=function(){st=null,He=!1},r.isValidAttribute=function(e,t,n){st||pt({});var r=ye(e),o=ye(t);return kt(r,o,n)},r.addHook=function(e,t){\"function\"==typeof t&&(he[e]=he[e]||[],T(he[e],t))},r.removeHook=function(e){if(he[e])return v(he[e])},r.removeHooks=function(e){he[e]&&(he[e]=[])},r.removeAllHooks=function(){he={}},r}();return te}));\n+/*! @license DOMPurify 2.4.2 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.2/LICENSE */\n+!function(e,t){\"object\"==typeof exports&&\"undefined\"!=typeof module?module.exports=t():\"function\"==typeof define&&define.amd?define(t):(e=\"undefined\"!=typeof globalThis?globalThis:e||self).DOMPurify=t()}(this,(function(){\"use strict\";function e(t){return e=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e},e(t)}function t(e,n){return t=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e},t(e,n)}function n(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}function r(e,o,a){return r=n()?Reflect.construct:function(e,n,r){var o=[null];o.push.apply(o,n);var a=new(Function.bind.apply(e,o));return r&&t(a,r.prototype),a},r.apply(null,arguments)}function o(e){return function(e){if(Array.isArray(e))return a(e)}(e)||function(e){if(\"undefined\"!=typeof Symbol&&null!=e[Symbol.iterator]||null!=e[\"@@iterator\"])return Array.from(e)}(e)||function(e,t){if(!e)return;if(\"string\"==typeof e)return a(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);\"Object\"===n&&e.constructor&&(n=e.constructor.name);if(\"Map\"===n||\"Set\"===n)return Array.from(e);if(\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return a(e,t)}(e)||function(){throw new TypeError(\"Invalid attempt to spread non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}()}function a(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=new Array(t);n<t;n++)r[n]=e[n];return r}var i=Object.hasOwnProperty,l=Object.setPrototypeOf,c=Object.isFrozen,u=Object.getPrototypeOf,s=Object.getOwnPropertyDescriptor,m=Object.freeze,f=Object.seal,p=Object.create,d=\"undefined\"!=typeof Reflect&&Reflect,h=d.apply,g=d.construct;h||(h=function(e,t,n){return e.apply(t,n)}),m||(m=function(e){return e}),f||(f=function(e){return e}),g||(g=function(e,t){return r(e,o(t))});var y,b=O(Array.prototype.forEach),v=O(Array.prototype.pop),T=O(Array.prototype.push),N=O(String.prototype.toLowerCase),A=O(String.prototype.toString),E=O(String.prototype.match),w=O(String.prototype.replace),S=O(String.prototype.indexOf),x=O(String.prototype.trim),_=O(RegExp.prototype.test),k=(y=TypeError,function(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return g(y,t)});function O(e){return function(t){for(var n=arguments.length,r=new Array(n>1?n-1:0),o=1;o<n;o++)r[o-1]=arguments[o];return h(e,t,r)}}function D(e,t,n){n=n||N,l&&l(e,null);for(var r=t.length;r--;){var o=t[r];if(\"string\"==typeof o){var a=n(o);a!==o&&(c(t)||(t[r]=a),o=a)}e[o]=!0}return e}function L(e){var t,n=p(null);for(t in e)!0===h(i,e,[t])&&(n[t]=e[t]);return n}function R(e,t){for(;null!==e;){var n=s(e,t);if(n){if(n.get)return O(n.get);if(\"function\"==typeof n.value)return O(n.value)}e=u(e)}return function(e){return console.warn(\"fallback value for\",e),null}}var M=m([\"a\",\"abbr\",\"acronym\",\"address\",\"area\",\"article\",\"aside\",\"audio\",\"b\",\"bdi\",\"bdo\",\"big\",\"blink\",\"blockquote\",\"body\",\"br\",\"button\",\"canvas\",\"caption\",\"center\",\"cite\",\"code\",\"col\",\"colgroup\",\"content\",\"data\",\"datalist\",\"dd\",\"decorator\",\"del\",\"details\",\"dfn\",\"dialog\",\"dir\",\"div\",\"dl\",\"dt\",\"element\",\"em\",\"fieldset\",\"figcaption\",\"figure\",\"font\",\"footer\",\"form\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"head\",\"header\",\"hgroup\",\"hr\",\"html\",\"i\",\"img\",\"input\",\"ins\",\"kbd\",\"label\",\"legend\",\"li\",\"main\",\"map\",\"mark\",\"marquee\",\"menu\",\"menuitem\",\"meter\",\"nav\",\"nobr\",\"ol\",\"optgroup\",\"option\",\"output\",\"p\",\"picture\",\"pre\",\"progress\",\"q\",\"rp\",\"rt\",\"ruby\",\"s\",\"samp\",\"section\",\"select\",\"shadow\",\"small\",\"source\",\"spacer\",\"span\",\"strike\",\"strong\",\"style\",\"sub\",\"summary\",\"sup\",\"table\",\"tbody\",\"td\",\"template\",\"textarea\",\"tfoot\",\"th\",\"thead\",\"time\",\"tr\",\"track\",\"tt\",\"u\",\"ul\",\"var\",\"video\",\"wbr\"]),C=m([\"svg\",\"a\",\"altglyph\",\"altglyphdef\",\"altglyphitem\",\"animatecolor\",\"animatemotion\",\"animatetransform\",\"circle\",\"clippath\",\"defs\",\"desc\",\"ellipse\",\"filter\",\"font\",\"g\",\"glyph\",\"glyphref\",\"hkern\",\"image\",\"line\",\"lineargradient\",\"marker\",\"mask\",\"metadata\",\"mpath\",\"path\",\"pattern\",\"polygon\",\"polyline\",\"radialgradient\",\"rect\",\"stop\",\"style\",\"switch\",\"symbol\",\"text\",\"textpath\",\"title\",\"tref\",\"tspan\",\"view\",\"vkern\"]),I=m([\"feBlend\",\"feColorMatrix\",\"feComponentTransfer\",\"feComposite\",\"feConvolveMatrix\",\"feDiffuseLighting\",\"feDisplacementMap\",\"feDistantLight\",\"feFlood\",\"feFuncA\",\"feFuncB\",\"feFuncG\",\"feFuncR\",\"feGaussianBlur\",\"feImage\",\"feMerge\",\"feMergeNode\",\"feMorphology\",\"feOffset\",\"fePointLight\",\"feSpecularLighting\",\"feSpotLight\",\"feTile\",\"feTurbulence\"]),F=m([\"animate\",\"color-profile\",\"cursor\",\"discard\",\"fedropshadow\",\"font-face\",\"font-face-format\",\"font-face-name\",\"font-face-src\",\"font-face-uri\",\"foreignobject\",\"hatch\",\"hatchpath\",\"mesh\",\"meshgradient\",\"meshpatch\",\"meshrow\",\"missing-glyph\",\"script\",\"set\",\"solidcolor\",\"unknown\",\"use\"]),U=m([\"math\",\"menclose\",\"merror\",\"mfenced\",\"mfrac\",\"mglyph\",\"mi\",\"mlabeledtr\",\"mmultiscripts\",\"mn\",\"mo\",\"mover\",\"mpadded\",\"mphantom\",\"mroot\",\"mrow\",\"ms\",\"mspace\",\"msqrt\",\"mstyle\",\"msub\",\"msup\",\"msubsup\",\"mtable\",\"mtd\",\"mtext\",\"mtr\",\"munder\",\"munderover\"]),H=m([\"maction\",\"maligngroup\",\"malignmark\",\"mlongdiv\",\"mscarries\",\"mscarry\",\"msgroup\",\"mstack\",\"msline\",\"msrow\",\"semantics\",\"annotation\",\"annotation-xml\",\"mprescripts\",\"none\"]),z=m([\"#text\"]),P=m([\"accept\",\"action\",\"align\",\"alt\",\"autocapitalize\",\"autocomplete\",\"autopictureinpicture\",\"autoplay\",\"background\",\"bgcolor\",\"border\",\"capture\",\"cellpadding\",\"cellspacing\",\"checked\",\"cite\",\"class\",\"clear\",\"color\",\"cols\",\"colspan\",\"controls\",\"controlslist\",\"coords\",\"crossorigin\",\"datetime\",\"decoding\",\"default\",\"dir\",\"disabled\",\"disablepictureinpicture\",\"disableremoteplayback\",\"download\",\"draggable\",\"enctype\",\"enterkeyhint\",\"face\",\"for\",\"headers\",\"height\",\"hidden\",\"high\",\"href\",\"hreflang\",\"id\",\"inputmode\",\"integrity\",\"ismap\",\"kind\",\"label\",\"lang\",\"list\",\"loading\",\"loop\",\"low\",\"max\",\"maxlength\",\"media\",\"method\",\"min\",\"minlength\",\"multiple\",\"muted\",\"name\",\"nonce\",\"noshade\",\"novalidate\",\"nowrap\",\"open\",\"optimum\",\"pattern\",\"placeholder\",\"playsinline\",\"poster\",\"preload\",\"pubdate\",\"radiogroup\",\"readonly\",\"rel\",\"required\",\"rev\",\"reversed\",\"role\",\"rows\",\"rowspan\",\"spellcheck\",\"scope\",\"selected\",\"shape\",\"size\",\"sizes\",\"span\",\"srclang\",\"start\",\"src\",\"srcset\",\"step\",\"style\",\"summary\",\"tabindex\",\"title\",\"translate\",\"type\",\"usemap\",\"valign\",\"value\",\"width\",\"xmlns\",\"slot\"]),j=m([\"accent-height\",\"accumulate\",\"additive\",\"alignment-baseline\",\"ascent\",\"attributename\",\"attributetype\",\"azimuth\",\"basefrequency\",\"baseline-shift\",\"begin\",\"bias\",\"by\",\"class\",\"clip\",\"clippathunits\",\"clip-path\",\"clip-rule\",\"color\",\"color-interpolation\",\"color-interpolation-filters\",\"color-profile\",\"color-rendering\",\"cx\",\"cy\",\"d\",\"dx\",\"dy\",\"diffuseconstant\",\"direction\",\"display\",\"divisor\",\"dur\",\"edgemode\",\"elevation\",\"end\",\"fill\",\"fill-opacity\",\"fill-rule\",\"filter\",\"filterunits\",\"flood-color\",\"flood-opacity\",\"font-family\",\"font-size\",\"font-size-adjust\",\"font-stretch\",\"font-style\",\"font-variant\",\"font-weight\",\"fx\",\"fy\",\"g1\",\"g2\",\"glyph-name\",\"glyphref\",\"gradientunits\",\"gradienttransform\",\"height\",\"href\",\"id\",\"image-rendering\",\"in\",\"in2\",\"k\",\"k1\",\"k2\",\"k3\",\"k4\",\"kerning\",\"keypoints\",\"keysplines\",\"keytimes\",\"lang\",\"lengthadjust\",\"letter-spacing\",\"kernelmatrix\",\"kernelunitlength\",\"lighting-color\",\"local\",\"marker-end\",\"marker-mid\",\"marker-start\",\"markerheight\",\"markerunits\",\"markerwidth\",\"maskcontentunits\",\"maskunits\",\"max\",\"mask\",\"media\",\"method\",\"mode\",\"min\",\"name\",\"numoctaves\",\"offset\",\"operator\",\"opacity\",\"order\",\"orient\",\"orientation\",\"origin\",\"overflow\",\"paint-order\",\"path\",\"pathlength\",\"patterncontentunits\",\"patterntransform\",\"patternunits\",\"points\",\"preservealpha\",\"preserveaspectratio\",\"primitiveunits\",\"r\",\"rx\",\"ry\",\"radius\",\"refx\",\"refy\",\"repeatcount\",\"repeatdur\",\"restart\",\"result\",\"rotate\",\"scale\",\"seed\",\"shape-rendering\",\"specularconstant\",\"specularexponent\",\"spreadmethod\",\"startoffset\",\"stddeviation\",\"stitchtiles\",\"stop-color\",\"stop-opacity\",\"stroke-dasharray\",\"stroke-dashoffset\",\"stroke-linecap\",\"stroke-linejoin\",\"stroke-miterlimit\",\"stroke-opacity\",\"stroke\",\"stroke-width\",\"style\",\"surfacescale\",\"systemlanguage\",\"tabindex\",\"targetx\",\"targety\",\"transform\",\"transform-origin\",\"text-anchor\",\"text-decoration\",\"text-rendering\",\"textlength\",\"type\",\"u1\",\"u2\",\"unicode\",\"values\",\"viewbox\",\"visibility\",\"version\",\"vert-adv-y\",\"vert-origin-x\",\"vert-origin-y\",\"width\",\"word-spacing\",\"wrap\",\"writing-mode\",\"xchannelselector\",\"ychannelselector\",\"x\",\"x1\",\"x2\",\"xmlns\",\"y\",\"y1\",\"y2\",\"z\",\"zoomandpan\"]),B=m([\"accent\",\"accentunder\",\"align\",\"bevelled\",\"close\",\"columnsalign\",\"columnlines\",\"columnspan\",\"denomalign\",\"depth\",\"dir\",\"display\",\"displaystyle\",\"encoding\",\"fence\",\"frame\",\"height\",\"href\",\"id\",\"largeop\",\"length\",\"linethickness\",\"lspace\",\"lquote\",\"mathbackground\",\"mathcolor\",\"mathsize\",\"mathvariant\",\"maxsize\",\"minsize\",\"movablelimits\",\"notation\",\"numalign\",\"open\",\"rowalign\",\"rowlines\",\"rowspacing\",\"rowspan\",\"rspace\",\"rquote\",\"scriptlevel\",\"scriptminsize\",\"scriptsizemultiplier\",\"selection\",\"separator\",\"separators\",\"stretchy\",\"subscriptshift\",\"supscriptshift\",\"symmetric\",\"voffset\",\"width\",\"xmlns\"]),G=m([\"xlink:href\",\"xml:id\",\"xlink:title\",\"xml:space\",\"xmlns:xlink\"]),W=f(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm),q=f(/<%[\\w\\W]*|[\\w\\W]*%>/gm),Y=f(/\\${[\\w\\W]*}/gm),$=f(/^data-[\\-\\w.\\u00B7-\\uFFFF]/),K=f(/^aria-[\\-\\w]+$/),V=f(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i),X=f(/^(?:\\w+script|data):/i),Z=f(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g),J=f(/^html$/i),Q=function(){return\"undefined\"==typeof window?null:window},ee=function(t,n){if(\"object\"!==e(t)||\"function\"!=typeof t.createPolicy)return null;var r=null,o=\"data-tt-policy-suffix\";n.currentScript&&n.currentScript.hasAttribute(o)&&(r=n.currentScript.getAttribute(o));var a=\"dompurify\"+(r?\"#\"+r:\"\");try{return t.createPolicy(a,{createHTML:function(e){return e},createScriptURL:function(e){return e}})}catch(e){return console.warn(\"TrustedTypes policy \"+a+\" could not be created.\"),null}};var te=function t(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Q(),r=function(e){return t(e)};if(r.version=\"2.4.2\",r.removed=[],!n||!n.document||9!==n.document.nodeType)return r.isSupported=!1,r;var a=n.document,i=n.document,l=n.DocumentFragment,c=n.HTMLTemplateElement,u=n.Node,s=n.Element,f=n.NodeFilter,p=n.NamedNodeMap,d=void 0===p?n.NamedNodeMap||n.MozNamedAttrMap:p,h=n.HTMLFormElement,g=n.DOMParser,y=n.trustedTypes,O=s.prototype,te=R(O,\"cloneNode\"),ne=R(O,\"nextSibling\"),re=R(O,\"childNodes\"),oe=R(O,\"parentNode\");if(\"function\"==typeof c){var ae=i.createElement(\"template\");ae.content&&ae.content.ownerDocument&&(i=ae.content.ownerDocument)}var ie=ee(y,a),le=ie?ie.createHTML(\"\"):\"\",ce=i,ue=ce.implementation,se=ce.createNodeIterator,me=ce.createDocumentFragment,fe=ce.getElementsByTagName,pe=a.importNode,de={};try{de=L(i).documentMode?i.documentMode:{}}catch(e){}var he={};r.isSupported=\"function\"==typeof oe&&ue&&void 0!==ue.createHTMLDocument&&9!==de;var ge,ye,be=W,ve=q,Te=Y,Ne=$,Ae=K,Ee=X,we=Z,Se=V,xe=null,_e=D({},[].concat(o(M),o(C),o(I),o(U),o(z))),ke=null,Oe=D({},[].concat(o(P),o(j),o(B),o(G))),De=Object.seal(Object.create(null,{tagNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},attributeNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},allowCustomizedBuiltInElements:{writable:!0,configurable:!1,enumerable:!0,value:!1}})),Le=null,Re=null,Me=!0,Ce=!0,Ie=!1,Fe=!1,Ue=!1,He=!1,ze=!1,Pe=!1,je=!1,Be=!1,Ge=!0,We=!1,qe=\"user-content-\",Ye=!0,$e=!1,Ke={},Ve=null,Xe=D({},[\"annotation-xml\",\"audio\",\"colgroup\",\"desc\",\"foreignobject\",\"head\",\"iframe\",\"math\",\"mi\",\"mn\",\"mo\",\"ms\",\"mtext\",\"noembed\",\"noframes\",\"noscript\",\"plaintext\",\"script\",\"style\",\"svg\",\"template\",\"thead\",\"title\",\"video\",\"xmp\"]),Ze=null,Je=D({},[\"audio\",\"video\",\"img\",\"source\",\"image\",\"track\"]),Qe=null,et=D({},[\"alt\",\"class\",\"for\",\"id\",\"label\",\"name\",\"pattern\",\"placeholder\",\"role\",\"summary\",\"title\",\"value\",\"style\",\"xmlns\"]),tt=\"http://www.w3.org/1998/Math/MathML\",nt=\"http://www.w3.org/2000/svg\",rt=\"http://www.w3.org/1999/xhtml\",ot=rt,at=!1,it=null,lt=D({},[tt,nt,rt],A),ct=[\"application/xhtml+xml\",\"text/html\"],ut=\"text/html\",st=null,mt=i.createElement(\"form\"),ft=function(e){return e instanceof RegExp||e instanceof Function},pt=function(t){st&&st===t||(t&&\"object\"===e(t)||(t={}),t=L(t),ge=ge=-1===ct.indexOf(t.PARSER_MEDIA_TYPE)?ut:t.PARSER_MEDIA_TYPE,ye=\"application/xhtml+xml\"===ge?A:N,xe=\"ALLOWED_TAGS\"in t?D({},t.ALLOWED_TAGS,ye):_e,ke=\"ALLOWED_ATTR\"in t?D({},t.ALLOWED_ATTR,ye):Oe,it=\"ALLOWED_NAMESPACES\"in t?D({},t.ALLOWED_NAMESPACES,A):lt,Qe=\"ADD_URI_SAFE_ATTR\"in t?D(L(et),t.ADD_URI_SAFE_ATTR,ye):et,Ze=\"ADD_DATA_URI_TAGS\"in t?D(L(Je),t.ADD_DATA_URI_TAGS,ye):Je,Ve=\"FORBID_CONTENTS\"in t?D({},t.FORBID_CONTENTS,ye):Xe,Le=\"FORBID_TAGS\"in t?D({},t.FORBID_TAGS,ye):{},Re=\"FORBID_ATTR\"in t?D({},t.FORBID_ATTR,ye):{},Ke=\"USE_PROFILES\"in t&&t.USE_PROFILES,Me=!1!==t.ALLOW_ARIA_ATTR,Ce=!1!==t.ALLOW_DATA_ATTR,Ie=t.ALLOW_UNKNOWN_PROTOCOLS||!1,Fe=t.SAFE_FOR_TEMPLATES||!1,Ue=t.WHOLE_DOCUMENT||!1,Pe=t.RETURN_DOM||!1,je=t.RETURN_DOM_FRAGMENT||!1,Be=t.RETURN_TRUSTED_TYPE||!1,ze=t.FORCE_BODY||!1,Ge=!1!==t.SANITIZE_DOM,We=t.SANITIZE_NAMED_PROPS||!1,Ye=!1!==t.KEEP_CONTENT,$e=t.IN_PLACE||!1,Se=t.ALLOWED_URI_REGEXP||Se,ot=t.NAMESPACE||rt,t.CUSTOM_ELEMENT_HANDLING&&ft(t.CUSTOM_ELEMENT_HANDLING.tagNameCheck)&&(De.tagNameCheck=t.CUSTOM_ELEMENT_HANDLING.tagNameCheck),t.CUSTOM_ELEMENT_HANDLING&&ft(t.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)&&(De.attributeNameCheck=t.CUSTOM_ELEMENT_HANDLING.attributeNameCheck),t.CUSTOM_ELEMENT_HANDLING&&\"boolean\"==typeof t.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements&&(De.allowCustomizedBuiltInElements=t.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements),Fe&&(Ce=!1),je&&(Pe=!0),Ke&&(xe=D({},o(z)),ke=[],!0===Ke.html&&(D(xe,M),D(ke,P)),!0===Ke.svg&&(D(xe,C),D(ke,j),D(ke,G)),!0===Ke.svgFilters&&(D(xe,I),D(ke,j),D(ke,G)),!0===Ke.mathMl&&(D(xe,U),D(ke,B),D(ke,G))),t.ADD_TAGS&&(xe===_e&&(xe=L(xe)),D(xe,t.ADD_TAGS,ye)),t.ADD_ATTR&&(ke===Oe&&(ke=L(ke)),D(ke,t.ADD_ATTR,ye)),t.ADD_URI_SAFE_ATTR&&D(Qe,t.ADD_URI_SAFE_ATTR,ye),t.FORBID_CONTENTS&&(Ve===Xe&&(Ve=L(Ve)),D(Ve,t.FORBID_CONTENTS,ye)),Ye&&(xe[\"#text\"]=!0),Ue&&D(xe,[\"html\",\"head\",\"body\"]),xe.table&&(D(xe,[\"tbody\"]),delete Le.tbody),m&&m(t),st=t)},dt=D({},[\"mi\",\"mo\",\"mn\",\"ms\",\"mtext\"]),ht=D({},[\"foreignobject\",\"desc\",\"title\",\"annotation-xml\"]),gt=D({},[\"title\",\"style\",\"font\",\"a\",\"script\"]),yt=D({},C);D(yt,I),D(yt,F);var bt=D({},U);D(bt,H);var vt=function(e){var t=oe(e);t&&t.tagName||(t={namespaceURI:ot,tagName:\"template\"});var n=N(e.tagName),r=N(t.tagName);return!!it[e.namespaceURI]&&(e.namespaceURI===nt?t.namespaceURI===rt?\"svg\"===n:t.namespaceURI===tt?\"svg\"===n&&(\"annotation-xml\"===r||dt[r]):Boolean(yt[n]):e.namespaceURI===tt?t.namespaceURI===rt?\"math\"===n:t.namespaceURI===nt?\"math\"===n&&ht[r]:Boolean(bt[n]):e.namespaceURI===rt?!(t.namespaceURI===nt&&!ht[r])&&(!(t.namespaceURI===tt&&!dt[r])&&(!bt[n]&&(gt[n]||!yt[n]))):!(\"application/xhtml+xml\"!==ge||!it[e.namespaceURI]))},Tt=function(e){T(r.removed,{element:e});try{e.parentNode.removeChild(e)}catch(t){try{e.outerHTML=le}catch(t){e.remove()}}},Nt=function(e,t){try{T(r.removed,{attribute:t.getAttributeNode(e),from:t})}catch(e){T(r.removed,{attribute:null,from:t})}if(t.removeAttribute(e),\"is\"===e&&!ke[e])if(Pe||je)try{Tt(t)}catch(e){}else try{t.setAttribute(e,\"\")}catch(e){}},At=function(e){var t,n;if(ze)e=\"<remove></remove>\"+e;else{var r=E(e,/^[\\r\\n\\t ]+/);n=r&&r[0]}\"application/xhtml+xml\"===ge&&ot===rt&&(e='<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>'+e+\"</body></html>\");var o=ie?ie.createHTML(e):e;if(ot===rt)try{t=(new g).parseFromString(o,ge)}catch(e){}if(!t||!t.documentElement){t=ue.createDocument(ot,\"template\",null);try{t.documentElement.innerHTML=at?le:o}catch(e){}}var a=t.body||t.documentElement;return e&&n&&a.insertBefore(i.createTextNode(n),a.childNodes[0]||null),ot===rt?fe.call(t,Ue?\"html\":\"body\")[0]:Ue?t.documentElement:a},Et=function(e){return se.call(e.ownerDocument||e,e,f.SHOW_ELEMENT|f.SHOW_COMMENT|f.SHOW_TEXT,null,!1)},wt=function(e){return e instanceof h&&(\"string\"!=typeof e.nodeName||\"string\"!=typeof e.textContent||\"function\"!=typeof e.removeChild||!(e.attributes instanceof d)||\"function\"!=typeof e.removeAttribute||\"function\"!=typeof e.setAttribute||\"string\"!=typeof e.namespaceURI||\"function\"!=typeof e.insertBefore||\"function\"!=typeof e.hasChildNodes)},St=function(t){return\"object\"===e(u)?t instanceof u:t&&\"object\"===e(t)&&\"number\"==typeof t.nodeType&&\"string\"==typeof t.nodeName},xt=function(e,t,n){he[e]&&b(he[e],(function(e){e.call(r,t,n,st)}))},_t=function(e){var t;if(xt(\"beforeSanitizeElements\",e,null),wt(e))return Tt(e),!0;if(_(/[\\u0080-\\uFFFF]/,e.nodeName))return Tt(e),!0;var n=ye(e.nodeName);if(xt(\"uponSanitizeElement\",e,{tagName:n,allowedTags:xe}),e.hasChildNodes()&&!St(e.firstElementChild)&&(!St(e.content)||!St(e.content.firstElementChild))&&_(/<[/\\w]/g,e.innerHTML)&&_(/<[/\\w]/g,e.textContent))return Tt(e),!0;if(\"select\"===n&&_(/<template/i,e.innerHTML))return Tt(e),!0;if(!xe[n]||Le[n]){if(!Le[n]&&Ot(n)){if(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,n))return!1;if(De.tagNameCheck instanceof Function&&De.tagNameCheck(n))return!1}if(Ye&&!Ve[n]){var o=oe(e)||e.parentNode,a=re(e)||e.childNodes;if(a&&o)for(var i=a.length-1;i>=0;--i)o.insertBefore(te(a[i],!0),ne(e))}return Tt(e),!0}return e instanceof s&&!vt(e)?(Tt(e),!0):\"noscript\"!==n&&\"noembed\"!==n||!_(/<\\/no(script|embed)/i,e.innerHTML)?(Fe&&3===e.nodeType&&(t=e.textContent,t=w(t,be,\" \"),t=w(t,ve,\" \"),t=w(t,Te,\" \"),e.textContent!==t&&(T(r.removed,{element:e.cloneNode()}),e.textContent=t)),xt(\"afterSanitizeElements\",e,null),!1):(Tt(e),!0)},kt=function(e,t,n){if(Ge&&(\"id\"===t||\"name\"===t)&&(n in i||n in mt))return!1;if(Ce&&!Re[t]&&_(Ne,t));else if(Me&&_(Ae,t));else if(!ke[t]||Re[t]){if(!(Ot(e)&&(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,e)||De.tagNameCheck instanceof Function&&De.tagNameCheck(e))&&(De.attributeNameCheck instanceof RegExp&&_(De.attributeNameCheck,t)||De.attributeNameCheck instanceof Function&&De.attributeNameCheck(t))||\"is\"===t&&De.allowCustomizedBuiltInElements&&(De.tagNameCheck instanceof RegExp&&_(De.tagNameCheck,n)||De.tagNameCheck instanceof Function&&De.tagNameCheck(n))))return!1}else if(Qe[t]);else if(_(Se,w(n,we,\"\")));else if(\"src\"!==t&&\"xlink:href\"!==t&&\"href\"!==t||\"script\"===e||0!==S(n,\"data:\")||!Ze[e]){if(Ie&&!_(Ee,w(n,we,\"\")));else if(n)return!1}else;return!0},Ot=function(e){return e.indexOf(\"-\")>0},Dt=function(t){var n,o,a,i;xt(\"beforeSanitizeAttributes\",t,null);var l=t.attributes;if(l){var c={attrName:\"\",attrValue:\"\",keepAttr:!0,allowedAttributes:ke};for(i=l.length;i--;){var u=n=l[i],s=u.name,m=u.namespaceURI;if(o=\"value\"===s?n.value:x(n.value),a=ye(s),c.attrName=a,c.attrValue=o,c.keepAttr=!0,c.forceKeepAttr=void 0,xt(\"uponSanitizeAttribute\",t,c),o=c.attrValue,!c.forceKeepAttr&&(Nt(s,t),c.keepAttr))if(_(/\\/>/i,o))Nt(s,t);else{Fe&&(o=w(o,be,\" \"),o=w(o,ve,\" \"),o=w(o,Te,\" \"));var f=ye(t.nodeName);if(kt(f,a,o)){if(!We||\"id\"!==a&&\"name\"!==a||(Nt(s,t),o=qe+o),ie&&\"object\"===e(y)&&\"function\"==typeof y.getAttributeType)if(m);else switch(y.getAttributeType(f,a)){case\"TrustedHTML\":o=ie.createHTML(o);break;case\"TrustedScriptURL\":o=ie.createScriptURL(o)}try{m?t.setAttributeNS(m,s,o):t.setAttribute(s,o),v(r.removed)}catch(e){}}}}xt(\"afterSanitizeAttributes\",t,null)}},Lt=function e(t){var n,r=Et(t);for(xt(\"beforeSanitizeShadowDOM\",t,null);n=r.nextNode();)xt(\"uponSanitizeShadowNode\",n,null),_t(n)||(n.content instanceof l&&e(n.content),Dt(n));xt(\"afterSanitizeShadowDOM\",t,null)};return r.sanitize=function(t){var o,i,c,s,m,f=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if((at=!t)&&(t=\"\\x3c!--\\x3e\"),\"string\"!=typeof t&&!St(t)){if(\"function\"!=typeof t.toString)throw k(\"toString is not a function\");if(\"string\"!=typeof(t=t.toString()))throw k(\"dirty is not a string, aborting\")}if(!r.isSupported){if(\"object\"===e(n.toStaticHTML)||\"function\"==typeof n.toStaticHTML){if(\"string\"==typeof t)return n.toStaticHTML(t);if(St(t))return n.toStaticHTML(t.outerHTML)}return t}if(He||pt(f),r.removed=[],\"string\"==typeof t&&($e=!1),$e){if(t.nodeName){var p=ye(t.nodeName);if(!xe[p]||Le[p])throw k(\"root node is forbidden and cannot be sanitized in-place\")}}else if(t instanceof u)1===(i=(o=At(\"\\x3c!----\\x3e\")).ownerDocument.importNode(t,!0)).nodeType&&\"BODY\"===i.nodeName||\"HTML\"===i.nodeName?o=i:o.appendChild(i);else{if(!Pe&&!Fe&&!Ue&&-1===t.indexOf(\"<\"))return ie&&Be?ie.createHTML(t):t;if(!(o=At(t)))return Pe?null:Be?le:\"\"}o&&ze&&Tt(o.firstChild);for(var d=Et($e?t:o);c=d.nextNode();)3===c.nodeType&&c===s||_t(c)||(c.content instanceof l&&Lt(c.content),Dt(c),s=c);if(s=null,$e)return t;if(Pe){if(je)for(m=me.call(o.ownerDocument);o.firstChild;)m.appendChild(o.firstChild);else m=o;return ke.shadowroot&&(m=pe.call(a,m,!0)),m}var h=Ue?o.outerHTML:o.innerHTML;return Ue&&xe[\"!doctype\"]&&o.ownerDocument&&o.ownerDocument.doctype&&o.ownerDocument.doctype.name&&_(J,o.ownerDocument.doctype.name)&&(h=\"<!DOCTYPE \"+o.ownerDocument.doctype.name+\">\\n\"+h),Fe&&(h=w(h,be,\" \"),h=w(h,ve,\" \"),h=w(h,Te,\" \")),ie&&Be?ie.createHTML(h):h},r.setConfig=function(e){pt(e),He=!0},r.clearConfig=function(){st=null,He=!1},r.isValidAttribute=function(e,t,n){st||pt({});var r=ye(e),o=ye(t);return kt(r,o,n)},r.addHook=function(e,t){\"function\"==typeof t&&(he[e]=he[e]||[],T(he[e],t))},r.removeHook=function(e){if(he[e])return v(he[e])},r.removeHooks=function(e){he[e]&&(he[e]=[])},r.removeAllHooks=function(){he={}},r}();return te}));\n //# sourceMappingURL=purify.min.js.map",
          "package-lock.json": "@@ -1,12 +1,12 @@\n {\n   \"name\": \"dompurify\",\n-  \"version\": \"2.4.0\",\n+  \"version\": \"2.4.1\",\n   \"lockfileVersion\": 2,\n   \"requires\": true,\n   \"packages\": {\n     \"\": {\n       \"name\": \"dompurify\",\n-      \"version\": \"2.4.0\",\n+      \"version\": \"2.4.1\",\n       \"license\": \"(MPL-2.0 OR Apache-2.0)\",\n       \"devDependencies\": {\n         \"@babel/core\": \"^7.17.8\",\n@@ -2140,13 +2140,11 @@\n       \"integrity\": \"sha512-1fMXF3YP4pZZVozF8j/ZLfvnR8NSIljt56UhbZ5PeeDmmGHpgpdwQt7ITlGvYaQukCvuBRMLEiKiYC+oeIg4cg==\",\n       \"dev\": true\n     },\n-    \"node_modules/@socket.io/base64-arraybuffer\": {\n-      \"version\": \"1.0.2\",\n-      \"dev\": true,\n-      \"license\": \"MIT\",\n-      \"engines\": {\n-        \"node\": \">= 0.6.0\"\n-      }\n+    \"node_modules/@socket.io/component-emitter\": {\n+      \"version\": \"3.1.0\",\n+      \"resolved\": \"https://registry.npmjs.org/@socket.io/component-emitter/-/component-emitter-3.1.0.tgz\",\n+      \"integrity\": \"sha512-+9jVqKhRSpsc591z5vX+X5Yyw+he/HCB4iQ/RYxw35CEPaY1gnsNE43nf9n9AaYjAQrTiI/mOwKUKdUs9vf7Xg==\",\n+      \"dev\": true\n     },\n     \"node_modules/@tootallnate/once\": {\n       \"version\": \"2.0.0\",\n@@ -2157,20 +2155,17 @@\n         \"node\": \">= 10\"\n       }\n     },\n-    \"node_modules/@types/component-emitter\": {\n-      \"version\": \"1.2.11\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n-    },\n     \"node_modules/@types/cookie\": {\n       \"version\": \"0.4.1\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n+      \"resolved\": \"https://registry.npmjs.org/@types/cookie/-/cookie-0.4.1.tgz\",\n+      \"integrity\": \"sha512-XW/Aa8APYr6jSVVA1y/DEIZX0/GMKLEVekNG727R8cs56ahETkRAy/3DR7+fJyh7oUgGwNQaRfXCun0+KbWY7Q==\",\n+      \"dev\": true\n     },\n     \"node_modules/@types/cors\": {\n       \"version\": \"2.8.12\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n+      \"resolved\": \"https://registry.npmjs.org/@types/cors/-/cors-2.8.12.tgz\",\n+      \"integrity\": \"sha512-vt+kDhq/M2ayberEtJcIN/hxXy1Pk+59g2FV/ZQceeaTyCtCucjL2Q7FXlFjtWn4n15KCr1NE2lNNFhp0lEThw==\",\n+      \"dev\": true\n     },\n     \"node_modules/@types/dompurify\": {\n       \"version\": \"2.3.3\",\n@@ -2418,8 +2413,9 @@\n     },\n     \"node_modules/accepts\": {\n       \"version\": \"1.3.8\",\n+      \"resolved\": \"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\",\n+      \"integrity\": \"sha512-PYAthTa2m2VKxuvSD3DPC/Gy+U+sOA1LAuT8mkmRuvw+NACSaeXEQ+NHcVF7rONl6qcaxV3Uuemwawk+7+SJLw==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"mime-types\": \"~2.1.34\",\n         \"negotiator\": \"0.6.3\"\n@@ -2650,8 +2646,9 @@\n     },\n     \"node_modules/base64id\": {\n       \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/base64id/-/base64id-2.0.0.tgz\",\n+      \"integrity\": \"sha512-lGe34o6EHj9y3Kts9R4ZYs/Gr+6N7MCaMlIFA3F1R2O5/m7K06AxfSeO5530PEERE6/WyEg3lsuyw4GHlPZHog==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \"^4.5.0 || >= 5.9\"\n       }\n@@ -2665,31 +2662,39 @@\n       }\n     },\n     \"node_modules/body-parser\": {\n-      \"version\": \"1.19.0\",\n+      \"version\": \"1.20.1\",\n+      \"resolved\": \"https://registry.npmjs.org/body-parser/-/body-parser-1.20.1.tgz\",\n+      \"integrity\": \"sha512-jWi7abTbYwajOytWCQc37VulmWiRae5RyTpaCyDcS5/lMdtwSz5lOpDE67srw/HYe35f1z3fDQw+3txg7gNtWw==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n-        \"bytes\": \"3.1.0\",\n+        \"bytes\": \"3.1.2\",\n         \"content-type\": \"~1.0.4\",\n         \"debug\": \"2.6.9\",\n-        \"depd\": \"~1.1.2\",\n-        \"http-errors\": \"1.7.2\",\n+        \"depd\": \"2.0.0\",\n+        \"destroy\": \"1.2.0\",\n+        \"http-errors\": \"2.0.0\",\n         \"iconv-lite\": \"0.4.24\",\n-        \"on-finished\": \"~2.3.0\",\n-        \"qs\": \"6.7.0\",\n-        \"raw-body\": \"2.4.0\",\n-        \"type-is\": \"~1.6.17\"\n+        \"on-finished\": \"2.4.1\",\n+        \"qs\": \"6.11.0\",\n+        \"raw-body\": \"2.5.1\",\n+        \"type-is\": \"~1.6.18\",\n+        \"unpipe\": \"1.0.0\"\n       },\n       \"engines\": {\n-        \"node\": \">= 0.8\"\n+        \"node\": \">= 0.8\",\n+        \"npm\": \"1.2.8000 || >= 1.4.16\"\n       }\n     },\n-    \"node_modules/body-parser/node_modules/qs\": {\n-      \"version\": \"6.7.0\",\n+    \"node_modules/body-parser/node_modules/on-finished\": {\n+      \"version\": \"2.4.1\",\n+      \"resolved\": \"https://registry.npmjs.org/on-finished/-/on-finished-2.4.1.tgz\",\n+      \"integrity\": \"sha512-oVlzkg3ENAhCk2zdv7IJwd/QUD4z2RxRwpkcGY8psCVcCYZNq4wYnVWALHM+brtuJjePWiYF/ClmuDr8Ch5+kg==\",\n       \"dev\": true,\n-      \"license\": \"BSD-3-Clause\",\n+      \"dependencies\": {\n+        \"ee-first\": \"1.1.1\"\n+      },\n       \"engines\": {\n-        \"node\": \">=0.6\"\n+        \"node\": \">= 0.8\"\n       }\n     },\n     \"node_modules/brace-expansion\": {\n@@ -2824,9 +2829,10 @@\n       }\n     },\n     \"node_modules/bytes\": {\n-      \"version\": \"3.1.0\",\n+      \"version\": \"3.1.2\",\n+      \"resolved\": \"https://registry.npmjs.org/bytes/-/bytes-3.1.2.tgz\",\n+      \"integrity\": \"sha512-/Nf7TyzTx6S3yRJObOAV7956r8cr2+Oj8AC5dt8wSP3BQAoeX58NoHyCU8P8zGkNXStjTSi6fzO6F0pBdcYbEg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">= 0.8\"\n       }\n@@ -3019,11 +3025,6 @@\n       \"dev\": true,\n       \"license\": \"MIT\"\n     },\n-    \"node_modules/component-emitter\": {\n-      \"version\": \"1.3.0\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n-    },\n     \"node_modules/concat-map\": {\n       \"version\": \"0.0.1\",\n       \"dev\": true,\n@@ -3080,8 +3081,9 @@\n     },\n     \"node_modules/cookie\": {\n       \"version\": \"0.4.2\",\n+      \"resolved\": \"https://registry.npmjs.org/cookie/-/cookie-0.4.2.tgz\",\n+      \"integrity\": \"sha512-aSWTXFzaKWkvHO1Ny/s+ePFpvKsPnjc551iI41v3ny/ow6tBG5Vd+FuqGNhh1LxOmVzOlGUriIlOaokOvhaStA==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">= 0.6\"\n       }\n@@ -3116,8 +3118,9 @@\n     },\n     \"node_modules/cors\": {\n       \"version\": \"2.8.5\",\n+      \"resolved\": \"https://registry.npmjs.org/cors/-/cors-2.8.5.tgz\",\n+      \"integrity\": \"sha512-KIHbLJqu73RGr/hnbrO9uBeixNGuvSQjul/jdFvS/KFSIH1hWVd1ng7zOHx+YrEfInLG7q4n6GHQ9cDtxv/P6g==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"object-assign\": \"^4\",\n         \"vary\": \"^1\"\n@@ -3350,11 +3353,22 @@\n       }\n     },\n     \"node_modules/depd\": {\n-      \"version\": \"1.1.2\",\n+      \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/depd/-/depd-2.0.0.tgz\",\n+      \"integrity\": \"sha512-g7nH6P6dyDioJogAAGprGpCtVImJhpPk/roCzdb3fIh61/s/nPsfR6onyMwkCAR/OlC3yBC0lESvUoQEAssIrw==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n-        \"node\": \">= 0.6\"\n+        \"node\": \">= 0.8\"\n+      }\n+    },\n+    \"node_modules/destroy\": {\n+      \"version\": \"1.2.0\",\n+      \"resolved\": \"https://registry.npmjs.org/destroy/-/destroy-1.2.0.tgz\",\n+      \"integrity\": \"sha512-2sJGJTaXIIaR1w4iJSNoN0hnMY7Gpc/n8D4qSCJw8QqFWXf7cuAgnEHxBpweaVcPevC2l3KpjYCx3NypQQgaJg==\",\n+      \"dev\": true,\n+      \"engines\": {\n+        \"node\": \">= 0.8\",\n+        \"npm\": \"1.2.8000 || >= 1.4.16\"\n       }\n     },\n     \"node_modules/di\": {\n@@ -3445,9 +3459,10 @@\n       }\n     },\n     \"node_modules/engine.io\": {\n-      \"version\": \"6.1.2\",\n+      \"version\": \"6.2.1\",\n+      \"resolved\": \"https://registry.npmjs.org/engine.io/-/engine.io-6.2.1.tgz\",\n+      \"integrity\": \"sha512-ECceEFcAaNRybd3lsGQKas3ZlMVjN3cyWwMP25D2i0zWfyiytVbTpRPa34qrr+FHddtpBVOmq4H/DCv1O0lZRA==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"@types/cookie\": \"^0.4.1\",\n         \"@types/cors\": \"^2.8.12\",\n@@ -3457,28 +3472,27 @@\n         \"cookie\": \"~0.4.1\",\n         \"cors\": \"~2.8.5\",\n         \"debug\": \"~4.3.1\",\n-        \"engine.io-parser\": \"~5.0.0\",\n+        \"engine.io-parser\": \"~5.0.3\",\n         \"ws\": \"~8.2.3\"\n       },\n       \"engines\": {\n         \"node\": \">=10.0.0\"\n       }\n     },\n     \"node_modules/engine.io-parser\": {\n-      \"version\": \"5.0.3\",\n+      \"version\": \"5.0.4\",\n+      \"resolved\": \"https://registry.npmjs.org/engine.io-parser/-/engine.io-parser-5.0.4.tgz\",\n+      \"integrity\": \"sha512-+nVFp+5z1E3HcToEnO7ZIj3g+3k9389DvWtvJZz0T6/eOCPIyyxehFcedoYrZQrp0LgQbD9pPXhpMBKMd5QURg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n-      \"dependencies\": {\n-        \"@socket.io/base64-arraybuffer\": \"~1.0.2\"\n-      },\n       \"engines\": {\n         \"node\": \">=10.0.0\"\n       }\n     },\n     \"node_modules/engine.io/node_modules/debug\": {\n-      \"version\": \"4.3.3\",\n+      \"version\": \"4.3.4\",\n+      \"resolved\": \"https://registry.npmjs.org/debug/-/debug-4.3.4.tgz\",\n+      \"integrity\": \"sha512-PRWFHuSU3eDtQJPvnNY7Jcket1j0t5OuOsFzPPzsekD52Zl8qUfFIPEiswXqIvHWGVHOgX+7G/vCNNhehwxfkQ==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"ms\": \"2.1.2\"\n       },\n@@ -3493,13 +3507,15 @@\n     },\n     \"node_modules/engine.io/node_modules/ms\": {\n       \"version\": \"2.1.2\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n+      \"resolved\": \"https://registry.npmjs.org/ms/-/ms-2.1.2.tgz\",\n+      \"integrity\": \"sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==\",\n+      \"dev\": true\n     },\n     \"node_modules/engine.io/node_modules/ws\": {\n       \"version\": \"8.2.3\",\n+      \"resolved\": \"https://registry.npmjs.org/ws/-/ws-8.2.3.tgz\",\n+      \"integrity\": \"sha512-wBuoj1BDpC6ZQ1B7DWQBYVLphPWkm8i9Y0/3YdHjHKHiohOJ1ws+3OccDWtH+PoC9DZD5WOTrJvNbWvjS6JWaA==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">=10.0.0\"\n       },\n@@ -5244,18 +5260,28 @@\n       }\n     },\n     \"node_modules/http-errors\": {\n-      \"version\": \"1.7.2\",\n+      \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/http-errors/-/http-errors-2.0.0.tgz\",\n+      \"integrity\": \"sha512-FtwrG/euBzaEjYeRqOgly7G0qviiXoJWnvEH2Z1plBdXgbyjv34pHTSb9zoeHMyDy33+DWy5Wt9Wo+TURtOYSQ==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n-        \"depd\": \"~1.1.2\",\n-        \"inherits\": \"2.0.3\",\n-        \"setprototypeof\": \"1.1.1\",\n-        \"statuses\": \">= 1.5.0 < 2\",\n-        \"toidentifier\": \"1.0.0\"\n+        \"depd\": \"2.0.0\",\n+        \"inherits\": \"2.0.4\",\n+        \"setprototypeof\": \"1.2.0\",\n+        \"statuses\": \"2.0.1\",\n+        \"toidentifier\": \"1.0.1\"\n       },\n       \"engines\": {\n-        \"node\": \">= 0.6\"\n+        \"node\": \">= 0.8\"\n+      }\n+    },\n+    \"node_modules/http-errors/node_modules/statuses\": {\n+      \"version\": \"2.0.1\",\n+      \"resolved\": \"https://registry.npmjs.org/statuses/-/statuses-2.0.1.tgz\",\n+      \"integrity\": \"sha512-RwNA9Z/7PrK06rYLIzFMlaF+l73iwpzsqRIFgbMLbTcLD6cOao82TaWefPXQvB2fOC4AjuYSEndS7N/mTCbkdQ==\",\n+      \"dev\": true,\n+      \"engines\": {\n+        \"node\": \">= 0.8\"\n       }\n     },\n     \"node_modules/http-proxy\": {\n@@ -5355,8 +5381,9 @@\n     },\n     \"node_modules/iconv-lite\": {\n       \"version\": \"0.4.24\",\n+      \"resolved\": \"https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz\",\n+      \"integrity\": \"sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"safer-buffer\": \">= 2.1.2 < 3\"\n       },\n@@ -5429,9 +5456,10 @@\n       }\n     },\n     \"node_modules/inherits\": {\n-      \"version\": \"2.0.3\",\n-      \"dev\": true,\n-      \"license\": \"ISC\"\n+      \"version\": \"2.0.4\",\n+      \"resolved\": \"https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz\",\n+      \"integrity\": \"sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==\",\n+      \"dev\": true\n     },\n     \"node_modules/internal-slot\": {\n       \"version\": \"1.0.3\",\n@@ -6797,8 +6825,9 @@\n     },\n     \"node_modules/negotiator\": {\n       \"version\": \"0.6.3\",\n+      \"resolved\": \"https://registry.npmjs.org/negotiator/-/negotiator-0.6.3.tgz\",\n+      \"integrity\": \"sha512-+EUsqGPLsM+j/zdChZjsnX51g4XrHFOIXwfnCVPGlQk/k5giakcKsuxCObBRu6DSm9opw/O6slWbJdghQM4bBg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">= 0.6\"\n       }\n@@ -6999,8 +7028,9 @@\n     },\n     \"node_modules/object-assign\": {\n       \"version\": \"4.1.1\",\n+      \"resolved\": \"https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz\",\n+      \"integrity\": \"sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">=0.10.0\"\n       }\n@@ -7571,6 +7601,21 @@\n         \"node\": \">=0.9\"\n       }\n     },\n+    \"node_modules/qs\": {\n+      \"version\": \"6.11.0\",\n+      \"resolved\": \"https://registry.npmjs.org/qs/-/qs-6.11.0.tgz\",\n+      \"integrity\": \"sha512-MvjoMCJwEarSbUYk5O+nmoSzSutSsTwF85zcHPQ9OrlFoZOYIjaqBAJIqIXjptyD5vThxGq52Xu/MaJzRkIk4Q==\",\n+      \"dev\": true,\n+      \"dependencies\": {\n+        \"side-channel\": \"^1.0.4\"\n+      },\n+      \"engines\": {\n+        \"node\": \">=0.6\"\n+      },\n+      \"funding\": {\n+        \"url\": \"https://github.com/sponsors/ljharb\"\n+      }\n+    },\n     \"node_modules/queue-microtask\": {\n       \"version\": \"1.2.3\",\n       \"dev\": true,\n@@ -7655,12 +7700,13 @@\n       }\n     },\n     \"node_modules/raw-body\": {\n-      \"version\": \"2.4.0\",\n+      \"version\": \"2.5.1\",\n+      \"resolved\": \"https://registry.npmjs.org/raw-body/-/raw-body-2.5.1.tgz\",\n+      \"integrity\": \"sha512-qqJBtEyVgS0ZmPGdCFPWJ3FreoqvG4MVQln/kCgF7Olq95IbOp0/BWyMwbdtn4VTvkM8Y7khCQ2Xgk/tcrCXig==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n-        \"bytes\": \"3.1.0\",\n-        \"http-errors\": \"1.7.2\",\n+        \"bytes\": \"3.1.2\",\n+        \"http-errors\": \"2.0.0\",\n         \"iconv-lite\": \"0.4.24\",\n         \"unpipe\": \"1.0.0\"\n       },\n@@ -8134,9 +8180,10 @@\n       }\n     },\n     \"node_modules/setprototypeof\": {\n-      \"version\": \"1.1.1\",\n-      \"dev\": true,\n-      \"license\": \"ISC\"\n+      \"version\": \"1.2.0\",\n+      \"resolved\": \"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\",\n+      \"integrity\": \"sha512-E5LDX7Wrp85Kil5bhZv46j8jOeboKq5JMmYM3gVGdGH8xFpPWXUMsNrlODCrkoxMEeNi/XZIwuRvY4XNwYMJpw==\",\n+      \"dev\": true\n     },\n     \"node_modules/shebang-command\": {\n       \"version\": \"2.0.0\",\n@@ -8182,44 +8229,46 @@\n       \"license\": \"ISC\"\n     },\n     \"node_modules/socket.io\": {\n-      \"version\": \"4.4.1\",\n+      \"version\": \"4.5.3\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io/-/socket.io-4.5.3.tgz\",\n+      \"integrity\": \"sha512-zdpnnKU+H6mOp7nYRXH4GNv1ux6HL6+lHL8g7Ds7Lj8CkdK1jJK/dlwsKDculbyOHifcJ0Pr/yeXnZQ5GeFrcg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"accepts\": \"~1.3.4\",\n         \"base64id\": \"~2.0.0\",\n         \"debug\": \"~4.3.2\",\n-        \"engine.io\": \"~6.1.0\",\n-        \"socket.io-adapter\": \"~2.3.3\",\n-        \"socket.io-parser\": \"~4.0.4\"\n+        \"engine.io\": \"~6.2.0\",\n+        \"socket.io-adapter\": \"~2.4.0\",\n+        \"socket.io-parser\": \"~4.2.0\"\n       },\n       \"engines\": {\n         \"node\": \">=10.0.0\"\n       }\n     },\n     \"node_modules/socket.io-adapter\": {\n-      \"version\": \"2.3.3\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n+      \"version\": \"2.4.0\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io-adapter/-/socket.io-adapter-2.4.0.tgz\",\n+      \"integrity\": \"sha512-W4N+o69rkMEGVuk2D/cvca3uYsvGlMwsySWV447y99gUPghxq42BxqLNMndb+a1mm/5/7NeXVQS7RLa2XyXvYg==\",\n+      \"dev\": true\n     },\n     \"node_modules/socket.io-parser\": {\n-      \"version\": \"4.0.5\",\n-      \"resolved\": \"https://registry.npmjs.org/socket.io-parser/-/socket.io-parser-4.0.5.tgz\",\n-      \"integrity\": \"sha512-sNjbT9dX63nqUFIOv95tTVm6elyIU4RvB1m8dOeZt+IgWwcWklFDOdmGcfo3zSiRsnR/3pJkjY5lfoGqEe4Eig==\",\n+      \"version\": \"4.2.1\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io-parser/-/socket.io-parser-4.2.1.tgz\",\n+      \"integrity\": \"sha512-V4GrkLy+HeF1F/en3SpUaM+7XxYXpuMUWLGde1kSSh5nQMN4hLrbPIkD+otwh6q9R6NOQBN4AMaOZ2zVjui82g==\",\n       \"dev\": true,\n       \"dependencies\": {\n-        \"@types/component-emitter\": \"^1.2.10\",\n-        \"component-emitter\": \"~1.3.0\",\n+        \"@socket.io/component-emitter\": \"~3.1.0\",\n         \"debug\": \"~4.3.1\"\n       },\n       \"engines\": {\n         \"node\": \">=10.0.0\"\n       }\n     },\n     \"node_modules/socket.io-parser/node_modules/debug\": {\n-      \"version\": \"4.3.3\",\n+      \"version\": \"4.3.4\",\n+      \"resolved\": \"https://registry.npmjs.org/debug/-/debug-4.3.4.tgz\",\n+      \"integrity\": \"sha512-PRWFHuSU3eDtQJPvnNY7Jcket1j0t5OuOsFzPPzsekD52Zl8qUfFIPEiswXqIvHWGVHOgX+7G/vCNNhehwxfkQ==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"dependencies\": {\n         \"ms\": \"2.1.2\"\n       },\n@@ -8234,8 +8283,9 @@\n     },\n     \"node_modules/socket.io-parser/node_modules/ms\": {\n       \"version\": \"2.1.2\",\n-      \"dev\": true,\n-      \"license\": \"MIT\"\n+      \"resolved\": \"https://registry.npmjs.org/ms/-/ms-2.1.2.tgz\",\n+      \"integrity\": \"sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==\",\n+      \"dev\": true\n     },\n     \"node_modules/socket.io/node_modules/debug\": {\n       \"version\": \"4.3.3\",\n@@ -8753,9 +8803,10 @@\n       }\n     },\n     \"node_modules/toidentifier\": {\n-      \"version\": \"1.0.0\",\n+      \"version\": \"1.0.1\",\n+      \"resolved\": \"https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz\",\n+      \"integrity\": \"sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">=0.6\"\n       }\n@@ -9013,8 +9064,9 @@\n     },\n     \"node_modules/vary\": {\n       \"version\": \"1.1.2\",\n+      \"resolved\": \"https://registry.npmjs.org/vary/-/vary-1.1.2.tgz\",\n+      \"integrity\": \"sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==\",\n       \"dev\": true,\n-      \"license\": \"MIT\",\n       \"engines\": {\n         \"node\": \">= 0.8\"\n       }\n@@ -11648,8 +11700,10 @@\n         }\n       }\n     },\n-    \"@socket.io/base64-arraybuffer\": {\n-      \"version\": \"1.0.2\",\n+    \"@socket.io/component-emitter\": {\n+      \"version\": \"3.1.0\",\n+      \"resolved\": \"https://registry.npmjs.org/@socket.io/component-emitter/-/component-emitter-3.1.0.tgz\",\n+      \"integrity\": \"sha512-+9jVqKhRSpsc591z5vX+X5Yyw+he/HCB4iQ/RYxw35CEPaY1gnsNE43nf9n9AaYjAQrTiI/mOwKUKdUs9vf7Xg==\",\n       \"dev\": true\n     },\n     \"@tootallnate/once\": {\n@@ -11658,16 +11712,16 @@\n       \"integrity\": \"sha512-XCuKFP5PS55gnMVu3dty8KPatLqUoy/ZYzDzAGCQ8JNFCkLXzmI7vNHCR+XpbZaMWQK/vQubr7PkYq8g470J/A==\",\n       \"dev\": true\n     },\n-    \"@types/component-emitter\": {\n-      \"version\": \"1.2.11\",\n-      \"dev\": true\n-    },\n     \"@types/cookie\": {\n       \"version\": \"0.4.1\",\n+      \"resolved\": \"https://registry.npmjs.org/@types/cookie/-/cookie-0.4.1.tgz\",\n+      \"integrity\": \"sha512-XW/Aa8APYr6jSVVA1y/DEIZX0/GMKLEVekNG727R8cs56ahETkRAy/3DR7+fJyh7oUgGwNQaRfXCun0+KbWY7Q==\",\n       \"dev\": true\n     },\n     \"@types/cors\": {\n       \"version\": \"2.8.12\",\n+      \"resolved\": \"https://registry.npmjs.org/@types/cors/-/cors-2.8.12.tgz\",\n+      \"integrity\": \"sha512-vt+kDhq/M2ayberEtJcIN/hxXy1Pk+59g2FV/ZQceeaTyCtCucjL2Q7FXlFjtWn4n15KCr1NE2lNNFhp0lEThw==\",\n       \"dev\": true\n     },\n     \"@types/dompurify\": {\n@@ -11891,6 +11945,8 @@\n     },\n     \"accepts\": {\n       \"version\": \"1.3.8\",\n+      \"resolved\": \"https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz\",\n+      \"integrity\": \"sha512-PYAthTa2m2VKxuvSD3DPC/Gy+U+sOA1LAuT8mkmRuvw+NACSaeXEQ+NHcVF7rONl6qcaxV3Uuemwawk+7+SJLw==\",\n       \"dev\": true,\n       \"requires\": {\n         \"mime-types\": \"~2.1.34\",\n@@ -12047,31 +12103,42 @@\n     },\n     \"base64id\": {\n       \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/base64id/-/base64id-2.0.0.tgz\",\n+      \"integrity\": \"sha512-lGe34o6EHj9y3Kts9R4ZYs/Gr+6N7MCaMlIFA3F1R2O5/m7K06AxfSeO5530PEERE6/WyEg3lsuyw4GHlPZHog==\",\n       \"dev\": true\n     },\n     \"binary-extensions\": {\n       \"version\": \"2.2.0\",\n       \"dev\": true\n     },\n     \"body-parser\": {\n-      \"version\": \"1.19.0\",\n+      \"version\": \"1.20.1\",\n+      \"resolved\": \"https://registry.npmjs.org/body-parser/-/body-parser-1.20.1.tgz\",\n+      \"integrity\": \"sha512-jWi7abTbYwajOytWCQc37VulmWiRae5RyTpaCyDcS5/lMdtwSz5lOpDE67srw/HYe35f1z3fDQw+3txg7gNtWw==\",\n       \"dev\": true,\n       \"requires\": {\n-        \"bytes\": \"3.1.0\",\n+        \"bytes\": \"3.1.2\",\n         \"content-type\": \"~1.0.4\",\n         \"debug\": \"2.6.9\",\n-        \"depd\": \"~1.1.2\",\n-        \"http-errors\": \"1.7.2\",\n+        \"depd\": \"2.0.0\",\n+        \"destroy\": \"1.2.0\",\n+        \"http-errors\": \"2.0.0\",\n         \"iconv-lite\": \"0.4.24\",\n-        \"on-finished\": \"~2.3.0\",\n-        \"qs\": \"6.7.0\",\n-        \"raw-body\": \"2.4.0\",\n-        \"type-is\": \"~1.6.17\"\n+        \"on-finished\": \"2.4.1\",\n+        \"qs\": \"6.11.0\",\n+        \"raw-body\": \"2.5.1\",\n+        \"type-is\": \"~1.6.18\",\n+        \"unpipe\": \"1.0.0\"\n       },\n       \"dependencies\": {\n-        \"qs\": {\n-          \"version\": \"6.7.0\",\n-          \"dev\": true\n+        \"on-finished\": {\n+          \"version\": \"2.4.1\",\n+          \"resolved\": \"https://registry.npmjs.org/on-finished/-/on-finished-2.4.1.tgz\",\n+          \"integrity\": \"sha512-oVlzkg3ENAhCk2zdv7IJwd/QUD4z2RxRwpkcGY8psCVcCYZNq4wYnVWALHM+brtuJjePWiYF/ClmuDr8Ch5+kg==\",\n+          \"dev\": true,\n+          \"requires\": {\n+            \"ee-first\": \"1.1.1\"\n+          }\n         }\n       }\n     },\n@@ -12160,7 +12227,9 @@\n       \"dev\": true\n     },\n     \"bytes\": {\n-      \"version\": \"3.1.0\",\n+      \"version\": \"3.1.2\",\n+      \"resolved\": \"https://registry.npmjs.org/bytes/-/bytes-3.1.2.tgz\",\n+      \"integrity\": \"sha512-/Nf7TyzTx6S3yRJObOAV7956r8cr2+Oj8AC5dt8wSP3BQAoeX58NoHyCU8P8zGkNXStjTSi6fzO6F0pBdcYbEg==\",\n       \"dev\": true\n     },\n     \"call-bind\": {\n@@ -12277,10 +12346,6 @@\n       \"version\": \"1.0.1\",\n       \"dev\": true\n     },\n-    \"component-emitter\": {\n-      \"version\": \"1.3.0\",\n-      \"dev\": true\n-    },\n     \"concat-map\": {\n       \"version\": \"0.0.1\",\n       \"dev\": true\n@@ -12322,6 +12387,8 @@\n     },\n     \"cookie\": {\n       \"version\": \"0.4.2\",\n+      \"resolved\": \"https://registry.npmjs.org/cookie/-/cookie-0.4.2.tgz\",\n+      \"integrity\": \"sha512-aSWTXFzaKWkvHO1Ny/s+ePFpvKsPnjc551iI41v3ny/ow6tBG5Vd+FuqGNhh1LxOmVzOlGUriIlOaokOvhaStA==\",\n       \"dev\": true\n     },\n     \"core-js-compat\": {\n@@ -12348,6 +12415,8 @@\n     },\n     \"cors\": {\n       \"version\": \"2.8.5\",\n+      \"resolved\": \"https://registry.npmjs.org/cors/-/cors-2.8.5.tgz\",\n+      \"integrity\": \"sha512-KIHbLJqu73RGr/hnbrO9uBeixNGuvSQjul/jdFvS/KFSIH1hWVd1ng7zOHx+YrEfInLG7q4n6GHQ9cDtxv/P6g==\",\n       \"dev\": true,\n       \"requires\": {\n         \"object-assign\": \"^4\",\n@@ -12504,7 +12573,15 @@\n       \"dev\": true\n     },\n     \"depd\": {\n-      \"version\": \"1.1.2\",\n+      \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/depd/-/depd-2.0.0.tgz\",\n+      \"integrity\": \"sha512-g7nH6P6dyDioJogAAGprGpCtVImJhpPk/roCzdb3fIh61/s/nPsfR6onyMwkCAR/OlC3yBC0lESvUoQEAssIrw==\",\n+      \"dev\": true\n+    },\n+    \"destroy\": {\n+      \"version\": \"1.2.0\",\n+      \"resolved\": \"https://registry.npmjs.org/destroy/-/destroy-1.2.0.tgz\",\n+      \"integrity\": \"sha512-2sJGJTaXIIaR1w4iJSNoN0hnMY7Gpc/n8D4qSCJw8QqFWXf7cuAgnEHxBpweaVcPevC2l3KpjYCx3NypQQgaJg==\",\n       \"dev\": true\n     },\n     \"di\": {\n@@ -12573,7 +12650,9 @@\n       \"dev\": true\n     },\n     \"engine.io\": {\n-      \"version\": \"6.1.2\",\n+      \"version\": \"6.2.1\",\n+      \"resolved\": \"https://registry.npmjs.org/engine.io/-/engine.io-6.2.1.tgz\",\n+      \"integrity\": \"sha512-ECceEFcAaNRybd3lsGQKas3ZlMVjN3cyWwMP25D2i0zWfyiytVbTpRPa34qrr+FHddtpBVOmq4H/DCv1O0lZRA==\",\n       \"dev\": true,\n       \"requires\": {\n         \"@types/cookie\": \"^0.4.1\",\n@@ -12584,34 +12663,39 @@\n         \"cookie\": \"~0.4.1\",\n         \"cors\": \"~2.8.5\",\n         \"debug\": \"~4.3.1\",\n-        \"engine.io-parser\": \"~5.0.0\",\n+        \"engine.io-parser\": \"~5.0.3\",\n         \"ws\": \"~8.2.3\"\n       },\n       \"dependencies\": {\n         \"debug\": {\n-          \"version\": \"4.3.3\",\n+          \"version\": \"4.3.4\",\n+          \"resolved\": \"https://registry.npmjs.org/debug/-/debug-4.3.4.tgz\",\n+          \"integrity\": \"sha512-PRWFHuSU3eDtQJPvnNY7Jcket1j0t5OuOsFzPPzsekD52Zl8qUfFIPEiswXqIvHWGVHOgX+7G/vCNNhehwxfkQ==\",\n           \"dev\": true,\n           \"requires\": {\n             \"ms\": \"2.1.2\"\n           }\n         },\n         \"ms\": {\n           \"version\": \"2.1.2\",\n+          \"resolved\": \"https://registry.npmjs.org/ms/-/ms-2.1.2.tgz\",\n+          \"integrity\": \"sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==\",\n           \"dev\": true\n         },\n         \"ws\": {\n           \"version\": \"8.2.3\",\n+          \"resolved\": \"https://registry.npmjs.org/ws/-/ws-8.2.3.tgz\",\n+          \"integrity\": \"sha512-wBuoj1BDpC6ZQ1B7DWQBYVLphPWkm8i9Y0/3YdHjHKHiohOJ1ws+3OccDWtH+PoC9DZD5WOTrJvNbWvjS6JWaA==\",\n           \"dev\": true,\n           \"requires\": {}\n         }\n       }\n     },\n     \"engine.io-parser\": {\n-      \"version\": \"5.0.3\",\n-      \"dev\": true,\n-      \"requires\": {\n-        \"@socket.io/base64-arraybuffer\": \"~1.0.2\"\n-      }\n+      \"version\": \"5.0.4\",\n+      \"resolved\": \"https://registry.npmjs.org/engine.io-parser/-/engine.io-parser-5.0.4.tgz\",\n+      \"integrity\": \"sha512-+nVFp+5z1E3HcToEnO7ZIj3g+3k9389DvWtvJZz0T6/eOCPIyyxehFcedoYrZQrp0LgQbD9pPXhpMBKMd5QURg==\",\n+      \"dev\": true\n     },\n     \"enhance-visitors\": {\n       \"version\": \"1.0.0\",\n@@ -13714,14 +13798,24 @@\n       }\n     },\n     \"http-errors\": {\n-      \"version\": \"1.7.2\",\n+      \"version\": \"2.0.0\",\n+      \"resolved\": \"https://registry.npmjs.org/http-errors/-/http-errors-2.0.0.tgz\",\n+      \"integrity\": \"sha512-FtwrG/euBzaEjYeRqOgly7G0qviiXoJWnvEH2Z1plBdXgbyjv34pHTSb9zoeHMyDy33+DWy5Wt9Wo+TURtOYSQ==\",\n       \"dev\": true,\n       \"requires\": {\n-        \"depd\": \"~1.1.2\",\n-        \"inherits\": \"2.0.3\",\n-        \"setprototypeof\": \"1.1.1\",\n-        \"statuses\": \">= 1.5.0 < 2\",\n-        \"toidentifier\": \"1.0.0\"\n+        \"depd\": \"2.0.0\",\n+        \"inherits\": \"2.0.4\",\n+        \"setprototypeof\": \"1.2.0\",\n+        \"statuses\": \"2.0.1\",\n+        \"toidentifier\": \"1.0.1\"\n+      },\n+      \"dependencies\": {\n+        \"statuses\": {\n+          \"version\": \"2.0.1\",\n+          \"resolved\": \"https://registry.npmjs.org/statuses/-/statuses-2.0.1.tgz\",\n+          \"integrity\": \"sha512-RwNA9Z/7PrK06rYLIzFMlaF+l73iwpzsqRIFgbMLbTcLD6cOao82TaWefPXQvB2fOC4AjuYSEndS7N/mTCbkdQ==\",\n+          \"dev\": true\n+        }\n       }\n     },\n     \"http-proxy\": {\n@@ -13797,6 +13891,8 @@\n     },\n     \"iconv-lite\": {\n       \"version\": \"0.4.24\",\n+      \"resolved\": \"https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz\",\n+      \"integrity\": \"sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==\",\n       \"dev\": true,\n       \"requires\": {\n         \"safer-buffer\": \">= 2.1.2 < 3\"\n@@ -13841,7 +13937,9 @@\n       }\n     },\n     \"inherits\": {\n-      \"version\": \"2.0.3\",\n+      \"version\": \"2.0.4\",\n+      \"resolved\": \"https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz\",\n+      \"integrity\": \"sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==\",\n       \"dev\": true\n     },\n     \"internal-slot\": {\n@@ -14711,6 +14809,8 @@\n     },\n     \"negotiator\": {\n       \"version\": \"0.6.3\",\n+      \"resolved\": \"https://registry.npmjs.org/negotiator/-/negotiator-0.6.3.tgz\",\n+      \"integrity\": \"sha512-+EUsqGPLsM+j/zdChZjsnX51g4XrHFOIXwfnCVPGlQk/k5giakcKsuxCObBRu6DSm9opw/O6slWbJdghQM4bBg==\",\n       \"dev\": true\n     },\n     \"neo-async\": {\n@@ -14849,6 +14949,8 @@\n     },\n     \"object-assign\": {\n       \"version\": \"4.1.1\",\n+      \"resolved\": \"https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz\",\n+      \"integrity\": \"sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==\",\n       \"dev\": true\n     },\n     \"object-inspect\": {\n@@ -15183,6 +15285,15 @@\n       \"version\": \"1.2.0\",\n       \"dev\": true\n     },\n+    \"qs\": {\n+      \"version\": \"6.11.0\",\n+      \"resolved\": \"https://registry.npmjs.org/qs/-/qs-6.11.0.tgz\",\n+      \"integrity\": \"sha512-MvjoMCJwEarSbUYk5O+nmoSzSutSsTwF85zcHPQ9OrlFoZOYIjaqBAJIqIXjptyD5vThxGq52Xu/MaJzRkIk4Q==\",\n+      \"dev\": true,\n+      \"requires\": {\n+        \"side-channel\": \"^1.0.4\"\n+      }\n+    },\n     \"queue-microtask\": {\n       \"version\": \"1.2.3\",\n       \"dev\": true\n@@ -15226,11 +15337,13 @@\n       \"dev\": true\n     },\n     \"raw-body\": {\n-      \"version\": \"2.4.0\",\n+      \"version\": \"2.5.1\",\n+      \"resolved\": \"https://registry.npmjs.org/raw-body/-/raw-body-2.5.1.tgz\",\n+      \"integrity\": \"sha512-qqJBtEyVgS0ZmPGdCFPWJ3FreoqvG4MVQln/kCgF7Olq95IbOp0/BWyMwbdtn4VTvkM8Y7khCQ2Xgk/tcrCXig==\",\n       \"dev\": true,\n       \"requires\": {\n-        \"bytes\": \"3.1.0\",\n-        \"http-errors\": \"1.7.2\",\n+        \"bytes\": \"3.1.2\",\n+        \"http-errors\": \"2.0.0\",\n         \"iconv-lite\": \"0.4.24\",\n         \"unpipe\": \"1.0.0\"\n       }\n@@ -15538,7 +15651,9 @@\n       }\n     },\n     \"setprototypeof\": {\n-      \"version\": \"1.1.1\",\n+      \"version\": \"1.2.0\",\n+      \"resolved\": \"https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz\",\n+      \"integrity\": \"sha512-E5LDX7Wrp85Kil5bhZv46j8jOeboKq5JMmYM3gVGdGH8xFpPWXUMsNrlODCrkoxMEeNi/XZIwuRvY4XNwYMJpw==\",\n       \"dev\": true\n     },\n     \"shebang-command\": {\n@@ -15572,15 +15687,17 @@\n       \"dev\": true\n     },\n     \"socket.io\": {\n-      \"version\": \"4.4.1\",\n+      \"version\": \"4.5.3\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io/-/socket.io-4.5.3.tgz\",\n+      \"integrity\": \"sha512-zdpnnKU+H6mOp7nYRXH4GNv1ux6HL6+lHL8g7Ds7Lj8CkdK1jJK/dlwsKDculbyOHifcJ0Pr/yeXnZQ5GeFrcg==\",\n       \"dev\": true,\n       \"requires\": {\n         \"accepts\": \"~1.3.4\",\n         \"base64id\": \"~2.0.0\",\n         \"debug\": \"~4.3.2\",\n-        \"engine.io\": \"~6.1.0\",\n-        \"socket.io-adapter\": \"~2.3.3\",\n-        \"socket.io-parser\": \"~4.0.4\"\n+        \"engine.io\": \"~6.2.0\",\n+        \"socket.io-adapter\": \"~2.4.0\",\n+        \"socket.io-parser\": \"~4.2.0\"\n       },\n       \"dependencies\": {\n         \"debug\": {\n@@ -15597,29 +15714,34 @@\n       }\n     },\n     \"socket.io-adapter\": {\n-      \"version\": \"2.3.3\",\n+      \"version\": \"2.4.0\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io-adapter/-/socket.io-adapter-2.4.0.tgz\",\n+      \"integrity\": \"sha512-W4N+o69rkMEGVuk2D/cvca3uYsvGlMwsySWV447y99gUPghxq42BxqLNMndb+a1mm/5/7NeXVQS7RLa2XyXvYg==\",\n       \"dev\": true\n     },\n     \"socket.io-parser\": {\n-      \"version\": \"4.0.5\",\n-      \"resolved\": \"https://registry.npmjs.org/socket.io-parser/-/socket.io-parser-4.0.5.tgz\",\n-      \"integrity\": \"sha512-sNjbT9dX63nqUFIOv95tTVm6elyIU4RvB1m8dOeZt+IgWwcWklFDOdmGcfo3zSiRsnR/3pJkjY5lfoGqEe4Eig==\",\n+      \"version\": \"4.2.1\",\n+      \"resolved\": \"https://registry.npmjs.org/socket.io-parser/-/socket.io-parser-4.2.1.tgz\",\n+      \"integrity\": \"sha512-V4GrkLy+HeF1F/en3SpUaM+7XxYXpuMUWLGde1kSSh5nQMN4hLrbPIkD+otwh6q9R6NOQBN4AMaOZ2zVjui82g==\",\n       \"dev\": true,\n       \"requires\": {\n-        \"@types/component-emitter\": \"^1.2.10\",\n-        \"component-emitter\": \"~1.3.0\",\n+        \"@socket.io/component-emitter\": \"~3.1.0\",\n         \"debug\": \"~4.3.1\"\n       },\n       \"dependencies\": {\n         \"debug\": {\n-          \"version\": \"4.3.3\",\n+          \"version\": \"4.3.4\",\n+          \"resolved\": \"https://registry.npmjs.org/debug/-/debug-4.3.4.tgz\",\n+          \"integrity\": \"sha512-PRWFHuSU3eDtQJPvnNY7Jcket1j0t5OuOsFzPPzsekD52Zl8qUfFIPEiswXqIvHWGVHOgX+7G/vCNNhehwxfkQ==\",\n           \"dev\": true,\n           \"requires\": {\n             \"ms\": \"2.1.2\"\n           }\n         },\n         \"ms\": {\n           \"version\": \"2.1.2\",\n+          \"resolved\": \"https://registry.npmjs.org/ms/-/ms-2.1.2.tgz\",\n+          \"integrity\": \"sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==\",\n           \"dev\": true\n         }\n       }\n@@ -15955,7 +16077,9 @@\n       }\n     },\n     \"toidentifier\": {\n-      \"version\": \"1.0.0\",\n+      \"version\": \"1.0.1\",\n+      \"resolved\": \"https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz\",\n+      \"integrity\": \"sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==\",\n       \"dev\": true\n     },\n     \"tough-cookie\": {\n@@ -16121,6 +16245,8 @@\n     },\n     \"vary\": {\n       \"version\": \"1.1.2\",\n+      \"resolved\": \"https://registry.npmjs.org/vary/-/vary-1.1.2.tgz\",\n+      \"integrity\": \"sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==\",\n       \"dev\": true\n     },\n     \"void-elements\": {",
          "package.json": "@@ -98,7 +98,7 @@\n   },\n   \"name\": \"dompurify\",\n   \"description\": \"DOMPurify is a DOM-only, super-fast, uber-tolerant XSS sanitizer for HTML, MathML and SVG. It's written in JavaScript and works in all modern browsers (Safari, Opera (15+), Internet Explorer (10+), Firefox and Chrome - as well as almost anything else using Blink or WebKit). DOMPurify is written by security people who have vast background in web attacks and XSS. Fear not.\",\n-  \"version\": \"2.4.1\",\n+  \"version\": \"2.4.2\",\n   \"directories\": {\n     \"test\": \"test\"\n   },",
          "src/purify.js": "@@ -844,7 +844,9 @@ function createDOMPurify(window = getGlobal()) {\n     if (!doc || !doc.documentElement) {\n       doc = implementation.createDocument(NAMESPACE, 'template', null);\n       try {\n-        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? '' : dirtyPayload;\n+        doc.documentElement.innerHTML = IS_EMPTY_INPUT\n+          ? emptyHTML\n+          : dirtyPayload;\n       } catch (_) {\n         // Syntax error if dirtyPayload is invalid xml\n       }",
          "src/utils.js": "@@ -95,7 +95,7 @@ export function clone(object) {\n \n   let property;\n   for (property in object) {\n-    if (apply(hasOwnProperty, object, [property])) {\n+    if (apply(hasOwnProperty, object, [property]) === true) {\n       newObject[property] = object[property];\n     }\n   }",
          "test/test-suite.js": "@@ -1687,7 +1687,7 @@\n       }\n     );\n     QUnit.test(\n-      'Test protection from prototype pollution attacks',\n+      'Test protection from prototype pollution attacks 1/2',\n       function (assert) {\n         const obj = JSON.parse(\n           '{\"ALLOWED_ATTR\":[\"onerror\",\"src\"], \"documentMode\":9}'\n@@ -1701,6 +1701,16 @@\n         assert.equal(clean, '<img src=\"x\">');\n       }\n     );\n+    QUnit.test(\n+      'Test protection from prototype pollution attacks 2/2',\n+      function (assert) {\n+        var obj = {};\n+        obj.__proto__.hasOwnProperty = Object;\n+        obj.constructor.prototype.ALLOWED_ATTR = [\"src\", \"onerror\"];\n+        var clean = DOMPurify.sanitize('<img src=x onerror=alert(1)>');\n+        assert.equal(clean, '<img src=\"x\">');\n+      }\n+    );\n     QUnit.test('Test if namespaces are properly enforced', function (assert) {\n       var tests = [\n         {",
          "website/index.html": "@@ -2,7 +2,7 @@\n <html>\n     <head>\n         <meta charset=\"UTF-8\">\n-        <title>DOMPurify 2.4.1 \"Kitten Winds\"</title>\n+        <title>DOMPurify 2.4.2 \"Queen Bee\"</title>\n         <script src=\"../dist/purify.min.js\"></script>\n         <!-- we don't actually need it - just to demo and test the $(html) sanitation -->\n         <script src=\"//code.jquery.com/jquery-3.2.0.min.js\"></script>\n@@ -23,7 +23,7 @@\n         </script>\n     </head>\n     <body>\n-        <h4>DOMPurify 2.4.1 \"Kitten Winds\"</h4>\n+        <h4>DOMPurify 2.4.2 \"Queen Bee\"</h4>\n         <p>\n             <a href=\"http://badge.fury.io/js/dompurify\" rel=\"nofollow\"><img alt=\"npm version\" src=\"https://badge.fury.io/js/dompurify.svg\"></a>  \n             <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/cure53/DOMPurify/workflows/Build%20and%20Test/badge.svg?branch=main\"><img src=\"https://github.com/cure53/DOMPurify/workflows/Build%20and%20Test/badge.svg?branch=main\" alt=\"Build and Test\"></a>  "
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N",
      "score": 9.1
    },
    "cwes": [
      {
        "cwe_id": "CWE-1321",
        "name": "Improperly Controlled Modification of Object Prototype Attributes ('Prototype Pollution')"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "eslerm",
          "id": 16212934,
          "node_id": "MDQ6VXNlcjE2MjEyOTM0",
          "avatar_url": "https://avatars.githubusercontent.com/u/16212934?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/eslerm",
          "html_url": "https://github.com/eslerm",
          "followers_url": "https://api.github.com/users/eslerm/followers",
          "following_url": "https://api.github.com/users/eslerm/following{/other_user}",
          "gists_url": "https://api.github.com/users/eslerm/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/eslerm/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/eslerm/subscriptions",
          "organizations_url": "https://api.github.com/users/eslerm/orgs",
          "repos_url": "https://api.github.com/users/eslerm/repos",
          "events_url": "https://api.github.com/users/eslerm/events{/privacy}",
          "received_events_url": "https://api.github.com/users/eslerm/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N",
        "score": 9.1
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:N/SC:N/SI:N/SA:N",
        "score": 9.3
      }
    },
    "epss": {
      "percentage": 0.00043,
      "percentile": 0.09902
    },
    "cve_description": "DOMPurify is a DOM-only, super-fast, uber-tolerant XSS sanitizer for HTML, MathML and SVG. DOMPurify was vulnerable to prototype pollution. This vulnerability is fixed in 2.4.2."
  },
  {
    "ghsa_id": "GHSA-r9mq-3c9r-fmjq",
    "cve_id": "CVE-2024-48914",
    "url": "https://api.github.com/advisories/GHSA-r9mq-3c9r-fmjq",
    "html_url": "https://github.com/advisories/GHSA-r9mq-3c9r-fmjq",
    "summary": "Vendure asset server plugin has local file read vulnerability with AssetServerPlugin & LocalAssetStorageStrategy",
    "description": "# Description\n\n## Path traversal\n\nThis vulnerability allows an attacker to craft a request which is able to traverse the server file system and retrieve the contents of arbitrary files, including sensitive data such as configuration files, environment variables, and other critical data stored on the server.\n\nFrom Rajesh Sharma who discovered the vulnerability:\n\nPOC: `curl --path-as-is http://localhost:3000/assets/../package.json` gives you the content of package.json present in the local directory.\n\nThe vulnerability stems from usage of decodedReqPath directly in path.join without performing any path normalization i.e path.normalize in node.js\n\nhttps://github.com/vendure-ecommerce/vendure/blob/801980e8f599c28c5059657a9d85dd03e3827992/packages/asset-server-plugin/src/plugin.ts#L352-L358\n\nIf the vendure service is behind some server like nginx, apache, etc. Path normalization is performed on the root server level but still the actual client's request path will be sent to vendure service but not the resultant normalized path. However, depending the type of root server one can try various payloads to bypass such normalization. \n\nThe reporter found a customer website which uses local asset plugin and using above mentioned vulnerability, and was able to find secrets like email credentials.\n\n\n## DOS via malformed URI\n\nIn the same code path is an additional vector for crashing the server via a malformed URI\n\nAgain from Rajesh:\n\nThere is also a potential Denial of Service (DoS) issue when incorrectly encoded URI characters are passed as part of the asset URL. When these malformed requests are processed, they can lead to system crashes or resource exhaustion, rendering the service unavailable to users.\nExploit: `curl  --path-as-is http://localhost:3000/assets/%80package.json` , here `%80` is not a valid url-encoded character hence the decodeURIComponent is called on it, the entire app crashes. \n\n```\n[:server] /Users/abc/mywork/vendure/packages/asset-server-plugin/src/plugin.ts:353\n[:server]         const decodedReqPath = decodeURIComponent(req.path);\n[:server]                                ^\n[:server] URIError: URI malformed\n```\n\n### Patches\nv3.0.5, v2.3.3\n\n### Workarounds\n- Use object storage rather than the local file system, e.g. MinIO or S3\n- Define middleware which detects and blocks requests with urls containing `/../`\n\n",
    "type": "reviewed",
    "severity": "critical",
    "repository_advisory_url": "https://api.github.com/repos/vendure-ecommerce/vendure/security-advisories/GHSA-r9mq-3c9r-fmjq",
    "source_code_location": "https://github.com/vendure-ecommerce/vendure",
    "identifiers": [
      {
        "value": "GHSA-r9mq-3c9r-fmjq",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-48914",
        "type": "CVE"
      }
    ],
    "references": [
      "https://github.com/vendure-ecommerce/vendure/security/advisories/GHSA-r9mq-3c9r-fmjq",
      "https://github.com/vendure-ecommerce/vendure/commit/e2ee0c43159b3d13b51b78654481094fdd4850c5",
      "https://github.com/vendure-ecommerce/vendure/commit/e4b58af6822d38a9c92a1d8573e19288b8edaa1c",
      "https://github.com/vendure-ecommerce/vendure/blob/801980e8f599c28c5059657a9d85dd03e3827992/packages/asset-server-plugin/src/plugin.ts#L352-L358",
      "https://nvd.nist.gov/vuln/detail/CVE-2024-48914",
      "https://github.com/advisories/GHSA-r9mq-3c9r-fmjq"
    ],
    "published_at": "2024-10-15T18:00:02Z",
    "updated_at": "2024-10-15T19:56:27Z",
    "github_reviewed_at": "2024-10-15T18:00:02Z",
    "nvd_published_at": "2024-10-15T16:15:06Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "@vendure/asset-server-plugin"
        },
        "vulnerable_version_range": "< 2.3.3",
        "first_patched_version": "2.3.3",
        "vulnerable_functions": [],
        "vulnerable_version": "2.3.2",
        "patches": {
          "CHANGELOG.md": "@@ -1,3 +1,6 @@\n+## <small>2.3.3 (2024-10-15)</small>\n+\n+\n ## <small>2.3.2 (2024-10-04)</small>\n \n ",
          "lerna.json": "@@ -1,6 +1,6 @@\n {\n     \"packages\": [\"packages/*\"],\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"npmClient\": \"npm\",\n     \"command\": {\n         \"version\": {",
          "package-lock.json": "@@ -31943,7 +31943,7 @@\n         },\n         \"packages/admin-ui\": {\n             \"name\": \"@vendure/admin-ui\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@angular/animations\": \"^17.2.4\",\n@@ -31966,7 +31966,7 @@\n                 \"@ng-select/ng-select\": \"^12.0.7\",\n                 \"@ngx-translate/core\": \"^15.0.0\",\n                 \"@ngx-translate/http-loader\": \"^8.0.0\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"@webcomponents/custom-elements\": \"^1.6.0\",\n                 \"apollo-angular\": \"^6.0.0\",\n                 \"apollo-upload-client\": \"^18.0.1\",\n@@ -32037,7 +32037,7 @@\n         },\n         \"packages/admin-ui-plugin\": {\n             \"name\": \"@vendure/admin-ui-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"date-fns\": \"^2.30.0\",\n@@ -32046,9 +32046,9 @@\n             \"devDependencies\": {\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n-                \"@vendure/admin-ui\": \"^2.3.2\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/admin-ui\": \"^2.3.3\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"express\": \"^4.18.3\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.4.2\"\n@@ -32079,7 +32079,7 @@\n         },\n         \"packages/asset-server-plugin\": {\n             \"name\": \"@vendure/asset-server-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"file-type\": \"^19.0.0\",\n@@ -32092,8 +32092,8 @@\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/node-fetch\": \"^2.6.11\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"express\": \"^4.18.3\",\n                 \"node-fetch\": \"^2.7.0\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -32105,11 +32105,11 @@\n         },\n         \"packages/cli\": {\n             \"name\": \"@vendure/cli\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@clack/prompts\": \"^0.7.0\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"change-case\": \"^4.1.2\",\n                 \"commander\": \"^11.0.0\",\n                 \"dotenv\": \"^16.4.5\",\n@@ -32123,7 +32123,7 @@\n                 \"vendure\": \"dist/cli.js\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"typescript\": \"5.3.3\"\n             },\n             \"funding\": {\n@@ -32160,7 +32160,7 @@\n         },\n         \"packages/common\": {\n             \"name\": \"@vendure/common\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"devDependencies\": {\n                 \"rimraf\": \"^5.0.5\",\n@@ -32172,7 +32172,7 @@\n         },\n         \"packages/core\": {\n             \"name\": \"@vendure/core\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@apollo/server\": \"^4.10.4\",\n@@ -32186,7 +32186,7 @@\n                 \"@nestjs/testing\": \"~10.3.10\",\n                 \"@nestjs/typeorm\": \"~10.0.2\",\n                 \"@types/fs-extra\": \"^9.0.1\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"bcrypt\": \"^5.1.1\",\n                 \"body-parser\": \"^1.20.2\",\n                 \"cookie-session\": \"^2.1.0\",\n@@ -32322,11 +32322,11 @@\n         },\n         \"packages/create\": {\n             \"name\": \"@vendure/create\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@clack/prompts\": \"^0.7.0\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"commander\": \"^11.0.0\",\n                 \"cross-spawn\": \"^7.0.3\",\n                 \"detect-port\": \"^1.5.1\",\n@@ -32345,7 +32345,7 @@\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/handlebars\": \"^4.1.0\",\n                 \"@types/semver\": \"^7.5.8\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"ts-node\": \"^10.9.2\",\n                 \"typescript\": \"5.3.3\"\n@@ -32362,21 +32362,21 @@\n             }\n         },\n         \"packages/dev-server\": {\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@nestjs/axios\": \"^3.0.2\",\n-                \"@vendure/admin-ui-plugin\": \"^2.3.2\",\n-                \"@vendure/asset-server-plugin\": \"^2.3.2\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n-                \"@vendure/elasticsearch-plugin\": \"^2.3.2\",\n-                \"@vendure/email-plugin\": \"^2.3.2\",\n+                \"@vendure/admin-ui-plugin\": \"^2.3.3\",\n+                \"@vendure/asset-server-plugin\": \"^2.3.3\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n+                \"@vendure/elasticsearch-plugin\": \"^2.3.3\",\n+                \"@vendure/email-plugin\": \"^2.3.3\",\n                 \"typescript\": \"5.3.3\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/testing\": \"^2.3.2\",\n-                \"@vendure/ui-devkit\": \"^2.3.2\",\n+                \"@vendure/testing\": \"^2.3.3\",\n+                \"@vendure/ui-devkit\": \"^2.3.3\",\n                 \"commander\": \"^12.0.0\",\n                 \"concurrently\": \"^8.2.2\",\n                 \"csv-stringify\": \"^6.4.6\",\n@@ -32394,16 +32394,16 @@\n         },\n         \"packages/elasticsearch-plugin\": {\n             \"name\": \"@vendure/elasticsearch-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@elastic/elasticsearch\": \"~7.9.1\",\n                 \"deepmerge\": \"^4.2.2\",\n                 \"fast-deep-equal\": \"^3.1.3\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n@@ -32413,7 +32413,7 @@\n         },\n         \"packages/email-plugin\": {\n             \"name\": \"@vendure/email-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@types/nodemailer\": \"^6.4.9\",\n@@ -32429,8 +32429,8 @@\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/mjml\": \"^4.7.4\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n@@ -32440,27 +32440,27 @@\n         },\n         \"packages/harden-plugin\": {\n             \"name\": \"@vendure/harden-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"graphql-query-complexity\": \"^0.12.0\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\"\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n             }\n         },\n         \"packages/job-queue-plugin\": {\n             \"name\": \"@vendure/job-queue-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"devDependencies\": {\n                 \"@google-cloud/pubsub\": \"^2.8.0\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"bullmq\": \"^5.4.2\",\n                 \"ioredis\": \"^5.3.2\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -32472,7 +32472,7 @@\n         },\n         \"packages/payments-plugin\": {\n             \"name\": \"@vendure/payments-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"currency.js\": \"2.0.4\"\n@@ -32481,9 +32481,9 @@\n                 \"@mollie/api-client\": \"^3.7.0\",\n                 \"@types/braintree\": \"^3.3.11\",\n                 \"@types/localtunnel\": \"2.0.4\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n-                \"@vendure/testing\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\",\n+                \"@vendure/testing\": \"^2.3.3\",\n                 \"braintree\": \"^3.22.0\",\n                 \"localtunnel\": \"2.0.2\",\n                 \"nock\": \"^13.1.4\",\n@@ -32539,12 +32539,12 @@\n         },\n         \"packages/sentry-plugin\": {\n             \"name\": \"@vendure/sentry-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"devDependencies\": {\n                 \"@sentry/node\": \"^7.106.1\",\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\"\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n@@ -32555,26 +32555,26 @@\n         },\n         \"packages/stellate-plugin\": {\n             \"name\": \"@vendure/stellate-plugin\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"node-fetch\": \"^2.7.0\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^2.3.2\",\n-                \"@vendure/core\": \"^2.3.2\"\n+                \"@vendure/common\": \"^2.3.3\",\n+                \"@vendure/core\": \"^2.3.3\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n             }\n         },\n         \"packages/testing\": {\n             \"name\": \"@vendure/testing\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@graphql-typed-document-node/core\": \"^3.2.0\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"faker\": \"^4.1.0\",\n                 \"form-data\": \"^4.0.0\",\n                 \"graphql\": \"~16.9.0\",\n@@ -32587,7 +32587,7 @@\n                 \"@types/mysql\": \"^2.15.26\",\n                 \"@types/node-fetch\": \"^2.6.4\",\n                 \"@types/pg\": \"^8.11.2\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"mysql\": \"^2.18.1\",\n                 \"pg\": \"^8.11.3\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -32603,15 +32603,15 @@\n         },\n         \"packages/ui-devkit\": {\n             \"name\": \"@vendure/ui-devkit\",\n-            \"version\": \"2.3.2\",\n+            \"version\": \"2.3.3\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@angular-devkit/build-angular\": \"^17.2.3\",\n                 \"@angular/cli\": \"^17.2.3\",\n                 \"@angular/compiler\": \"^17.2.4\",\n                 \"@angular/compiler-cli\": \"^17.2.4\",\n-                \"@vendure/admin-ui\": \"^2.3.2\",\n-                \"@vendure/common\": \"^2.3.2\",\n+                \"@vendure/admin-ui\": \"^2.3.3\",\n+                \"@vendure/common\": \"^2.3.3\",\n                 \"chalk\": \"^4.1.0\",\n                 \"chokidar\": \"^3.6.0\",\n                 \"fs-extra\": \"^11.2.0\",\n@@ -32622,7 +32622,7 @@\n                 \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n                 \"@rollup/plugin-terser\": \"^0.4.4\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n-                \"@vendure/core\": \"^2.3.2\",\n+                \"@vendure/core\": \"^2.3.3\",\n                 \"react\": \"^18.2.0\",\n                 \"react-dom\": \"^18.2.0\",\n                 \"rimraf\": \"^5.0.5\",",
          "packages/admin-ui-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/admin-ui-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n     \"files\": [\n@@ -21,9 +21,9 @@\n     \"devDependencies\": {\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n-        \"@vendure/admin-ui\": \"^2.3.2\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/admin-ui\": \"^2.3.3\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"express\": \"^4.18.3\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.4.2\"",
          "packages/admin-ui/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/admin-ui\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"scripts\": {\n         \"ng\": \"ng\",\n@@ -49,7 +49,7 @@\n         \"@ng-select/ng-select\": \"^12.0.7\",\n         \"@ngx-translate/core\": \"^15.0.0\",\n         \"@ngx-translate/http-loader\": \"^8.0.0\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"@webcomponents/custom-elements\": \"^1.6.0\",\n         \"apollo-angular\": \"^6.0.0\",\n         \"apollo-upload-client\": \"^18.0.1\",",
          "packages/admin-ui/src/lib/core/src/common/version.ts": "@@ -1,2 +1,2 @@\n // Auto-generated by the set-version.js script.\n-export const ADMIN_UI_VERSION = '2.3.2';\n+export const ADMIN_UI_VERSION = '2.3.3';",
          "packages/asset-server-plugin/e2e/asset-server-plugin.e2e-spec.ts": "@@ -1,7 +1,8 @@\n /* eslint-disable @typescript-eslint/no-non-null-assertion */\n-import { mergeConfig } from '@vendure/core';\n+import { ConfigService, mergeConfig } from '@vendure/core';\n import { AssetFragment } from '@vendure/core/e2e/graphql/generated-e2e-admin-types';\n import { createTestEnvironment } from '@vendure/testing';\n+import { exec } from 'child_process';\n import fs from 'fs-extra';\n import gql from 'graphql-tag';\n import fetch from 'node-fetch';\n@@ -193,6 +194,41 @@ describe('AssetServerPlugin', () => {\n         it('does not error on non-integer height', async () => {\n             return fetch(`${asset.preview}?h=10.5`);\n         });\n+\n+        // https://github.com/vendure-ecommerce/vendure/security/advisories/GHSA-r9mq-3c9r-fmjq\n+        describe('path traversal', () => {\n+            function curlWithPathAsIs(url: string) {\n+                return new Promise<string>((resolve, reject) => {\n+                    // We use curl here rather than node-fetch or any other fetch-type function because\n+                    // those will automatically perform path normalization which will mask the path traversal\n+                    return exec(`curl --path-as-is ${url}`, (err, stdout, stderr) => {\n+                        if (err) {\n+                            reject(err);\n+                        }\n+                        resolve(stdout);\n+                    });\n+                });\n+            }\n+\n+            function testPathTraversalOnUrl(urlPath: string) {\n+                return async () => {\n+                    const port = server.app.get(ConfigService).apiOptions.port;\n+                    const result = await curlWithPathAsIs(`http://localhost:${port}/assets${urlPath}`);\n+                    expect(result).not.toContain('@vendure/asset-server-plugin');\n+                    expect(result.toLowerCase()).toContain('resource not found');\n+                };\n+            }\n+\n+            it('blocks path traversal 1', testPathTraversalOnUrl(`/../../package.json`));\n+            it('blocks path traversal 2', testPathTraversalOnUrl(`/foo/../../../package.json`));\n+            it('blocks path traversal 3', testPathTraversalOnUrl(`/foo/../../../foo/../package.json`));\n+            it('blocks path traversal 4', testPathTraversalOnUrl(`/%2F..%2F..%2Fpackage.json`));\n+            it('blocks path traversal 5', testPathTraversalOnUrl(`/%2E%2E/%2E%2E/package.json`));\n+            it('blocks path traversal 6', testPathTraversalOnUrl(`/..//..//package.json`));\n+            it('blocks path traversal 7', testPathTraversalOnUrl(`/.%2F.%2F.%2Fpackage.json`));\n+            it('blocks path traversal 8', testPathTraversalOnUrl(`/..\\\\\\\\..\\\\\\\\package.json`));\n+            it('blocks path traversal 9', testPathTraversalOnUrl(`/\\\\\\\\\\\\..\\\\\\\\\\\\..\\\\\\\\\\\\package.json`));\n+        });\n     });\n \n     describe('deletion', () => {\n@@ -268,7 +304,7 @@ describe('AssetServerPlugin', () => {\n     // https://github.com/vendure-ecommerce/vendure/issues/1563\n     it('falls back to binary preview if image file cannot be processed', async () => {\n         const filesToUpload = [path.join(__dirname, 'fixtures/assets/bad-image.jpg')];\n-        const { createAssets }: CreateAssets.Mutation = await adminClient.fileUploadMutation({\n+        const { createAssets }: CreateAssetsMutation = await adminClient.fileUploadMutation({\n             mutation: CREATE_ASSETS,\n             filePaths: filesToUpload,\n             mapVariables: filePaths => ({",
          "packages/asset-server-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/asset-server-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n     \"files\": [\n@@ -26,8 +26,8 @@\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/node-fetch\": \"^2.6.11\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"express\": \"^4.18.3\",\n         \"node-fetch\": \"^2.7.0\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/asset-server-plugin/src/plugin.ts": "@@ -281,7 +281,7 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n         return async (err: any, req: Request, res: Response, next: NextFunction) => {\n             if (err && (err.status === 404 || err.statusCode === 404)) {\n                 if (req.query) {\n-                    const decodedReqPath = decodeURIComponent(req.path);\n+                    const decodedReqPath = this.sanitizeFilePath(req.path);\n                     Logger.debug(`Pre-cached Asset not found: ${decodedReqPath}`, loggerCtx);\n                     let file: Buffer;\n                     try {\n@@ -347,9 +347,7 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n             imageParamsString += quality;\n         }\n \n-        /* eslint-enable @typescript-eslint/restrict-template-expressions */\n-\n-        const decodedReqPath = decodeURIComponent(req.path);\n+        const decodedReqPath = this.sanitizeFilePath(req.path);\n         if (imageParamsString !== '') {\n             const imageParamHash = this.md5(imageParamsString);\n             return path.join(this.cacheDir, this.addSuffix(decodedReqPath, imageParamHash, imageFormat));\n@@ -358,6 +356,20 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n         }\n     }\n \n+    /**\n+     * Sanitize the file path to prevent directory traversal attacks.\n+     */\n+    private sanitizeFilePath(filePath: string): string {\n+        let decodedPath: string;\n+        try {\n+            decodedPath = decodeURIComponent(filePath);\n+        } catch (e: any) {\n+            Logger.error((e.message as string) + ': ' + filePath, loggerCtx);\n+            return '';\n+        }\n+        return path.normalize(decodedPath).replace(/(\\.\\.[\\/\\\\])+/, '');\n+    }\n+\n     private md5(input: string): string {\n         return createHash('md5').update(input).digest('hex');\n     }",
          "packages/cli/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/cli\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"description\": \"A modern, headless ecommerce framework\",\n     \"repository\": {\n         \"type\": \"git\",\n@@ -35,7 +35,7 @@\n     ],\n     \"dependencies\": {\n         \"@clack/prompts\": \"^0.7.0\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"change-case\": \"^4.1.2\",\n         \"commander\": \"^11.0.0\",\n         \"dotenv\": \"^16.4.5\",\n@@ -46,7 +46,7 @@\n         \"tsconfig-paths\": \"^4.2.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"typescript\": \"5.3.3\"\n     }\n }",
          "packages/common/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/common\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"main\": \"index.js\",\n     \"license\": \"MIT\",\n     \"scripts\": {",
          "packages/core/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/core\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"description\": \"A modern, headless ecommerce framework\",\n     \"repository\": {\n         \"type\": \"git\",\n@@ -51,7 +51,7 @@\n         \"@nestjs/testing\": \"~10.3.10\",\n         \"@nestjs/typeorm\": \"~10.0.2\",\n         \"@types/fs-extra\": \"^9.0.1\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"bcrypt\": \"^5.1.1\",\n         \"body-parser\": \"^1.20.2\",\n         \"cookie-session\": \"^2.1.0\",",
          "packages/create/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/create\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"bin\": {\n         \"create\": \"./index.js\"\n@@ -28,14 +28,14 @@\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/handlebars\": \"^4.1.0\",\n         \"@types/semver\": \"^7.5.8\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"rimraf\": \"^5.0.5\",\n         \"ts-node\": \"^10.9.2\",\n         \"typescript\": \"5.3.3\"\n     },\n     \"dependencies\": {\n         \"@clack/prompts\": \"^0.7.0\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"commander\": \"^11.0.0\",\n         \"cross-spawn\": \"^7.0.3\",\n         \"detect-port\": \"^1.5.1\",",
          "packages/dev-server/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"dev-server\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"main\": \"index.js\",\n     \"license\": \"MIT\",\n     \"private\": true,\n@@ -15,17 +15,17 @@\n     },\n     \"dependencies\": {\n         \"@nestjs/axios\": \"^3.0.2\",\n-        \"@vendure/admin-ui-plugin\": \"^2.3.2\",\n-        \"@vendure/asset-server-plugin\": \"^2.3.2\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n-        \"@vendure/elasticsearch-plugin\": \"^2.3.2\",\n-        \"@vendure/email-plugin\": \"^2.3.2\",\n+        \"@vendure/admin-ui-plugin\": \"^2.3.3\",\n+        \"@vendure/asset-server-plugin\": \"^2.3.3\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n+        \"@vendure/elasticsearch-plugin\": \"^2.3.3\",\n+        \"@vendure/email-plugin\": \"^2.3.3\",\n         \"typescript\": \"5.3.3\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/testing\": \"^2.3.2\",\n-        \"@vendure/ui-devkit\": \"^2.3.2\",\n+        \"@vendure/testing\": \"^2.3.3\",\n+        \"@vendure/ui-devkit\": \"^2.3.3\",\n         \"commander\": \"^12.0.0\",\n         \"concurrently\": \"^8.2.2\",\n         \"csv-stringify\": \"^6.4.6\",",
          "packages/elasticsearch-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/elasticsearch-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -26,8 +26,8 @@\n         \"fast-deep-equal\": \"^3.1.3\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.3.3\"\n     }",
          "packages/email-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/email-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -34,8 +34,8 @@\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/mjml\": \"^4.7.4\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.3.3\"\n     }",
          "packages/harden-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/harden-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -21,7 +21,7 @@\n         \"graphql-query-complexity\": \"^0.12.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\"\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\"\n     }\n }",
          "packages/job-queue-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/job-queue-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"package/index.js\",\n     \"types\": \"package/index.d.ts\",\n@@ -23,8 +23,8 @@\n     },\n     \"devDependencies\": {\n         \"@google-cloud/pubsub\": \"^2.8.0\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"bullmq\": \"^5.4.2\",\n         \"ioredis\": \"^5.3.2\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/payments-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/payments-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"package/index.js\",\n     \"types\": \"package/index.d.ts\",\n@@ -46,9 +46,9 @@\n         \"@mollie/api-client\": \"^3.7.0\",\n         \"@types/braintree\": \"^3.3.11\",\n         \"@types/localtunnel\": \"2.0.4\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n-        \"@vendure/testing\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\",\n+        \"@vendure/testing\": \"^2.3.3\",\n         \"braintree\": \"^3.22.0\",\n         \"localtunnel\": \"2.0.2\",\n         \"nock\": \"^13.1.4\",",
          "packages/sentry-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/sentry-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -22,7 +22,7 @@\n     },\n     \"devDependencies\": {\n         \"@sentry/node\": \"^7.106.1\",\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\"\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\"\n     }\n }",
          "packages/stellate-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/stellate-plugin\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"license\": \"MIT\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -21,7 +21,7 @@\n         \"node-fetch\": \"^2.7.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^2.3.2\",\n-        \"@vendure/core\": \"^2.3.2\"\n+        \"@vendure/common\": \"^2.3.3\",\n+        \"@vendure/core\": \"^2.3.3\"\n     }\n }",
          "packages/testing/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/testing\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"description\": \"End-to-end testing tools for Vendure projects\",\n     \"keywords\": [\n         \"vendure\",\n@@ -37,7 +37,7 @@\n     },\n     \"dependencies\": {\n         \"@graphql-typed-document-node/core\": \"^3.2.0\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"faker\": \"^4.1.0\",\n         \"form-data\": \"^4.0.0\",\n         \"graphql\": \"~16.9.0\",\n@@ -50,7 +50,7 @@\n         \"@types/mysql\": \"^2.15.26\",\n         \"@types/node-fetch\": \"^2.6.4\",\n         \"@types/pg\": \"^8.11.2\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"mysql\": \"^2.18.1\",\n         \"pg\": \"^8.11.3\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/ui-devkit/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/ui-devkit\",\n-    \"version\": \"2.3.2\",\n+    \"version\": \"2.3.3\",\n     \"description\": \"A library for authoring Vendure Admin UI extensions\",\n     \"keywords\": [\n         \"vendure\",\n@@ -40,8 +40,8 @@\n         \"@angular/cli\": \"^17.2.3\",\n         \"@angular/compiler\": \"^17.2.4\",\n         \"@angular/compiler-cli\": \"^17.2.4\",\n-        \"@vendure/admin-ui\": \"^2.3.2\",\n-        \"@vendure/common\": \"^2.3.2\",\n+        \"@vendure/admin-ui\": \"^2.3.3\",\n+        \"@vendure/common\": \"^2.3.3\",\n         \"chalk\": \"^4.1.0\",\n         \"chokidar\": \"^3.6.0\",\n         \"fs-extra\": \"^11.2.0\",\n@@ -52,7 +52,7 @@\n         \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n         \"@rollup/plugin-terser\": \"^0.4.4\",\n         \"@types/fs-extra\": \"^11.0.4\",\n-        \"@vendure/core\": \"^2.3.2\",\n+        \"@vendure/core\": \"^2.3.3\",\n         \"react\": \"^18.2.0\",\n         \"react-dom\": \"^18.2.0\",\n         \"rimraf\": \"^5.0.5\","
        }
      },
      {
        "package": {
          "ecosystem": "npm",
          "name": "@vendure/asset-server-plugin"
        },
        "vulnerable_version_range": ">= 3.0.0, < 3.0.5",
        "first_patched_version": "3.0.5",
        "vulnerable_functions": [],
        "vulnerable_version": "3.0.4",
        "patches": {
          "CHANGELOG.md": "@@ -1,3 +1,18 @@\n+## <small>3.0.5 (2024-10-15)</small>\n+\n+\n+#### Fixes\n+\n+* **admin-ui** Fix theme & ui language switcher ([c93589b](https://github.com/vendure-ecommerce/vendure/commit/c93589b)), closes [#3111](https://github.com/vendure-ecommerce/vendure/issues/3111)\n+* **core** Do not include deleted variants when indexing productInStock (#3110) ([73cb190](https://github.com/vendure-ecommerce/vendure/commit/73cb190)), closes [#3110](https://github.com/vendure-ecommerce/vendure/issues/3110) [#3109](https://github.com/vendure-ecommerce/vendure/issues/3109)\n+* **core** Fix coupon code validation across multiple channels ([e57cc1b](https://github.com/vendure-ecommerce/vendure/commit/e57cc1b)), closes [#2052](https://github.com/vendure-ecommerce/vendure/issues/2052)\n+* **core** Fix filtering on list queries of tree entities ([227da05](https://github.com/vendure-ecommerce/vendure/commit/227da05)), closes [#3107](https://github.com/vendure-ecommerce/vendure/issues/3107)\n+* **core** Improve error message on populating without tax rates ([7e36131](https://github.com/vendure-ecommerce/vendure/commit/7e36131)), closes [#1926](https://github.com/vendure-ecommerce/vendure/issues/1926)\n+\n+#### Features\n+\n+* **create** Improved getting started experience (#3128) ([adb4384](https://github.com/vendure-ecommerce/vendure/commit/adb4384)), closes [#3128](https://github.com/vendure-ecommerce/vendure/issues/3128)\n+\n ## <small>3.0.4 (2024-10-04)</small>\n \n \n@@ -8,7 +23,6 @@\n * **admin-ui** Add support for custom fields on CustomerGroup list ([7128a33](https://github.com/vendure-ecommerce/vendure/commit/7128a33))\n * **admin-ui** Enable selective loading of custom fields ([9d7744b](https://github.com/vendure-ecommerce/vendure/commit/9d7744b)), closes [#3097](https://github.com/vendure-ecommerce/vendure/issues/3097)\n * **admin-ui** Fix bad locale detection regex ([f336d7f](https://github.com/vendure-ecommerce/vendure/commit/f336d7f))\n-* **admin-ui** Fix import errors ([2c60761](https://github.com/vendure-ecommerce/vendure/commit/2c60761))\n * **admin-ui** Lazy-load only selected custom fields in list views ([690dd0f](https://github.com/vendure-ecommerce/vendure/commit/690dd0f)), closes [#3097](https://github.com/vendure-ecommerce/vendure/issues/3097)\n * **admin-ui** Unsubscribe from alerts when logging out (#3071) ([f38340b](https://github.com/vendure-ecommerce/vendure/commit/f38340b)), closes [#3071](https://github.com/vendure-ecommerce/vendure/issues/3071) [#2188](https://github.com/vendure-ecommerce/vendure/issues/2188)\n * **asset-server-plugin** Do not return raw error message on error ([801980e](https://github.com/vendure-ecommerce/vendure/commit/801980e))\n@@ -23,7 +37,8 @@\n \n #### Perf\n \n-* **core** Fix performance when using FacetValue-based checks ([a735bdf](https://github.com/vendure-ecommerce/vendure/commit/a735bdf)), closes [#3075](https://github.com/vendure-ecommerce/vendure/issues/3075)\n+* **core** Fix performance when using FacetValue-based checks ([a735bdf](https://github.com/vendure-ecommerce/vendure/commit/a735bdf))\n+* **admin-ui** List views only load the visible custom fields, closes [#3097](https://github.com/vendure-ecommerce/vendure/issues/3097)\n \n ## <small>3.0.3 (2024-09-11)</small>\n ",
          "docs/docs/guides/developer-guide/security/index.md": "@@ -0,0 +1,235 @@\n+---\n+title: \"Security\"\n+---\n+\n+Security of your Vendure application includes considering how to prevent and protect against common security threats such as:\n+\n+- Data breaches\n+- Unauthorized access\n+- Attacks aimed at disrupting the service\n+\n+Vendure itself is designed with security in mind, but you must also consider the security of your own application code, the server environment, and the network architecture.\n+\n+## Basics\n+\n+Here are some basic measures you should use to secure your Vendure application. These are not exhaustive, but they are a good starting point.\n+\n+### Change the default credentials\n+\n+Do not deploy any public Vendure instance with the default superadmin credentials (`superadmin:superadmin`). Use your hosting platform's environment variables to set a **strong** password for the Superadmin account.\n+\n+```ts\n+import { VendureConfig } from '@vendure/core';\n+\n+export const config: VendureConfig = {\n+  authOptions: {\n+    tokenMethod: ['bearer', 'cookie'],\n+    superadminCredentials: {\n+      identifier: process.env.SUPERADMIN_USERNAME,\n+      password: process.env.SUPERADMIN_PASSWORD,\n+    },\n+  },\n+  // ...\n+};\n+```\n+\n+### Use the HardenPlugin\n+\n+It is recommended that you install and configure the [HardenPlugin](/reference/core-plugins/harden-plugin/) for all production deployments. This plugin locks down your schema \n+(disabling introspection and field suggestions) and protects your Shop API against malicious queries that could otherwise overwhelm your server.\n+\n+Install the plugin:\n+\n+```sh\n+npm install @vendure/harden-plugin\n+\n+# or\n+\n+yarn add @vendure/harden-plugin\n+```\n+\n+Then add it to your VendureConfig:\n+\n+```ts\n+import { VendureConfig } from '@vendure/core';\n+import { HardenPlugin } from '@vendure/harden-plugin';\n+\n+const IS_DEV = process.env.APP_ENV === 'dev';\n+\n+export const config: VendureConfig = {\n+  // ...\n+  plugins: [\n+    HardenPlugin.init({\n+      maxQueryComplexity: 500,\n+      apiMode: IS_DEV ? 'dev' : 'prod',\n+    }),\n+    // ...\n+  ]\n+};\n+```\n+\n+:::info\n+For a detailed explanation of how to best configure this plugin, see the [HardenPlugin docs](/reference/core-plugins/harden-plugin/).\n+:::\n+\n+\n+## OWASP Top Ten Security Assessment\n+\n+The Open Worldwide Application Security Project (OWASP) is a nonprofit foundation that works to improve the security of software.\n+\n+It publishes a top 10 list of common web application vulnerabilities: https://owasp.org/Top10\n+\n+This section assesses Vendure against this list, stating what is covered **out of the box** (built in to the framework or easily configurable) and what needs to be **additionally considered.**\n+\n+### 1. Broken Access Control\n+\n+Reference: https://owasp.org/Top10/A01_2021-Broken_Access_Control/\n+\n+Out of the box:\n+\n+- Vendure uses role-based access control\n+- We deny by default for non-public API requests\n+- Built-in CORS controls for session cookies\n+- Directory listing is not possible via default configuration (e.g. exposing web root dir contents)\n+- Stateful session identifiers should be invalidated on the server after logout. On logout we delete all session records from the DB & session cache.\n+\n+To consider:\n+\n+- Rate limit API and controller access to minimize the harm from automated attack tooling.\n+\n+### 2. Cryptographic Failures\n+\n+Reference: https://owasp.org/Top10/A02_2021-Cryptographic_Failures/\n+\n+Out of the box:\n+\n+- Vendure defaults to bcrypt with 12 salt rounds for storing passwords. This strategy is configurable if security requirements mandate alternative algorithms.\n+- No deprecated hash functions (SHA1, MD5) are used in security-related contexts (only for things like creating cache keys).\n+- Payment information is not stored in Vendure by default. Payment integrations rely on the payment provider to store all sensitive data.\n+\n+To consider:\n+\n+- The Vendure server will not use TLS be default. The usual configuration is to handle this at the gateway level on your production platform.\n+- If a network caching layer is used (e.g. Stellate), ensure it is configured to not cache user-related data (customer details, active order etc)\n+\n+### 3. Injection\n+\n+Reference: https://owasp.org/Top10/A03_2021-Injection/\n+\n+Out of the box:\n+\n+- GraphQL has built-in validation of incoming data\n+- All database operations are parameterized - no string concatenation using user-supplied data.\n+- List queries apply default limits to prevent mass disclosure of records.\n+\n+To consider:\n+\n+- If using custom fields, you should consider defining a validation function to prevent bad data from getting into the database.\n+\n+### 4. Insecure Design\n+\n+Reference: https://owasp.org/Top10/A04_2021-Insecure_Design/\n+\n+Out of the box:\n+\n+- Use of established libraries for the critical underlying components: NestJS, TypeORM, Angular.\n+- End-to-end tests of security-related flows such as authentication, verification, and RBAC permissions controls.\n+- Harden plugin provides pre-configured protections against common attack vectors targeting GraphQL APIs.\n+\n+To consider:\n+\n+- Tiered exposure such as an API gateway which prevents exposure of the Admin API to the public internet.\n+- Limit resource usage of Vendure server & worker instances via containerization.\n+- Rate limiting & other network-level protections (such as Cloudflare) should be considered.\n+\n+### 5. Security Misconfiguration\n+\n+Reference: https://owasp.org/Top10/A05_2021-Security_Misconfiguration/\n+\n+Out of the box:\n+\n+- Single point of configuration for the entire application, reducing the chance of misconfiguration.\n+- A default setup only requires a database, which means there are few components to configure and harden.\n+- Stack traces are not leaked in API errors\n+\n+To consider:\n+\n+- Ensure the default superadmin credentials are not used in production\n+- Use environment variables to turn off development features such as the GraphQL playground\n+- Use the HardenPlugin in production to automatically turn of development features and restrict system information leaking via API.\n+- Use fine-grained permissions and roles for your administrator accounts to reduce the attack surface if an account is compromised.\n+\n+### 6. Vulnerable and Outdated Components\n+\n+Reference: https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/\n+\n+Out of the box:\n+\n+- All dependencies are updated to current versions with each minor release\n+- Modular design limits the number of dependencies for core packages.\n+- Automated code & dependency scanning is used in the Vendure repo\n+\n+To consider:\n+\n+- Run your own audits on your code base.\n+- Use version override mechanisms if needed to patch and critical Vendure dependencies that did not yet get updated.\n+\n+### 7. Identification and Authentication Failures\n+\n+Reference: https://owasp.org/Top10/A07_2021-Identification_and_Authentication_Failures/\n+\n+Out of the box:\n+\n+- Valid usernames are not leaked via mechanisms such as account reset\n+- Does not permit \"knowlege-based\" account recovery\n+- Uses strong password hashing (bcrypt with 12 salt rounds)\n+- Session identifiers are not exposed in API urls (instead we use headers/cookies)\n+- New session tokens always regenerated after successful login\n+- Sessions deleted during logout\n+- Cryptographically-strong, high-entropy session tokens are used (crypto.randomBytes API)\n+\n+To consider:\n+\n+- Implementing a multi-factor authentication flow\n+- Do not use default superadmin credentials in production\n+- Implementing a custom PasswordValidationStrategy to disallow weak/common passwords\n+- Subscribe to AttemptedLoginEvent to implement detection of brute-force attacks\n+\n+### 8. Software and Data Integrity Failures\n+\n+Reference: https://owasp.org/Top10/A08_2021-Software_and_Data_Integrity_Failures/\n+\n+To consider:\n+\n+- Exercise caution when introducing new dependencies to your project.\n+- Do not use untrusted Vendure plugins. Where possible review the code prior to use.\n+- Exercise caution if using auto-updating mechanisms for dependencies.\n+- If storing serialized data in custom fields, implement validation to prevent untrusted data getting into the database.\n+- Evaluate your CI/CD pipeline against the OWASP recommendations for this point\n+\n+### 9. Security Logging and Monitoring Failures\n+\n+Reference: https://owasp.org/Top10/A09_2021-Security_Logging_and_Monitoring_Failures/\n+\n+Out of the box:\n+\n+- APIs for integrating logging & monitoring tools & services, e.g. configurable Logger interface & ErrorHandlerStrategy\n+- Official Sentry integration for application performance monitoring\n+\n+To consider:\n+\n+- Integrate with dedicated logging tools for improved log management\n+- Integrate with monitoring tools such as Sentry\n+- Use the EventBus to monitor events such as repeated failed login attempts and high-value orders\n+\n+### 10. Server-Side Request Forgery (SSRF)\n+\n+Reference: [https://owasp.org/Top10/A10_2021-Server-Side_Request_Forgery_(SSRF)/](https://owasp.org/Top10/A10_2021-Server-Side_Request_Forgery_%28SSRF%29/)\n+\n+Out of the box:\n+\n+- By default Vendure does not rely on requests to remote servers for core functionality\n+\n+To consider:\n+\n+- Review the OWASP recommendations against your network architecture",
          "docs/docs/guides/getting-started/installation/index.md": "@@ -66,8 +66,6 @@ Follow the instructions to move into the new directory created for your project,\n ```bash\n cd my-shop\n \n-yarn dev\n-# or\n npm run dev\n ```\n ",
          "docs/sidebars.js": "@@ -72,6 +72,7 @@ const sidebars = {\n                 'guides/developer-guide/events/index',\n                 'guides/developer-guide/migrations/index',\n                 'guides/developer-guide/plugins/index',\n+                'guides/developer-guide/security/index',\n                 'guides/developer-guide/strategies-configurable-operations/index',\n                 'guides/developer-guide/testing/index',\n                 'guides/developer-guide/updating/index',",
          "lerna.json": "@@ -1,6 +1,6 @@\n {\n     \"packages\": [\"packages/*\"],\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"npmClient\": \"npm\",\n     \"command\": {\n         \"version\": {",
          "license/signatures/version1/cla.json": "@@ -191,6 +191,22 @@\n       \"created_at\": \"2024-09-30T12:27:50Z\",\n       \"repoId\": 136938012,\n       \"pullRequestNo\": 3096\n+    },\n+    {\n+      \"name\": \"kyunal\",\n+      \"id\": 33372279,\n+      \"comment_id\": 2395056311,\n+      \"created_at\": \"2024-10-05T13:21:30Z\",\n+      \"repoId\": 136938012,\n+      \"pullRequestNo\": 3110\n+    },\n+    {\n+      \"name\": \"LeftoversTodayAppAdmin\",\n+      \"id\": 139936478,\n+      \"comment_id\": 2395622489,\n+      \"created_at\": \"2024-10-06T23:06:02Z\",\n+      \"repoId\": 136938012,\n+      \"pullRequestNo\": 3112\n     }\n   ]\n }\n\\ No newline at end of file",
          "package-lock.json": "@@ -28269,7 +28269,6 @@\n             \"version\": \"8.4.2\",\n             \"resolved\": \"https://registry.npmjs.org/open/-/open-8.4.2.tgz\",\n             \"integrity\": \"sha512-7x81NCL719oNbsq/3mh+hVrAWmFuEYUqrq/Iw3kUzH8ReypT9QQ0BLoJS7/G9k6N81XjW4qHWtjWwe/9eLy1EQ==\",\n-            \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"define-lazy-prop\": \"^2.0.0\",\n                 \"is-docker\": \"^2.1.1\",\n@@ -36483,7 +36482,7 @@\n         },\n         \"packages/admin-ui\": {\n             \"name\": \"@vendure/admin-ui\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@angular/animations\": \"^17.2.4\",\n@@ -36506,7 +36505,7 @@\n                 \"@ng-select/ng-select\": \"^12.0.7\",\n                 \"@ngx-translate/core\": \"^15.0.0\",\n                 \"@ngx-translate/http-loader\": \"^8.0.0\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"@webcomponents/custom-elements\": \"^1.6.0\",\n                 \"apollo-angular\": \"^6.0.0\",\n                 \"apollo-upload-client\": \"^18.0.1\",\n@@ -36577,7 +36576,7 @@\n         },\n         \"packages/admin-ui-plugin\": {\n             \"name\": \"@vendure/admin-ui-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"date-fns\": \"^2.30.0\",\n@@ -36587,9 +36586,9 @@\n             \"devDependencies\": {\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n-                \"@vendure/admin-ui\": \"^3.0.4\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/admin-ui\": \"^3.0.5\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"express\": \"^4.18.3\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.4.2\"\n@@ -36631,7 +36630,7 @@\n         },\n         \"packages/asset-server-plugin\": {\n             \"name\": \"@vendure/asset-server-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"file-type\": \"^19.0.0\",\n@@ -36644,8 +36643,8 @@\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/node-fetch\": \"^2.6.11\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"express\": \"^4.18.3\",\n                 \"node-fetch\": \"^2.7.0\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -36657,11 +36656,11 @@\n         },\n         \"packages/cli\": {\n             \"name\": \"@vendure/cli\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@clack/prompts\": \"^0.7.0\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"change-case\": \"^4.1.2\",\n                 \"commander\": \"^11.0.0\",\n                 \"dotenv\": \"^16.4.5\",\n@@ -36675,7 +36674,7 @@\n                 \"vendure\": \"dist/cli.js\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n             \"funding\": {\n@@ -36707,7 +36706,7 @@\n         },\n         \"packages/common\": {\n             \"name\": \"@vendure/common\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"devDependencies\": {\n                 \"rimraf\": \"^5.0.5\",\n@@ -36719,7 +36718,7 @@\n         },\n         \"packages/core\": {\n             \"name\": \"@vendure/core\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@apollo/server\": \"^4.10.4\",\n@@ -36733,7 +36732,7 @@\n                 \"@nestjs/testing\": \"~10.3.10\",\n                 \"@nestjs/typeorm\": \"~10.0.2\",\n                 \"@types/fs-extra\": \"^9.0.1\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"bcrypt\": \"^5.1.1\",\n                 \"body-parser\": \"^1.20.2\",\n                 \"cookie-session\": \"^2.1.0\",\n@@ -36828,15 +36827,16 @@\n         },\n         \"packages/create\": {\n             \"name\": \"@vendure/create\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@clack/prompts\": \"^0.7.0\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"commander\": \"^11.0.0\",\n                 \"cross-spawn\": \"^7.0.3\",\n                 \"fs-extra\": \"^11.2.0\",\n                 \"handlebars\": \"^4.7.8\",\n+                \"open\": \"^8.4.2\",\n                 \"picocolors\": \"^1.0.0\",\n                 \"semver\": \"^7.5.4\",\n                 \"tcp-port-used\": \"^1.0.2\"\n@@ -36849,7 +36849,7 @@\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/handlebars\": \"^4.1.0\",\n                 \"@types/semver\": \"^7.5.8\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"ts-node\": \"^10.9.2\",\n                 \"typescript\": \"5.3.3\"\n@@ -36859,21 +36859,21 @@\n             }\n         },\n         \"packages/dev-server\": {\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@nestjs/axios\": \"^3.0.2\",\n-                \"@vendure/admin-ui-plugin\": \"^3.0.4\",\n-                \"@vendure/asset-server-plugin\": \"^3.0.4\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n-                \"@vendure/elasticsearch-plugin\": \"^3.0.4\",\n-                \"@vendure/email-plugin\": \"^3.0.4\",\n+                \"@vendure/admin-ui-plugin\": \"^3.0.5\",\n+                \"@vendure/asset-server-plugin\": \"^3.0.5\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n+                \"@vendure/elasticsearch-plugin\": \"^3.0.5\",\n+                \"@vendure/email-plugin\": \"^3.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/testing\": \"^3.0.4\",\n-                \"@vendure/ui-devkit\": \"^3.0.4\",\n+                \"@vendure/testing\": \"^3.0.5\",\n+                \"@vendure/ui-devkit\": \"^3.0.5\",\n                 \"commander\": \"^12.0.0\",\n                 \"concurrently\": \"^8.2.2\",\n                 \"csv-stringify\": \"^6.4.6\",\n@@ -36916,16 +36916,16 @@\n         },\n         \"packages/elasticsearch-plugin\": {\n             \"name\": \"@vendure/elasticsearch-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@elastic/elasticsearch\": \"~7.9.1\",\n                 \"deepmerge\": \"^4.2.2\",\n                 \"fast-deep-equal\": \"^3.1.3\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n@@ -36935,7 +36935,7 @@\n         },\n         \"packages/email-plugin\": {\n             \"name\": \"@vendure/email-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@types/nodemailer\": \"^6.4.9\",\n@@ -36951,8 +36951,8 @@\n                 \"@types/express\": \"^4.17.21\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n                 \"@types/mjml\": \"^4.7.4\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"rimraf\": \"^5.0.5\",\n                 \"typescript\": \"5.3.3\"\n             },\n@@ -36971,27 +36971,27 @@\n         },\n         \"packages/harden-plugin\": {\n             \"name\": \"@vendure/harden-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"graphql-query-complexity\": \"^0.12.0\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\"\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n             }\n         },\n         \"packages/job-queue-plugin\": {\n             \"name\": \"@vendure/job-queue-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"devDependencies\": {\n                 \"@google-cloud/pubsub\": \"^2.8.0\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"bullmq\": \"^5.4.2\",\n                 \"ioredis\": \"^5.3.2\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -37003,7 +37003,7 @@\n         },\n         \"packages/payments-plugin\": {\n             \"name\": \"@vendure/payments-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"currency.js\": \"2.0.4\"\n@@ -37012,9 +37012,9 @@\n                 \"@mollie/api-client\": \"^3.7.0\",\n                 \"@types/braintree\": \"^3.3.11\",\n                 \"@types/localtunnel\": \"2.0.4\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n-                \"@vendure/testing\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\",\n+                \"@vendure/testing\": \"^3.0.5\",\n                 \"braintree\": \"^3.22.0\",\n                 \"localtunnel\": \"2.0.2\",\n                 \"nock\": \"^13.1.4\",\n@@ -37058,12 +37058,12 @@\n         },\n         \"packages/sentry-plugin\": {\n             \"name\": \"@vendure/sentry-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"devDependencies\": {\n                 \"@sentry/node\": \"^7.106.1\",\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\"\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n@@ -37074,26 +37074,26 @@\n         },\n         \"packages/stellate-plugin\": {\n             \"name\": \"@vendure/stellate-plugin\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"node-fetch\": \"^2.7.0\"\n             },\n             \"devDependencies\": {\n-                \"@vendure/common\": \"^3.0.4\",\n-                \"@vendure/core\": \"^3.0.4\"\n+                \"@vendure/common\": \"^3.0.5\",\n+                \"@vendure/core\": \"^3.0.5\"\n             },\n             \"funding\": {\n                 \"url\": \"https://github.com/sponsors/michaelbromley\"\n             }\n         },\n         \"packages/testing\": {\n             \"name\": \"@vendure/testing\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@graphql-typed-document-node/core\": \"^3.2.0\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"faker\": \"^4.1.0\",\n                 \"form-data\": \"^4.0.0\",\n                 \"graphql\": \"~16.9.0\",\n@@ -37106,7 +37106,7 @@\n                 \"@types/mysql\": \"^2.15.26\",\n                 \"@types/node-fetch\": \"^2.6.4\",\n                 \"@types/pg\": \"^8.11.2\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"mysql\": \"^2.18.1\",\n                 \"pg\": \"^8.11.3\",\n                 \"rimraf\": \"^5.0.5\",\n@@ -37124,15 +37124,15 @@\n         },\n         \"packages/ui-devkit\": {\n             \"name\": \"@vendure/ui-devkit\",\n-            \"version\": \"3.0.4\",\n+            \"version\": \"3.0.5\",\n             \"license\": \"GPL-3.0-or-later\",\n             \"dependencies\": {\n                 \"@angular-devkit/build-angular\": \"^17.2.3\",\n                 \"@angular/cli\": \"^17.2.3\",\n                 \"@angular/compiler\": \"^17.2.4\",\n                 \"@angular/compiler-cli\": \"^17.2.4\",\n-                \"@vendure/admin-ui\": \"^3.0.4\",\n-                \"@vendure/common\": \"^3.0.4\",\n+                \"@vendure/admin-ui\": \"^3.0.5\",\n+                \"@vendure/common\": \"^3.0.5\",\n                 \"chalk\": \"^4.1.0\",\n                 \"chokidar\": \"^3.6.0\",\n                 \"fs-extra\": \"^11.2.0\",\n@@ -37143,7 +37143,7 @@\n                 \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n                 \"@rollup/plugin-terser\": \"^0.4.4\",\n                 \"@types/fs-extra\": \"^11.0.4\",\n-                \"@vendure/core\": \"^3.0.4\",\n+                \"@vendure/core\": \"^3.0.5\",\n                 \"react\": \"^18.2.0\",\n                 \"react-dom\": \"^18.2.0\",\n                 \"rimraf\": \"^5.0.5\",",
          "packages/admin-ui-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/admin-ui-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n     \"files\": [\n@@ -21,9 +21,9 @@\n     \"devDependencies\": {\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n-        \"@vendure/admin-ui\": \"^3.0.4\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/admin-ui\": \"^3.0.5\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"express\": \"^4.18.3\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.4.2\"",
          "packages/admin-ui/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/admin-ui\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"scripts\": {\n         \"ng\": \"ng\",\n@@ -49,7 +49,7 @@\n         \"@ng-select/ng-select\": \"^12.0.7\",\n         \"@ngx-translate/core\": \"^15.0.0\",\n         \"@ngx-translate/http-loader\": \"^8.0.0\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"@webcomponents/custom-elements\": \"^1.6.0\",\n         \"apollo-angular\": \"^6.0.0\",\n         \"apollo-upload-client\": \"^18.0.1\",",
          "packages/admin-ui/src/lib/core/src/common/version.ts": "@@ -1,2 +1,2 @@\n // Auto-generated by the set-version.js script.\n-export const ADMIN_UI_VERSION = '3.0.4';\n+export const ADMIN_UI_VERSION = '3.0.5';",
          "packages/admin-ui/src/lib/core/src/data/query-result.ts": "@@ -3,7 +3,17 @@ import { notNullOrUndefined } from '@vendure/common/lib/shared-utils';\n import { Apollo, QueryRef } from 'apollo-angular';\n import { DocumentNode } from 'graphql';\n import { merge, Observable, Subject, Subscription } from 'rxjs';\n-import { distinctUntilChanged, filter, finalize, map, skip, take, takeUntil } from 'rxjs/operators';\n+import {\n+    distinctUntilChanged,\n+    filter,\n+    finalize,\n+    map,\n+    shareReplay,\n+    skip,\n+    startWith,\n+    take,\n+    takeUntil,\n+} from 'rxjs/operators';\n \n import { CustomFieldConfig, GetUserStatusQuery } from '../common/generated-types';\n \n@@ -194,7 +204,9 @@ export class QueryResult<T, V extends Record<string, any> = Record<string, any>>\n                 this.subscribeToQueryRef(this.queryRef);\n                 this.queryRefSubscribed.set(this.queryRef, true);\n             }\n-            this.valueChangeSubject.subscribe(subscriber);\n+            this.valueChangeSubject\n+                .pipe(startWith(this.queryRef.getCurrentResult()), shareReplay(1))\n+                .subscribe(subscriber);\n             return () => {\n                 this.queryRefSubscribed.delete(this.queryRef);\n             };",
          "packages/asset-server-plugin/e2e/asset-server-plugin.e2e-spec.ts": "@@ -1,7 +1,8 @@\n /* eslint-disable @typescript-eslint/no-non-null-assertion */\n-import { mergeConfig } from '@vendure/core';\n+import { ConfigService, mergeConfig } from '@vendure/core';\n import { AssetFragment } from '@vendure/core/e2e/graphql/generated-e2e-admin-types';\n import { createTestEnvironment } from '@vendure/testing';\n+import { exec } from 'child_process';\n import fs from 'fs-extra';\n import gql from 'graphql-tag';\n import fetch from 'node-fetch';\n@@ -193,6 +194,41 @@ describe('AssetServerPlugin', () => {\n         it('does not error on non-integer height', async () => {\n             return fetch(`${asset.preview}?h=10.5`);\n         });\n+\n+        // https://github.com/vendure-ecommerce/vendure/security/advisories/GHSA-r9mq-3c9r-fmjq\n+        describe('path traversal', () => {\n+            function curlWithPathAsIs(url: string) {\n+                return new Promise<string>((resolve, reject) => {\n+                    // We use curl here rather than node-fetch or any other fetch-type function because\n+                    // those will automatically perform path normalization which will mask the path traversal\n+                    return exec(`curl --path-as-is ${url}`, (err, stdout, stderr) => {\n+                        if (err) {\n+                            reject(err);\n+                        }\n+                        resolve(stdout);\n+                    });\n+                });\n+            }\n+\n+            function testPathTraversalOnUrl(urlPath: string) {\n+                return async () => {\n+                    const port = server.app.get(ConfigService).apiOptions.port;\n+                    const result = await curlWithPathAsIs(`http://localhost:${port}/assets${urlPath}`);\n+                    expect(result).not.toContain('@vendure/asset-server-plugin');\n+                    expect(result.toLowerCase()).toContain('resource not found');\n+                };\n+            }\n+\n+            it('blocks path traversal 1', testPathTraversalOnUrl(`/../../package.json`));\n+            it('blocks path traversal 2', testPathTraversalOnUrl(`/foo/../../../package.json`));\n+            it('blocks path traversal 3', testPathTraversalOnUrl(`/foo/../../../foo/../package.json`));\n+            it('blocks path traversal 4', testPathTraversalOnUrl(`/%2F..%2F..%2Fpackage.json`));\n+            it('blocks path traversal 5', testPathTraversalOnUrl(`/%2E%2E/%2E%2E/package.json`));\n+            it('blocks path traversal 6', testPathTraversalOnUrl(`/..//..//package.json`));\n+            it('blocks path traversal 7', testPathTraversalOnUrl(`/.%2F.%2F.%2Fpackage.json`));\n+            it('blocks path traversal 8', testPathTraversalOnUrl(`/..\\\\\\\\..\\\\\\\\package.json`));\n+            it('blocks path traversal 9', testPathTraversalOnUrl(`/\\\\\\\\\\\\..\\\\\\\\\\\\..\\\\\\\\\\\\package.json`));\n+        });\n     });\n \n     describe('deletion', () => {\n@@ -268,7 +304,7 @@ describe('AssetServerPlugin', () => {\n     // https://github.com/vendure-ecommerce/vendure/issues/1563\n     it('falls back to binary preview if image file cannot be processed', async () => {\n         const filesToUpload = [path.join(__dirname, 'fixtures/assets/bad-image.jpg')];\n-        const { createAssets }: CreateAssets.Mutation = await adminClient.fileUploadMutation({\n+        const { createAssets }: CreateAssetsMutation = await adminClient.fileUploadMutation({\n             mutation: CREATE_ASSETS,\n             filePaths: filesToUpload,\n             mapVariables: filePaths => ({",
          "packages/asset-server-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/asset-server-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n     \"files\": [\n@@ -26,8 +26,8 @@\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/node-fetch\": \"^2.6.11\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"express\": \"^4.18.3\",\n         \"node-fetch\": \"^2.7.0\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/asset-server-plugin/src/plugin.ts": "@@ -281,7 +281,7 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n         return async (err: any, req: Request, res: Response, next: NextFunction) => {\n             if (err && (err.status === 404 || err.statusCode === 404)) {\n                 if (req.query) {\n-                    const decodedReqPath = decodeURIComponent(req.path);\n+                    const decodedReqPath = this.sanitizeFilePath(req.path);\n                     Logger.debug(`Pre-cached Asset not found: ${decodedReqPath}`, loggerCtx);\n                     let file: Buffer;\n                     try {\n@@ -347,9 +347,7 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n             imageParamsString += quality;\n         }\n \n-        /* eslint-enable @typescript-eslint/restrict-template-expressions */\n-\n-        const decodedReqPath = decodeURIComponent(req.path);\n+        const decodedReqPath = this.sanitizeFilePath(req.path);\n         if (imageParamsString !== '') {\n             const imageParamHash = this.md5(imageParamsString);\n             return path.join(this.cacheDir, this.addSuffix(decodedReqPath, imageParamHash, imageFormat));\n@@ -358,6 +356,20 @@ export class AssetServerPlugin implements NestModule, OnApplicationBootstrap {\n         }\n     }\n \n+    /**\n+     * Sanitize the file path to prevent directory traversal attacks.\n+     */\n+    private sanitizeFilePath(filePath: string): string {\n+        let decodedPath: string;\n+        try {\n+            decodedPath = decodeURIComponent(filePath);\n+        } catch (e: any) {\n+            Logger.error((e.message as string) + ': ' + filePath, loggerCtx);\n+            return '';\n+        }\n+        return path.normalize(decodedPath).replace(/(\\.\\.[\\/\\\\])+/, '');\n+    }\n+\n     private md5(input: string): string {\n         return createHash('md5').update(input).digest('hex');\n     }",
          "packages/cli/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/cli\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"description\": \"A modern, headless ecommerce framework\",\n     \"repository\": {\n         \"type\": \"git\",\n@@ -35,7 +35,7 @@\n     ],\n     \"dependencies\": {\n         \"@clack/prompts\": \"^0.7.0\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"change-case\": \"^4.1.2\",\n         \"commander\": \"^11.0.0\",\n         \"dotenv\": \"^16.4.5\",\n@@ -46,7 +46,7 @@\n         \"tsconfig-paths\": \"^4.2.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"typescript\": \"5.3.3\"\n     }\n }",
          "packages/common/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/common\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"main\": \"index.js\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"scripts\": {",
          "packages/core/e2e/collection.e2e-spec.ts": "@@ -735,6 +735,21 @@ describe('Collection resolver', () => {\n                 139900, 219900, 229900,\n             ]);\n         });\n+\n+        // https://github.com/vendure-ecommerce/vendure/issues/3107\n+        it('collection list with translations, filtered by name', async () => {\n+            const { collections } = await adminClient.query(GET_COLLECTION_LIST_WITH_TRANSLATIONS, {\n+                options: {\n+                    filter: {\n+                        name: {\n+                            eq: 'Electronics',\n+                        },\n+                    },\n+                },\n+            });\n+\n+            expect(collections.items.length).toBeDefined();\n+        });\n     });\n \n     describe('moveCollection', () => {\n@@ -2413,6 +2428,21 @@ export const GET_COLLECTION_LIST = gql`\n     ${COLLECTION_FRAGMENT}\n `;\n \n+export const GET_COLLECTION_LIST_WITH_TRANSLATIONS = gql`\n+    query GetCollectionListWithTranslations($options: CollectionListOptions) {\n+        collections(options: $options) {\n+            items {\n+                id\n+                name\n+                translations {\n+                    id\n+                    name\n+                }\n+            }\n+        }\n+    }\n+`;\n+\n export const MOVE_COLLECTION = gql`\n     mutation MoveCollection($input: MoveCollectionInput!) {\n         moveCollection(input: $input) {",
          "packages/core/e2e/list-query-builder.e2e-spec.ts": "@@ -56,14 +56,16 @@ describe('ListQueryBuilder', () => {\n \n             expect(testEntities.totalItems).toBe(6);\n             expect(getItemLabels(testEntities.items)).toEqual(['A', 'B', 'C', 'D', 'E', 'F']);\n-            expect(testEntities.items.map((i: any) => i.name)).toEqual(expect.arrayContaining([\n-                'apple',\n-                'bike',\n-                'cake',\n-                'dog',\n-                'egg',\n-                'baum', // if default en lang does not exist, use next available lang\n-            ]));\n+            expect(testEntities.items.map((i: any) => i.name)).toEqual(\n+                expect.arrayContaining([\n+                    'apple',\n+                    'bike',\n+                    'cake',\n+                    'dog',\n+                    'egg',\n+                    'baum', // if default en lang does not exist, use next available lang\n+                ]),\n+            );\n         });\n \n         it('all de', async () => {\n@@ -77,14 +79,16 @@ describe('ListQueryBuilder', () => {\n \n             expect(testEntities.totalItems).toBe(6);\n             expect(getItemLabels(testEntities.items)).toEqual(['A', 'B', 'C', 'D', 'E', 'F']);\n-            expect(testEntities.items.map((i: any) => i.name)).toEqual(expect.arrayContaining([\n-                'apfel',\n-                'fahrrad',\n-                'kuchen',\n-                'hund',\n-                'egg', // falls back to en translation when de doesn't exist\n-                'baum',\n-            ]));\n+            expect(testEntities.items.map((i: any) => i.name)).toEqual(\n+                expect.arrayContaining([\n+                    'apfel',\n+                    'fahrrad',\n+                    'kuchen',\n+                    'hund',\n+                    'egg', // falls back to en translation when de doesn't exist\n+                    'baum',\n+                ]),\n+            );\n         });\n \n         it('take', async () => {\n@@ -278,6 +282,20 @@ describe('ListQueryBuilder', () => {\n             expect(getItemLabels(testEntities.items)).toEqual(['A', 'C', 'E']);\n         });\n \n+        it('filtering on translatable string', async () => {\n+            const { testEntities } = await adminClient.query(GET_LIST_WITH_TRANSLATIONS, {\n+                options: {\n+                    filter: {\n+                        name: {\n+                            contains: 'g',\n+                        },\n+                    },\n+                },\n+            });\n+\n+            expect(getItemLabels(testEntities.items)).toEqual(['D', 'E']);\n+        });\n+\n         describe('regex', () => {\n             it('simple substring', async () => {\n                 const { testEntities } = await adminClient.query(GET_LIST, {\n@@ -1208,7 +1226,10 @@ describe('ListQueryBuilder', () => {\n     // https://github.com/vendure-ecommerce/vendure/issues/1586\n     it('using the getMany() of the resulting QueryBuilder', async () => {\n         const { testEntitiesGetMany } = await adminClient.query(GET_ARRAY_LIST, {});\n-        const actualPrices = testEntitiesGetMany.sort(sortById).map((x: any) => x.price).sort((a: number, b: number) => a - b);\n+        const actualPrices = testEntitiesGetMany\n+            .sort(sortById)\n+            .map((x: any) => x.price)\n+            .sort((a: number, b: number) => a - b);\n         const expectedPrices = [11, 9, 22, 14, 13, 33].sort((a, b) => a - b);\n         expect(actualPrices).toEqual(expectedPrices);\n     });",
          "packages/core/e2e/order-promotion.e2e-spec.ts": "@@ -925,9 +925,8 @@ describe('Promotions applied to Orders', () => {\n                 expect(removeCouponCode!.total).toBe(2200);\n                 expect(removeCouponCode!.totalWithTax).toBe(2640);\n \n-                const { activeOrder } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                    GET_ACTIVE_ORDER,\n-                );\n+                const { activeOrder } =\n+                    await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n                 expect(getItemSale1Line(activeOrder!.lines).discounts.length).toBe(0);\n                 expect(activeOrder!.total).toBe(2200);\n                 expect(activeOrder!.totalWithTax).toBe(2640);\n@@ -986,9 +985,8 @@ describe('Promotions applied to Orders', () => {\n                 expect(removeCouponCode!.total).toBe(2200);\n                 expect(removeCouponCode!.totalWithTax).toBe(2640);\n \n-                const { activeOrder } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                    GET_ACTIVE_ORDER,\n-                );\n+                const { activeOrder } =\n+                    await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n                 expect(getItemSale1Line(activeOrder!.lines).discounts.length).toBe(0);\n                 expect(activeOrder!.total).toBe(2200);\n                 expect(activeOrder!.totalWithTax).toBe(2640);\n@@ -1534,9 +1532,8 @@ describe('Promotions applied to Orders', () => {\n \n                 await addGuestCustomerToOrder();\n \n-                const { activeOrder } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                    GET_ACTIVE_ORDER,\n-                );\n+                const { activeOrder } =\n+                    await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n                 expect(activeOrder!.couponCodes).toEqual([]);\n                 expect(activeOrder!.totalWithTax).toBe(6000);\n             });\n@@ -1627,9 +1624,8 @@ describe('Promotions applied to Orders', () => {\n \n                 await logInAsRegisteredCustomer();\n \n-                const { activeOrder } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                    GET_ACTIVE_ORDER,\n-                );\n+                const { activeOrder } =\n+                    await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n                 expect(activeOrder!.totalWithTax).toBe(6000);\n                 expect(activeOrder!.couponCodes).toEqual([]);\n             });\n@@ -1883,9 +1879,8 @@ describe('Promotions applied to Orders', () => {\n         expect(addItemToOrder.discounts.length).toBe(1);\n         expect(addItemToOrder.discounts[0].description).toBe('Test Promo');\n \n-        const { activeOrder: check1 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-            GET_ACTIVE_ORDER,\n-        );\n+        const { activeOrder: check1 } =\n+            await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n         expect(check1!.discounts.length).toBe(1);\n         expect(check1!.discounts[0].description).toBe('Test Promo');\n \n@@ -1899,9 +1894,8 @@ describe('Promotions applied to Orders', () => {\n         orderResultGuard.assertSuccess(removeOrderLine);\n         expect(removeOrderLine.discounts.length).toBe(0);\n \n-        const { activeOrder: check2 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-            GET_ACTIVE_ORDER,\n-        );\n+        const { activeOrder: check2 } =\n+            await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n         expect(check2!.discounts.length).toBe(0);\n     });\n \n@@ -2043,9 +2037,8 @@ describe('Promotions applied to Orders', () => {\n                 quantity: 1,\n             });\n \n-            const { activeOrder: check1 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                GET_ACTIVE_ORDER,\n-            );\n+            const { activeOrder: check1 } =\n+                await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n \n             expect(check1!.lines[0].discountedUnitPriceWithTax).toBe(0);\n             expect(check1!.totalWithTax).toBe(0);\n@@ -2055,9 +2048,8 @@ describe('Promotions applied to Orders', () => {\n                 CodegenShop.ApplyCouponCodeMutationVariables\n             >(APPLY_COUPON_CODE, { couponCode: couponCode2 });\n \n-            const { activeOrder: check2 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                GET_ACTIVE_ORDER,\n-            );\n+            const { activeOrder: check2 } =\n+                await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n             expect(check2!.lines[0].discountedUnitPriceWithTax).toBe(0);\n             expect(check2!.totalWithTax).toBe(0);\n         });\n@@ -2080,9 +2072,8 @@ describe('Promotions applied to Orders', () => {\n                 quantity: 1,\n             });\n \n-            const { activeOrder: check1 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                GET_ACTIVE_ORDER,\n-            );\n+            const { activeOrder: check1 } =\n+                await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n \n             expect(check1!.lines[0].discountedUnitPriceWithTax).toBe(0);\n             expect(check1!.totalWithTax).toBe(0);\n@@ -2092,14 +2083,152 @@ describe('Promotions applied to Orders', () => {\n                 CodegenShop.ApplyCouponCodeMutationVariables\n             >(APPLY_COUPON_CODE, { couponCode: couponCode2 });\n \n-            const { activeOrder: check2 } = await shopClient.query<CodegenShop.GetActiveOrderQuery>(\n-                GET_ACTIVE_ORDER,\n-            );\n+            const { activeOrder: check2 } =\n+                await shopClient.query<CodegenShop.GetActiveOrderQuery>(GET_ACTIVE_ORDER);\n             expect(check2!.lines[0].discountedUnitPriceWithTax).toBe(0);\n             expect(check2!.totalWithTax).toBe(0);\n         });\n     });\n \n+    // https://github.com/vendure-ecommerce/vendure/issues/2052\n+    describe('multi-channel usage', () => {\n+        const SECOND_CHANNEL_TOKEN = 'second_channel_token';\n+        const THIRD_CHANNEL_TOKEN = 'third_channel_token';\n+        const promoCode = 'TEST_COMMON_CODE';\n+\n+        async function createChannelAndAssignProducts(code: string, token: string) {\n+            const result = await adminClient.query<\n+                Codegen.CreateChannelMutation,\n+                Codegen.CreateChannelMutationVariables\n+            >(CREATE_CHANNEL, {\n+                input: {\n+                    code,\n+                    token,\n+                    defaultLanguageCode: LanguageCode.en,\n+                    currencyCode: CurrencyCode.GBP,\n+                    pricesIncludeTax: true,\n+                    defaultShippingZoneId: 'T_1',\n+                    defaultTaxZoneId: 'T_1',\n+                },\n+            });\n+\n+            await adminClient.query<\n+                Codegen.AssignProductsToChannelMutation,\n+                Codegen.AssignProductsToChannelMutationVariables\n+            >(ASSIGN_PRODUCT_TO_CHANNEL, {\n+                input: {\n+                    channelId: (result.createChannel as Codegen.ChannelFragment).id,\n+                    priceFactor: 1,\n+                    productIds: products.map(p => p.id),\n+                },\n+            });\n+\n+            return result.createChannel as Codegen.ChannelFragment;\n+        }\n+\n+        async function addItemAndApplyPromoCode() {\n+            await shopClient.asAnonymousUser();\n+            await shopClient.query<\n+                CodegenShop.AddItemToOrderMutation,\n+                CodegenShop.AddItemToOrderMutationVariables\n+            >(ADD_ITEM_TO_ORDER, {\n+                productVariantId: getVariantBySlug('item-5000').id,\n+                quantity: 1,\n+            });\n+\n+            const { applyCouponCode } = await shopClient.query<\n+                CodegenShop.ApplyCouponCodeMutation,\n+                CodegenShop.ApplyCouponCodeMutationVariables\n+            >(APPLY_COUPON_CODE, {\n+                couponCode: promoCode,\n+            });\n+\n+            orderResultGuard.assertSuccess(applyCouponCode);\n+            return applyCouponCode;\n+        }\n+\n+        beforeAll(async () => {\n+            await createChannelAndAssignProducts('second-channel', SECOND_CHANNEL_TOKEN);\n+            await createChannelAndAssignProducts('third-channel', THIRD_CHANNEL_TOKEN);\n+        });\n+\n+        it('create promotion in second channel', async () => {\n+            adminClient.setChannelToken(SECOND_CHANNEL_TOKEN);\n+\n+            const result = await createPromotion({\n+                enabled: true,\n+                name: 'common-promotion-second-channel',\n+                couponCode: promoCode,\n+                actions: [\n+                    {\n+                        code: orderPercentageDiscount.code,\n+                        arguments: [{ name: 'discount', value: '20' }],\n+                    },\n+                ],\n+                conditions: [],\n+            });\n+\n+            expect(result.name).toBe('common-promotion-second-channel');\n+        });\n+\n+        it('create promotion in third channel', async () => {\n+            adminClient.setChannelToken(THIRD_CHANNEL_TOKEN);\n+\n+            const result = await createPromotion({\n+                enabled: true,\n+                name: 'common-promotion-third-channel',\n+                couponCode: promoCode,\n+                actions: [\n+                    {\n+                        code: orderPercentageDiscount.code,\n+                        arguments: [{ name: 'discount', value: '20' }],\n+                    },\n+                ],\n+                conditions: [],\n+            });\n+\n+            expect(result.name).toBe('common-promotion-third-channel');\n+        });\n+\n+        it('applies promotion in second channel', async () => {\n+            shopClient.setChannelToken(SECOND_CHANNEL_TOKEN);\n+\n+            const result = await addItemAndApplyPromoCode();\n+            expect(result.discounts.length).toBe(1);\n+            expect(result.discounts[0].description).toBe('common-promotion-second-channel');\n+        });\n+\n+        it('applies promotion in third channel', async () => {\n+            shopClient.setChannelToken(THIRD_CHANNEL_TOKEN);\n+\n+            const result = await addItemAndApplyPromoCode();\n+            expect(result.discounts.length).toBe(1);\n+            expect(result.discounts[0].description).toBe('common-promotion-third-channel');\n+        });\n+\n+        it('applies promotion from current channel, not default channel', async () => {\n+            adminClient.setChannelToken(E2E_DEFAULT_CHANNEL_TOKEN);\n+            const defaultChannelPromotion = await createPromotion({\n+                enabled: true,\n+                name: 'common-promotion-default-channel',\n+                couponCode: promoCode,\n+                actions: [\n+                    {\n+                        code: orderPercentageDiscount.code,\n+                        arguments: [{ name: 'discount', value: '20' }],\n+                    },\n+                ],\n+                conditions: [],\n+            });\n+\n+            shopClient.setChannelToken(SECOND_CHANNEL_TOKEN);\n+\n+            const result = await addItemAndApplyPromoCode();\n+            expect(result.discounts.length).toBe(1);\n+            expect(result.discounts[0].description).toBe('common-promotion-second-channel');\n+        });\n+    });\n+\n     async function getProducts() {\n         const result = await adminClient.query<Codegen.GetProductsWithVariantPricesQuery>(\n             GET_PRODUCTS_WITH_VARIANT_PRICES,",
          "packages/core/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/core\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"description\": \"A modern, headless ecommerce framework\",\n     \"repository\": {\n         \"type\": \"git\",\n@@ -51,7 +51,7 @@\n         \"@nestjs/testing\": \"~10.3.10\",\n         \"@nestjs/typeorm\": \"~10.0.2\",\n         \"@types/fs-extra\": \"^9.0.1\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"bcrypt\": \"^5.1.1\",\n         \"body-parser\": \"^1.20.2\",\n         \"cookie-session\": \"^2.1.0\",",
          "packages/core/src/cli/populate.ts": "@@ -150,5 +150,9 @@ export async function importProductsFromCsv(\n         languageCode,\n         channelOrToken: channel,\n     });\n-    return lastValueFrom(importer.parseAndImport(productData, ctx, true));\n+    const createEnvVar: import('@vendure/common/lib/shared-constants').CREATING_VENDURE_APP =\n+        'CREATING_VENDURE_APP';\n+    // Turn off progress bar when running in the context of the @vendure/create script\n+    const reportProgress = process.env[createEnvVar] === 'true' ? false : true;\n+    return lastValueFrom(importer.parseAndImport(productData, ctx, reportProgress));\n }",
          "packages/core/src/data-import/providers/importer/importer.ts": "@@ -10,6 +10,7 @@ import { RequestContext } from '../../../api/common/request-context';\n import { InternalServerError } from '../../../common/error/errors';\n import { ConfigService } from '../../../config/config.service';\n import { CustomFieldConfig } from '../../../config/custom-field/custom-field-types';\n+import { Logger } from '../../../config/index';\n import { Facet } from '../../../entity/facet/facet.entity';\n import { FacetValue } from '../../../entity/facet-value/facet-value.entity';\n import { TaxCategory } from '../../../entity/tax-category/tax-category.entity';\n@@ -159,6 +160,17 @@ export class Importer {\n         let imported = 0;\n         const languageCode = ctx.languageCode;\n         const taxCategories = await this.taxCategoryService.findAll(ctx);\n+        if (taxCategories.totalItems === 0) {\n+            Logger.error(\n+                [\n+                    `No TaxCategories found in the database. Ensure that at least one TaxCategory exists.`,\n+                    `If you are populating from an InitialData object, ensure the 'taxRates' array is not empty.`,\n+                ].join('\\n'),\n+            );\n+            throw new Error(\n+                `No TaxCategories found in the database. Ensure the IntialData.taxRates array is not empty.`,\n+            );\n+        }\n         await this.fastImporter.initialize(ctx.channel);\n         for (const { product, variants } of rows) {\n             const productMainTranslation = this.getTranslationByCodeOrFirst(\n@@ -207,7 +219,7 @@ export class Importer {\n                 );\n                 const groupId = await this.fastImporter.createProductOptionGroup({\n                     code,\n-                    options: optionGroupMainTranslation.values.map(name => ({} as any)),\n+                    options: optionGroupMainTranslation.values.map(name => ({}) as any),\n                     translations: optionGroup.translations.map(translation => {\n                         return {\n                             languageCode: translation.languageCode,\n@@ -363,7 +375,10 @@ export class Importer {\n         return facetValueIds;\n     }\n \n-    protected processCustomFieldValues(customFields: { [field: string]: string }, config: CustomFieldConfig[]) {\n+    protected processCustomFieldValues(\n+        customFields: { [field: string]: string },\n+        config: CustomFieldConfig[],\n+    ) {\n         const processed: { [field: string]: string | string[] | boolean | undefined } = {};\n         for (const fieldDef of config) {\n             const value = customFields[fieldDef.name];",
          "packages/core/src/data-import/providers/populator/populator.ts": "@@ -271,8 +271,6 @@ export class Populator {\n         taxRates: Array<{ name: string; percentage: number }>,\n         zoneMap: ZoneMap,\n     ) {\n-        const taxCategories: TaxCategory[] = [];\n-\n         for (const taxRate of taxRates) {\n             const category = await this.taxCategoryService.create(ctx, { name: taxRate.name });\n ",
          "packages/core/src/plugin/default-search-plugin/indexer/indexer.controller.ts": "@@ -481,6 +481,7 @@ export class IndexerController {\n                                         loadEagerRelations: false,\n                                         where: {\n                                             productId: variant.productId,\n+                                            deletedAt: IsNull(),\n                                         },\n                                     })\n                                     .then(_variants =>",
          "packages/core/src/service/helpers/utils/tree-relations-qb-joiner.ts": "@@ -110,7 +110,7 @@ export function joinTreeRelationsDynamically<T extends VendureEntity>(\n         }\n         const nextAlias = DriverUtils.buildAlias(\n             qb.connection.driver,\n-            { shorten: false },\n+            { shorten: false, joiner: joinConnector },\n             currentAlias,\n             part.replace(/\\./g, '_'),\n         );",
          "packages/core/src/service/services/promotion.service.ts": "@@ -250,6 +250,7 @@ export class PromotionService {\n                 couponCode,\n                 enabled: true,\n                 deletedAt: IsNull(),\n+                channels: { id: ctx.channelId },\n             },\n             relations: ['channels'],\n         });",
          "packages/create/README.md": "@@ -4,47 +4,17 @@ A CLI tool for rapidly scaffolding a new Vendure server application. Heavily ins\n \n ## Usage\n \n-Vendure Create requires [Node.js](https://nodejs.org/en/) v8.9.0+ to be installed.\n-\n-To create a new project, you may choose one of the following methods:\n-\n-### npx\n+Vendure Create requires [Node.js](https://nodejs.org/en/) v18+ to be installed.\n \n ```sh\n npx @vendure/create my-app\n ```\n \n-*[npx](https://medium.com/@maybekatz/introducing-npx-an-npm-package-runner-55f7d4bd282b) comes with npm 5.2+ and higher.*\n-\n-### npm\n-\n-```sh\n-npm init @vendure my-app\n-```\n-\n-*`npm init <initializer>` is available in npm 6+*\n-\n-### Yarn\n-\n-```sh\n-yarn create @vendure my-app\n-```\n-\n-*`yarn create` is available in Yarn 0.25+*\n-\n-\n-It will create a directory called `my-app` inside the current folder.\n-\n ## Options\n \n-### `--use-npm`\n-\n-By default, Vendure Create will detect whether a compatible version of Yarn is installed, and if so will display a prompt to select the preferred package manager.\n-You can override this and force it to use npm with the `--use-npm` flag.\n-\n ### `--log-level`\n \n-You can control how much output is generated during the installation and setup with this flag. Valid options are `silent`, `info` and `verbose`. The default is `silent`\n+You can control how much output is generated during the installation and setup with this flag. Valid options are `silent`, `info` and `verbose`. The default is `info`\n \n Example:\n ",
          "packages/create/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/create\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"bin\": {\n         \"create\": \"./index.js\"\n@@ -27,18 +27,19 @@\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/handlebars\": \"^4.1.0\",\n         \"@types/semver\": \"^7.5.8\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"rimraf\": \"^5.0.5\",\n         \"ts-node\": \"^10.9.2\",\n         \"typescript\": \"5.3.3\"\n     },\n     \"dependencies\": {\n         \"@clack/prompts\": \"^0.7.0\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"commander\": \"^11.0.0\",\n         \"cross-spawn\": \"^7.0.3\",\n         \"fs-extra\": \"^11.2.0\",\n         \"handlebars\": \"^4.7.8\",\n+        \"open\": \"^8.4.2\",\n         \"picocolors\": \"^1.0.0\",\n         \"semver\": \"^7.5.4\",\n         \"tcp-port-used\": \"^1.0.2\"",
          "packages/create/src/create-vendure-app.ts": "@@ -1,24 +1,33 @@\n-/* eslint-disable no-console */\n import { intro, note, outro, select, spinner } from '@clack/prompts';\n import { program } from 'commander';\n import fs from 'fs-extra';\n+import { ChildProcess, spawn } from 'node:child_process';\n+import { setTimeout as sleep } from 'node:timers/promises';\n+import open from 'open';\n import os from 'os';\n import path from 'path';\n import pc from 'picocolors';\n \n import { REQUIRED_NODE_VERSION, SERVER_PORT } from './constants';\n-import { checkCancel, gatherCiUserResponses, gatherUserResponses } from './gather-user-responses';\n import {\n+    getCiConfiguration,\n+    getManualConfiguration,\n+    getQuickStartConfiguration,\n+} from './gather-user-responses';\n+import {\n+    checkCancel,\n     checkDbConnection,\n     checkNodeVersion,\n     checkThatNpmCanReadCwd,\n+    cleanUpDockerResources,\n     getDependencies,\n     installPackages,\n     isSafeToCreateProjectIn,\n     isServerPortInUse,\n     scaffoldAlreadyExists,\n-    yarnIsAvailable,\n+    startPostgresDatabase,\n } from './helpers';\n+import { log, setLogLevel } from './logger';\n import { CliLogLevel, PackageManager } from './types';\n \n // eslint-disable-next-line @typescript-eslint/no-var-requires\n@@ -44,21 +53,31 @@ program\n         '--log-level <logLevel>',\n         \"Log level, either 'silent', 'info', or 'verbose'\",\n         /^(silent|info|verbose)$/i,\n-        'silent',\n+        'info',\n+    )\n+    .option('--verbose', 'Alias for --log-level verbose', false)\n+    .option(\n+        '--use-npm',\n+        'Uses npm rather than as the default package manager. DEPRECATED: Npm is now the default',\n     )\n-    .option('--use-npm', 'Uses npm rather than Yarn as the default package manager')\n-    .option('--ci', 'Runs without prompts for use in CI scenarios')\n+    .option('--ci', 'Runs without prompts for use in CI scenarios', false)\n     .parse(process.argv);\n \n const options = program.opts();\n-void createVendureApp(projectName, options.useNpm, options.logLevel || 'silent', options.ci);\n+void createVendureApp(\n+    projectName,\n+    options.useNpm,\n+    options.verbose ? 'verbose' : options.logLevel || 'info',\n+    options.ci,\n+);\n \n export async function createVendureApp(\n     name: string | undefined,\n     useNpm: boolean,\n     logLevel: CliLogLevel,\n     isCi: boolean = false,\n ) {\n+    setLogLevel(logLevel);\n     if (!runPreChecks(name, useNpm)) {\n         return;\n     }\n@@ -67,6 +86,22 @@ export async function createVendureApp(\n         `Let's create a ${pc.blue(pc.bold('Vendure App'))} ✨ ${pc.dim(`v${packageJson.version as string}`)}`,\n     );\n \n+    const mode = isCi\n+        ? 'ci'\n+        : ((await select({\n+              message: 'How should we proceed?',\n+              options: [\n+                  { label: 'Quick Start', value: 'quick', hint: 'Get up an running in a single step' },\n+                  {\n+                      label: 'Manual Configuration',\n+                      value: 'manual',\n+                      hint: 'Customize your Vendure project with more advanced settings',\n+                  },\n+              ],\n+              initialValue: 'quick' as 'quick' | 'manual',\n+          })) as 'quick' | 'manual');\n+    checkCancel(mode);\n+\n     const portSpinner = spinner();\n     let port = SERVER_PORT;\n     const attemptedPortRange = 20;\n@@ -90,27 +125,15 @@ export async function createVendureApp(\n     const appName = path.basename(root);\n     const scaffoldExists = scaffoldAlreadyExists(root, name);\n \n-    const yarnAvailable = yarnIsAvailable();\n-    let packageManager: PackageManager = 'npm';\n-    if (yarnAvailable && !useNpm) {\n-        packageManager = (await select({\n-            message: 'Which package manager should be used?',\n-            options: [\n-                { label: 'npm', value: 'npm' },\n-                { label: 'yarn', value: 'yarn' },\n-            ],\n-            initialValue: 'yarn' as PackageManager,\n-        })) as PackageManager;\n-        checkCancel(packageManager);\n-    }\n+    const packageManager: PackageManager = 'npm';\n \n     if (scaffoldExists) {\n-        console.log(\n+        log(\n             pc.yellow(\n                 'It appears that a new Vendure project scaffold already exists. Re-using the existing files...',\n             ),\n+            { newline: 'after' },\n         );\n-        console.log();\n     }\n     const {\n         dbType,\n@@ -123,10 +146,12 @@ export async function createVendureApp(\n         dockerfileSource,\n         dockerComposeSource,\n         populateProducts,\n-    } = isCi\n-        ? await gatherCiUserResponses(root, packageManager)\n-        : await gatherUserResponses(root, scaffoldExists, packageManager);\n-    const originalDirectory = process.cwd();\n+    } =\n+        mode === 'ci'\n+            ? await getCiConfiguration(root, packageManager)\n+            : mode === 'manual'\n+              ? await getManualConfiguration(root, packageManager)\n+              : await getQuickStartConfiguration(root, packageManager);\n     process.chdir(root);\n     if (packageManager !== 'npm' && !checkThatNpmCanReadCwd()) {\n         process.exit(1);\n@@ -139,11 +164,11 @@ export async function createVendureApp(\n         scripts: {\n             'dev:server': 'ts-node ./src/index.ts',\n             'dev:worker': 'ts-node ./src/index-worker.ts',\n-            dev: packageManager === 'yarn' ? 'concurrently yarn:dev:*' : 'concurrently npm:dev:*',\n+            dev: 'concurrently npm:dev:*',\n             build: 'tsc',\n             'start:server': 'node ./dist/index.js',\n             'start:worker': 'node ./dist/index-worker.js',\n-            start: packageManager === 'yarn' ? 'concurrently yarn:start:*' : 'concurrently npm:start:*',\n+            start: 'concurrently npm:start:*',\n         },\n     };\n \n@@ -152,7 +177,6 @@ export async function createVendureApp(\n         `Setting up your new Vendure project in ${pc.green(root)}\\nThis may take a few minutes...`,\n     );\n \n-    const rootPathScript = (fileName: string): string => path.join(root, `${fileName}.ts`);\n     const srcPathScript = (fileName: string): string => path.join(root, 'src', `${fileName}.ts`);\n \n     fs.writeFileSync(path.join(root, 'package.json'), JSON.stringify(packageJsonContents, null, 2) + os.EOL);\n@@ -162,9 +186,9 @@ export async function createVendureApp(\n     const installSpinner = spinner();\n     installSpinner.start(`Installing ${dependencies[0]} + ${dependencies.length - 1} more dependencies`);\n     try {\n-        await installPackages(root, packageManager === 'yarn', dependencies, false, logLevel, isCi);\n+        await installPackages({ dependencies, logLevel });\n     } catch (e) {\n-        outro(pc.red(`Failed to install dependencies. Please try again.`));\n+        outro(pc.red(`Failed to inst all dependencies. Please try again.`));\n         process.exit(1);\n     }\n     installSpinner.stop(`Successfully installed ${dependencies.length} dependencies`);\n@@ -175,7 +199,7 @@ export async function createVendureApp(\n             `Installing ${devDependencies[0]} + ${devDependencies.length - 1} more dev dependencies`,\n         );\n         try {\n-            await installPackages(root, packageManager === 'yarn', devDependencies, true, logLevel, isCi);\n+            await installPackages({ dependencies: devDependencies, isDevDependencies: true, logLevel });\n         } catch (e) {\n             outro(pc.red(`Failed to install dev dependencies. Please try again.`));\n             process.exit(1);\n@@ -185,6 +209,10 @@ export async function createVendureApp(\n \n     const scaffoldSpinner = spinner();\n     scaffoldSpinner.start(`Generating app scaffold`);\n+    // We add this pause so that the above output is displayed before the\n+    // potentially lengthy file operations begin, which can prevent that\n+    // from displaying and thus make the user think that the process has hung.\n+    await sleep(500);\n     fs.ensureDirSync(path.join(root, 'src'));\n     const assetPath = (fileName: string) => path.join(__dirname, '../assets', fileName);\n     const configFile = srcPathScript('vendure-config');\n@@ -199,34 +227,87 @@ export async function createVendureApp(\n             .then(() => fs.writeFile(path.join(root, 'README.md'), readmeSource))\n             .then(() => fs.writeFile(path.join(root, 'Dockerfile'), dockerfileSource))\n             .then(() => fs.writeFile(path.join(root, 'docker-compose.yml'), dockerComposeSource))\n-            .then(() => fs.mkdir(path.join(root, 'src/plugins')))\n+            .then(() => fs.ensureDir(path.join(root, 'src/plugins')))\n             .then(() => fs.copyFile(assetPath('gitignore.template'), path.join(root, '.gitignore')))\n             .then(() => fs.copyFile(assetPath('tsconfig.template.json'), path.join(root, 'tsconfig.json')))\n             .then(() => createDirectoryStructure(root))\n             .then(() => copyEmailTemplates(root));\n-    } catch (e) {\n-        outro(pc.red(`Failed to create app scaffold. Please try again.`));\n+    } catch (e: any) {\n+        outro(pc.red(`Failed to create app scaffold: ${e.message as string}`));\n         process.exit(1);\n     }\n     scaffoldSpinner.stop(`Generated app scaffold`);\n \n+    if (mode === 'quick' && dbType === 'postgres') {\n+        cleanUpDockerResources(name);\n+        await startPostgresDatabase(root);\n+    }\n+\n     const populateSpinner = spinner();\n     populateSpinner.start(`Initializing your new Vendure server`);\n+\n+    // We want to display a set of tips and instructions to the user\n+    // as the initialization process is running because it can take\n+    // a few minutes to complete.\n+    const tips = [\n+        populateProducts\n+            ? 'We are populating sample data so that you can start testing right away'\n+            : 'We are setting up your Vendure server',\n+        '☕ This can take a minute or two, so grab a coffee',\n+        `✨ We'd love it if you drop us a star on GitHub: https://github.com/vendure-ecommerce/vendure`,\n+        `📖 Check out the Vendure documentation at https://docs.vendure.io`,\n+        `💬 Join our Discord community to chat with other Vendure developers: https://vendure.io/community`,\n+        '💡 In the mean time, here are some tips to get you started',\n+        `Vendure provides dedicated GraphQL APIs for both the Admin and Shop`,\n+        `Almost every aspect of Vendure is customizable via plugins`,\n+        `You can run 'vendure add' from the command line to add new plugins & features`,\n+        `Use the EventBus in your plugins to react to events in the system`,\n+        `Vendure supports multiple languages & currencies out of the box`,\n+        `☕ Did we mention this can take a while?`,\n+        `Our custom fields feature allows you to add any kind of data to your entities`,\n+        `Vendure is built with TypeScript, so you get full type safety`,\n+        `Combined with GraphQL's static schema, your type safety is end-to-end`,\n+        `☕ Almost there now... thanks for your patience!`,\n+        `Collections allow you to group products together`,\n+        `Our AssetServerPlugin allows you to dynamically resize & optimize images`,\n+        `Order flows are fully customizable to suit your business requirements`,\n+        `Role-based permissions allow you to control access to every part of the system`,\n+        `Customers can be grouped for targeted promotions & custom pricing`,\n+        `You can find integrations in the Vendure Hub: https://vendure.io/hub`,\n+    ];\n+\n+    let tipIndex = 0;\n+    let timer: any;\n+    const tipInterval = 10_000;\n+\n+    function displayTip() {\n+        populateSpinner.message(tips[tipIndex]);\n+        tipIndex++;\n+        if (tipIndex >= tips.length) {\n+            // skip the intro tips if looping\n+            tipIndex = 3;\n+        }\n+        timer = setTimeout(displayTip, tipInterval);\n+    }\n+\n+    timer = setTimeout(displayTip, tipInterval);\n+\n     // register ts-node so that the config file can be loaded\n     // eslint-disable-next-line @typescript-eslint/no-var-requires\n     require(path.join(root, 'node_modules/ts-node')).register();\n \n+    let superAdminCredentials: { identifier: string; password: string } | undefined;\n     try {\n         const { populate } = await import(path.join(root, 'node_modules/@vendure/core/cli/populate'));\n-        const { bootstrap, DefaultLogger, LogLevel, JobQueueService } = await import(\n+        const { bootstrap, DefaultLogger, LogLevel, JobQueueService, ConfigModule } = await import(\n             path.join(root, 'node_modules/@vendure/core/dist/index')\n         );\n         const { config } = await import(configFile);\n         const assetsDir = path.join(__dirname, '../assets');\n-\n+        superAdminCredentials = config.authOptions.superadminCredentials;\n         const initialDataPath = path.join(assetsDir, 'initial-data.json');\n         const vendureLogLevel =\n-            logLevel === 'silent'\n+            logLevel === 'info' || logLevel === 'silent'\n                 ? LogLevel.Error\n                 : logLevel === 'verbose'\n                   ? LogLevel.Verbose\n@@ -240,7 +321,6 @@ export async function createVendureApp(\n                     ...(config.apiOptions ?? {}),\n                     port,\n                 },\n-                silent: logLevel === 'silent',\n                 dbConnectionOptions: {\n                     ...config.dbConnectionOptions,\n                     synchronize: true,\n@@ -262,35 +342,116 @@ export async function createVendureApp(\n \n         // Pause to ensure the worker jobs have time to complete.\n         if (isCi) {\n-            console.log('[CI] Pausing before close...');\n+            log('[CI] Pausing before close...');\n         }\n-        await new Promise(resolve => setTimeout(resolve, isCi ? 30000 : 2000));\n+        await sleep(isCi ? 30000 : 2000);\n         await app.close();\n         if (isCi) {\n-            console.log('[CI] Pausing after close...');\n-            await new Promise(resolve => setTimeout(resolve, 10000));\n+            log('[CI] Pausing after close...');\n+            await sleep(10000);\n         }\n-    } catch (e) {\n-        console.log(e);\n+        populateSpinner.stop(`Server successfully initialized${populateProducts ? ' and populated' : ''}`);\n+        clearTimeout(timer);\n+        /**\n+         * This is currently disabled because I am running into issues actually getting the server\n+         * to quite properly in response to a SIGINT.\n+         * This means that the server runs, but cannot be ended, without forcefully\n+         * killing the process.\n+         *\n+         * Once this has been resolved, the following code can be re-enabled by\n+         * setting `autoRunServer` to `true`.\n+         */\n+        const autoRunServer = false;\n+        if (mode === 'quick' && autoRunServer) {\n+            // In quick-start mode, we want to now run the server and open up\n+            // a browser window to the Admin UI.\n+            try {\n+                const adminUiUrl = `http://localhost:${port}/admin`;\n+                const quickStartInstructions = [\n+                    'Use the following credentials to log in to the Admin UI:',\n+                    `Username: ${pc.green(config.authOptions.superadminCredentials?.identifier)}`,\n+                    `Password: ${pc.green(config.authOptions.superadminCredentials?.password)}`,\n+                    `Open your browser and navigate to: ${pc.green(adminUiUrl)}`,\n+                    '',\n+                ];\n+                note(quickStartInstructions.join('\\n'));\n+\n+                const npmCommand = os.platform() === 'win32' ? 'npm.cmd' : 'npm';\n+                let quickStartProcess: ChildProcess | undefined;\n+                try {\n+                    quickStartProcess = spawn(npmCommand, ['run', 'dev'], {\n+                        cwd: root,\n+                        stdio: 'inherit',\n+                    });\n+                } catch (e: any) {\n+                    /* empty */\n+                }\n+\n+                // process.stdin.resume();\n+                process.on('SIGINT', function () {\n+                    displayOutro(root, name, superAdminCredentials);\n+                    quickStartProcess?.kill('SIGINT');\n+                    process.exit(0);\n+                });\n+\n+                // Give enough time for the server to get up and running\n+                // before opening the window.\n+                await sleep(10_000);\n+                try {\n+                    await open(adminUiUrl, {\n+                        newInstance: true,\n+                    });\n+                } catch (e: any) {\n+                    /* empty */\n+                }\n+            } catch (e: any) {\n+                log(pc.red(`Failed to start the server: ${e.message as string}`), {\n+                    newline: 'after',\n+                    level: 'verbose',\n+                });\n+            }\n+        } else {\n+            clearTimeout(timer);\n+            displayOutro(root, name, superAdminCredentials);\n+            process.exit(0);\n+        }\n+    } catch (e: any) {\n+        log(e.toString());\n         outro(pc.red(`Failed to initialize server. Please try again.`));\n         process.exit(1);\n     }\n-    populateSpinner.stop(`Server successfully initialized${populateProducts ? ' and populated' : ''}`);\n+}\n \n-    const startCommand = packageManager === 'yarn' ? 'yarn dev' : 'npm run dev';\n+function displayOutro(\n+    root: string,\n+    name: string,\n+    superAdminCredentials?: { identifier: string; password: string },\n+) {\n+    const startCommand = 'npm run dev';\n     const nextSteps = [\n-        `${pc.green('Success!')} Created a new Vendure server at:`,\n-        `\\n`,\n-        pc.italic(root),\n-        `\\n`,\n-        `We suggest that you start by typing:`,\n+        `Your new Vendure server was created!`,\n+        pc.gray(root),\n         `\\n`,\n+        `Next, run:`,\n         pc.gray('$ ') + pc.blue(pc.bold(`cd ${name}`)),\n         pc.gray('$ ') + pc.blue(pc.bold(`${startCommand}`)),\n+        `\\n`,\n+        `This will start the server in development mode.`,\n+        `To access the Admin UI, open your browser and navigate to:`,\n+        `\\n`,\n+        pc.green(`http://localhost:3000/admin`),\n+        `\\n`,\n+        `Use the following credentials to log in:`,\n+        `Username: ${pc.green(superAdminCredentials?.identifier ?? 'superadmin')}`,\n+        `Password: ${pc.green(superAdminCredentials?.password ?? 'superadmin')}`,\n+        '\\n',\n+        '➡️ Docs: https://docs.vendure.io',\n+        '➡️ Discord community: https://vendure.io/community',\n+        '➡️ Star us on GitHub:',\n+        '   https://github.com/vendure-ecommerce/vendure',\n     ];\n-    note(nextSteps.join('\\n'));\n+    note(nextSteps.join('\\n'), pc.green('Setup complete!'));\n     outro(`Happy hacking!`);\n-    process.exit(0);\n }\n \n /**\n@@ -299,17 +460,21 @@ export async function createVendureApp(\n  */\n function runPreChecks(name: string | undefined, useNpm: boolean): name is string {\n     if (typeof name === 'undefined') {\n-        console.error('Please specify the project directory:');\n-        console.log(`  ${pc.cyan(program.name())} ${pc.green('<project-directory>')}`);\n-        console.log();\n-        console.log('For example:');\n-        console.log(`  ${pc.cyan(program.name())} ${pc.green('my-vendure-app')}`);\n+        log(pc.red(`Please specify the project directory:`));\n+        log(`  ${pc.cyan(program.name())} ${pc.green('<project-directory>')}`, { newline: 'after' });\n+        log('For example:');\n+        log(`  ${pc.cyan(program.name())} ${pc.green('my-vendure-app')}`);\n         process.exit(1);\n         return false;\n     }\n \n     const root = path.resolve(name);\n-    fs.ensureDirSync(name);\n+    try {\n+        fs.ensureDirSync(name);\n+    } catch (e: any) {\n+        log(pc.red(`Could not create project directory ${name}: ${e.message as string}`));\n+        return false;\n+    }\n     if (!isSafeToCreateProjectIn(root, name)) {\n         process.exit(1);\n     }\n@@ -332,6 +497,6 @@ async function copyEmailTemplates(root: string) {\n     try {\n         await fs.copy(templateDir, path.join(root, 'static', 'email', 'templates'));\n     } catch (err: any) {\n-        console.error(pc.red('Failed to copy email templates.'));\n+        log(pc.red('Failed to copy email templates.'));\n     }\n }",
          "packages/create/src/gather-user-responses.ts": "@@ -1,10 +1,11 @@\n-import { cancel, isCancel, select, text } from '@clack/prompts';\n+import { select, text } from '@clack/prompts';\n import { SUPER_ADMIN_USER_IDENTIFIER, SUPER_ADMIN_USER_PASSWORD } from '@vendure/common/lib/shared-constants';\n import { randomBytes } from 'crypto';\n import fs from 'fs-extra';\n import Handlebars from 'handlebars';\n import path from 'path';\n \n+import { checkCancel, isDockerAvailable } from './helpers';\n import { DbType, FileSources, PackageManager, UserResponses } from './types';\n \n interface PromptAnswers {\n@@ -23,12 +24,72 @@ interface PromptAnswers {\n \n /* eslint-disable no-console */\n \n+export async function getQuickStartConfiguration(\n+    root: string,\n+    packageManager: PackageManager,\n+): Promise<UserResponses> {\n+    // First we want to detect whether Docker is running\n+    const { result: dockerStatus } = await isDockerAvailable();\n+    let usePostgres: boolean;\n+    switch (dockerStatus) {\n+        case 'running':\n+            usePostgres = true;\n+            break;\n+        case 'not-found':\n+            usePostgres = false;\n+            break;\n+        case 'not-running': {\n+            let useSqlite = false;\n+            let dockerIsNowRunning = false;\n+            do {\n+                const useSqliteResponse = await select({\n+                    message: 'We could not automatically start Docker. How should we proceed?',\n+                    options: [\n+                        { label: `Let's use SQLite as the database`, value: true },\n+                        { label: 'I have manually started Docker', value: false },\n+                    ],\n+                    initialValue: true,\n+                });\n+                checkCancel(useSqlite);\n+                useSqlite = useSqliteResponse as boolean;\n+                if (useSqlite === false) {\n+                    const { result: dockerStatusManual } = await isDockerAvailable();\n+                    dockerIsNowRunning = dockerStatusManual === 'running';\n+                }\n+            } while (dockerIsNowRunning !== true && useSqlite === false);\n+            usePostgres = !useSqlite;\n+            break;\n+        }\n+    }\n+    const quickStartAnswers: PromptAnswers = {\n+        dbType: usePostgres ? 'postgres' : 'sqlite',\n+        dbHost: usePostgres ? 'localhost' : '',\n+        dbPort: usePostgres ? '6543' : '',\n+        dbName: usePostgres ? 'vendure' : '',\n+        dbUserName: usePostgres ? 'vendure' : '',\n+        dbPassword: usePostgres ? randomBytes(16).toString('base64url') : '',\n+        dbSchema: usePostgres ? 'public' : '',\n+        populateProducts: true,\n+        superadminIdentifier: SUPER_ADMIN_USER_IDENTIFIER,\n+        superadminPassword: SUPER_ADMIN_USER_PASSWORD,\n+    };\n+\n+    const responses = {\n+        ...(await generateSources(root, quickStartAnswers, packageManager)),\n+        dbType: quickStartAnswers.dbType,\n+        populateProducts: quickStartAnswers.populateProducts as boolean,\n+        superadminIdentifier: quickStartAnswers.superadminIdentifier as string,\n+        superadminPassword: quickStartAnswers.superadminPassword as string,\n+    };\n+\n+    return responses;\n+}\n+\n /**\n  * Prompts the user to determine how the new Vendure app should be configured.\n  */\n-export async function gatherUserResponses(\n+export async function getManualConfiguration(\n     root: string,\n-    alreadyRanScaffold: boolean,\n     packageManager: PackageManager,\n ): Promise<UserResponses> {\n     const dbType = (await select({\n@@ -38,13 +99,12 @@ export async function gatherUserResponses(\n             { label: 'MariaDB', value: 'mariadb' },\n             { label: 'Postgres', value: 'postgres' },\n             { label: 'SQLite', value: 'sqlite' },\n-            { label: 'SQL.js', value: 'sqljs' },\n         ],\n         initialValue: 'sqlite' as DbType,\n     })) as DbType;\n     checkCancel(dbType);\n \n-    const hasConnection = dbType !== 'sqlite' && dbType !== 'sqljs';\n+    const hasConnection = dbType !== 'sqlite';\n     const dbHost = hasConnection\n         ? await text({\n               message: \"What's the database host address?\",\n@@ -146,7 +206,7 @@ export async function gatherUserResponses(\n /**\n  * Returns mock \"user response\" without prompting, for use in CI\n  */\n-export async function gatherCiUserResponses(\n+export async function getCiConfiguration(\n     root: string,\n     packageManager: PackageManager,\n ): Promise<UserResponses> {\n@@ -171,14 +231,6 @@ export async function gatherCiUserResponses(\n     };\n }\n \n-export function checkCancel<T>(value: T | symbol): value is T {\n-    if (isCancel(value)) {\n-        cancel('Setup cancelled.');\n-        process.exit(0);\n-    }\n-    return true;\n-}\n-\n /**\n  * Create the server index, worker and config source code based on the options specified by the CLI prompts.\n  */\n@@ -200,12 +252,10 @@ async function generateSources(\n \n     const templateContext = {\n         ...answers,\n-        useYarn: packageManager === 'yarn',\n         dbType: answers.dbType === 'sqlite' ? 'better-sqlite3' : answers.dbType,\n         name: path.basename(root),\n         isSQLite: answers.dbType === 'sqlite',\n-        isSQLjs: answers.dbType === 'sqljs',\n-        requiresConnection: answers.dbType !== 'sqlite' && answers.dbType !== 'sqljs',\n+        requiresConnection: answers.dbType !== 'sqlite',\n         cookieSecret: randomBytes(16).toString('base64url'),\n     };\n \n@@ -233,10 +283,6 @@ function defaultDBPort(dbType: DbType): number {\n             return 3306;\n         case 'postgres':\n             return 5432;\n-        case 'mssql':\n-            return 1433;\n-        case 'oracle':\n-            return 1521;\n         default:\n             return 3306;\n     }",
          "packages/create/src/helpers.ts": "@@ -1,12 +1,15 @@\n-/* eslint-disable no-console */\n-import { execSync } from 'child_process';\n+import { cancel, isCancel, spinner } from '@clack/prompts';\n import spawn from 'cross-spawn';\n import fs from 'fs-extra';\n+import { execFile, execSync, execFileSync } from 'node:child_process';\n+import { platform } from 'node:os';\n+import { promisify } from 'node:util';\n import path from 'path';\n import pc from 'picocolors';\n import semver from 'semver';\n \n-import { SERVER_PORT, TYPESCRIPT_VERSION } from './constants';\n+import { TYPESCRIPT_VERSION } from './constants';\n+import { log } from './logger';\n import { CliLogLevel, DbType } from './types';\n \n /**\n@@ -46,7 +49,6 @@ export function isSafeToCreateProjectIn(root: string, name: string) {\n         'tsconfig.json',\n         'yarn.lock',\n     ];\n-    console.log();\n \n     const conflicts = fs\n         .readdirSync(root)\n@@ -57,13 +59,13 @@ export function isSafeToCreateProjectIn(root: string, name: string) {\n         .filter(file => !errorLogFilePatterns.some(pattern => file.indexOf(pattern) === 0));\n \n     if (conflicts.length > 0) {\n-        console.log(`The directory ${pc.green(name)} contains files that could conflict:`);\n-        console.log();\n+        log(`The directory ${pc.green(name)} contains files that could conflict:`, { newline: 'after' });\n         for (const file of conflicts) {\n-            console.log(`  ${file}`);\n+            log(`  ${file}`);\n         }\n-        console.log();\n-        console.log('Either try using a new directory name, or remove the files listed above.');\n+        log('Either try using a new directory name, or remove the files listed above.', {\n+            newline: 'before',\n+        });\n \n         return false;\n     }\n@@ -89,38 +91,23 @@ export function scaffoldAlreadyExists(root: string, name: string): boolean {\n \n export function checkNodeVersion(requiredVersion: string) {\n     if (!semver.satisfies(process.version, requiredVersion)) {\n-        console.error(\n+        log(\n             pc.red(\n-                'You are running Node %s.\\n' +\n-                    'Vendure requires Node %s or higher. \\n' +\n+                `You are running Node ${process.version}.` +\n+                    `Vendure requires Node ${requiredVersion} or higher.` +\n                     'Please update your version of Node.',\n             ),\n-            process.version,\n-            requiredVersion,\n         );\n         process.exit(1);\n     }\n }\n \n-export function yarnIsAvailable() {\n-    try {\n-        const yarnVersion = execSync('yarnpkg --version');\n-        if (semver.major(yarnVersion.toString()) > 1) {\n-            return true;\n-        } else {\n-            return false;\n-        }\n-    } catch (e: any) {\n-        return false;\n-    }\n-}\n-\n // Bun support should not be exposed yet, see\n // https://github.com/oven-sh/bun/issues/4947\n // https://github.com/lovell/sharp/issues/3511\n export function bunIsAvailable() {\n     try {\n-        execSync('bun --version', { stdio: 'ignore' });\n+        execFileSync('bun', ['--version'], { stdio: 'ignore' });\n         return true;\n     } catch (e: any) {\n         return false;\n@@ -160,7 +147,7 @@ export function checkThatNpmCanReadCwd() {\n     if (npmCWD === cwd) {\n         return true;\n     }\n-    console.error(\n+    log(\n         pc.red(\n             'Could not start an npm process in the right directory.\\n\\n' +\n                 `The current directory is: ${pc.bold(cwd)}\\n` +\n@@ -169,7 +156,7 @@ export function checkThatNpmCanReadCwd() {\n         ),\n     );\n     if (process.platform === 'win32') {\n-        console.error(\n+        log(\n             pc.red('On Windows, this can usually be fixed by running:\\n\\n') +\n                 `  ${pc.cyan('reg')} delete \"HKCU\\\\Software\\\\Microsoft\\\\Command Processor\" /v AutoRun /f\\n` +\n                 `  ${pc.cyan(\n@@ -185,61 +172,32 @@ export function checkThatNpmCanReadCwd() {\n }\n \n /**\n- * Install packages via npm or yarn.\n+ * Install packages via npm.\n  * Based on the install function from https://github.com/facebook/create-react-app\n  */\n-export function installPackages(\n-    root: string,\n-    useYarn: boolean,\n-    dependencies: string[],\n-    isDev: boolean,\n-    logLevel: CliLogLevel,\n-    isCi: boolean = false,\n-): Promise<void> {\n+export function installPackages(options: {\n+    dependencies: string[];\n+    isDevDependencies?: boolean;\n+    logLevel: CliLogLevel;\n+}): Promise<void> {\n+    const { dependencies, isDevDependencies = false, logLevel } = options;\n     return new Promise((resolve, reject) => {\n-        let command: string;\n-        let args: string[];\n-        if (useYarn) {\n-            command = 'yarnpkg';\n-            args = ['add', '--exact', '--ignore-engines'];\n-            if (isDev) {\n-                args.push('--dev');\n-            }\n-            if (isCi) {\n-                // In CI, publish to Verdaccio\n-                // See https://github.com/yarnpkg/yarn/issues/6029\n-                args.push('--registry http://localhost:4873/');\n-                // Increase network timeout\n-                // See https://github.com/yarnpkg/yarn/issues/4890#issuecomment-358179301\n-                args.push('--network-timeout 300000');\n-            }\n-            args = args.concat(dependencies);\n-\n-            // Explicitly set cwd() to work around issues like\n-            // https://github.com/facebook/create-react-app/issues/3326.\n-            // Unfortunately we can only do this for Yarn because npm support for\n-            // equivalent --prefix flag doesn't help with this issue.\n-            // This is why for npm, we run checkThatNpmCanReadCwd() early instead.\n-            args.push('--cwd');\n-            args.push(root);\n-        } else {\n-            command = 'npm';\n-            args = ['install', '--save', '--save-exact', '--loglevel', 'error'].concat(dependencies);\n-            if (isDev) {\n-                args.push('--save-dev');\n-            }\n+        const command = 'npm';\n+        const args = ['install', '--save', '--save-exact', '--loglevel', 'error'].concat(dependencies);\n+        if (isDevDependencies) {\n+            args.push('--save-dev');\n         }\n \n         if (logLevel === 'verbose') {\n             args.push('--verbose');\n         }\n \n-        const child = spawn(command, args, { stdio: logLevel === 'silent' ? 'ignore' : 'inherit' });\n+        const child = spawn(command, args, { stdio: logLevel === 'verbose' ? 'inherit' : 'ignore' });\n         child.on('close', code => {\n             if (code !== 0) {\n                 let message = 'An error occurred when installing dependencies.';\n                 if (logLevel === 'silent') {\n-                    message += ' Try running with `--log-level info` or `--log-level verbose` to diagnose.';\n+                    message += ' Try running with `--log-level verbose` to diagnose.';\n                 }\n                 reject({\n                     message,\n@@ -285,15 +243,9 @@ function dbDriverPackage(dbType: DbType): string {\n             return 'pg';\n         case 'sqlite':\n             return 'better-sqlite3';\n-        case 'sqljs':\n-            return 'sql.js';\n-        case 'mssql':\n-            return 'mssql';\n-        case 'oracle':\n-            return 'oracledb';\n         default:\n             const n: never = dbType;\n-            console.error(pc.red(`No driver package configured for type \"${dbType as string}\"`));\n+            log(pc.red(`No driver package configured for type \"${dbType as string}\"`));\n             return '';\n     }\n }\n@@ -383,6 +335,133 @@ async function checkPostgresDbExists(options: any, root: string): Promise<true>\n     return true;\n }\n \n+/**\n+ * Check to see if Docker is installed and running.\n+ * If not, attempt to start it.\n+ * If that is not possible, return false.\n+ *\n+ * Refs:\n+ * - https://stackoverflow.com/a/48843074/772859\n+ */\n+export async function isDockerAvailable(): Promise<{ result: 'not-found' | 'not-running' | 'running' }> {\n+    const dockerSpinner = spinner();\n+\n+    function isDaemonRunning(): boolean {\n+        try {\n+            execFileSync('docker', ['stats', '--no-stream'], { stdio: 'ignore' });\n+            return true;\n+        } catch (e: any) {\n+            return false;\n+        }\n+    }\n+\n+    dockerSpinner.start('Checking for Docker');\n+    try {\n+        execFileSync('docker', ['-v'], { stdio: 'ignore' });\n+        dockerSpinner.message('Docker was found!');\n+    } catch (e: any) {\n+        dockerSpinner.stop('Docker was not found on this machine. We will use SQLite for the database.');\n+        return { result: 'not-found' };\n+    }\n+    // Now we need to check if the docker daemon is running\n+    const isRunning = isDaemonRunning();\n+    if (isRunning) {\n+        dockerSpinner.stop('Docker is running');\n+        return { result: 'running' };\n+    }\n+    dockerSpinner.message('Docker daemon is not running. Attempting to start');\n+    // detect the current OS\n+    const currentPlatform = platform();\n+    try {\n+        if (currentPlatform === 'win32') {\n+            // https://stackoverflow.com/a/44182489/772859\n+            execSync('\"C:\\\\Program Files\\\\Docker\\\\Docker\\\\Docker Desktop.exe\"', { stdio: 'ignore' });\n+        } else if (currentPlatform === 'darwin') {\n+            execSync('open -a Docker', { stdio: 'ignore' });\n+        } else {\n+            execSync('systemctl start docker', { stdio: 'ignore' });\n+        }\n+    } catch (e: any) {\n+        dockerSpinner.stop('Could not start Docker.');\n+        log(e.message, { level: 'verbose' });\n+        return { result: 'not-running' };\n+    }\n+    // Verify that the daemon is now running\n+    let attempts = 1;\n+    do {\n+        log(`Checking for Docker daemon... (attempt ${attempts})`, { level: 'verbose' });\n+        if (isDaemonRunning()) {\n+            log(`Docker daemon is now running (after ${attempts} attempts).`, { level: 'verbose' });\n+            dockerSpinner.stop('Docker is running');\n+            return { result: 'running' };\n+        }\n+        await new Promise(resolve => setTimeout(resolve, 50));\n+        attempts++;\n+    } while (attempts < 100);\n+    dockerSpinner.stop('Docker daemon could not be started');\n+    return { result: 'not-running' };\n+}\n+\n+export async function startPostgresDatabase(root: string): Promise<boolean> {\n+    // Now we need to run the postgres database via Docker\n+    let containerName: string | undefined;\n+    const postgresContainerSpinner = spinner();\n+    postgresContainerSpinner.start('Starting PostgreSQL database');\n+    try {\n+        const result = await promisify(execFile)(`docker`, [\n+            `compose`,\n+            `-f`,\n+            path.join(root, 'docker-compose.yml'),\n+            `up`,\n+            `-d`,\n+            `postgres_db`,\n+        ]);\n+        containerName = result.stderr.match(/Container\\s+(.+-postgres_db[^ ]*)/)?.[1];\n+        if (!containerName) {\n+            // guess the container name based on the directory name\n+            containerName = path.basename(root).replace(/[^a-z0-9]/gi, '') + '-postgres_db-1';\n+            postgresContainerSpinner.message(\n+                'Could not find container name. Guessing it is: ' + containerName,\n+            );\n+            log(pc.red('Could not find container name. Guessing it is: ' + containerName), {\n+                newline: 'before',\n+                level: 'verbose',\n+            });\n+        } else {\n+            log(pc.green(`Started PostgreSQL database in container \"${containerName}\"`), {\n+                newline: 'before',\n+                level: 'verbose',\n+            });\n+        }\n+    } catch (e: any) {\n+        log(pc.red(`Failed to start PostgreSQL database: ${e.message as string}`));\n+        postgresContainerSpinner.stop('Failed to start PostgreSQL database');\n+        return false;\n+    }\n+    postgresContainerSpinner.message(`Waiting for PostgreSQL database to be ready...`);\n+    let attempts = 1;\n+    let isReady = false;\n+    do {\n+        // We now need to ensure that the database is ready to accept connections\n+        try {\n+            const result = execFileSync(`docker`, [`exec`, `-i`, containerName, `pg_isready`]);\n+            isReady = result?.toString().includes('accepting connections');\n+            if (!isReady) {\n+                log(pc.yellow(`PostgreSQL database not yet ready. Attempt ${attempts}...`), {\n+                    level: 'verbose',\n+                });\n+            }\n+        } catch (e: any) {\n+            // ignore\n+            log('is_ready error:' + (e.message as string), { level: 'verbose', newline: 'before' });\n+        }\n+        await new Promise(resolve => setTimeout(resolve, 50));\n+        attempts++;\n+    } while (!isReady && attempts < 100);\n+    postgresContainerSpinner.stop('PostgreSQL database is ready');\n+    return true;\n+}\n+\n function throwConnectionError(err: any) {\n     throw new Error(\n         'Could not connect to the database. ' +\n@@ -420,7 +499,35 @@ export function isServerPortInUse(port: number): Promise<boolean> {\n     try {\n         return tcpPortUsed.check(port);\n     } catch (e: any) {\n-        console.log(pc.yellow(`Warning: could not determine whether port ${port} is available`));\n+        log(pc.yellow(`Warning: could not determine whether port ${port} is available`));\n         return Promise.resolve(false);\n     }\n }\n+\n+/**\n+ * Checks if the response from a Clack prompt was a cancellation symbol, and if so,\n+ * ends the interactive process.\n+ */\n+export function checkCancel<T>(value: T | symbol): value is T {\n+    if (isCancel(value)) {\n+        cancel('Setup cancelled.');\n+        process.exit(0);\n+    }\n+    return true;\n+}\n+\n+export function cleanUpDockerResources(name: string) {\n+    try {\n+        execSync(`docker stop $(docker ps -a -q --filter \"label=io.vendure.create.name=${name}\")`, {\n+            stdio: 'ignore',\n+        });\n+        execSync(`docker rm $(docker ps -a -q --filter \"label=io.vendure.create.name=${name}\")`, {\n+            stdio: 'ignore',\n+        });\n+        execSync(`docker volume rm $(docker volume ls --filter \"label=io.vendure.create.name=${name}\" -q)`, {\n+            stdio: 'ignore',\n+        });\n+    } catch (e) {\n+        log(pc.yellow(`Could not clean up Docker resources`), { level: 'verbose' });\n+    }\n+}",
          "packages/create/src/logger.ts": "@@ -0,0 +1,24 @@\n+/* eslint-disable no-console */\n+import { CliLogLevel } from './types';\n+\n+let logLevel: CliLogLevel = 'info';\n+\n+export function setLogLevel(level: CliLogLevel = 'info') {\n+    logLevel = level;\n+}\n+\n+export function log(\n+    message?: string,\n+    options?: { level?: CliLogLevel; newline?: 'before' | 'after' | 'both' },\n+) {\n+    const { level = 'info' } = options || {};\n+    if (logLevel !== 'silent' && (logLevel === 'verbose' || level === 'info')) {\n+        if (options?.newline === 'before' || options?.newline === 'both') {\n+            console.log();\n+        }\n+        console.log('   ' + (message ?? ''));\n+        if (options?.newline === 'after' || options?.newline === 'both') {\n+            console.log();\n+        }\n+    }\n+}",
          "packages/create/src/types.ts": "@@ -1,4 +1,4 @@\n-export type DbType = 'mysql' | 'mariadb' | 'postgres' | 'sqlite' | 'sqljs' | 'mssql' | 'oracle';\n+export type DbType = 'mysql' | 'mariadb' | 'postgres' | 'sqlite';\n \n export interface FileSources {\n     indexSource: string;\n@@ -18,6 +18,6 @@ export interface UserResponses extends FileSources {\n     superadminPassword: string;\n }\n \n-export type PackageManager = 'npm' | 'yarn';\n+export type PackageManager = 'npm';\n \n export type CliLogLevel = 'silent' | 'info' | 'verbose';",
          "packages/create/templates/Dockerfile.hbs": "@@ -3,7 +3,7 @@ FROM node:20\n WORKDIR /usr/src/app\n \n COPY package.json ./\n-COPY {{#if useYarn}}yarn.lock{{else}}package-lock.json{{/if}} ./\n-RUN {{#if useYarn}}yarn{{else}}npm install{{/if}} --production\n+COPY package-lock.json ./\n+RUN npm install --production\n COPY . .\n-RUN {{#if useYarn}}yarn{{else}}npm run{{/if}} build\n+RUN npm run build",
          "packages/create/templates/docker-compose.hbs": "@@ -1,39 +1,115 @@\n-version: \"3\"\n+# INFORMATION\n+# We are not exposing the default ports for the services in this file.\n+# This is to avoid conflicts with existing services on your machine.\n+# In case you don't have any services running on the default ports, you can expose them by changing the\n+# ports section in the services block. Please don't forget to update the ports in the .env file as well.\n+\n services:\n-  server:\n-    build:\n-      context: .\n-      dockerfile: Dockerfile\n-    ports:\n-      - 3000:3000\n-    command: [{{#if useYarn}}\"yarn\"{{else}}\"npm\", \"run\"{{/if}}, \"start:server\"]\n-    volumes:\n-      - /usr/src/app\n-    environment:\n-      DB_HOST: database\n-      DB_PORT: 5432\n-      DB_NAME: vendure\n-      DB_USERNAME: postgres\n-      DB_PASSWORD: password\n-  worker:\n-    build:\n-      context: .\n-      dockerfile: Dockerfile\n-    command: [{{#if useYarn}}\"yarn\"{{else}}\"npm\", \"run\"{{/if}}, \"start:worker\"]\n-    volumes:\n-      - /usr/src/app\n-    environment:\n-      DB_HOST: database\n-      DB_PORT: 5432\n-      DB_NAME: vendure\n-      DB_USERNAME: postgres\n-      DB_PASSWORD: password\n-  database:\n-    image: postgres\n-    volumes:\n-      - /var/lib/postgresql/data\n-    ports:\n-      - 5432:5432\n-    environment:\n-      POSTGRES_PASSWORD: password\n-      POSTGRES_DB: vendure\n+    postgres_db:\n+        image: postgres:16-alpine\n+        volumes:\n+            - postgres_db_data:/var/lib/postgresql/data\n+        ports:\n+            - \"6543:5432\"\n+        environment:\n+            POSTGRES_DB: {{{ escapeSingle dbName }}}\n+            POSTGRES_USER: {{{ escapeSingle dbUserName }}}\n+            POSTGRES_PASSWORD: {{{ escapeSingle dbPassword }}}\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+    mysql_db:\n+        image: mysql:8\n+        volumes:\n+            - mysql_db_data:/var/lib/mysql\n+        environment:\n+            MYSQL_ROOT_PASSWORD: 'ROOT'\n+            MYSQL_DATABASE: {{{ escapeSingle dbName }}}\n+            MYSQL_USER: {{{ escapeSingle dbUserName }}}\n+            MYSQL_PASSWORD: {{{ escapeSingle dbPassword }}}\n+        ports:\n+            - \"4306:3306\"\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+    mariadb_db:\n+        image: mariadb:10\n+        volumes:\n+            - mariadb_db_data:/var/lib/mysql\n+        environment:\n+            MARIADB_ROOT_PASSWORD: 'ROOT'\n+            MARIADB_DATABASE: {{{ escapeSingle dbName }}}\n+            MARIADB_USER: {{{ escapeSingle dbUserName }}}\n+            MARIADB_PASSWORD: {{{ escapeSingle dbPassword }}}\n+        ports:\n+            - \"3306:3306\"\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+    # RECOMMENDED (especially for production)\n+    # Want to use our BullMQ with Redis instead of our default database job queue?\n+    # Checkout our BullMQ plugin: https://docs.vendure.io/reference/core-plugins/job-queue-plugin/bull-mqjob-queue-plugin/\n+    redis:\n+        image: redis:7-alpine\n+        ports:\n+            - \"6479:6379\"\n+        volumes:\n+            - redis_data:/data\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+    # RECOMMENDED\n+    # Want to use Typesense instead of our default search engine?\n+    # Checkout our advanced search plugin: https://vendure.io/hub/vendure-plus-advanced-search-plugin\n+    # To run the typesense container run \"docker compose up -d typesense\"\n+    typesense:\n+        image: typesense/typesense:27\n+        command: [ '--data-dir', '/data', '--api-key', 'SuperSecret' ]\n+        ports:\n+            - \"8208:8108\"\n+        volumes:\n+            - typesense_data:/data\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+    # Want to use Elasticsearch instead of our default database engine?\n+    # Checkout our Elasticsearch plugin: https://docs.vendure.io/reference/core-plugins/elasticsearch-plugin/\n+    # To run the elasticsearch container run \"docker compose up -d elasticsearch\"\n+    elasticsearch:\n+        image: docker.elastic.co/elasticsearch/elasticsearch:7.1.1\n+        environment:\n+            discovery.type: single-node\n+            bootstrap.memory_lock: true\n+            ES_JAVA_OPTS: -Xms512m -Xmx512m\n+        volumes:\n+            - elasticsearch_data:/usr/share/elasticsearch/data\n+        ports:\n+            - \"9300:9200\"\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+\n+volumes:\n+    postgres_db_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+    mysql_db_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+    mariadb_db_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+    typesense_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+    elasticsearch_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"\n+    redis_data:\n+        driver: local\n+        labels:\n+            - \"io.vendure.create.name={{{ escapeSingle name }}}\"",
          "packages/create/templates/readme.hbs": "@@ -17,7 +17,7 @@ Useful links:\n ## Development\n \n ```\n-{{#if useYarn}}yarn dev{{else}}npm run dev{{/if}}\n+npm run dev\n ```\n \n will start the Vendure server and [worker](https://www.vendure.io/docs/developer-guide/vendure-worker/) processes from\n@@ -26,7 +26,7 @@ the `src` directory.\n ## Build\n \n ```\n-{{#if useYarn}}yarn build{{else}}npm run build{{/if}}\n+npm run build\n ```\n \n will compile the TypeScript sources into the `/dist` directory.\n@@ -41,7 +41,7 @@ hosting environment.\n You can run the built files directly with the `start` script:\n \n ```\n-{{#if useYarn}}yarn start{{else}}npm run start{{/if}}\n+npm run start\n ```\n \n You could also consider using a process manager like [pm2](https://pm2.keymetrics.io/) to run and manage\n@@ -79,10 +79,24 @@ used in development.\n - `vendure` - we are referencing the tag we set up during the build.\n - `npm run start:server` - this last part is the actual command that should be run inside the container.\n \n-### Docker compose\n+### Docker Compose\n \n-We've included a sample [docker-compose.yml](./docker-compose.yml) file which demonstrates how the server, worker, and\n-database may be orchestrated with Docker Compose.\n+We've included a [docker-compose.yml](./docker-compose.yml) file which includes configuration for commonly-used\n+services such as PostgreSQL, MySQL, MariaDB, Elasticsearch and Redis.\n+\n+To use Docker Compose, you will need to have Docker installed on your machine. Here are installation\n+instructions for [Mac](https://docs.docker.com/desktop/install/mac-install/), [Windows](https://docs.docker.com/desktop/install/windows-install/),\n+and [Linux](https://docs.docker.com/desktop/install/linux/).\n+\n+You can start the services with:\n+\n+```shell\n+docker-compose up <service>\n+\n+# examples:\n+docker-compose up postgres_db\n+docker-compose up redis\n+```\n \n ## Plugins\n \n@@ -92,7 +106,7 @@ These should be located in the `./src/plugins` directory.\n To create a new plugin run:\n \n ```\n-{{#if useYarn}}yarn{{else}}npx{{/if}} vendure add\n+npx vendure add\n ```\n \n and select `[Plugin] Create a new Vendure plugin`.\n@@ -105,7 +119,7 @@ will be required whenever you make changes to the `customFields` config or defin\n To generate a new migration, run:\n \n ```\n-{{#if useYarn}}yarn{{else}}npx{{/if}} vendure migrate\n+npx vendure migrate\n ```\n \n The generated migration file will be found in the `./src/migrations/` directory, and should be committed to source control.",
          "packages/dev-server/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"dev-server\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"main\": \"index.js\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"private\": true,\n@@ -15,17 +15,17 @@\n     },\n     \"dependencies\": {\n         \"@nestjs/axios\": \"^3.0.2\",\n-        \"@vendure/admin-ui-plugin\": \"^3.0.4\",\n-        \"@vendure/asset-server-plugin\": \"^3.0.4\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n-        \"@vendure/elasticsearch-plugin\": \"^3.0.4\",\n-        \"@vendure/email-plugin\": \"^3.0.4\",\n+        \"@vendure/admin-ui-plugin\": \"^3.0.5\",\n+        \"@vendure/asset-server-plugin\": \"^3.0.5\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n+        \"@vendure/elasticsearch-plugin\": \"^3.0.5\",\n+        \"@vendure/email-plugin\": \"^3.0.5\",\n         \"typescript\": \"5.3.3\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/testing\": \"^3.0.4\",\n-        \"@vendure/ui-devkit\": \"^3.0.4\",\n+        \"@vendure/testing\": \"^3.0.5\",\n+        \"@vendure/ui-devkit\": \"^3.0.5\",\n         \"commander\": \"^12.0.0\",\n         \"concurrently\": \"^8.2.2\",\n         \"csv-stringify\": \"^6.4.6\",",
          "packages/elasticsearch-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/elasticsearch-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -26,8 +26,8 @@\n         \"fast-deep-equal\": \"^3.1.3\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.3.3\"\n     }",
          "packages/email-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/email-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -34,8 +34,8 @@\n         \"@types/express\": \"^4.17.21\",\n         \"@types/fs-extra\": \"^11.0.4\",\n         \"@types/mjml\": \"^4.7.4\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"rimraf\": \"^5.0.5\",\n         \"typescript\": \"5.3.3\"\n     }",
          "packages/harden-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/harden-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -21,7 +21,7 @@\n         \"graphql-query-complexity\": \"^0.12.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\"\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\"\n     }\n }",
          "packages/job-queue-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/job-queue-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"package/index.js\",\n     \"types\": \"package/index.d.ts\",\n@@ -23,8 +23,8 @@\n     },\n     \"devDependencies\": {\n         \"@google-cloud/pubsub\": \"^2.8.0\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"bullmq\": \"^5.4.2\",\n         \"ioredis\": \"^5.3.2\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/payments-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/payments-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"package/index.js\",\n     \"types\": \"package/index.d.ts\",\n@@ -46,9 +46,9 @@\n         \"@mollie/api-client\": \"^3.7.0\",\n         \"@types/braintree\": \"^3.3.11\",\n         \"@types/localtunnel\": \"2.0.4\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n-        \"@vendure/testing\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\",\n+        \"@vendure/testing\": \"^3.0.5\",\n         \"braintree\": \"^3.22.0\",\n         \"localtunnel\": \"2.0.2\",\n         \"nock\": \"^13.1.4\",",
          "packages/sentry-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/sentry-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -22,7 +22,7 @@\n     },\n     \"devDependencies\": {\n         \"@sentry/node\": \"^7.106.1\",\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\"\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\"\n     }\n }",
          "packages/stellate-plugin/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/stellate-plugin\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"license\": \"GPL-3.0-or-later\",\n     \"main\": \"lib/index.js\",\n     \"types\": \"lib/index.d.ts\",\n@@ -21,7 +21,7 @@\n         \"node-fetch\": \"^2.7.0\"\n     },\n     \"devDependencies\": {\n-        \"@vendure/common\": \"^3.0.4\",\n-        \"@vendure/core\": \"^3.0.4\"\n+        \"@vendure/common\": \"^3.0.5\",\n+        \"@vendure/core\": \"^3.0.5\"\n     }\n }",
          "packages/testing/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/testing\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"description\": \"End-to-end testing tools for Vendure projects\",\n     \"keywords\": [\n         \"vendure\",\n@@ -37,7 +37,7 @@\n     },\n     \"dependencies\": {\n         \"@graphql-typed-document-node/core\": \"^3.2.0\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"faker\": \"^4.1.0\",\n         \"form-data\": \"^4.0.0\",\n         \"graphql\": \"~16.9.0\",\n@@ -50,7 +50,7 @@\n         \"@types/mysql\": \"^2.15.26\",\n         \"@types/node-fetch\": \"^2.6.4\",\n         \"@types/pg\": \"^8.11.2\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"mysql\": \"^2.18.1\",\n         \"pg\": \"^8.11.3\",\n         \"rimraf\": \"^5.0.5\",",
          "packages/ui-devkit/package.json": "@@ -1,6 +1,6 @@\n {\n     \"name\": \"@vendure/ui-devkit\",\n-    \"version\": \"3.0.4\",\n+    \"version\": \"3.0.5\",\n     \"description\": \"A library for authoring Vendure Admin UI extensions\",\n     \"keywords\": [\n         \"vendure\",\n@@ -40,8 +40,8 @@\n         \"@angular/cli\": \"^17.2.3\",\n         \"@angular/compiler\": \"^17.2.4\",\n         \"@angular/compiler-cli\": \"^17.2.4\",\n-        \"@vendure/admin-ui\": \"^3.0.4\",\n-        \"@vendure/common\": \"^3.0.4\",\n+        \"@vendure/admin-ui\": \"^3.0.5\",\n+        \"@vendure/common\": \"^3.0.5\",\n         \"chalk\": \"^4.1.0\",\n         \"chokidar\": \"^3.6.0\",\n         \"fs-extra\": \"^11.2.0\",\n@@ -52,7 +52,7 @@\n         \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n         \"@rollup/plugin-terser\": \"^0.4.4\",\n         \"@types/fs-extra\": \"^11.0.4\",\n-        \"@vendure/core\": \"^3.0.4\",\n+        \"@vendure/core\": \"^3.0.5\",\n         \"react\": \"^18.2.0\",\n         \"react-dom\": \"^18.2.0\",\n         \"rimraf\": \"^5.0.5\","
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:H",
      "score": 9.1
    },
    "cwes": [
      {
        "cwe_id": "CWE-20",
        "name": "Improper Input Validation"
      },
      {
        "cwe_id": "CWE-22",
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')"
      }
    ],
    "credits": [],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:H",
        "score": 9.1
      },
      "cvss_v4": {
        "vector_string": null,
        "score": 0
      }
    },
    "epss": {
      "percentage": 0.00065,
      "percentile": 0.29681
    },
    "cve_description": "Vendure is an open-source headless commerce platform. Prior to versions 3.0.5 and 2.3.3, a vulnerability in Vendure's asset server plugin allows an attacker to craft a request which is able to traverse the server file system and retrieve the contents of arbitrary files, including sensitive data such as configuration files, environment variables, and other critical data stored on the server. In the same code path is an additional vector for crashing the server via a malformed URI. Patches are available in versions 3.0.5 and 2.3.3. Some workarounds are also available. One may use object storage rather than the local file system, e.g. MinIO or S3, or define middleware which detects and blocks requests with urls containing `/../`."
  },
  {
    "ghsa_id": "GHSA-vgxq-6rcf-qwrw",
    "cve_id": "CVE-2024-42640",
    "url": "https://api.github.com/advisories/GHSA-vgxq-6rcf-qwrw",
    "html_url": "https://github.com/advisories/GHSA-vgxq-6rcf-qwrw",
    "summary": "angular-base64-upload vulnerable to unauthenticated remote code execution",
    "description": "angular-base64-upload versions prior to v0.1.21 are vulnerable to unauthenticated remote code execution via the `angular-base64-upload/demo/server.php` endpoint. Exploitation of this vulnerability involves uploading arbitrary file content to the server, which can subsequently accessed through the `angular-base64-upload/demo/uploads` endpoint. This leads to the execution of previously uploaded content which enables the attacker to achieve code execution on the server.\n\nNOTE: This vulnerability only affects products that are no longer supported by the maintainer.",
    "type": "reviewed",
    "severity": "critical",
    "repository_advisory_url": null,
    "source_code_location": "https://github.com/adonespitogo/angular-base64-upload",
    "identifiers": [
      {
        "value": "GHSA-vgxq-6rcf-qwrw",
        "type": "GHSA"
      },
      {
        "value": "CVE-2024-42640",
        "type": "CVE"
      }
    ],
    "references": [
      "https://nvd.nist.gov/vuln/detail/CVE-2024-42640",
      "https://github.com/adonespitogo/angular-base64-upload",
      "https://www.zyenra.com/blog/unauthenticated-rce-in-angular-base64-upload.html",
      "https://github.com/rvizx/CVE-2024-42640",
      "https://github.com/advisories/GHSA-vgxq-6rcf-qwrw"
    ],
    "published_at": "2024-10-11T18:32:49Z",
    "updated_at": "2024-10-11T19:44:07Z",
    "github_reviewed_at": "2024-10-11T19:44:05Z",
    "nvd_published_at": "2024-10-11T16:15:08Z",
    "withdrawn_at": null,
    "vulnerabilities": [
      {
        "package": {
          "ecosystem": "npm",
          "name": "angular-base64-upload"
        },
        "vulnerable_version_range": "< 0.1.21",
        "first_patched_version": "0.1.21",
        "vulnerable_functions": [],
        "vulnerable_version": "0.1.19",
        "patches": {
          ".travis.yml": "@@ -1,12 +1,12 @@\n language: node_js\n node_js:\n-  - '0.10.37'\n+  - '4.0'\n before_script:\n   - 'export DISPLAY=:99.0'\n   - 'sh -e /etc/init.d/xvfb start'\n-  - 'npm install -g bower karma grunt-cli jshint'\n+  - 'npm install -g bower jshint karma gulp gulp-cli'\n   - 'npm install' # install npm packages\n   - 'bower install' # install bower packages\n \n after_script:\n-  - 'grunt test' # or other command for build, run tests, etc\n+  - 'gulp test' # or other command for build, run tests, etc",
          "CHANGELOG.md": "@@ -1,6 +1,13 @@\n Change Log\n --------\n \n+v0.1.21\n+ - Migrate to `Gulp` build\n+ - Clear validation errors after clearing input with empty value - [see PR#90](https://github.com/adonespitogo/angular-base64-upload/pull/90)\n+\n+v0.1.20\n+ - Added do-not-parse-if-oversize flag to prevent images above maximum size to be converted to base64.\n+\n v0.1.19\n  - Refactored unit tests - separated into multiple files for easier navigation.\n  - Set view value only once",
          "Gruntfile.js": "@@ -1,118 +0,0 @@\n-/*global module:false*/\n-module.exports = function(grunt) {\n-\n-  // Project configuration.\n-  grunt.initConfig({\n-    // Metadata.\n-    pkg: grunt.file.readJSON('package.json'),\n-    banner:\n-      '/*! <%= pkg.title || pkg.name %> - <%= pkg.version %>\\n' +\n-      '* <%= pkg.homepage %>\\n' +\n-      '* Copyright (c) <%= pkg.author %> [<%= grunt.template.today(\"mmmm dd, yyyy\") %>]\\n' +\n-      '* Licensed <%= pkg.license %> */\\n',\n-    config: {\n-      dist:'./dist',\n-      src: './src',\n-      demo: './demo',\n-      js: [\n-        '<%= config.src %>/**/*.js'\n-      ]\n-    },\n-    copy: {\n-      distToDemo: {\n-        expand: true,\n-        src: '**/angular-base64-upload.js*',\n-        cwd: '<%= config.dist %>/',\n-        dest: '<%= config.demo %>/'\n-      }\n-    },\n-    clean: [\n-      '<%= config.dist %>',\n-      '<%= config.demo %>/<%= pkg.name %>.min.js',\n-      '<%= config.demo %>/<%= pkg.name %>.min.js.map'\n-    ],\n-\n-    // Task configuration.\n-    concat: {\n-      options: {\n-        banner: '<%= banner %>',\n-        stripBanners: true\n-      },\n-      dist: {\n-        src: ['<%= config.js %>'],\n-        dest: '<%= config.dist %>/<%= pkg.name %>.js'\n-      }\n-    },\n-    uglify: {\n-      options: {\n-        banner: '<%= banner %>',\n-        sourceMap: true\n-      },\n-      dist: {\n-        src: '<%= concat.dist.dest %>',\n-        dest: '<%= config.dist %>/<%= pkg.name %>.min.js'\n-      }\n-    },\n-    jshint: {\n-      options: {\n-        curly: true,\n-        eqeqeq: true,\n-        immed: true,\n-        latedef: true,\n-        newcap: true,\n-        noarg: true,\n-        sub: true,\n-        undef: true,\n-        unused: true,\n-        boss: true,\n-        eqnull: true,\n-        browser: true,\n-        globals: {\n-          angular: true\n-        }\n-      },\n-      gruntfile: {\n-        options: {\n-          undef: false\n-        },\n-        src: 'Gruntfile.js'\n-      },\n-      'angular-base64-upload': {\n-        src: 'src/angular-base64-upload.js'\n-      },\n-      tests: {\n-        options: {\n-          undef: false,\n-          unused: false\n-        },\n-        src: ['test/**/*.js']\n-      }\n-    },\n-    karma: {\n-      options: {\n-        configFile: './test/config/karma.conf.js'\n-      },\n-      unit: {\n-      }\n-    },\n-    watch: {\n-      src: {\n-        files: ['<%= config.src %>/<%= pkg.name %>.js'],\n-        tasks: ['build']\n-      }\n-    }\n-  });\n-\n-  // load plugins\n-  require('load-grunt-tasks')(grunt);\n-\n-  grunt.registerTask('default', ['build']);\n-  grunt.registerTask('build', ['clean', 'jshint', 'concat', 'uglify', 'copy']);\n-\n-  grunt.registerTask('test', function () {\n-    var TestRunner = require('./test/config/grunt_test_runner.js');\n-    var runner = new TestRunner(grunt);\n-    runner.run();\n-  });\n-\n-};",
          "Gulpfile.js": "@@ -0,0 +1,53 @@\n+'use strict'\n+\n+const gulp = require('gulp')\n+const uglify = require('gulp-uglify')\n+const jshint = require('gulp-jshint')\n+const insert = require('gulp-insert')\n+const rename = require('gulp-rename')\n+const sourcemaps = require('gulp-sourcemaps')\n+const del = require('del')\n+const gutil = require('gulp-util')\n+const TestRunner = require('./test/config/test_runner')\n+const banner = require('./banner')\n+const src = 'src/**/*.js'\n+const dist = './dist'\n+\n+gulp.task('clean', done => {\n+  del([dist + '/**/*']).then(paths => {\n+    console.log('Deleted files and folders:\\n', paths.join('\\n'))\n+    done()\n+  })\n+})\n+\n+gulp.task('jshint', () => {\n+  return gulp.src(src)\n+    .pipe(jshint())\n+    .pipe(jshint.reporter('default'))\n+})\n+\n+gulp.task('test', ['jshint'], done => {\n+  new TestRunner(done)\n+})\n+\n+gulp.task('debug', ['clean'], () => {\n+  return gulp.src(src)\n+    .pipe(insert.prepend(banner))\n+    .pipe(gulp.dest(dist))\n+})\n+\n+gulp.task('uglify', ['clean'], () => {\n+  return gulp.src(src)\n+    .pipe(sourcemaps.init())\n+    .pipe(uglify())\n+    .pipe(insert.prepend(banner))\n+    .pipe(rename(path => {\n+      path.basename += '.min'\n+    }))\n+    .pipe(sourcemaps.write('./'))\n+    .pipe(gulp.dest(dist))\n+})\n+\n+gulp.task('build', ['clean', 'jshint', 'debug', 'uglify'])\n+\n+gulp.task('default', ['build'])",
          "README.md": "@@ -30,7 +30,7 @@ Installation\n \n Example\n --------------------------\n-See [plunker](http://embed.plnkr.co/MTzfQASN8ZVeocAq7VcM/preview) or the [./demo](https://github.com/adonespitogo/angular-base64-upload/tree/master/demo) folder.\n+See [plunker](http://embed.plnkr.co/MTzfQASN8ZVeocAq7VcM/preview).\n \n Usage\n -------\n@@ -71,32 +71,56 @@ Validations\n </form>\n ```\n \n+Options\n+-------------------\n+ - `do-not-parse-if-oversize` = Prevents the image from being converted to base64 whenever its size exceeds the maximum file size; this can be useful to prevent the browser from freezing whenever an exceedingly large file is uploaded. If this flag is set, the base64 attribute in the model will be set to null whenever an oversized image is uploaded.\n+ \n+```html\n+<form name=\"form\">\n+  <input type=\"file\" ng-model=\"files\" name=\"files\" base-sixty-four-input do-not-parse-if-oversize>\n+</form>\n+```\n+\n Custom Parser\n -------------------\n You can implement your own parsing logic before the data gets added into the model.\n \n-Use case: You want images to be auto-resized after selecting files and add custom model attributes.\n+Use case: You want images to be auto-resized after selecting files and add custom model attributes ([Jimp](https://github.com/oliver-moran/jimp) has been used in the example below).\n+\n \n ```\n-app.controller('ctrl', function ($scope, $q, imageProcessor) {\n+app.controller('ctrl', function ($scope, $q) {\n \n   $scope.resizeImage = function ( file, base64_object ) {\n-\n+    // file is an instance of File constructor.\n+    // base64_object is an object that contains compiled base64 image data from file.\n     var deferred = $q.defer();\n-\n-    imageProcessor.run(file).then(function (resized) {\n-      var modelVal = {\n-        file: file,\n-        resized: resized\n-      };\n-      deferred.resolve(modelVal); // resolved value is appended to the model\n+    var url = URL.createObjectURL(file);// creates url for file object.\n+    Jimp.read(url)\n+    .then(function (item) {\n+      item\n+      .resize(1280, Jimp.AUTO)// width of 1280px, auto-adjusted height\n+      .quality(50)//drops the image quality to 50%\n+      .getBase64(file.type, function (err, newBase64) {\n+        if (err) {throw err;}\n+        var bytes = Math.round((3/4)*newBase64.length);\n+        base64Object.filetype = file.type;\n+        base64Object.filesize = bytes;\n+        base64Object.base64 = newBase64;\n+        // Note that base64 in this package doesn't contain \"data:image/jpeg;base64,\" part,\n+        // while base64 string from Jimp does. It should be taken care of in back-end side.\n+        deferred.resolve(base64Object);\n+      });\n+    })\n+    .catch(function (err) {\n+      return console.log(err);// error handling\n     });\n-\n     return deferred.promise;\n   };\n \n });\n \n+<script src='/js/jimp.min.js'></script>\n <input type=\"file\" base-sixty-four-input ng-model=\"images\" parser=\"resizeImage\" multiple>\n \n ```\n@@ -145,7 +169,7 @@ Events\n \n Example event handler implementation:\n    ```\n-   $scope.errorHandler = function (event, reader, fileList, fileObjs, file) {\n+   $scope.errorHandler = function (event, reader, file, fileList, fileObjs, object) {\n      console.log(\"An error occurred while reading file: \"+file.name);\n      reader.abort();\n    };\n@@ -199,10 +223,12 @@ end\n \n Contribution\n ------------\n- - Using [Grunt](http://gruntjs.com) as build tool\n  - Uses [jasmine 1.3](http://jasmine.github.io/1.3/introduction.html) in writing unit test specs\n- - `grunt test` to run unit tests\n- - `grunt build` to build the project\n+ - `npm install -g gulp gulp-cli bower`\n+ - `npm install`\n+ - `bower install`\n+ - `gulp test` to run unit tests\n+ - `gulp build` to build the project\n  - Update `README.md` and `CHANGELOG.md` to reflect the new changes\n  - Update the version number of `package.json` and `bower.json`\n ",
          "banner.js": "@@ -0,0 +1,11 @@\n+`use strict`\n+\n+const pkg = require('./package')\n+var date = new Date();\n+var n = date.toDateString();\n+// var time = date.toLocaleTimeString();\n+\n+module.exports = `/*! ${pkg.title || pkg.name} - ${pkg.version}\\n` +\n+      `* ${pkg.homepage}\\n` +\n+      `* Copyright (c) ${pkg.author} [${n}]\\n` +\n+      `* Licensed ${pkg.license} */\\n`\n\\ No newline at end of file",
          "bower.json": "@@ -1,7 +1,7 @@\n {\n   \"name\": \"angular-base64-upload\",\n   \"main\": \"src/angular-base64-upload.js\",\n-  \"version\": \"v0.1.19\",\n+  \"version\": \"v0.1.21\",\n   \"homepage\": \"https://github.com/adonespitogo/angular-base64-upload\",\n   \"authors\": [\n     \"Adones Pitogo <pitogo.adones@gmail.com>\"",
          "demo/README.md": "@@ -1,13 +0,0 @@\n-angular-base-64-upload-demo\n-===========================\n-\n-![alt tag](https://raw.github.com/adonespitogo/angular-base64-upload/master/banner.png)\n-\n-Running\n--------\n-<p>Inside your clone's directory:</p>\n-\n-```\n-$ cd demo/\n-$ php -S 127.0.0.1:8000\n-```",
          "demo/angular-base64-upload.js": "@@ -1,287 +0,0 @@\n-/*! angular-base64-upload - v0.1.19\n-* https://github.com/adonespitogo/angular-base64-upload\n-* Copyright (c) Adones Pitogo <pitogo.adones@gmail.com> [March 13, 2016]\n-* Licensed MIT */\n-(function(window, undefined) {\n-\n-  'use strict';\n-\n-  /* istanbul ignore next */\n-  window._arrayBufferToBase64 = function(buffer) { //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\n-    var binary = '';\n-    var bytes = new Uint8Array(buffer);\n-    var len = bytes.byteLength;\n-    for (var i = 0; i < len; i++) {\n-      binary += String.fromCharCode(bytes[i]);\n-    }\n-    return window.btoa(binary);\n-  };\n-\n-\n-  var mod = window.angular.module('naif.base64', []);\n-\n-  mod.directive('baseSixtyFourInput', [\n-    '$window',\n-    '$q',\n-    function($window, $q) {\n-\n-      var isolateScope = {\n-        onChange: '&',\n-        onAfterValidate: '&',\n-        parser: '&'\n-      };\n-\n-      var FILE_READER_EVENTS = ['onabort', 'onerror', 'onloadstart', 'onloadend', 'onprogress', 'onload'];\n-      for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-        var e = FILE_READER_EVENTS[i];\n-        isolateScope[e] = '&';\n-      }\n-\n-      return {\n-        restrict: 'A',\n-        require: 'ngModel',\n-        scope: isolateScope,\n-        link: function(scope, elem, attrs, ngModel) {\n-\n-          /* istanbul ignore if */\n-          if (!ngModel) {\n-            return;\n-          }\n-\n-          var rawFiles = [];\n-          var fileObjects = [];\n-\n-          elem.on('change', function(e) {\n-\n-            if (!e.target.files.length) {\n-              return;\n-            }\n-\n-            fileObjects = [];\n-            fileObjects = angular.copy(fileObjects);\n-            rawFiles = e.target.files; // use event target so we can mock the files from test\n-            _readFiles();\n-            _onChange(e);\n-            _onAfterValidate(e);\n-          });\n-\n-          function _readFiles() {\n-            for (var i = rawFiles.length - 1; i >= 0; i--) {\n-              var reader = new $window.FileReader();\n-              var file = rawFiles[i];\n-              var fileObject = {};\n-              var promises = [];\n-\n-              fileObject.filetype = file.type;\n-              fileObject.filename = file.name;\n-              fileObject.filesize = file.size;\n-\n-              // append file a new promise, that waits until resolved\n-              rawFiles[i].deferredObj = $q.defer();\n-              promises.push(rawFiles[i].deferredObj.promise);\n-\n-              // set view value once all files are read\n-              $q.all(promises).then(_setViewValue);\n-              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\n-\n-              _attachEventHandlers(reader, file, fileObject);\n-              reader.readAsArrayBuffer(file);\n-            }\n-          }\n-\n-          function _onChange(e) {\n-            if (attrs.onChange) {\n-              scope.onChange()(e, rawFiles);\n-            }\n-          }\n-\n-          function _onAfterValidate(e) {\n-            if (attrs.onAfterValidate) {\n-              // wait for all promises, in rawFiles,\n-              //   then call onAfterValidate\n-              var promises = [];\n-              for (var i = rawFiles.length - 1; i >= 0; i--) {\n-                promises.push(rawFiles[i].deferredObj.promise);\n-              }\n-              $q.all(promises).then(function() {\n-                scope.onAfterValidate()(e, fileObjects, rawFiles);\n-              });\n-            }\n-          }\n-\n-          function _attachEventHandlers(fReader, file, fileObject) {\n-\n-            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-              var e = FILE_READER_EVENTS[i];\n-              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\n-                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\n-              }\n-            }\n-\n-            fReader.onload = _readerOnLoad(fReader, file, fileObject);\n-          }\n-\n-          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\n-            fReader[eventName] = function(e) {\n-              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-            };\n-          }\n-\n-          function _readerOnLoad(fReader, file, fileObject) {\n-\n-            return function(e) {\n-\n-              var buffer = e.target.result;\n-              var promise;\n-\n-              fileObject.base64 = $window._arrayBufferToBase64(buffer);\n-\n-              if (attrs.parser) {\n-                promise = $q.when(scope.parser()(file, fileObject));\n-              } else {\n-                promise = $q.when(fileObject);\n-              }\n-\n-              promise.then(function(fileObj) {\n-                fileObjects.push(fileObj);\n-                // _setViewValue();\n-\n-                // fulfill the promise here.\n-                file.deferredObj.resolve();\n-              });\n-\n-              if (attrs.onload) {\n-                scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-              }\n-\n-            };\n-\n-          }\n-\n-          function _setViewValue() {\n-            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\n-            ngModel.$setViewValue(newVal);\n-            _maxsize(newVal);\n-            _minsize(newVal);\n-            _maxnum(newVal);\n-            _minnum(newVal);\n-            _accept(newVal);\n-          }\n-\n-          ngModel.$isEmpty = function(val) {\n-            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\n-          };\n-\n-          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\n-          scope._clearInput = function() {\n-            elem[0].value = '';\n-          };\n-\n-          scope.$watch(function() {\n-            return ngModel.$viewValue;\n-          }, function(val, oldVal) {\n-            if (ngModel.$isEmpty(oldVal)) {\n-              return; }\n-            if (ngModel.$isEmpty(val)) {\n-              scope._clearInput();\n-            }\n-          });\n-\n-          // VALIDATIONS =========================================================\n-\n-          function _maxnum(val) {\n-            if (attrs.maxnum && attrs.multiple && val) {\n-              var valid = val.length <= parseInt(attrs.maxnum);\n-              ngModel.$setValidity('maxnum', valid);\n-            }\n-            return val;\n-          }\n-\n-          function _minnum(val) {\n-            if (attrs.minnum && attrs.multiple && val) {\n-              var valid = val.length >= parseInt(attrs.minnum);\n-              ngModel.$setValidity('minnum', valid);\n-            }\n-            return val;\n-          }\n-\n-          function _maxsize(val) {\n-            var valid = true;\n-\n-            if (attrs.maxsize && val) {\n-              var max = parseFloat(attrs.maxsize) * 1000;\n-\n-              if (attrs.multiple) {\n-                for (var i = 0; i < val.length; i++) {\n-                  var file = val[i];\n-                  if (file.filesize > max) {\n-                    valid = false;\n-                    break;\n-                  }\n-                }\n-              } else {\n-                valid = val.filesize <= max;\n-              }\n-              ngModel.$setValidity('maxsize', valid);\n-            }\n-\n-            return val;\n-          }\n-\n-          function _minsize(val) {\n-            var valid = true;\n-            var min = parseFloat(attrs.minsize) * 1000;\n-\n-            if (attrs.minsize && val) {\n-              if (attrs.multiple) {\n-                for (var i = 0; i < val.length; i++) {\n-                  var file = val[i];\n-                  if (file.filesize < min) {\n-                    valid = false;\n-                    break;\n-                  }\n-                }\n-              } else {\n-                valid = val.filesize >= min;\n-              }\n-              ngModel.$setValidity('minsize', valid);\n-            }\n-\n-            return val;\n-          }\n-\n-          function _accept(val) {\n-            var valid = true;\n-            var regExp, exp, fileExt;\n-            if (attrs.accept) {\n-              exp = attrs.accept.trim().replace(/[,\\s]+/gi, \"|\").replace(/\\./g, \"\\\\.\").replace(/\\/\\*/g, \"/.*\");\n-              regExp = new RegExp(exp);\n-            }\n-\n-            if (attrs.accept && val) {\n-              if (attrs.multiple) {\n-                for (var i = 0; i < val.length; i++) {\n-                  var file = val[i];\n-                  fileExt = \".\" + file.filename.split('.').pop();\n-                  valid = regExp.test(file.filetype) || regExp.test(fileExt);\n-\n-                  if (!valid) {\n-                    break; }\n-                }\n-              } else {\n-                fileExt = \".\" + val.filename.split('.').pop();\n-                valid = regExp.test(val.filetype) || regExp.test(fileExt);\n-              }\n-              ngModel.$setValidity('accept', valid);\n-            }\n-\n-            return val;\n-          }\n-\n-        }\n-      };\n-\n-    }\n-  ]);\n-\n-})(window);",
          "demo/index.html": "@@ -1,131 +0,0 @@\n-<!DOCTYPE html>\n-<html lang=\"en\" ng-app='myApp'>\n-<head>\n-  <meta charset=\"UTF-8\">\n-  <title>Angular Base64 Upload Demo</title>\n-  <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css\">\n-  <script type=\"text/javascript\" src=\"//cdnjs.cloudflare.com/ajax/libs/angular.js/1.3.5/angular.min.js\"></script>\n-  <script type=\"text/javascript\" src=\"angular-base64-upload.js\"></script>\n-  <script type=\"text/javascript\">\n-    angular.module('myApp', ['naif.base64'])\n-    .controller('ctrl', function($scope, $http, $window, $rootScope){\n-\n-      var uploadedCount = 0;\n-\n-      $scope.files = [];\n-      $scope.file = {};\n-\n-      $scope.uploadFiles = function() {\n-\n-        var files = angular.copy($scope.files);\n-\n-        if ($scope.file) {\n-          files.push($scope.file);\n-        }\n-\n-        if (files.length === 0) {\n-          $window.alert('Please select files!');\n-          return false;\n-        }\n-\n-        for (var i = files.length - 1; i >= 0; i--) {\n-          var file = files[i];\n-          $http.post('server.php', file)\n-          .success(function(res){\n-            uploadedCount ++;\n-            if (uploadedCount == files.length) {\n-              $window.alert('View uploaded files?');\n-              $window.location.assign('/uploads');\n-            }\n-          });\n-        }\n-      };\n-    });\n-  </script>\n-  <style>\n-    body{padding-bottom: 50px;}\n-    .alert{margin-top: 15px;}\n-  </style>\n-</head>\n-<body ng-controller=\"ctrl\">\n-  <div class=\"container\">\n-    <form name=\"form\">\n-    <h3>Single File Selection</h3>\n-      <div class=\"input-group\">\n-        <label for=\"file\">Select File</label>\n-        <input class=\"form-control\" type=\"file\" ng-model=\"file\" name=\"inputFile\" base-sixty-four-input required maxsize=\"500\" accept=\"image/*\">\n-        <span class=\"help-block\">\n-          <a ng-click=\"file=null\" class=\"btn btn-default\">Clear input</a>\n-          <ul>\n-            <li>required</li>\n-            <li>maxsize = 500</li>\n-            <li>accept = image/*</li>\n-          </ul>\n-        </span>\n-        <div class=\"alert\" ng-class=\"{'alert-danger': form.inputFile.$invalid, 'alert-success': form.inputFile.$valid}\">\n-          form.inputFile.$error: {{ form.inputFile.$error }}\n-        </div>\n-      </div>\n-    <b>Model Value:</b>\n-    <table class=\"table table-bordered table-striped\">\n-      <tr>\n-        <th>filename</th>\n-        <th>filetype</th>\n-        <th>filesize (<i><small>KB</small></i>)</th>\n-        <th>base64</th>\n-      </tr>\n-      <tr ng-show=\"file\">\n-        <td>{{file.filename}}</td>\n-        <td>{{file.filetype}}</td>\n-        <td>{{file.filesize / 1000}}</td>\n-        <td>{{file.base64.substring(0, 30)}}...</td>\n-      </tr>\n-      <tr>\n-        <td colspan=\"4\" ng-show=\"!file\">\n-          <small><i>No file selected.</i></small>\n-        </td>\n-      </tr>\n-    </table>\n-    <hr>\n-    <h3>Multiple Files Selection</h3>\n-      <div class=\"input-group\">\n-        <label for=\"file\">Select Files</label>\n-        <span class=\"help-block\">\n-          <ul>\n-            <li>required</li>\n-            <li>minsize = 500</li>\n-            <li>accept = image/*, .zip</li>\n-            <li>minnum = 2</li>\n-          </ul>\n-        </span>\n-        <input class=\"form-control\" type=\"file\" ng-model=\"files\" name=\"files\" base-sixty-four-input multiple accept=\"image/*, .zip\" minsize=\"500\" required minnum=\"2\">\n-        <a ng-click=\"files=null\" class=\"help-block btn btn-default\">Clear input</a>\n-      </div>\n-      <div class=\"alert\" ng-class=\"{'alert-danger': form.files.$invalid, 'alert-success': form.files.$valid}\">\n-        form.files.$error: {{form.files.$error}}<br>\n-      </div>\n-    <b>Model Value:</b>\n-    <table class=\"table table-bordered table-striped\">\n-      <tr>\n-        <th>filename</th>\n-        <th>filetype</th>\n-        <th>filesize (<i><small>KB</small></i>)</th>\n-        <th>base64</th>\n-      </tr>\n-      <tr ng-repeat=\"file in files\">\n-        <td>{{file.filename}}</td>\n-        <td>{{file.filetype}}</td>\n-        <td>{{file.filesize / 1000}}</td>\n-        <td>{{file.base64.substring(0, 30)}}...</td>\n-      </tr>\n-      <tr>\n-        <td colspan=\"4\" ng-show=\"files.length == 0\">\n-          <small><i>No file selected.</i></small>\n-        </td>\n-      </tr>\n-    </table>\n-    </form>\n-  </div>\n-</body>\n-\n-</html>",
          "demo/server.php": "@@ -1,36 +0,0 @@\n-<?php\n-\n-class Base64File\n-{\n-\n-  public $base64 = '';\n-  public $filename = '';\n-\n-  private $folder = 'uploads';\n-\n-  function __construct($attrs)\n-  {\n-    $this->base64 = $attrs['base64'];\n-    $this->filename = $this->folder.'/'.$attrs['filename'];\n-    $this->decodeBase64File();\n-    return $this;\n-  }\n-\n-  function decodeBase64File() {\n-      $ifp = fopen($this->filename, 'w');\n-      fwrite( $ifp, base64_decode( $this->base64) );\n-      fclose($ifp);\n-      return $ifp;\n-  }\n-\n-}\n-\n-//parse request payload\n-$postdata = file_get_contents(\"php://input\");\n-$request = json_decode($postdata, true);\n-//end parse\n-\n-$file = new Base64File($request);\n-echo $file->filename;\n-\n-?>\n\\ No newline at end of file",
          "demo/uploads/index.php": "@@ -1,12 +0,0 @@\n-<?php\n-\n-$dir = dirname(__FILE__);\n-\n-$files = scandir($dir);\n-\n-foreach ($files as $file) {\n-  if($file{0} != '.' && $file != 'index.php') {\n-    echo \"<a href='uploads/$file'>$file</a><br>\";\n-  }\n-}\n-",
          "dist/angular-base64-upload.js": "@@ -1,17 +1,19 @@\n-/*! angular-base64-upload - v0.1.19\n+/*! angular-base64-upload - v0.1.21\n * https://github.com/adonespitogo/angular-base64-upload\n-* Copyright (c) Adones Pitogo <pitogo.adones@gmail.com> [March 13, 2016]\n+* Copyright (c) Adones Pitogo <pitogo.adones@gmail.com> [Wed Apr 26 2017]\n * Licensed MIT */\n (function(window, undefined) {\n \n   'use strict';\n \n   /* istanbul ignore next */\n-  window._arrayBufferToBase64 = function(buffer) { //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\n+  //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\n+  window._arrayBufferToBase64 = function(buffer) {\n     var binary = '';\n     var bytes = new Uint8Array(buffer);\n     var len = bytes.byteLength;\n-    for (var i = 0; i < len; i++) {\n+\n+    for (var i = 0; i < len; i += 1) {\n       binary += String.fromCharCode(bytes[i]);\n     }\n     return window.btoa(binary);\n@@ -32,161 +34,25 @@\n       };\n \n       var FILE_READER_EVENTS = ['onabort', 'onerror', 'onloadstart', 'onloadend', 'onprogress', 'onload'];\n-      for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-        var e = FILE_READER_EVENTS[i];\n+\n+      FILE_READER_EVENTS.forEach(function(e) {\n         isolateScope[e] = '&';\n-      }\n+      });\n \n       return {\n         restrict: 'A',\n         require: 'ngModel',\n         scope: isolateScope,\n         link: function(scope, elem, attrs, ngModel) {\n \n-          /* istanbul ignore if */\n-          if (!ngModel) {\n-            return;\n-          }\n-\n           var rawFiles = [];\n           var fileObjects = [];\n \n-          elem.on('change', function(e) {\n-\n-            if (!e.target.files.length) {\n-              return;\n-            }\n-\n-            fileObjects = [];\n-            fileObjects = angular.copy(fileObjects);\n-            rawFiles = e.target.files; // use event target so we can mock the files from test\n-            _readFiles();\n-            _onChange(e);\n-            _onAfterValidate(e);\n-          });\n-\n-          function _readFiles() {\n-            for (var i = rawFiles.length - 1; i >= 0; i--) {\n-              var reader = new $window.FileReader();\n-              var file = rawFiles[i];\n-              var fileObject = {};\n-              var promises = [];\n-\n-              fileObject.filetype = file.type;\n-              fileObject.filename = file.name;\n-              fileObject.filesize = file.size;\n-\n-              // append file a new promise, that waits until resolved\n-              rawFiles[i].deferredObj = $q.defer();\n-              promises.push(rawFiles[i].deferredObj.promise);\n-\n-              // set view value once all files are read\n-              $q.all(promises).then(_setViewValue);\n-              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\n-\n-              _attachEventHandlers(reader, file, fileObject);\n-              reader.readAsArrayBuffer(file);\n-            }\n-          }\n-\n-          function _onChange(e) {\n-            if (attrs.onChange) {\n-              scope.onChange()(e, rawFiles);\n-            }\n-          }\n-\n-          function _onAfterValidate(e) {\n-            if (attrs.onAfterValidate) {\n-              // wait for all promises, in rawFiles,\n-              //   then call onAfterValidate\n-              var promises = [];\n-              for (var i = rawFiles.length - 1; i >= 0; i--) {\n-                promises.push(rawFiles[i].deferredObj.promise);\n-              }\n-              $q.all(promises).then(function() {\n-                scope.onAfterValidate()(e, fileObjects, rawFiles);\n-              });\n-            }\n-          }\n-\n-          function _attachEventHandlers(fReader, file, fileObject) {\n-\n-            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-              var e = FILE_READER_EVENTS[i];\n-              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\n-                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\n-              }\n-            }\n-\n-            fReader.onload = _readerOnLoad(fReader, file, fileObject);\n-          }\n-\n-          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\n-            fReader[eventName] = function(e) {\n-              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-            };\n-          }\n-\n-          function _readerOnLoad(fReader, file, fileObject) {\n-\n-            return function(e) {\n-\n-              var buffer = e.target.result;\n-              var promise;\n-\n-              fileObject.base64 = $window._arrayBufferToBase64(buffer);\n-\n-              if (attrs.parser) {\n-                promise = $q.when(scope.parser()(file, fileObject));\n-              } else {\n-                promise = $q.when(fileObject);\n-              }\n-\n-              promise.then(function(fileObj) {\n-                fileObjects.push(fileObj);\n-                // _setViewValue();\n-\n-                // fulfill the promise here.\n-                file.deferredObj.resolve();\n-              });\n-\n-              if (attrs.onload) {\n-                scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-              }\n-\n-            };\n-\n-          }\n-\n-          function _setViewValue() {\n-            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\n-            ngModel.$setViewValue(newVal);\n-            _maxsize(newVal);\n-            _minsize(newVal);\n-            _maxnum(newVal);\n-            _minnum(newVal);\n-            _accept(newVal);\n+          /* istanbul ignore if */\n+          if (!ngModel) {\n+            return;\n           }\n \n-          ngModel.$isEmpty = function(val) {\n-            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\n-          };\n-\n-          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\n-          scope._clearInput = function() {\n-            elem[0].value = '';\n-          };\n-\n-          scope.$watch(function() {\n-            return ngModel.$viewValue;\n-          }, function(val, oldVal) {\n-            if (ngModel.$isEmpty(oldVal)) {\n-              return; }\n-            if (ngModel.$isEmpty(val)) {\n-              scope._clearInput();\n-            }\n-          });\n-\n           // VALIDATIONS =========================================================\n \n           function _maxnum(val) {\n@@ -266,7 +132,8 @@\n                   valid = regExp.test(file.filetype) || regExp.test(fileExt);\n \n                   if (!valid) {\n-                    break; }\n+                    break;\n+                  }\n                 }\n               } else {\n                 fileExt = \".\" + val.filename.split('.').pop();\n@@ -278,6 +145,162 @@\n             return val;\n           }\n \n+          //end validations ===============\n+\n+          function _setViewValue() {\n+            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\n+            ngModel.$setViewValue(newVal);\n+            _maxsize(newVal);\n+            _minsize(newVal);\n+            _maxnum(newVal);\n+            _minnum(newVal);\n+            _accept(newVal);\n+          }\n+\n+          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\n+            fReader[eventName] = function(e) {\n+              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\n+            };\n+          }\n+\n+          function _readerOnLoad(fReader, file, fileObject) {\n+\n+            return function(e) {\n+\n+              var buffer = e.target.result;\n+              var promise;\n+\n+              // do not convert the image to base64 if it exceeds the maximum\n+              // size to prevent the browser from freezing\n+              var exceedsMaxSize = attrs.maxsize && file.size > attrs.maxsize * 1024;\n+              if (attrs.doNotParseIfOversize !== undefined && exceedsMaxSize) {\n+                fileObject.base64 = null;\n+              } else {\n+                fileObject.base64 = $window._arrayBufferToBase64(buffer);\n+              }\n+\n+              if (attrs.parser) {\n+                promise = $q.when(scope.parser()(file, fileObject));\n+              } else {\n+                promise = $q.when(fileObject);\n+              }\n+\n+              promise.then(function(fileObj) {\n+                fileObjects.push(fileObj);\n+                // fulfill the promise here.\n+                file.deferredObj.resolve();\n+              });\n+\n+              if (attrs.onload) {\n+                if (scope.onload && typeof scope.onload() === \"function\") {\n+                  scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\n+                } else {\n+                  scope.onload(e, rawFiles);\n+                }\n+              }\n+\n+            };\n+\n+          }\n+\n+          function _attachEventHandlers(fReader, file, fileObject) {\n+\n+            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n+              var e = FILE_READER_EVENTS[i];\n+              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\n+                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\n+              }\n+            }\n+\n+            fReader.onload = _readerOnLoad(fReader, file, fileObject);\n+          }\n+\n+          function _readFiles() {\n+            var promises = [];\n+            var i;\n+            for (i = rawFiles.length - 1; i >= 0; i--) {\n+              // append file a new promise, that waits until resolved\n+              rawFiles[i].deferredObj = $q.defer();\n+              promises.push(rawFiles[i].deferredObj.promise);\n+              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\n+            }\n+\n+            // set view value once all files are read\n+            $q.all(promises).then(_setViewValue);\n+\n+            for (i = rawFiles.length - 1; i >= 0; i--) {\n+              var reader = new $window.FileReader();\n+              var file = rawFiles[i];\n+              var fileObject = {};\n+\n+              fileObject.filetype = file.type;\n+              fileObject.filename = file.name;\n+              fileObject.filesize = file.size;\n+\n+              _attachEventHandlers(reader, file, fileObject);\n+              reader.readAsArrayBuffer(file);\n+            }\n+          }\n+\n+          function _onChange(e) {\n+            if (attrs.onChange) {\n+              if (scope.onChange && typeof scope.onChange() === \"function\") {\n+                scope.onChange()(e, rawFiles);\n+              } else {\n+                scope.onChange(e, rawFiles);\n+              }\n+            }\n+          }\n+\n+          function _onAfterValidate(e) {\n+            if (attrs.onAfterValidate) {\n+              // wait for all promises, in rawFiles,\n+              //   then call onAfterValidate\n+              var promises = [];\n+              for (var i = rawFiles.length - 1; i >= 0; i--) {\n+                promises.push(rawFiles[i].deferredObj.promise);\n+              }\n+              $q.all(promises).then(function() {\n+                if (scope.onAfterValidate && typeof scope.onAfterValidate() === \"function\") {\n+                  scope.onAfterValidate()(e, fileObjects, rawFiles);\n+                } else {\n+                  scope.onAfterValidate(e, fileObjects, rawFiles);\n+                }\n+              });\n+            }\n+          }\n+\n+          ngModel.$isEmpty = function(val) {\n+            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\n+          };\n+\n+          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\n+          scope._clearInput = function() {\n+            elem[0].value = '';\n+          };\n+\n+          scope.$watch(function() {\n+            return ngModel.$viewValue;\n+          }, function(val) {\n+            if (ngModel.$isEmpty(val)) {\n+              scope._clearInput();\n+            }\n+          });\n+\n+          elem.on('change', function(e) {\n+\n+            if (!e.target.files.length) {\n+              return;\n+            }\n+\n+            fileObjects = [];\n+            fileObjects = angular.copy(fileObjects);\n+            rawFiles = e.target.files; // use event target so we can mock the files from test\n+            _readFiles();\n+            _onChange(e);\n+            _onAfterValidate(e);\n+          });\n+\n         }\n       };\n ",
          "dist/angular-base64-upload.min.js": "@@ -1,7 +1,6 @@\n-/*! angular-base64-upload - v0.1.19\n+/*! angular-base64-upload - v0.1.21\n * https://github.com/adonespitogo/angular-base64-upload\n-* Copyright (c) Adones Pitogo <pitogo.adones@gmail.com> [March 13, 2016]\n+* Copyright (c) Adones Pitogo <pitogo.adones@gmail.com> [Wed Apr 26 2017]\n * Licensed MIT */\n-\n-!function(a,b){\"use strict\";a._arrayBufferToBase64=function(b){for(var c=\"\",d=new Uint8Array(b),e=d.byteLength,f=0;e>f;f++)c+=String.fromCharCode(d[f]);return a.btoa(c)};var c=a.angular.module(\"naif.base64\",[]);c.directive(\"baseSixtyFourInput\",[\"$window\",\"$q\",function(a,b){for(var c={onChange:\"&\",onAfterValidate:\"&\",parser:\"&\"},d=[\"onabort\",\"onerror\",\"onloadstart\",\"onloadend\",\"onprogress\",\"onload\"],e=d.length-1;e>=0;e--){var f=d[e];c[f]=\"&\"}return{restrict:\"A\",require:\"ngModel\",scope:c,link:function(c,e,f,g){function h(){for(var c=t.length-1;c>=0;c--){var d=new a.FileReader,e=t[c],f={},g=[];f.filetype=e.type,f.filename=e.name,f.filesize=e.size,t[c].deferredObj=b.defer(),g.push(t[c].deferredObj.promise),b.all(g).then(n),k(d,e,f),d.readAsArrayBuffer(e)}}function i(a){f.onChange&&c.onChange()(a,t)}function j(a){if(f.onAfterValidate){for(var d=[],e=t.length-1;e>=0;e--)d.push(t[e].deferredObj.promise);b.all(d).then(function(){c.onAfterValidate()(a,u,t)})}}function k(a,b,e){for(var g=d.length-1;g>=0;g--){var h=d[g];f[h]&&\"onload\"!==h&&l(h,c[h],a,b,e)}a.onload=m(a,b,e)}function l(a,b,c,d,e){c[a]=function(a){b()(a,c,d,t,u,e)}}function m(d,e,g){return function(h){var i,j=h.target.result;g.base64=a._arrayBufferToBase64(j),i=f.parser?b.when(c.parser()(e,g)):b.when(g),i.then(function(a){u.push(a),e.deferredObj.resolve()}),f.onload&&c.onload()(h,d,e,t,u,g)}}function n(){var a=f.multiple?u:u[0];g.$setViewValue(a),q(a),r(a),o(a),p(a),s(a)}function o(a){if(f.maxnum&&f.multiple&&a){var b=a.length<=parseInt(f.maxnum);g.$setValidity(\"maxnum\",b)}return a}function p(a){if(f.minnum&&f.multiple&&a){var b=a.length>=parseInt(f.minnum);g.$setValidity(\"minnum\",b)}return a}function q(a){var b=!0;if(f.maxsize&&a){var c=1e3*parseFloat(f.maxsize);if(f.multiple)for(var d=0;d<a.length;d++){var e=a[d];if(e.filesize>c){b=!1;break}}else b=a.filesize<=c;g.$setValidity(\"maxsize\",b)}return a}function r(a){var b=!0,c=1e3*parseFloat(f.minsize);if(f.minsize&&a){if(f.multiple)for(var d=0;d<a.length;d++){var e=a[d];if(e.filesize<c){b=!1;break}}else b=a.filesize>=c;g.$setValidity(\"minsize\",b)}return a}function s(a){var b,c,d,e=!0;if(f.accept&&(c=f.accept.trim().replace(/[,\\s]+/gi,\"|\").replace(/\\./g,\"\\\\.\").replace(/\\/\\*/g,\"/.*\"),b=new RegExp(c)),f.accept&&a){if(f.multiple)for(var h=0;h<a.length;h++){var i=a[h];if(d=\".\"+i.filename.split(\".\").pop(),e=b.test(i.filetype)||b.test(d),!e)break}else d=\".\"+a.filename.split(\".\").pop(),e=b.test(a.filetype)||b.test(d);g.$setValidity(\"accept\",e)}return a}if(g){var t=[],u=[];e.on(\"change\",function(a){a.target.files.length&&(u=[],u=angular.copy(u),t=a.target.files,h(),i(a),j(a))}),g.$isEmpty=function(a){return!a||(angular.isArray(a)?0===a.length:!a.base64)},c._clearInput=function(){e[0].value=\"\"},c.$watch(function(){return g.$viewValue},function(a,b){g.$isEmpty(b)||g.$isEmpty(a)&&c._clearInput()})}}}}])}(window);\n-//# sourceMappingURL=angular-base64-upload.min.js.map\n\\ No newline at end of file\n+!function(e,n){\"use strict\";e._arrayBufferToBase64=function(n){for(var t=\"\",r=new Uint8Array(n),a=r.byteLength,i=0;i<a;i+=1)t+=String.fromCharCode(r[i]);return e.btoa(t)},e.angular.module(\"naif.base64\",[]).directive(\"baseSixtyFourInput\",[\"$window\",\"$q\",function(e,t){var r={onChange:\"&\",onAfterValidate:\"&\",parser:\"&\"},a=[\"onabort\",\"onerror\",\"onloadstart\",\"onloadend\",\"onprogress\",\"onload\"];return a.forEach(function(e){r[e]=\"&\"}),{restrict:\"A\",require:\"ngModel\",scope:r,link:function(r,i,o,l){function f(e){if(o.maxnum&&o.multiple&&e){var n=e.length<=parseInt(o.maxnum);l.$setValidity(\"maxnum\",n)}return e}function u(e){if(o.minnum&&o.multiple&&e){var n=e.length>=parseInt(o.minnum);l.$setValidity(\"minnum\",n)}return e}function s(e){var n=!0;if(o.maxsize&&e){var t=1e3*parseFloat(o.maxsize);if(o.multiple)for(var r=0;r<e.length;r++){var a=e[r];if(a.filesize>t){n=!1;break}}else n=e.filesize<=t;l.$setValidity(\"maxsize\",n)}return e}function c(e){var n=!0,t=1e3*parseFloat(o.minsize);if(o.minsize&&e){if(o.multiple)for(var r=0;r<e.length;r++){var a=e[r];if(a.filesize<t){n=!1;break}}else n=e.filesize>=t;l.$setValidity(\"minsize\",n)}return e}function p(e){var n,t,r,a=!0;if(o.accept&&(t=o.accept.trim().replace(/[,\\s]+/gi,\"|\").replace(/\\./g,\"\\\\.\").replace(/\\/\\*/g,\"/.*\"),n=new RegExp(t)),o.accept&&e){if(o.multiple)for(var i=0;i<e.length;i++){var f=e[i];if(r=\".\"+f.filename.split(\".\").pop(),!(a=n.test(f.filetype)||n.test(r)))break}else r=\".\"+e.filename.split(\".\").pop(),a=n.test(e.filetype)||n.test(r);l.$setValidity(\"accept\",a)}return e}function d(){var e=o.multiple?V:V[0];l.$setViewValue(e),s(e),c(e),f(e),u(e),p(e)}function m(e,n,t,r,a){t[e]=function(e){n()(e,t,r,b,V,a)}}function g(a,i,l){return function(f){var u,s=f.target.result,c=o.maxsize&&i.size>1024*o.maxsize;o.doNotParseIfOversize!==n&&c?l.base64=null:l.base64=e._arrayBufferToBase64(s),u=o.parser?t.when(r.parser()(i,l)):t.when(l),u.then(function(e){V.push(e),i.deferredObj.resolve()}),o.onload&&(r.onload&&\"function\"==typeof r.onload()?r.onload()(f,a,i,b,V,l):r.onload(f,b))}}function h(e,n,t){for(var i=a.length-1;i>=0;i--){var l=a[i];o[l]&&\"onload\"!==l&&m(l,r[l],e,n,t)}e.onload=g(e,n,t)}function v(){var n,r=[];for(n=b.length-1;n>=0;n--)b[n].deferredObj=t.defer(),r.push(b[n].deferredObj.promise);for(t.all(r).then(d),n=b.length-1;n>=0;n--){var a=new e.FileReader,i=b[n],o={};o.filetype=i.type,o.filename=i.name,o.filesize=i.size,h(a,i,o),a.readAsArrayBuffer(i)}}function y(e){o.onChange&&(r.onChange&&\"function\"==typeof r.onChange()?r.onChange()(e,b):r.onChange(e,b))}function z(e){if(o.onAfterValidate){for(var n=[],a=b.length-1;a>=0;a--)n.push(b[a].deferredObj.promise);t.all(n).then(function(){r.onAfterValidate&&\"function\"==typeof r.onAfterValidate()?r.onAfterValidate()(e,V,b):r.onAfterValidate(e,V,b)})}}var b=[],V=[];l&&(l.$isEmpty=function(e){return!e||(angular.isArray(e)?0===e.length:!e.base64)},r._clearInput=function(){i[0].value=\"\"},r.$watch(function(){return l.$viewValue},function(e){l.$isEmpty(e)&&r._clearInput()}),i.on(\"change\",function(e){e.target.files.length&&(V=[],V=angular.copy(V),b=e.target.files,v(),y(e),z(e))}))}}}])}(window);\n+//# sourceMappingURL=angular-base64-upload.min.js.map",
          "dist/angular-base64-upload.min.js.map": "@@ -1 +1 @@\n-{\"version\":3,\"sources\":[\"angular-base64-upload.js\"],\"names\":[\"window\",\"undefined\",\"_arrayBufferToBase64\",\"buffer\",\"binary\",\"bytes\",\"Uint8Array\",\"len\",\"byteLength\",\"i\",\"String\",\"fromCharCode\",\"btoa\",\"mod\",\"angular\",\"module\",\"directive\",\"$window\",\"$q\",\"isolateScope\",\"onChange\",\"onAfterValidate\",\"parser\",\"FILE_READER_EVENTS\",\"length\",\"e\",\"restrict\",\"require\",\"scope\",\"link\",\"elem\",\"attrs\",\"ngModel\",\"_readFiles\",\"rawFiles\",\"reader\",\"FileReader\",\"file\",\"fileObject\",\"promises\",\"filetype\",\"type\",\"filename\",\"name\",\"filesize\",\"size\",\"deferredObj\",\"defer\",\"push\",\"promise\",\"all\",\"then\",\"_setViewValue\",\"_attachEventHandlers\",\"readAsArrayBuffer\",\"_onChange\",\"_onAfterValidate\",\"fileObjects\",\"fReader\",\"_attachHandlerForEvent\",\"onload\",\"_readerOnLoad\",\"eventName\",\"handler\",\"target\",\"result\",\"base64\",\"when\",\"fileObj\",\"resolve\",\"newVal\",\"multiple\",\"$setViewValue\",\"_maxsize\",\"_minsize\",\"_maxnum\",\"_minnum\",\"_accept\",\"val\",\"maxnum\",\"valid\",\"parseInt\",\"$setValidity\",\"minnum\",\"maxsize\",\"max\",\"parseFloat\",\"min\",\"minsize\",\"regExp\",\"exp\",\"fileExt\",\"accept\",\"trim\",\"replace\",\"RegExp\",\"split\",\"pop\",\"test\",\"on\",\"files\",\"copy\",\"$isEmpty\",\"isArray\",\"_clearInput\",\"value\",\"$watch\",\"$viewValue\",\"oldVal\"],\"mappings\":\";;;;;CAIA,SAAUA,EAAQC,GAEhB,YAGAD,GAAOE,qBAAuB,SAASC,GAIrC,IAAK,GAHDC,GAAS,GACTC,EAAQ,GAAIC,YAAWH,GACvBI,EAAMF,EAAMG,WACPC,EAAI,EAAOF,EAAJE,EAASA,IACvBL,GAAUM,OAAOC,aAAaN,EAAMI,GAEtC,OAAOT,GAAOY,KAAKR,GAIrB,IAAIS,GAAMb,EAAOc,QAAQC,OAAO,iBAEhCF,GAAIG,UAAU,sBACZ,UACA,KACA,SAASC,EAASC,GAShB,IAAK,GAPDC,IACFC,SAAU,IACVC,gBAAiB,IACjBC,OAAQ,KAGNC,GAAsB,UAAW,UAAW,cAAe,YAAa,aAAc,UACjFd,EAAIc,EAAmBC,OAAS,EAAGf,GAAK,EAAGA,IAAK,CACvD,GAAIgB,GAAIF,EAAmBd,EAC3BU,GAAaM,GAAK,IAGpB,OACEC,SAAU,IACVC,QAAS,UACTC,MAAOT,EACPU,KAAM,SAASD,EAAOE,EAAMC,EAAOC,GAwBjC,QAASC,KACP,IAAK,GAAIxB,GAAIyB,EAASV,OAAS,EAAGf,GAAK,EAAGA,IAAK,CAC7C,GAAI0B,GAAS,GAAIlB,GAAQmB,WACrBC,EAAOH,EAASzB,GAChB6B,KACAC,IAEJD,GAAWE,SAAWH,EAAKI,KAC3BH,EAAWI,SAAWL,EAAKM,KAC3BL,EAAWM,SAAWP,EAAKQ,KAG3BX,EAASzB,GAAGqC,YAAc5B,EAAG6B,QAC7BR,EAASS,KAAKd,EAASzB,GAAGqC,YAAYG,SAGtC/B,EAAGgC,IAAIX,GAAUY,KAAKC,GAGtBC,EAAqBlB,EAAQE,EAAMC,GACnCH,EAAOmB,kBAAkBjB,IAI7B,QAASkB,GAAU9B,GACbM,EAAMX,UACRQ,EAAMR,WAAWK,EAAGS,GAIxB,QAASsB,GAAiB/B,GACxB,GAAIM,EAAMV,gBAAiB,CAIzB,IAAK,GADDkB,MACK9B,EAAIyB,EAASV,OAAS,EAAGf,GAAK,EAAGA,IACxC8B,EAASS,KAAKd,EAASzB,GAAGqC,YAAYG,QAExC/B,GAAGgC,IAAIX,GAAUY,KAAK,WACpBvB,EAAMP,kBAAkBI,EAAGgC,EAAavB,MAK9C,QAASmB,GAAqBK,EAASrB,EAAMC,GAE3C,IAAK,GAAI7B,GAAIc,EAAmBC,OAAS,EAAGf,GAAK,EAAGA,IAAK,CACvD,GAAIgB,GAAIF,EAAmBd,EACvBsB,GAAMN,IAAY,WAANA,GACdkC,EAAuBlC,EAAGG,EAAMH,GAAIiC,EAASrB,EAAMC,GAIvDoB,EAAQE,OAASC,EAAcH,EAASrB,EAAMC,GAGhD,QAASqB,GAAuBG,EAAWC,EAASL,EAASrB,EAAMC,GACjEoB,EAAQI,GAAa,SAASrC,GAC5BsC,IAAUtC,EAAGiC,EAASrB,EAAMH,EAAUuB,EAAanB,IAIvD,QAASuB,GAAcH,EAASrB,EAAMC,GAEpC,MAAO,UAASb,GAEd,GACIwB,GADA9C,EAASsB,EAAEuC,OAAOC,MAGtB3B,GAAW4B,OAASjD,EAAQf,qBAAqBC,GAG/C8C,EADElB,EAAMT,OACEJ,EAAGiD,KAAKvC,EAAMN,SAASe,EAAMC,IAE7BpB,EAAGiD,KAAK7B,GAGpBW,EAAQE,KAAK,SAASiB,GACpBX,EAAYT,KAAKoB,GAIjB/B,EAAKS,YAAYuB,YAGftC,EAAM6B,QACRhC,EAAMgC,SAASnC,EAAGiC,EAASrB,EAAMH,EAAUuB,EAAanB,IAO9D,QAASc,KACP,GAAIkB,GAASvC,EAAMwC,SAAWd,EAAcA,EAAY,EACxDzB,GAAQwC,cAAcF,GACtBG,EAASH,GACTI,EAASJ,GACTK,EAAQL,GACRM,EAAQN,GACRO,EAAQP,GAwBV,QAASK,GAAQG,GACf,GAAI/C,EAAMgD,QAAUhD,EAAMwC,UAAYO,EAAK,CACzC,GAAIE,GAAQF,EAAItD,QAAUyD,SAASlD,EAAMgD,OACzC/C,GAAQkD,aAAa,SAAUF,GAEjC,MAAOF,GAGT,QAASF,GAAQE,GACf,GAAI/C,EAAMoD,QAAUpD,EAAMwC,UAAYO,EAAK,CACzC,GAAIE,GAAQF,EAAItD,QAAUyD,SAASlD,EAAMoD,OACzCnD,GAAQkD,aAAa,SAAUF,GAEjC,MAAOF,GAGT,QAASL,GAASK,GAChB,GAAIE,IAAQ,CAEZ,IAAIjD,EAAMqD,SAAWN,EAAK,CACxB,GAAIO,GAAkC,IAA5BC,WAAWvD,EAAMqD,QAE3B,IAAIrD,EAAMwC,SACR,IAAK,GAAI9D,GAAI,EAAGA,EAAIqE,EAAItD,OAAQf,IAAK,CACnC,GAAI4B,GAAOyC,EAAIrE,EACf,IAAI4B,EAAKO,SAAWyC,EAAK,CACvBL,GAAQ,CACR,YAIJA,GAAQF,EAAIlC,UAAYyC,CAE1BrD,GAAQkD,aAAa,UAAWF,GAGlC,MAAOF,GAGT,QAASJ,GAASI,GAChB,GAAIE,IAAQ,EACRO,EAAkC,IAA5BD,WAAWvD,EAAMyD,QAE3B,IAAIzD,EAAMyD,SAAWV,EAAK,CACxB,GAAI/C,EAAMwC,SACR,IAAK,GAAI9D,GAAI,EAAGA,EAAIqE,EAAItD,OAAQf,IAAK,CACnC,GAAI4B,GAAOyC,EAAIrE,EACf,IAAI4B,EAAKO,SAAW2C,EAAK,CACvBP,GAAQ,CACR,YAIJA,GAAQF,EAAIlC,UAAY2C,CAE1BvD,GAAQkD,aAAa,UAAWF,GAGlC,MAAOF,GAGT,QAASD,GAAQC,GACf,GACIW,GAAQC,EAAKC,EADbX,GAAQ,CAOZ,IALIjD,EAAM6D,SACRF,EAAM3D,EAAM6D,OAAOC,OAAOC,QAAQ,WAAY,KAAKA,QAAQ,MAAO,OAAOA,QAAQ,QAAS,OAC1FL,EAAS,GAAIM,QAAOL,IAGlB3D,EAAM6D,QAAUd,EAAK,CACvB,GAAI/C,EAAMwC,SACR,IAAK,GAAI9D,GAAI,EAAGA,EAAIqE,EAAItD,OAAQf,IAAK,CACnC,GAAI4B,GAAOyC,EAAIrE,EAIf,IAHAkF,EAAU,IAAMtD,EAAKK,SAASsD,MAAM,KAAKC,MACzCjB,EAAQS,EAAOS,KAAK7D,EAAKG,WAAaiD,EAAOS,KAAKP,IAE7CX,EACH,UAGJW,GAAU,IAAMb,EAAIpC,SAASsD,MAAM,KAAKC,MACxCjB,EAAQS,EAAOS,KAAKpB,EAAItC,WAAaiD,EAAOS,KAAKP,EAEnD3D,GAAQkD,aAAa,SAAUF,GAGjC,MAAOF,GAvOT,GAAK9C,EAAL,CAIA,GAAIE,MACAuB,IAEJ3B,GAAKqE,GAAG,SAAU,SAAS1E,GAEpBA,EAAEuC,OAAOoC,MAAM5E,SAIpBiC,KACAA,EAAc3C,QAAQuF,KAAK5C,GAC3BvB,EAAWT,EAAEuC,OAAOoC,MACpBnE,IACAsB,EAAU9B,GACV+B,EAAiB/B,MA0GnBO,EAAQsE,SAAW,SAASxB,GAC1B,OAAQA,IAAQhE,QAAQyF,QAAQzB,GAAsB,IAAfA,EAAItD,QAAgBsD,EAAIZ,SAIjEtC,EAAM4E,YAAc,WAClB1E,EAAK,GAAG2E,MAAQ,IAGlB7E,EAAM8E,OAAO,WACX,MAAO1E,GAAQ2E,YACd,SAAS7B,EAAK8B,GACX5E,EAAQsE,SAASM,IAEjB5E,EAAQsE,SAASxB,IACnBlD,EAAM4E,uBAqGjBxG\",\"file\":\"angular-base64-upload.min.js\"}\n\\ No newline at end of file\n+{\"version\":3,\"sources\":[\"angular-base64-upload.js\"],\"names\":[\"window\",\"undefined\",\"_arrayBufferToBase64\",\"buffer\",\"binary\",\"bytes\",\"Uint8Array\",\"len\",\"byteLength\",\"i\",\"String\",\"fromCharCode\",\"btoa\",\"angular\",\"module\",\"directive\",\"$window\",\"$q\",\"isolateScope\",\"onChange\",\"onAfterValidate\",\"parser\",\"FILE_READER_EVENTS\",\"forEach\",\"e\",\"restrict\",\"require\",\"scope\",\"link\",\"elem\",\"attrs\",\"ngModel\",\"_maxnum\",\"val\",\"maxnum\",\"multiple\",\"valid\",\"length\",\"parseInt\",\"$setValidity\",\"_minnum\",\"minnum\",\"_maxsize\",\"maxsize\",\"max\",\"parseFloat\",\"file\",\"filesize\",\"_minsize\",\"min\",\"minsize\",\"_accept\",\"regExp\",\"exp\",\"fileExt\",\"accept\",\"trim\",\"replace\",\"RegExp\",\"filename\",\"split\",\"pop\",\"test\",\"filetype\",\"_setViewValue\",\"newVal\",\"fileObjects\",\"$setViewValue\",\"_attachHandlerForEvent\",\"eventName\",\"handler\",\"fReader\",\"fileObject\",\"rawFiles\",\"_readerOnLoad\",\"promise\",\"target\",\"result\",\"exceedsMaxSize\",\"size\",\"doNotParseIfOversize\",\"base64\",\"when\",\"then\",\"fileObj\",\"push\",\"deferredObj\",\"resolve\",\"onload\",\"_attachEventHandlers\",\"_readFiles\",\"promises\",\"defer\",\"all\",\"reader\",\"FileReader\",\"type\",\"name\",\"readAsArrayBuffer\",\"_onChange\",\"_onAfterValidate\",\"$isEmpty\",\"isArray\",\"_clearInput\",\"value\",\"$watch\",\"$viewValue\",\"on\",\"files\",\"copy\"],\"mappings\":\"CAAA,SAAUA,EAAQC,GAEhB,YAIAD,GAAOE,qBAAuB,SAASC,GAKrC,IAAK,GAJDC,GAAS,GACTC,EAAQ,GAAIC,YAAWH,GACvBI,EAAMF,EAAMG,WAEPC,EAAI,EAAGA,EAAIF,EAAKE,GAAK,EAC5BL,GAAUM,OAAOC,aAAaN,EAAMI,GAEtC,OAAOT,GAAOY,KAAKR,IAIXJ,EAAOa,QAAQC,OAAO,kBAE5BC,UAAU,sBACZ,UACA,KACA,SAASC,EAASC,GAEhB,GAAIC,IACFC,SAAU,IACVC,gBAAiB,IACjBC,OAAQ,KAGNC,GAAsB,UAAW,UAAW,cAAe,YAAa,aAAc,SAM1F,OAJAA,GAAmBC,QAAQ,SAASC,GAClCN,EAAaM,GAAK,OAIlBC,SAAU,IACVC,QAAS,UACTC,MAAOT,EACPU,KAAM,SAASD,EAAOE,EAAMC,EAAOC,GAYjC,QAASC,GAAQC,GACf,GAAIH,EAAMI,QAAUJ,EAAMK,UAAYF,EAAK,CACzC,GAAIG,GAAQH,EAAII,QAAUC,SAASR,EAAMI,OACzCH,GAAQQ,aAAa,SAAUH,GAEjC,MAAOH,GAGT,QAASO,GAAQP,GACf,GAAIH,EAAMW,QAAUX,EAAMK,UAAYF,EAAK,CACzC,GAAIG,GAAQH,EAAII,QAAUC,SAASR,EAAMW,OACzCV,GAAQQ,aAAa,SAAUH,GAEjC,MAAOH,GAGT,QAASS,GAAST,GAChB,GAAIG,IAAQ,CAEZ,IAAIN,EAAMa,SAAWV,EAAK,CACxB,GAAIW,GAAkC,IAA5BC,WAAWf,EAAMa,QAE3B,IAAIb,EAAMK,SACR,IAAK,GAAI1B,GAAI,EAAGA,EAAIwB,EAAII,OAAQ5B,IAAK,CACnC,GAAIqC,GAAOb,EAAIxB,EACf,IAAIqC,EAAKC,SAAWH,EAAK,CACvBR,GAAQ,CACR,YAIJA,GAAQH,EAAIc,UAAYH,CAE1Bb,GAAQQ,aAAa,UAAWH,GAGlC,MAAOH,GAGT,QAASe,GAASf,GAChB,GAAIG,IAAQ,EACRa,EAAkC,IAA5BJ,WAAWf,EAAMoB,QAE3B,IAAIpB,EAAMoB,SAAWjB,EAAK,CACxB,GAAIH,EAAMK,SACR,IAAK,GAAI1B,GAAI,EAAGA,EAAIwB,EAAII,OAAQ5B,IAAK,CACnC,GAAIqC,GAAOb,EAAIxB,EACf,IAAIqC,EAAKC,SAAWE,EAAK,CACvBb,GAAQ,CACR,YAIJA,GAAQH,EAAIc,UAAYE,CAE1BlB,GAAQQ,aAAa,UAAWH,GAGlC,MAAOH,GAGT,QAASkB,GAAQlB,GACf,GACImB,GAAQC,EAAKC,EADblB,GAAQ,CAOZ,IALIN,EAAMyB,SACRF,EAAMvB,EAAMyB,OAAOC,OAAOC,QAAQ,WAAY,KAAKA,QAAQ,MAAO,OAAOA,QAAQ,QAAS,OAC1FL,EAAS,GAAIM,QAAOL,IAGlBvB,EAAMyB,QAAUtB,EAAK,CACvB,GAAIH,EAAMK,SACR,IAAK,GAAI1B,GAAI,EAAGA,EAAIwB,EAAII,OAAQ5B,IAAK,CACnC,GAAIqC,GAAOb,EAAIxB,EAIf,IAHA6C,EAAU,IAAMR,EAAKa,SAASC,MAAM,KAAKC,QACzCzB,EAAQgB,EAAOU,KAAKhB,EAAKiB,WAAaX,EAAOU,KAAKR,IAGhD,UAIJA,GAAU,IAAMrB,EAAI0B,SAASC,MAAM,KAAKC,MACxCzB,EAAQgB,EAAOU,KAAK7B,EAAI8B,WAAaX,EAAOU,KAAKR,EAEnDvB,GAAQQ,aAAa,SAAUH,GAGjC,MAAOH,GAKT,QAAS+B,KACP,GAAIC,GAASnC,EAAMK,SAAW+B,EAAcA,EAAY,EACxDnC,GAAQoC,cAAcF,GACtBvB,EAASuB,GACTjB,EAASiB,GACTjC,EAAQiC,GACRzB,EAAQyB,GACRd,EAAQc,GAGV,QAASG,GAAuBC,EAAWC,EAASC,EAASzB,EAAM0B,GACjED,EAAQF,GAAa,SAAS7C,GAC5B8C,IAAU9C,EAAG+C,EAASzB,EAAM2B,EAAUP,EAAaM,IAIvD,QAASE,GAAcH,EAASzB,EAAM0B,GAEpC,MAAO,UAAShD,GAEd,GACImD,GADAxE,EAASqB,EAAEoD,OAAOC,OAKlBC,EAAiBhD,EAAMa,SAAWG,EAAKiC,KAAuB,KAAhBjD,EAAMa,OACpDb,GAAMkD,uBAAyB/E,GAAa6E,EAC9CN,EAAWS,OAAS,KAEpBT,EAAWS,OAASjE,EAAQd,qBAAqBC,GAIjDwE,EADE7C,EAAMT,OACEJ,EAAGiE,KAAKvD,EAAMN,SAASyB,EAAM0B,IAE7BvD,EAAGiE,KAAKV,GAGpBG,EAAQQ,KAAK,SAASC,GACpBlB,EAAYmB,KAAKD,GAEjBtC,EAAKwC,YAAYC,YAGfzD,EAAM0D,SACJ7D,EAAM6D,QAAoC,kBAAnB7D,GAAM6D,SAC/B7D,EAAM6D,SAAShE,EAAG+C,EAASzB,EAAM2B,EAAUP,EAAaM,GAExD7C,EAAM6D,OAAOhE,EAAGiD,KAQxB,QAASgB,GAAqBlB,EAASzB,EAAM0B,GAE3C,IAAK,GAAI/D,GAAIa,EAAmBe,OAAS,EAAG5B,GAAK,EAAGA,IAAK,CACvD,GAAIe,GAAIF,EAAmBb,EACvBqB,GAAMN,IAAY,WAANA,GACd4C,EAAuB5C,EAAGG,EAAMH,GAAI+C,EAASzB,EAAM0B,GAIvDD,EAAQiB,OAASd,EAAcH,EAASzB,EAAM0B,GAGhD,QAASkB,KACP,GACIjF,GADAkF,IAEJ,KAAKlF,EAAIgE,EAASpC,OAAS,EAAG5B,GAAK,EAAGA,IAEpCgE,EAAShE,GAAG6E,YAAcrE,EAAG2E,QAC7BD,EAASN,KAAKZ,EAAShE,GAAG6E,YAAYX,QAOxC,KAFA1D,EAAG4E,IAAIF,GAAUR,KAAKnB,GAEjBvD,EAAIgE,EAASpC,OAAS,EAAG5B,GAAK,EAAGA,IAAK,CACzC,GAAIqF,GAAS,GAAI9E,GAAQ+E,WACrBjD,EAAO2B,EAAShE,GAChB+D,IAEJA,GAAWT,SAAWjB,EAAKkD,KAC3BxB,EAAWb,SAAWb,EAAKmD,KAC3BzB,EAAWzB,SAAWD,EAAKiC,KAE3BU,EAAqBK,EAAQhD,EAAM0B,GACnCsB,EAAOI,kBAAkBpD,IAI7B,QAASqD,GAAU3E,GACbM,EAAMX,WACJQ,EAAMR,UAAwC,kBAArBQ,GAAMR,WACjCQ,EAAMR,WAAWK,EAAGiD,GAEpB9C,EAAMR,SAASK,EAAGiD,IAKxB,QAAS2B,GAAiB5E,GACxB,GAAIM,EAAMV,gBAAiB,CAIzB,IAAK,GADDuE,MACKlF,EAAIgE,EAASpC,OAAS,EAAG5B,GAAK,EAAGA,IACxCkF,EAASN,KAAKZ,EAAShE,GAAG6E,YAAYX,QAExC1D,GAAG4E,IAAIF,GAAUR,KAAK,WAChBxD,EAAMP,iBAAsD,kBAA5BO,GAAMP,kBACxCO,EAAMP,kBAAkBI,EAAG0C,EAAaO,GAExC9C,EAAMP,gBAAgBI,EAAG0C,EAAaO,MA3N9C,GAAIA,MACAP,IAGCnC,KA6NLA,EAAQsE,SAAW,SAASpE,GAC1B,OAAQA,IAAQpB,QAAQyF,QAAQrE,GAAsB,IAAfA,EAAII,QAAgBJ,EAAIgD,SAIjEtD,EAAM4E,YAAc,WAClB1E,EAAK,GAAG2E,MAAQ,IAGlB7E,EAAM8E,OAAO,WACX,MAAO1E,GAAQ2E,YACd,SAASzE,GACNF,EAAQsE,SAASpE,IACnBN,EAAM4E,gBAIV1E,EAAK8E,GAAG,SAAU,SAASnF,GAEpBA,EAAEoD,OAAOgC,MAAMvE,SAIpB6B,KACAA,EAAcrD,QAAQgG,KAAK3C,GAC3BO,EAAWjD,EAAEoD,OAAOgC,MACpBlB,IACAS,EAAU3E,GACV4E,EAAiB5E,aAS1BxB\",\"file\":\"angular-base64-upload.min.js\",\"sourcesContent\":[\"(function(window, undefined) {\\n\\n  'use strict';\\n\\n  /* istanbul ignore next */\\n  //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\\n  window._arrayBufferToBase64 = function(buffer) {\\n    var binary = '';\\n    var bytes = new Uint8Array(buffer);\\n    var len = bytes.byteLength;\\n\\n    for (var i = 0; i < len; i += 1) {\\n      binary += String.fromCharCode(bytes[i]);\\n    }\\n    return window.btoa(binary);\\n  };\\n\\n\\n  var mod = window.angular.module('naif.base64', []);\\n\\n  mod.directive('baseSixtyFourInput', [\\n    '$window',\\n    '$q',\\n    function($window, $q) {\\n\\n      var isolateScope = {\\n        onChange: '&',\\n        onAfterValidate: '&',\\n        parser: '&'\\n      };\\n\\n      var FILE_READER_EVENTS = ['onabort', 'onerror', 'onloadstart', 'onloadend', 'onprogress', 'onload'];\\n\\n      FILE_READER_EVENTS.forEach(function(e) {\\n        isolateScope[e] = '&';\\n      });\\n\\n      return {\\n        restrict: 'A',\\n        require: 'ngModel',\\n        scope: isolateScope,\\n        link: function(scope, elem, attrs, ngModel) {\\n\\n          var rawFiles = [];\\n          var fileObjects = [];\\n\\n          /* istanbul ignore if */\\n          if (!ngModel) {\\n            return;\\n          }\\n\\n          // VALIDATIONS =========================================================\\n\\n          function _maxnum(val) {\\n            if (attrs.maxnum && attrs.multiple && val) {\\n              var valid = val.length <= parseInt(attrs.maxnum);\\n              ngModel.$setValidity('maxnum', valid);\\n            }\\n            return val;\\n          }\\n\\n          function _minnum(val) {\\n            if (attrs.minnum && attrs.multiple && val) {\\n              var valid = val.length >= parseInt(attrs.minnum);\\n              ngModel.$setValidity('minnum', valid);\\n            }\\n            return val;\\n          }\\n\\n          function _maxsize(val) {\\n            var valid = true;\\n\\n            if (attrs.maxsize && val) {\\n              var max = parseFloat(attrs.maxsize) * 1000;\\n\\n              if (attrs.multiple) {\\n                for (var i = 0; i < val.length; i++) {\\n                  var file = val[i];\\n                  if (file.filesize > max) {\\n                    valid = false;\\n                    break;\\n                  }\\n                }\\n              } else {\\n                valid = val.filesize <= max;\\n              }\\n              ngModel.$setValidity('maxsize', valid);\\n            }\\n\\n            return val;\\n          }\\n\\n          function _minsize(val) {\\n            var valid = true;\\n            var min = parseFloat(attrs.minsize) * 1000;\\n\\n            if (attrs.minsize && val) {\\n              if (attrs.multiple) {\\n                for (var i = 0; i < val.length; i++) {\\n                  var file = val[i];\\n                  if (file.filesize < min) {\\n                    valid = false;\\n                    break;\\n                  }\\n                }\\n              } else {\\n                valid = val.filesize >= min;\\n              }\\n              ngModel.$setValidity('minsize', valid);\\n            }\\n\\n            return val;\\n          }\\n\\n          function _accept(val) {\\n            var valid = true;\\n            var regExp, exp, fileExt;\\n            if (attrs.accept) {\\n              exp = attrs.accept.trim().replace(/[,\\\\s]+/gi, \\\"|\\\").replace(/\\\\./g, \\\"\\\\\\\\.\\\").replace(/\\\\/\\\\*/g, \\\"/.*\\\");\\n              regExp = new RegExp(exp);\\n            }\\n\\n            if (attrs.accept && val) {\\n              if (attrs.multiple) {\\n                for (var i = 0; i < val.length; i++) {\\n                  var file = val[i];\\n                  fileExt = \\\".\\\" + file.filename.split('.').pop();\\n                  valid = regExp.test(file.filetype) || regExp.test(fileExt);\\n\\n                  if (!valid) {\\n                    break;\\n                  }\\n                }\\n              } else {\\n                fileExt = \\\".\\\" + val.filename.split('.').pop();\\n                valid = regExp.test(val.filetype) || regExp.test(fileExt);\\n              }\\n              ngModel.$setValidity('accept', valid);\\n            }\\n\\n            return val;\\n          }\\n\\n          //end validations ===============\\n\\n          function _setViewValue() {\\n            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\\n            ngModel.$setViewValue(newVal);\\n            _maxsize(newVal);\\n            _minsize(newVal);\\n            _maxnum(newVal);\\n            _minnum(newVal);\\n            _accept(newVal);\\n          }\\n\\n          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\\n            fReader[eventName] = function(e) {\\n              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\\n            };\\n          }\\n\\n          function _readerOnLoad(fReader, file, fileObject) {\\n\\n            return function(e) {\\n\\n              var buffer = e.target.result;\\n              var promise;\\n\\n              // do not convert the image to base64 if it exceeds the maximum\\n              // size to prevent the browser from freezing\\n              var exceedsMaxSize = attrs.maxsize && file.size > attrs.maxsize * 1024;\\n              if (attrs.doNotParseIfOversize !== undefined && exceedsMaxSize) {\\n                fileObject.base64 = null;\\n              } else {\\n                fileObject.base64 = $window._arrayBufferToBase64(buffer);\\n              }\\n\\n              if (attrs.parser) {\\n                promise = $q.when(scope.parser()(file, fileObject));\\n              } else {\\n                promise = $q.when(fileObject);\\n              }\\n\\n              promise.then(function(fileObj) {\\n                fileObjects.push(fileObj);\\n                // fulfill the promise here.\\n                file.deferredObj.resolve();\\n              });\\n\\n              if (attrs.onload) {\\n                if (scope.onload && typeof scope.onload() === \\\"function\\\") {\\n                  scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\\n                } else {\\n                  scope.onload(e, rawFiles);\\n                }\\n              }\\n\\n            };\\n\\n          }\\n\\n          function _attachEventHandlers(fReader, file, fileObject) {\\n\\n            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\\n              var e = FILE_READER_EVENTS[i];\\n              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\\n                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\\n              }\\n            }\\n\\n            fReader.onload = _readerOnLoad(fReader, file, fileObject);\\n          }\\n\\n          function _readFiles() {\\n            var promises = [];\\n            var i;\\n            for (i = rawFiles.length - 1; i >= 0; i--) {\\n              // append file a new promise, that waits until resolved\\n              rawFiles[i].deferredObj = $q.defer();\\n              promises.push(rawFiles[i].deferredObj.promise);\\n              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\\n            }\\n\\n            // set view value once all files are read\\n            $q.all(promises).then(_setViewValue);\\n\\n            for (i = rawFiles.length - 1; i >= 0; i--) {\\n              var reader = new $window.FileReader();\\n              var file = rawFiles[i];\\n              var fileObject = {};\\n\\n              fileObject.filetype = file.type;\\n              fileObject.filename = file.name;\\n              fileObject.filesize = file.size;\\n\\n              _attachEventHandlers(reader, file, fileObject);\\n              reader.readAsArrayBuffer(file);\\n            }\\n          }\\n\\n          function _onChange(e) {\\n            if (attrs.onChange) {\\n              if (scope.onChange && typeof scope.onChange() === \\\"function\\\") {\\n                scope.onChange()(e, rawFiles);\\n              } else {\\n                scope.onChange(e, rawFiles);\\n              }\\n            }\\n          }\\n\\n          function _onAfterValidate(e) {\\n            if (attrs.onAfterValidate) {\\n              // wait for all promises, in rawFiles,\\n              //   then call onAfterValidate\\n              var promises = [];\\n              for (var i = rawFiles.length - 1; i >= 0; i--) {\\n                promises.push(rawFiles[i].deferredObj.promise);\\n              }\\n              $q.all(promises).then(function() {\\n                if (scope.onAfterValidate && typeof scope.onAfterValidate() === \\\"function\\\") {\\n                  scope.onAfterValidate()(e, fileObjects, rawFiles);\\n                } else {\\n                  scope.onAfterValidate(e, fileObjects, rawFiles);\\n                }\\n              });\\n            }\\n          }\\n\\n          ngModel.$isEmpty = function(val) {\\n            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\\n          };\\n\\n          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\\n          scope._clearInput = function() {\\n            elem[0].value = '';\\n          };\\n\\n          scope.$watch(function() {\\n            return ngModel.$viewValue;\\n          }, function(val) {\\n            if (ngModel.$isEmpty(val)) {\\n              scope._clearInput();\\n            }\\n          });\\n\\n          elem.on('change', function(e) {\\n\\n            if (!e.target.files.length) {\\n              return;\\n            }\\n\\n            fileObjects = [];\\n            fileObjects = angular.copy(fileObjects);\\n            rawFiles = e.target.files; // use event target so we can mock the files from test\\n            _readFiles();\\n            _onChange(e);\\n            _onAfterValidate(e);\\n          });\\n\\n        }\\n      };\\n\\n    }\\n  ]);\\n\\n})(window);\\n\"]}\n\\ No newline at end of file",
          "package.json": "@@ -1,30 +1,28 @@\n {\n   \"name\": \"angular-base64-upload\",\n-  \"version\": \"v0.1.19\",\n+  \"version\": \"v0.1.21\",\n   \"description\": \"Converts files from file input into base64 encoded models.\",\n   \"main\": \"index.js\",\n   \"scripts\": {\n-    \"test\": \"grunt test\"\n+    \"test\": \"gulp test\"\n   },\n-  \"dependencies\": {},\n   \"devDependencies\": {\n-    \"bower\": \"^1.3.12\",\n-    \"grunt\": \"^0.4.5\",\n-    \"grunt-cli\": \"^0.1.13\",\n-    \"grunt-contrib-clean\": \"^0.6.0\",\n-    \"grunt-contrib-concat\": \"^0.5.1\",\n-    \"grunt-contrib-copy\": \"^0.8.0\",\n-    \"grunt-contrib-jshint\": \"^0.10.0\",\n-    \"grunt-contrib-uglify\": \"^0.9.1\",\n-    \"grunt-contrib-watch\": \"^0.6.1\",\n-    \"grunt-karma\": \"^0.9.0\",\n-    \"grunt-then\": \"^1.0.0\",\n-    \"karma\": \"^0.12.23\",\n-    \"karma-coverage\": \"^0.2.6\",\n-    \"karma-jasmine\": \"^0.1.5\",\n-    \"karma-phantomjs-launcher\": \"^0.1.4\",\n-    \"karma-story-reporter\": \"^0.3.1\",\n-    \"load-grunt-tasks\": \"^3.2.0\"\n+    \"bower\": \"^1.8.0\",\n+    \"del\": \"^2.2.2\",\n+    \"gulp\": \"^3.9.1\",\n+    \"gulp-insert\": \"^0.5.0\",\n+    \"gulp-jshint\": \"^2.0.4\",\n+    \"gulp-rename\": \"^1.2.2\",\n+    \"gulp-sourcemaps\": \"^2.6.0\",\n+    \"gulp-uglify\": \"^2.1.2\",\n+    \"gulp-util\": \"^3.0.8\",\n+    \"jasmine-core\": \"^2.6.0\",\n+    \"jshint\": \"^2.9.4\",\n+    \"karma\": \"^1.6.0\",\n+    \"karma-coverage\": \"^1.1.1\",\n+    \"karma-jasmine\": \"^0.1.0\",\n+    \"karma-phantomjs-launcher\": \"^1.0.4\",\n+    \"karma-story-reporter\": \"^0.3.1\"\n   },\n   \"repository\": {\n     \"type\": \"git\",\n@@ -45,5 +43,8 @@\n   \"bugs\": {\n     \"url\": \"https://github.com/adonespitogo/angular-base64-upload/issues\"\n   },\n-  \"homepage\": \"https://github.com/adonespitogo/angular-base64-upload\"\n+  \"homepage\": \"https://github.com/adonespitogo/angular-base64-upload\",\n+  \"engines\": {\n+    \"node\": \">=4.0\"\n+  }\n }",
          "src/angular-base64-upload.js": "@@ -3,11 +3,13 @@\n   'use strict';\n \n   /* istanbul ignore next */\n-  window._arrayBufferToBase64 = function(buffer) { //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\n+  //http://stackoverflow.com/questions/9267899/arraybuffer-to-base64-encoded-string\n+  window._arrayBufferToBase64 = function(buffer) {\n     var binary = '';\n     var bytes = new Uint8Array(buffer);\n     var len = bytes.byteLength;\n-    for (var i = 0; i < len; i++) {\n+\n+    for (var i = 0; i < len; i += 1) {\n       binary += String.fromCharCode(bytes[i]);\n     }\n     return window.btoa(binary);\n@@ -28,162 +30,25 @@\n       };\n \n       var FILE_READER_EVENTS = ['onabort', 'onerror', 'onloadstart', 'onloadend', 'onprogress', 'onload'];\n-      for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-        var e = FILE_READER_EVENTS[i];\n+\n+      FILE_READER_EVENTS.forEach(function(e) {\n         isolateScope[e] = '&';\n-      }\n+      });\n \n       return {\n         restrict: 'A',\n         require: 'ngModel',\n         scope: isolateScope,\n         link: function(scope, elem, attrs, ngModel) {\n \n-          /* istanbul ignore if */\n-          if (!ngModel) {\n-            return;\n-          }\n-\n           var rawFiles = [];\n           var fileObjects = [];\n \n-          elem.on('change', function(e) {\n-\n-            if (!e.target.files.length) {\n-              return;\n-            }\n-\n-            fileObjects = [];\n-            fileObjects = angular.copy(fileObjects);\n-            rawFiles = e.target.files; // use event target so we can mock the files from test\n-            _readFiles();\n-            _onChange(e);\n-            _onAfterValidate(e);\n-          });\n-\n-          function _readFiles() {\n-            var promises = [];\n-            var i;\n-            for (i = rawFiles.length - 1; i >= 0; i--) {\n-              // append file a new promise, that waits until resolved\n-              rawFiles[i].deferredObj = $q.defer();\n-              promises.push(rawFiles[i].deferredObj.promise);\n-              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\n-            }\n-\n-            // set view value once all files are read\n-            $q.all(promises).then(_setViewValue);\n-\n-            for (i = rawFiles.length - 1; i >= 0; i--) {\n-              var reader = new $window.FileReader();\n-              var file = rawFiles[i];\n-              var fileObject = {};\n-\n-              fileObject.filetype = file.type;\n-              fileObject.filename = file.name;\n-              fileObject.filesize = file.size;\n-              \n-              _attachEventHandlers(reader, file, fileObject);\n-              reader.readAsArrayBuffer(file);\n-            }\n-          }\n-\n-          function _onChange(e) {\n-            if (attrs.onChange) {\n-              scope.onChange()(e, rawFiles);\n-            }\n-          }\n-\n-          function _onAfterValidate(e) {\n-            if (attrs.onAfterValidate) {\n-              // wait for all promises, in rawFiles,\n-              //   then call onAfterValidate\n-              var promises = [];\n-              for (var i = rawFiles.length - 1; i >= 0; i--) {\n-                promises.push(rawFiles[i].deferredObj.promise);\n-              }\n-              $q.all(promises).then(function() {\n-                scope.onAfterValidate()(e, fileObjects, rawFiles);\n-              });\n-            }\n-          }\n-\n-          function _attachEventHandlers(fReader, file, fileObject) {\n-\n-            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n-              var e = FILE_READER_EVENTS[i];\n-              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\n-                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\n-              }\n-            }\n-\n-            fReader.onload = _readerOnLoad(fReader, file, fileObject);\n-          }\n-\n-          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\n-            fReader[eventName] = function(e) {\n-              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-            };\n-          }\n-\n-          function _readerOnLoad(fReader, file, fileObject) {\n-\n-            return function(e) {\n-\n-              var buffer = e.target.result;\n-              var promise;\n-\n-              fileObject.base64 = $window._arrayBufferToBase64(buffer);\n-\n-              if (attrs.parser) {\n-                promise = $q.when(scope.parser()(file, fileObject));\n-              } else {\n-                promise = $q.when(fileObject);\n-              }\n-\n-              promise.then(function(fileObj) {\n-                fileObjects.push(fileObj);\n-                // fulfill the promise here.\n-                file.deferredObj.resolve();\n-              });\n-\n-              if (attrs.onload) {\n-                scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\n-              }\n-\n-            };\n-\n-          }\n-\n-          function _setViewValue() {\n-            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\n-            ngModel.$setViewValue(newVal);\n-            _maxsize(newVal);\n-            _minsize(newVal);\n-            _maxnum(newVal);\n-            _minnum(newVal);\n-            _accept(newVal);\n+          /* istanbul ignore if */\n+          if (!ngModel) {\n+            return;\n           }\n \n-          ngModel.$isEmpty = function(val) {\n-            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\n-          };\n-\n-          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\n-          scope._clearInput = function() {\n-            elem[0].value = '';\n-          };\n-\n-          scope.$watch(function() {\n-            return ngModel.$viewValue;\n-          }, function(val, oldVal) {\n-            if (ngModel.$isEmpty(oldVal)) {\n-              return; }\n-            if (ngModel.$isEmpty(val)) {\n-              scope._clearInput();\n-            }\n-          });\n-\n           // VALIDATIONS =========================================================\n \n           function _maxnum(val) {\n@@ -263,7 +128,8 @@\n                   valid = regExp.test(file.filetype) || regExp.test(fileExt);\n \n                   if (!valid) {\n-                    break; }\n+                    break;\n+                  }\n                 }\n               } else {\n                 fileExt = \".\" + val.filename.split('.').pop();\n@@ -275,6 +141,168 @@\n             return val;\n           }\n \n+          //end validations ===============\n+\n+          function _setViewValue() {\n+            var newVal = attrs.multiple ? fileObjects : fileObjects[0];\n+            ngModel.$setViewValue(newVal);\n+            _maxsize(newVal);\n+            _minsize(newVal);\n+            _maxnum(newVal);\n+            _minnum(newVal);\n+            _accept(newVal);\n+          }\n+\n+          function _attachHandlerForEvent(eventName, handler, fReader, file, fileObject) {\n+            fReader[eventName] = function(e) {\n+              handler()(e, fReader, file, rawFiles, fileObjects, fileObject);\n+            };\n+          }\n+\n+          function _readerOnLoad(fReader, file, fileObject) {\n+\n+            return function(e) {\n+\n+              var buffer = e.target.result;\n+              var promise;\n+\n+              // do not convert the image to base64 if it exceeds the maximum\n+              // size to prevent the browser from freezing\n+              var exceedsMaxSize = attrs.maxsize && file.size > attrs.maxsize * 1024;\n+              if (attrs.doNotParseIfOversize !== undefined && exceedsMaxSize) {\n+                fileObject.base64 = null;\n+              } else {\n+                fileObject.base64 = $window._arrayBufferToBase64(buffer);\n+              }\n+\n+              if (attrs.parser) {\n+                promise = $q.when(scope.parser()(file, fileObject));\n+              } else {\n+                promise = $q.when(fileObject);\n+              }\n+\n+              promise.then(function(fileObj) {\n+                fileObjects.push(fileObj);\n+                // fulfill the promise here.\n+                file.deferredObj.resolve();\n+              });\n+\n+              if (attrs.onload) {\n+                if (scope.onload && typeof scope.onload() === \"function\") {\n+                  scope.onload()(e, fReader, file, rawFiles, fileObjects, fileObject);\n+                } else {\n+                  scope.onload(e, rawFiles);\n+                }\n+              }\n+\n+            };\n+\n+          }\n+\n+          function _attachEventHandlers(fReader, file, fileObject) {\n+\n+            for (var i = FILE_READER_EVENTS.length - 1; i >= 0; i--) {\n+              var e = FILE_READER_EVENTS[i];\n+              if (attrs[e] && e !== 'onload') { // don't attach handler to onload yet\n+                _attachHandlerForEvent(e, scope[e], fReader, file, fileObject);\n+              }\n+            }\n+\n+            fReader.onload = _readerOnLoad(fReader, file, fileObject);\n+          }\n+\n+          function _readFiles() {\n+            var promises = [];\n+            var i;\n+            for (i = rawFiles.length - 1; i >= 0; i--) {\n+              // append file a new promise, that waits until resolved\n+              rawFiles[i].deferredObj = $q.defer();\n+              promises.push(rawFiles[i].deferredObj.promise);\n+              // TODO: Make sure all promises are resolved even during file reader error, otherwise view value wont be updated\n+            }\n+\n+            // set view value once all files are read\n+            $q.all(promises).then(_setViewValue);\n+\n+            for (i = rawFiles.length - 1; i >= 0; i--) {\n+              var reader = new $window.FileReader();\n+              var file = rawFiles[i];\n+              var fileObject = {};\n+\n+              fileObject.filetype = file.type;\n+              fileObject.filename = file.name;\n+              fileObject.filesize = file.size;\n+\n+              _attachEventHandlers(reader, file, fileObject);\n+              reader.readAsArrayBuffer(file);\n+            }\n+          }\n+\n+          function _onChange(e) {\n+            if (attrs.onChange) {\n+              if (scope.onChange && typeof scope.onChange() === \"function\") {\n+                scope.onChange()(e, rawFiles);\n+              } else {\n+                scope.onChange(e, rawFiles);\n+              }\n+            }\n+          }\n+\n+          function _onAfterValidate(e) {\n+            if (attrs.onAfterValidate) {\n+              // wait for all promises, in rawFiles,\n+              //   then call onAfterValidate\n+              var promises = [];\n+              for (var i = rawFiles.length - 1; i >= 0; i--) {\n+                promises.push(rawFiles[i].deferredObj.promise);\n+              }\n+              $q.all(promises).then(function() {\n+                if (scope.onAfterValidate && typeof scope.onAfterValidate() === \"function\") {\n+                  scope.onAfterValidate()(e, fileObjects, rawFiles);\n+                } else {\n+                  scope.onAfterValidate(e, fileObjects, rawFiles);\n+                }\n+              });\n+            }\n+          }\n+\n+          ngModel.$isEmpty = function(val) {\n+            return !val || (angular.isArray(val) ? val.length === 0 : !val.base64);\n+          };\n+\n+          // http://stackoverflow.com/questions/1703228/how-can-i-clear-an-html-file-input-with-javascript\n+          scope._clearInput = function() {\n+            elem[0].value = '';\n+          };\n+\n+          scope.$watch(function() {\n+            return ngModel.$viewValue;\n+          }, function(val) {\n+            if (ngModel.$isEmpty(val) && ngModel.$dirty) {\n+              scope._clearInput();\n+              // Remove validation errors\n+              ngModel.$setValidity('maxnum', true);\n+              ngModel.$setValidity('minnum', true);\n+              ngModel.$setValidity('maxsize', true);\n+              ngModel.$setValidity('minsize', true);\n+              ngModel.$setValidity('accept', true);\n+            }\n+          });\n+\n+          elem.on('change', function(e) {\n+\n+            if (!e.target.files.length) {\n+              return;\n+            }\n+\n+            fileObjects = [];\n+            fileObjects = angular.copy(fileObjects);\n+            rawFiles = e.target.files; // use event target so we can mock the files from test\n+            _readFiles();\n+            _onChange(e);\n+            _onAfterValidate(e);\n+          });\n+\n         }\n       };\n ",
          "test/clear-input.spec.js": "@@ -20,7 +20,10 @@ describe('Clear input', function () {\n       scope = $ROOTSCOPE.$new();\n       directive = _compile({\n         attrs: [\n-          {attr: 'ng-model', val: 'file'}\n+          {attr: 'ng-model', val: 'file'},\n+          {attr: 'name', val: 'myinput'},\n+          {attr: 'maxsize', val: 1},\n+          {attr: 'required', val: 'required'},\n         ]\n       });\n       directive.$input.triggerHandler(event);\n@@ -34,18 +37,30 @@ describe('Clear input', function () {\n \n   it('should clear input when $viewValue is null', function () {\n     directive.$scope.file = null;\n+    $ROOTSCOPE.$apply();\n+    expect(spy).toHaveBeenCalled();\n   });\n \n   it('should clear input when $viewValue is empty object', function () {\n     directive.$scope.file = {};\n+    $ROOTSCOPE.$apply();\n+    expect(spy).toHaveBeenCalled();\n   });\n \n   it('should clear input when $viewValue is empty array', function () {\n     directive.$scope.file = [];\n-  });\n-\n-  afterEach(function () {\n     $ROOTSCOPE.$apply();\n     expect(spy).toHaveBeenCalled();\n   });\n+\n+  it('should clear validation errors', function() {\n+    expect(directive.$scope.form.myinput.$error.maxsize).toBeTruthy()\n+    expect(directive.$scope.form.myinput.$error.required).toBeFalsy()\n+    directive.$scope.file = null\n+    $ROOTSCOPE.$apply();\n+    expect(directive.$scope.form.myinput.$error.maxsize).toBeFalsy()\n+    expect(directive.$scope.form.myinput.$error.required).toBeTruthy()\n+    expect(directive.$scope.form.myinput.$dirty).toBeTruthy()\n+  })\n+\n });\n\\ No newline at end of file",
          "test/config/grunt_test_runner.js": "@@ -1,51 +0,0 @@\n-\n-function _make_version_range (version, start, end) {\n-  var versions = [];\n-  for (var i = start; i <= end; i ++) {\n-    versions.push(version + '.' + i);\n-  }\n-  return versions;\n-}\n-\n-var v12 = _make_version_range('1.2', 0, 28);\n-var v13 = _make_version_range('1.3', 0, 15);\n-var ANGULAR_VERSIONS = v12.concat(v13);\n-var fileLoader = require('./file_loader.js');\n-\n-function TestRunner (grunt) {\n-\n-  var self = this;\n-\n-  self._grunt = grunt;\n-  self.version_index = 0;\n-\n-  self.run = function () {\n-    grunt = self._grunt;\n-    grunt.registerTask('pre-test', ['jshint:angular-base64-upload', 'jshint:tests']);\n-    grunt.task.run('pre-test').then(self.runTest);\n-\n-  };\n-\n-  self.runTest = function () {\n-\n-    var VERSION = ANGULAR_VERSIONS[self.version_index];\n-    var files = fileLoader(VERSION);\n-\n-    grunt.config('karma.options.files', files);\n-    grunt.config('karma.options.reporters', self.version_index === 0? ['story', 'coverage'] : ['story']);\n-\n-    console.log('\\n\\n\\n\\t\\tRUNNING TEST AGAINST ANGULAR v'+VERSION);\n-\n-    grunt.task.run('karma:unit')\n-    .then(function () {\n-      self.version_index ++;\n-      if (ANGULAR_VERSIONS[self.version_index]) {\n-        self.runTest();\n-      }\n-    });\n-\n-  };\n-\n-}\n-\n-module.exports = TestRunner;\n\\ No newline at end of file",
          "test/config/test_runner.js": "@@ -0,0 +1,53 @@\n+'use strict'\n+\n+const Server = require('karma').Server\n+\n+function _make_version_range(version, start, end) {\n+  var versions = [];\n+  for (var i = start; i <= end; i++) {\n+    versions.push(version + '.' + i);\n+  }\n+  return versions;\n+}\n+\n+var v12 = _make_version_range('1.2', 0, 28);\n+var v13 = _make_version_range('1.3', 0, 15);\n+var ANGULAR_VERSIONS = v12.concat(v13);\n+var fileLoader = require('./file_loader.js');\n+\n+class TestRunner {\n+  constructor(done) {\n+    this.isDone = false\n+    this.done = done\n+    this.version_index = 0\n+    this.run()\n+  }\n+\n+  run() {\n+    let ng_version = ANGULAR_VERSIONS[this.version_index]\n+    let files = fileLoader(ng_version)\n+    let reporters = this.version_index === 0 ? ['story', 'coverage'] : ['story']\n+\n+    console.log('\\n\\n\\n\\t\\tRUNNING TEST AGAINST ANGULAR v' + ng_version)\n+\n+    new Server({\n+      configFile: __dirname + '/karma.conf.js',\n+      files,\n+      reporters\n+    }, exitCode => {\n+      if (this.version_index === ANGULAR_VERSIONS.length - 1) {\n+        if (!this.isDone) {\n+          this.done()\n+          this.isDone = true\n+          process.exit(exitCode)\n+        }\n+      } else {\n+        this.version_index += 1\n+        this.run()\n+      }\n+    }).start();\n+  }\n+\n+}\n+\n+module.exports = TestRunner;",
          "test/helpers/mocks.js": "@@ -1,10 +1,50 @@\n // contains mock objects/properties/functions used in testing\n \n+function File(opts) {\n+  opts = opts || {};\n+  this.name = opts.name || 'filename.txt';\n+  this.type = opts.type || 'image/jpeg';\n+  this.size = opts.size || (500 * 1000); //500kb\n+}\n+\n+function FileList(num_files) {\n+  num_files = num_files || 5;\n+  var list = [];\n+  for (var i = 0; i < num_files; i++) {\n+    list.push(new File());\n+  }\n+  return list;\n+}\n+\n+function FileObject(file) {\n+  file = file || new File();\n+  this.filename = file.name;\n+  this.filetype = file.type;\n+  this.filesize = file.size;\n+  this.base64 = $windowMock._arrayBufferToBase64();\n+}\n+\n+function FileObjects(num) {\n+  num = num || 5;\n+  var objs = [];\n+  for (var i = num - 1; i >= 0; i--) {\n+    objs.push(new FileObject());\n+  }\n+  return objs;\n+}\n+\n+function Event(opts) {\n+  opts = opts || {};\n+  this.type = opts.type || 'change';\n+  this.target = opts.target || {};\n+  this.target.files = opts.files || new FileList();\n+}\n+\n function FileReaderMock() {\n   var self = this;\n \n \n-  self.triggerEvent = function (eventName) {\n+  self.triggerEvent = function(eventName) {\n     var event = new Event();\n     if (typeof self[eventName] === 'function') {\n       event.target = self;\n@@ -13,7 +53,7 @@ function FileReaderMock() {\n     }\n   };\n \n-  self.readAsArrayBuffer = function (file) {\n+  self.readAsArrayBuffer = function(file) {\n \n     self.file = file;\n \n@@ -22,15 +62,14 @@ function FileReaderMock() {\n         var e = FILE_READER_EVENTS[i];\n         self.triggerEvent(e);\n       }\n-    }\n-    else {\n+    } else {\n       self.result = 'reader-result-buffer';\n       self.triggerEvent('onload');\n     }\n \n   };\n \n-  self.abort = function () {\n+  self.abort = function() {\n     self.triggerEvent('onabort');\n   };\n   return self;\n@@ -40,48 +79,8 @@ FileReaderMock.autoTriggerEvents = false;\n \n $windowMock = {\n   document: window.document,\n-  _arrayBufferToBase64: function () {\n+  _arrayBufferToBase64: function() {\n     return 'base64-mock-string';\n   },\n   FileReader: FileReaderMock\n };\n-\n-function File (opts) {\n-  opts = opts || {};\n-  this.name = opts.name || 'filename.txt';\n-  this.type = opts.type || 'image/jpeg';\n-  this.size = opts.size || (500 * 1000);//500kb\n-}\n-\n-function FileList (num_files) {\n-  num_files = num_files || 5;\n-  var list = [];\n-  for (var i = 0; i < num_files; i++) {\n-    list.push(new File());\n-  }\n-  return list;\n-}\n-\n-function Event (opts) {\n-  opts = opts || {};\n-  this.type = opts.type || 'change';\n-  this.target = opts.target || {};\n-  this.target.files = opts.files || new FileList();\n-}\n-\n-function FileObject (file) {\n-  file = file || new File();\n-  this.filename = file.name;\n-  this.filetype = file.type;\n-  this.filesize = file.size;\n-  this.base64 = $windowMock._arrayBufferToBase64();\n-}\n-\n-function FileObjects (num) {\n-  num = num || 5;\n-  var objs = [];\n-  for (var i = num - 1; i >= 0; i--) {\n-    objs.push(new FileObject());\n-  }\n-  return objs;\n-}",
          "test/validations.spec.js": "@@ -368,4 +368,125 @@ describe('Validations', function() {\n \n     });\n   });\n+\n+  describe('doNotParseIfOversize', function() {\n+    var maxsize;\n+    var attrs;\n+\n+    beforeEach(function() {\n+      maxsize = 500; //kb\n+\n+      attrs = [\n+        { attr: 'ng-model', val: 'model' },\n+        { attr: 'name', val: 'myinput' },\n+        { attr: 'maxsize', val: maxsize }\n+      ];\n+    });\n+\n+    it('should not parse single oversized files when doNotParseIfOversize is set', function() {\n+      attrs.push({ attr: 'do-not-parse-if-oversize', val: '' });\n+      var d = _compile({ attrs: attrs });\n+\n+      var testSize = function(size, shouldBeNull) {\n+\n+        var f1 = new File({ size: size * 1000 });\n+\n+        event.target.files = [f1];\n+        d.$input.triggerHandler(event);\n+        $ROOTSCOPE.$apply();\n+        if (shouldBeNull) {\n+          expect(d.$scope.model.base64).toBe(null);\n+        } else {\n+          expect(d.$scope.model.base64).not.toBe(null);\n+        }\n+        \n+      };\n+\n+      testSize(200, false);\n+      testSize(500, false);\n+      testSize(600, true);\n+\n+    });\n+\n+    it('should not parse multiple oversized files when doNotParseIfOversize is set', function() {\n+      attrs.push({ attr: 'do-not-parse-if-oversize', val: '' });\n+      attrs.push({ attr: 'multiple', val: true });\n+\n+      var d = _compile({ attrs: attrs });\n+\n+      expect(d.$scope.form.myinput.$error.maxsize).not.toBeDefined();\n+\n+      var testSize = function(size, size2, shouldBeNull, shouldBeNull2) {\n+\n+        var f1 = new File({ size: size * 1000 });\n+        var f2 = new File({ size: size2 * 1000 });\n+\n+        event.target.files = [f1, f2];\n+        d.$input.triggerHandler(event);\n+        $ROOTSCOPE.$apply();\n+        if (shouldBeNull) {\n+          expect(d.$scope.model[1].base64).toBe(null);\n+        } else {\n+          expect(d.$scope.model[1].base64).not.toBe(null);\n+        }\n+        if (shouldBeNull2) {\n+          expect(d.$scope.model[0].base64).toBe(null);\n+        } else {\n+          expect(d.$scope.model[0].base64).not.toBe(null);\n+        }\n+      };\n+\n+      testSize(200, 100, false, false);\n+      testSize(500, 123, false, false);\n+      testSize(200, 600, false, true);\n+      testSize(600, 100, true, false);\n+\n+    });\n+\n+    it('should parse single oversized files when doNotParseIfOversize is not set', function() {\n+      var d = _compile({ attrs: attrs });\n+\n+      var testSize = function(size) {\n+\n+        var f1 = new File({ size: size * 1000 });\n+\n+        event.target.files = [f1];\n+        d.$input.triggerHandler(event);\n+        $ROOTSCOPE.$apply();\n+        expect(d.$scope.model.base64).not.toBe(null);\n+        \n+      };\n+\n+      testSize(200);\n+      testSize(500);\n+      testSize(600);\n+\n+    });\n+\n+    it('should parse multiple oversized files when doNotParseIfOversize is not set', function() {\n+      attrs.push({ attr: 'multiple', val: true });\n+\n+      var d = _compile({ attrs: attrs });\n+\n+      expect(d.$scope.form.myinput.$error.maxsize).not.toBeDefined();\n+\n+      var testSize = function(size, size2) {\n+\n+        var f1 = new File({ size: size * 1000 });\n+        var f2 = new File({ size: size2 * 1000 });\n+\n+        event.target.files = [f1, f2];\n+        d.$input.triggerHandler(event);\n+        $ROOTSCOPE.$apply();\n+        expect(d.$scope.model[0].base64).not.toBe(null);\n+        expect(d.$scope.model[1].base64).not.toBe(null);\n+      };\n+\n+      testSize(200, 100);\n+      testSize(500, 123);\n+      testSize(200, 600);\n+      testSize(600, 100);\n+\n+    });\n+  });\n });"
        }
      }
    ],
    "cvss": {
      "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H",
      "score": 10
    },
    "cwes": [
      {
        "cwe_id": "CWE-434",
        "name": "Unrestricted Upload of File with Dangerous Type"
      }
    ],
    "credits": [
      {
        "user": {
          "login": "rvizx",
          "id": 84989569,
          "node_id": "MDQ6VXNlcjg0OTg5NTY5",
          "avatar_url": "https://avatars.githubusercontent.com/u/84989569?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/rvizx",
          "html_url": "https://github.com/rvizx",
          "followers_url": "https://api.github.com/users/rvizx/followers",
          "following_url": "https://api.github.com/users/rvizx/following{/other_user}",
          "gists_url": "https://api.github.com/users/rvizx/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/rvizx/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/rvizx/subscriptions",
          "organizations_url": "https://api.github.com/users/rvizx/orgs",
          "repos_url": "https://api.github.com/users/rvizx/repos",
          "events_url": "https://api.github.com/users/rvizx/events{/privacy}",
          "received_events_url": "https://api.github.com/users/rvizx/received_events",
          "type": "User",
          "user_view_type": "public",
          "site_admin": false
        },
        "type": "analyst"
      }
    ],
    "cvss_severities": {
      "cvss_v3": {
        "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H",
        "score": 10
      },
      "cvss_v4": {
        "vector_string": "CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N",
        "score": 9.3
      }
    },
    "epss": {
      "percentage": 0.00043,
      "percentile": 0.09695
    },
    "cve_description": "angular-base64-upload prior to v0.1.21 is vulnerable to unauthenticated remote code execution via demo/server.php. Exploiting this vulnerability allows an attacker to upload arbitrary content to the server, which can subsequently be accessed through demo/uploads. This leads to the execution of previously uploaded content and enables the attacker to achieve code execution on the server. NOTE: This vulnerability only affects products that are no longer supported by the maintainer."
  }
]
