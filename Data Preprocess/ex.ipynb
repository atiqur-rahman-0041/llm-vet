{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.191\n",
      "  Downloading langchain-0.0.191-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting PyYAML>=5.4.1 (from langchain==0.0.191)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.191)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.191)\n",
      "  Using cached aiohttp-3.10.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.191)\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.191)\n",
      "  Downloading numexpr-2.10.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting numpy<2,>=1 (from langchain==0.0.191)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.191)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain==0.0.191)\n",
      "  Downloading pydantic-1.10.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
      "Collecting requests<3,>=2 (from langchain==0.0.191)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.191)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached yarl-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic<2,>=1->langchain==0.0.191)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain==0.0.191)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain==0.0.191)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain==0.0.191)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain==0.0.191)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.0.191)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (24.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.0.191)\n",
      "  Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading langchain-0.0.191-py3-none-any.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.10.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Downloading numexpr-2.10.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Downloading pydantic-1.10.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, propcache, numpy, mypy-extensions, multidict, marshmallow, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, numexpr, aiosignal, openapi-schema-pydantic, dataclasses-json, aiohttp, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 dataclasses-json-0.5.14 frozenlist-1.5.0 greenlet-3.1.1 idna-3.10 langchain-0.0.191 marshmallow-3.23.1 multidict-6.1.0 mypy-extensions-1.0.0 numexpr-2.10.1 numpy-1.26.4 openapi-schema-pydantic-1.2.4 propcache-0.2.0 pydantic-1.10.19 requests-2.32.3 tenacity-8.5.0 typing-extensions-4.12.2 typing-inspect-0.9.0 urllib3-2.2.3 yarl-1.17.1\n",
      "Collecting llama-cpp-python==0.1.66\n",
      "  Downloading llama_cpp_python-0.1.66.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python==0.1.66) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python==0.1.66) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.66)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.66-cp312-cp312-linux_x86_64.whl size=274014 sha256=08ae4d265d1eae78a98d9e19efb8487d7300a985784044c3a813e30ee9e30364\n",
      "  Stored in directory: /home/nazia/.cache/pip/wheels/d9/60/b1/e608fa4c5fa70c61fec6a2c03718c7995ed6d32dca2bbe3fb1\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.66\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-75.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Using cached pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached setuptools-75.4.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, setuptools, scipy, safetensors, regex, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, fsspec, filelock, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.0 setuptools-75.4.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.0 transformers-4.46.2 triton-3.1.0\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Collecting auto-gptq==0.2.2\n",
      "  Downloading auto_gptq-0.2.2.tar.gz (52 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq==0.2.2)\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets (from auto-gptq==0.2.2)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from auto-gptq==0.2.2) (1.26.4)\n",
      "Collecting rouge (from auto-gptq==0.2.2)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.12/site-packages (from auto-gptq==0.2.2) (2.5.1)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from auto-gptq==0.2.2) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.26.1 in ./.venv/lib/python3.12/site-packages (from auto-gptq==0.2.2) (4.46.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (24.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq==0.2.2) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq==0.2.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq==0.2.2) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq==0.2.2) (4.67.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->auto-gptq==0.2.2)\n",
      "  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq==0.2.2)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->auto-gptq==0.2.2)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->auto-gptq==0.2.2)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->auto-gptq==0.2.2)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch>=1.13.0->auto-gptq==0.2.2)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq==0.2.2) (3.10.10)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.12/site-packages (from rouge->auto-gptq==0.2.2) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq==0.2.2) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.2) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->auto-gptq==0.2.2) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->auto-gptq==0.2.2) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq==0.2.2)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->auto-gptq==0.2.2)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->auto-gptq==0.2.2) (0.2.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.2.2-py3-none-any.whl size=58241 sha256=a5647a98aa9212c78fccc7ba957ed3805dcaa942d837aa111f2c290a715cfd1a\n",
      "  Stored in directory: /home/nazia/.cache/pip/wheels/df/56/7b/737da120d3f8ee99ea654d59f2e21b3e30af29789974a3a364\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: pytz, xxhash, tzdata, rouge, pyarrow, fsspec, dill, pandas, multiprocess, datasets, accelerate, auto-gptq\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed accelerate-1.1.1 auto-gptq-0.2.2 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pandas-2.2.3 pyarrow-18.0.0 pytz-2024.2 rouge-1.0.1 tzdata-2024.2 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.191\n",
    "!pip install llama-cpp-python==0.1.66\n",
    "!pip install sentence-transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install auto-gptq==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: click\n",
      "Successfully installed click-8.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in ./.venv/lib/python3.12/site-packages (0.1.66)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto-gptq in ./.venv/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: accelerate>=0.19.0 in ./.venv/lib/python3.12/site-packages (from auto-gptq) (1.1.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from auto-gptq) (3.1.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from auto-gptq) (1.26.4)\n",
      "Requirement already satisfied: rouge in ./.venv/lib/python3.12/site-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.12/site-packages (from auto-gptq) (2.5.1)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from auto-gptq) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.26.1 in ./.venv/lib/python3.12/site-packages (from auto-gptq) (4.46.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq) (24.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate>=0.19.0->auto-gptq) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers>=4.26.1->auto-gptq) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets->auto-gptq) (3.10.10)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.12/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->auto-gptq) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.26.1->auto-gptq) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->auto-gptq) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.1.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1 einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import logging\n",
    "import click\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# Transformers (check for latest version)\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LongformerTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Hugging Face Hub (for downloading models)\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# LangChain Modules (latest version recommended)\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLaMA C++ backend for local inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Metrics and other utilities\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m END_COLOR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     imported_modules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mlogging\u001b[49m),\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclick\u001b[39m\u001b[38;5;124m\"\u001b[39m, click),\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch),\n\u001b[1;32m      9\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m, transformers),\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mos\u001b[39m\u001b[38;5;124m\"\u001b[39m, os),\n\u001b[1;32m     11\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre\u001b[39m\u001b[38;5;124m\"\u001b[39m, re),\n\u001b[1;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutil\u001b[39m\u001b[38;5;124m\"\u001b[39m, shutil),\n\u001b[1;32m     13\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m, subprocess),\n\u001b[1;32m     14\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m, requests),\n\u001b[1;32m     15\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpathlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, Path),\n\u001b[1;32m     16\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_gptq\u001b[39m\u001b[38;5;124m\"\u001b[39m, AutoGPTQForCausalLM),\n\u001b[1;32m     17\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m, hf_hub_download),\n\u001b[1;32m     18\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_instruct_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, HuggingFaceInstructEmbeddings),\n\u001b[1;32m     19\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, HuggingFacePipeline),\n\u001b[1;32m     20\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m, LlamaCpp),\n\u001b[1;32m     21\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m\"\u001b[39m, PromptTemplate),\n\u001b[1;32m     22\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_chain\u001b[39m\u001b[38;5;124m\"\u001b[39m, LLMChain),\n\u001b[1;32m     23\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_auto_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, AutoTokenizer),\n\u001b[1;32m     24\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_auto_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, AutoModelForCausalLM),\n\u001b[1;32m     25\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_generation_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, GenerationConfig),\n\u001b[1;32m     26\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_llm_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, LlamaForCausalLM),\n\u001b[1;32m     27\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_llm_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, LlamaTokenizer),\n\u001b[1;32m     28\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_longformer_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongformerTokenizer), \n\u001b[1;32m     29\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline),\n\u001b[1;32m     30\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m, Rouge), \n\u001b[1;32m     31\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_splitter\u001b[39m\u001b[38;5;124m\"\u001b[39m, RecursiveCharacterTextSplitter),\n\u001b[1;32m     32\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m, tqdm), \n\u001b[1;32m     33\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtermcolor_colored\u001b[39m\u001b[38;5;124m\"\u001b[39m, colored),\n\u001b[1;32m     34\u001b[0m     ]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule(s) Imported:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m imported_modules:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "GREEN = '\\033[92m'\n",
    "END_COLOR = '\\033[0m'\n",
    "\n",
    "try:\n",
    "    imported_modules = [\n",
    "        (\"logging\", logging),\n",
    "        (\"click\", click),\n",
    "        (\"torch\", torch),\n",
    "        (\"transformers\", transformers),\n",
    "        (\"os\", os),\n",
    "        (\"re\", re),\n",
    "        (\"shutil\", shutil),\n",
    "        (\"subprocess\", subprocess),\n",
    "        (\"requests\", requests),\n",
    "        (\"pathlib\", Path),\n",
    "        (\"auto_gptq\", AutoGPTQForCausalLM),\n",
    "        (\"huggingface_hub\", hf_hub_download),\n",
    "        (\"huggingface_instruct_embeddings\", HuggingFaceInstructEmbeddings),\n",
    "        (\"langchain_pipeline\", HuggingFacePipeline),\n",
    "        (\"llama_cpp\", LlamaCpp),\n",
    "        (\"prompt_template\", PromptTemplate),\n",
    "        (\"llm_chain\", LLMChain),\n",
    "        (\"transformers_auto_tokenizer\", AutoTokenizer),\n",
    "        (\"transformers_auto_model\", AutoModelForCausalLM),\n",
    "        (\"transformers_generation_config\", GenerationConfig),\n",
    "        (\"transformers_llm_model\", LlamaForCausalLM),\n",
    "        (\"transformers_llm_tokenizer\", LlamaTokenizer),\n",
    "        (\"transformers_longformer_tokenizer\", LongformerTokenizer), \n",
    "        (\"transformers_pipeline\", pipeline),\n",
    "        (\"rouge\", Rouge), \n",
    "        (\"text_splitter\", RecursiveCharacterTextSplitter),\n",
    "        (\"tqdm\", tqdm), \n",
    "        (\"termcolor_colored\", colored),\n",
    "    ]\n",
    "\n",
    "    print(\"Module(s) Imported:\")\n",
    "    for module_name, module in imported_modules:\n",
    "        if module:\n",
    "            print(f\" - {module_name}\")\n",
    "\n",
    "            version = getattr(module, \"__version__\", None)\n",
    "            if version:\n",
    "                print(f\"   Version: {GREEN}{version}{END_COLOR}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import a module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device_type, model_id, model_basename=None):\n",
    "\n",
    "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
    "    logging.info(\"This action can take a few minutes!\")\n",
    "\n",
    "    if model_basename is not None:\n",
    "        if \".ggml\" in model_basename:\n",
    "            logging.info(\"Using Llamacpp for GGML quantized models\")\n",
    "            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
    "            max_ctx_size = 4096\n",
    "            kwargs = {\n",
    "                \"model_path\": model_path,\n",
    "                \"n_ctx\": max_ctx_size,\n",
    "                \"max_tokens\": max_ctx_size,\n",
    "            }\n",
    "            if device_type.lower() == \"mps\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "            if device_type.lower() == \"cuda\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "                kwargs[\"n_batch\"] = max_ctx_size\n",
    "            return Llama(**kwargs)\n",
    "\n",
    "        else:\n",
    "            logging.info(\"Using AutoGPTQForCausalLM for quantized models\")\n",
    "\n",
    "            if \".safetensors\" in model_basename:\n",
    "                # Remove the \".safetensors\" ending if present\n",
    "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "            logging.info(\"Tokenizer loaded\")\n",
    "\n",
    "            model = AutoGPTQForCausalLM.from_quantized(\n",
    "                model_id,\n",
    "                model_basename=model_basename,\n",
    "                use_safetensors=True,\n",
    "                trust_remote_code=True,\n",
    "                device=\"cuda:0\",\n",
    "                use_triton=False,\n",
    "                quantize_config=None,\n",
    "            )\n",
    "    elif (\n",
    "        device_type.lower() == \"cuda\"\n",
    "    ):  \n",
    "        logging.info(\"Using AutoModelForCausalLM for full models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        logging.info(\"Tokenizer loaded\")\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
    "        )\n",
    "        model.tie_weights()\n",
    "    else:\n",
    "        logging.info(\"Using LlamaTokenizer\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "\n",
    "    # Create a pipeline for text generation\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=2048,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    logging.info(\"Local LLM Loaded\")\n",
    "\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SHOW_SOURCES = True\n",
    "logging.info(f\"Running on: {DEVICE_TYPE}\")\n",
    "logging.info(f\"Display Source Documents set to: {SHOW_SOURCES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "model_basename = \"llama-2-7b-chat.ggmlv3.q4_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Llama.__del__ at 0x732b2ab96fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nazia/Desktop/Nazia/CSC591/project/.venv/lib/python3.12/site-packages/llama_cpp/llama.py\", line 1440, in __del__\n",
      "    if self.ctx is not None:\n",
      "       ^^^^^^^^\n",
      "AttributeError: 'Llama' object has no attribute 'ctx'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Llama.__init__() got an unexpected keyword argument 'max_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_basename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_basename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(device_type, model_id, model_basename)\u001b[0m\n\u001b[1;32m     19\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_gpu_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     20\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_ctx_size\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing AutoGPTQForCausalLM for quantized models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Llama.__init__() got an unexpected keyword argument 'max_tokens'"
     ]
    }
   ],
   "source": [
    "LLM = load_model(device_type=DEVICE_TYPE, model_id=model_id, model_basename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `HuggingFace2` has been saved to /home/nazia/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/nazia/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `HuggingFace2`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_generator(text):\n",
    "  llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})\n",
    "  summary_prompt = \"summarize this text: \" + text\n",
    "  return llm(summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus. Most people infected with the virus will experience mild to moderate respiratory illness and recover without requiring special treatment. However, some will become seriously ill and require medical attention. Older people and those with underlying medical conditions like cardiovascular disease, diabetes, chronic respiratory disease, or cancer are more likely to develop serious illness. Anyone can get sick with COVID-19 and become seriously ill or die at any age. The best way to prevent and slow down transmission is to be well informed about the disease and how the virus spreads. Protect yourself and others from infection by staying at least 1 metre apart from others, wearing a properly fitted mask, and washing your hands or using an alcohol-based rub frequently. Get vaccinated when it’s your turn and follow local guidance.The virus can spread from an infected person’s mouth or nose in small liquid particles when they cough, sneeze, speak, sing or breathe. These particles range from larger respiratory droplets to smaller aerosols. It is important to practice respiratory etiquette, for example by coughing into a flexed elbow, and to stay home and self-isolate until you recover if you feel unwell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_generator(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
